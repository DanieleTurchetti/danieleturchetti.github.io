[
  {
    "objectID": "Lab1_ExplContVar.html",
    "href": "Lab1_ExplContVar.html",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "This practical will lead you into producing some high quality standard visualisations of single continuous data variables, as seen in lectures.\n\n\nBy the end of the lab, you will have acquired the following skills:\n\n\nPlotting a histogram with hist\n\n\nPlotting a boxplot with boxplot\n\n\nExtracting simple numerical summaries with summary and fivenum\n\n\nCustomising plots with labels, titles, colour, etc.\n\n\nProducing multiple variations of the plots above\n\n\n\n\n\nTo begin with, let’s first see how to use R to produce standard plots of a variable, namely histograms, boxplots, and quantile (or QQ) plots.\nFor illustration, let us use the R built-in mtcars data set, which contains information on the characteristics of 23 cars.\n\ndata(mtcars)\n\n\n\nA histogram consists of parallel vertical bars that graphically shows the frequency distribution of a quantitative variable. The area of each bar is proportional to the frequency of items found in each class. A histogram is useful to look at when we want to see more detail on the full distribution of the data, and features relating to its shape.\nTo plot a histogram, we apply the hist function to the data vector. We can extract the mpg (miles-per-gallon) variable from the mtcars data set using the $ operator, and so we can draw a histogram as follows:\n\nhist(mtcars$mpg)\n\n\n\n\n\n\n\n\nThis seems to suggest that we have a peak somewhere between 15 and 20 mpg, and potentially another peak between 30 and 35 mpg - perhaps suggesting groups of ‘fuel efficient’ and ‘fuel inefficient’ cars.\nOne way to assess if the number of bars in the histogram is appropriate is to show the location of the data points on the horizontal axis. We can add a ‘rug plot’ to our histogram, which marks the positions of the data with lines on the axis:\n\nhist(mtcars$mpg)\nrug(mtcars$mpg) ## Note: the 'rug' function draws on top of an existing histogram\n\n\n\n\n\n\n\n\nNow we can also see where the data fall within the bars!\nThe default settings of hist will determine the number of bars to display algorithmically, and in this case it has drawn only 5. In general, this is probably too few to show any detail, but we don’t have many data points here. Fortunately, the display of the histogram can be adjusted by a number of arguments:\n\nbreaks - allows us to control the number of bars in the histogram. breaks can take a variety of different inputs:\n\nIf breaks is set to a single number, this will be used to (suggest) the number of bars in the histogram.\nIf breaks is set to a vector, the values will be used to indicate the endpoints of the bars of the histogram.\n\nfreq - if TRUE the histogram shows the simple frequencies or counts within each bar; if FALSE then the histogram shows probability densities rather than counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the hist function to draw histograms of miles-per-gallon which match those shown above (don’t worry about the labels).\n\n\n\nClick for solution\n\n\nhist(mtcars$mpg,freq=FALSE,main='freq=FALSE')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=10,main='breaks=10')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=50,main='breaks=50')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=c(10,12,20,30,35), main=\"breaks=c(10,12,20,30,35)\")\n\n\n\n\n\n\n\n\n\n\n\n\nA five-number numerical summary can be computed with the fivenum function, which takes a vector of numbers as input. To add a little more information, the summary function includes the mean of the data for a 6-number summary.\n\n\n\nCompute summaries of the mpg data in the mtcars dataset using fivenum and summary. What is your interpretation of the result?\n\n\nClick for solution\n\n\nfivenum(mtcars$mpg)\n\n[1] 10.40 15.35 19.20 22.80 33.90\n\n\nThe values returned are the sample minimum, lower quartile, median, upper quartile and maximum. We can see that the median across all the cars in the dataset is about 20 miles per gallon. This is pretty terrible by modern standards, but the data are from 1974 and the USA.\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nThe inclusion of the mean can be quite helpful. If the data have an approximately symmetric distribution then the mean and median values should be close, which can be used as a quick check for any potential skewness in the data. Given that the mean is fairly close to the median, there doesn’t appear to be a dramatic amount of skewness in the distribution of MPG.\n\n\n\n\n\nA boxplot provides a graphical view of the median, quartiles, maximum, and minimum of a data set. In many ways, it is simply a direct visualisation of the five number summary constructed above. The whiskers (vertical lines) capture roughly 99% of a normal distribution, and observations outside this range are plotted as points representing outliers (see the figure below).\n\nBoxplots are created for single variables using the boxplot function, but can be used to easily compare many variables or groups within the data. To draw a boxplot of a single variable, multiple variables, or all variables in a data frame, we simply pass the data directly to the boxplot function:\n\nboxplot(mtcars$mpg)\n\n\n\n\n\n\n\n\nAs the boxplot is based on the simple 5-number summary, it lacks the detail of a histogram. However, we can inspect it for features such as symmetry or skewness - a symmetric distribution will give a boxplot with a whiskers of equal length, a centrally-positioned box evenly divided by the median line.\nHere, we see the box is slightly off-centre, suggesting some slight skewness. We also note that there are no obvious outliers.\n\n\nDraw a a boxplot of all the variables in mtcars by passing the entire data frame to the boxplot function. Can you see anything useful in the plot?\n\n\nClick for solution\n\n\nboxplot(mtcars)\n\n\n\n\n\n\n\n\nUnfortunately, because of the different scales of variables we can’t see what’s going on with the smaller variables.\n\nThe boxplot is most useful when comparing how a variable behaves in different groups (i.e., the levels of a categorical variable). For example, we can compare the MPG with the number of engine cylinders\n\ncyl &lt;- factor(mtcars$cyl) # make the 'cyl' variable categorical\nboxplot(mtcars$mpg ~ cyl)\n\n\n\n\n\n\n\n\nWhat do you conclude about fuel efficiency in cars with more engine cylinders?\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data.\n\n\n\n\n\nFrancis Galton famously developed his ideas on correlation and regression using data which included the heights of parents and their children. This galton data set include data on heights for 928 children and their 205 ‘midparents’. Each ‘midparent’ height is the average of the father’s height and 1.08 times the mother’s height (to adjust for the usual gender differences). Similarly, the daughter’s heights have also been multiplied by 1.08. Note that we have one midparent height for each child, so that many midparent heights are repeated.\nThe variables are child and parent for the different heights recorded in inches.\n\n\n\nDownload the galton.Rda data set from the Ultra page and load the file\nDraw a boxplot of both variables in the data set.\nWhat features do you see? How do the heights compare in terms of location and spread?\nDoes this agree with what you expected?\nDraw and compare histograms of the two variables - can you detect any noticeable similarities or differences?\nRedraw your histograms and add a rugplot to each. Does this give you more information?\n\n\n\nClick for solution\n\n\nboxplot(galton)\n\n\n\n\n\n\n\n## parents are less spread than children, both seem to be centred around the \n## same values (about 68.5)\n## both appear symmetric\n## parent displays two outliers\n\n## expectations: similar heights? so yes.\n## not sure would we expect more variability in children than parents, though parents are an average!\n\nhist(galton$child)\nrug(galton$child)\n\n\n\n\n\n\n\nhist(galton$parent)\nrug(galton$parent)\n\n\n\n\n\n\n\n## histograms are clearly symmetric, normal distributions?\n## rugplot shows something strange - all of these data points are lining up on a few values - why?\n## lets look at the first 30 values\ngalton$child[1:30]\n\n [1] 61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 62.2 62.2 63.2 63.2 63.2\n[16] 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2\n\n## is there rounding here? all the values are at integers +0.2...\n## how many unique values are there?\nunique(galton$parent)\n\n [1] 70.5 68.5 65.5 64.5 64.0 67.5 66.5 69.5 71.5 72.5 73.0\n\n## only 11 unique values!\n\n\n\n\n\n\nUsing the par() command, draw histograms of parents and child heights side-by-side on the same plot (you may need to adjust the width of your plot window.)\nDo your plots reveal anything interesting?\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\nhist(galton$parent)\nhist(galton$child)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the difficulties of comparing multiple independent plots is we need to do more work to ensure consistency of presentation. In particular, we should ensure that our histogram intervals and axis ranges are the same for both plots, as the default presentation will change from plot to plot.\nMany high level plotting functions (plot, hist, boxplot, etc.) allow you to include additional options to customise how the plot is drawn (as well as other graphical parameters). We have seen examples of these already with the axis label arguments xlab and ylab, however we can customise the following plot features for finer control of how a plot is drawn.\nAxis limits\nTo control the ranges of the horizontal and vertical axes, we can add the xlim and ylim arguments to our original plotting function To set the horixontal axis limits, we pass a vector of two numbers to represent the lower and upper limits, xlim = c(lower, upper) specifying numerical values for upper and lower, and repeat the same for ylim to customise the vertical axis.\nAxis labels\nTo specify a label for the x- and y-axes we can supply a string to the xlab and ylab arguments. To give a plot a title, we pass the title as a string to the main argument.\nIt is easier to compare the shape of distributions in histograms when they are arranged vertically, they use equal horizontal axis limits, and the same binwidths.\n\n\n\n\nUse par to setup a column of two plots.\nPlot a histogram of child and then parent from galton using:\n\nx-axis limits of 60 to 75\nA bar width of 1 unit\nAn appropriate x-axis label and plot title\n\nDoes the comparison yield any new information that wasn’t conveyed in the boxplot?\nTry reducing your bar widths - what do you find?\n\n\n\nClick for solution\n\n\npar(mfrow=c(2,1))\nhist(galton$parent,xlim=c(60,75),breaks=60:75,xlab='Parent heights', main='')\nhist(galton$child,xlim=c(60,75),breaks=60:75,xlab='Child heights', main='')\n\n\n\n\n\n\n\n## they seem to line up nicely, with the children more spread. Much like the boxplots\n## why are the parents less variable? One set of parents has many children, so \n## there's a lot more variability from many children (between 1 and 15) there\n## due to repetition of the parent values \n## its actually a difference of the order of 1/sqrt(2), which you would expect for\n## the mean of 2 parents!\n\nIn interpreting the data, it is worth noting that:\n\nGalton obtained this data “through the offer of prizes” for the “bext Extracts from their own Family Records”, so the sample is hardly a random one\nthe data are clearly heavily rounded for tabulation\nfamily sizes vary from 1 child up to 15, so that there is a lot of repetition in the midparent heights.\n\nYou might expect that if we had the individual un-adjusted heights and the genders of the parents and children we would find that the height data distributions would be neatly bimodal with one peak for females and one for males. They are not. Apparently, height distributions are rarely like that.\n\n\n\n\n\nData science inevitably involves working with large data sets. The effort involved in preparing and making a large dataset usable for analysis should not be underestimated, but thankfully we’re going to look at a dataset “prepared earlier”. This movies data set (downloadable from Ultra) is reasonably large(ish), containing 24 different attribues of 28819 movies gathered from IMDB. One of the variables is the movie length in minutes, and it is interesting to look at this variable in some detail.\n\n\n\nLoad the movies data set and draw a histogram of the data.\nPlot a histogram of the length variable.\nWhat features can you see?\nAdd a rugplot to the histogram - what problems does this highlight?\nLet’s try a boxplot of the data - do we learn anything more?\nAre there any obvious outliers?\n\n\n\nClick for solution\n\n\nhist(movies$length)\nrug(movies$length)\n\n\n\n\n\n\n\n## all the data stack up at the left end near zero, but the range of the data is huge!\nboxplot(movies$length,horizontal = TRUE)\n\n\n\n\n\n\n\n## this confirms the same as the rugplot, but there are two ridiculously large outliers\n\nClearly, our data are distorted by some particularly egregious outliers.\n\n\n\n\nLook at the outliers more closely:\n\nWhat are the lengths of the two longest movies?\nDoes that seem sensible?\nExtract the subset of the dataframe containing all the variables for both of these movies\nInspect the variable values:\nThe variables r1 to r10 give the percentage of reviews which rated the movie as a 1 up to a 10 out of 10. Are these movies particularly popular?\nWhat are the names of the movies? Do a quick Google search to see if you can find out more.\n\n\n\nClick for solution\n\nThere are various way you can do this, I chose to sort the data and use the head or tail functions to extract the end/start of the list\n\ntail(sort(movies$length))\n\n[1]  647  773  873 1100 2880 5220\n\nhead(sort(movies$length,decreasing=TRUE))\n\n[1] 5220 2880 1100  873  773  647\n\n\nso 5220 and 2880 minutes - that’s 87 and 48 hours, or 14.5 and 8 days respectively. These are obviously not ‘genuine’ movies. Although it’s tempting to dismiss these as simple errors, it is worth checking if possible.\nTo look at all the variables for these two movies, we must subset the data frame using the square brackets\n\nmovies[movies$length&gt;2000,]\n\n                                                 title year length budget\n11937                           Cure for Insomnia, The 1987   5220     NA\n30574 Longest Most Meaningless Movie in the World, The 1970   2880     NA\n      rating votes   r1  r2  r3  r4 r5 r6 r7  r8  r9  r10 mpaa Action Animation\n11937    3.8    59 44.5 4.5 4.5 4.5  0  0  0 4.5 4.5 44.5           0         0\n30574    6.4    15 44.5 0.0 0.0 0.0  0  0  0 0.0 0.0 64.5           0         0\n      Comedy Drama Documentary Romance Short\n11937      0     0           0       0     0\n30574      0     0           0       0     0\n\n\nThe movies seem to be somewhat polarising, either rated 0 or 10, but only by a small number of reviews! Incidentally, this data set is no longer up-to-date and there are some even longer films now (though it’s a mystery why.)\n\nIn this case, the extreme outliers should be ignored, and for exploring the main distribution of movie lengths it makes sense to set some kind of upper limit. Over 99% of the data are less than three hours in length, so let’s restrict ourselve to those.\n\n\n\n\nExtract the all the movies of length at most three hours.\nDraw a histogram - what do you find?\n\n\n\nClick for solution\n\n\nmovies2 &lt;- movies[movies$length&lt;180,]\nhist(movies2$length)\n\n\n\n\n\n\n\n\nAt last, we see some structure! There are two clear peaks here (bi-modality): the first under 20 minutes, the second in [80,100] minutes. The latter group seems about right for the ‘average’ movie.\n\nUseful context:\n\nThe Oscars define a “short film” as anything under 40 minutes.\nAnimated shorts are typically between 5 and 8 minutes long, and are counted as individual movies (so, e.g., every ‘Tom and Jerry’ cartoon has its own entry).\n\n\n\n\n\nRedraw your histogram using a bin-width of 1 minute (you may need to enlarge your plot window).\nWhat do you see? How does the information above help explain the data?\nIs there any heaping in the data? At what values?\n\n\n\nClick for solution\n\nTo get a 1-minute bin width, I made a sequence of integers from 0 to 180 as my breakpoints. If you’re not familiar with this, look at the help for the ‘seq’ function in creating similar sequences.\n\nhist(movies2$length,breaks=0:180)\n\n\n\n\n\n\n\n\nThere’s a lot going on here! * there are still a fair few longer films over 2hrs, but its a minority * the clump of short movies correspond to the defined ‘short film’ and have a clear peak around 7mins. Coincidentally, most short cartoons are 6-8 mins long. * A big pronounced spike in values at exactly 90 minutes. This probably isn’t rounding, but rather that when making and editing movies they will have aimed to produce a film of that length. Perhaps we might have expected an ever sharper peak? * We also see some stacking around this peak, with certain lengths being favoured at 80, 85, 95, 100, etc\n\n\n\n\n\nUsing colour in a plot can be very effective, for example to highlight different groups within the data. Colour is adjusted by setting the col optional arugment to the plotting function, and what R does with that information depends on the value we supply.\n\ncol is assigned a single value: all points on a scatterplot, all bars of a histogram, all boxplots are coloured with the new colour\ncol is a vector:\n\nin a scatterplot, if col is a vector of the same length as the number of data points then each data point is coloured individually\nin a histogram, if col is a vector of the same length as the number of bars then each bar is coloured individually\nin a boxplot, if col is a vector of the same length as the number of boxplots then each boxplot is coloured individually\nif the vector is not of the correct length, it will be replicated until it is and the above rules apply\n\n\nNow that we know how the col argument works, we need to know how to specify colours. Again, there are a number of ways and you can mix and match as appropriate\n\nIntegers: The integers 1:8 are interpreted as colours (black, red, green, blue, …) and can be used as a quick shorthand for a common colour. Type palette() to see the sequence of colours R uses.\nNames: R recognises over 650 named colours is specified as text, e.g.\"steelblue\", \"darkorange\". You can see the list of recognised names by typing colors(), and a document showing the actual colors is available here\nHexadecimal: R can recognise colours specified as hexadecimal RGB codes (as used in HTML etc), so pure red can be specified as \"#ff0000\" and cyan as \"#00ffff\".\nColour functions: R has a number of functions that will generate a number of colours for use in plotting. These functions include rainbow, heat.colors, and terrain.colors and all take the number of desired colours as argument.\n\n\n## Colour example\n## 3 plots in one row\npar(mfrow=c(1,3))\n## colour the cars data by number of gears\nplot(x=mtcars$wt, y=mtcars$mpg, col=mtcars$gear, xlab=\"Weight\", ylab=\"MPG\", \n     main=\"MPG vs Weight\")\n## manually colour boxplots\nboxplot(mpg~cyl, data=mtcars, col=c(\"orange\",\"violet\",\"steelblue3\"),\n        main=\"Car Milage Data\", xlab=\"Number of Cylinders\", \n        ylab=\"Miles Per Gallon\")\n## use a colour function to shade histogram bars\nhist(mtcars$mpg,col=rainbow(5),main='MPG')\n\n\n\n\n\n\n\n\n\n\n\nShow the histograms of length of short movies next to that for ‘non-short’ movies, using a different colour for each histogram.\nExperiment with using the col argument to add colour to your histograms.\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\n## again, we can use subsetting to select a subset of the data. Here we don't need a ',' as we're subsetting a vector instead of a matrix \nhist(movies2$length[movies2$length&lt;=40],col='royalblue',xlab='length',main='Short films')\nhist(movies2$length[movies2$length&gt;40],col='tomato',xlab='length', main='Regular films')\n\n\n\n\n\n\n\n\n\n\n\n\n\n Download data: bundestag\nThese data contain the results of the 2009 elections for the German Bundestag, the first chamber of the German parliament. The contains the number of votes cast for the various political parties, for each state (“Bundesland”). Amongst the German political parties there are two on the left of the political spectrum, the SPD - similar to the UK’s Labour party - and Die Linke (“The Left”), a party even further to the left. Suppose we’re interested in the support for this “Die Linke” party.\n\n\n\nExplore the election support for ‘Die Linke’ party by examining the LINKE1 variable using. Compare the outputs and explain the difference you find:\n\nA histogram, with a rugplot\nA stacked dotplot\nA beewswarm plot, setting horizontal=TRUE\n\n\n\n\nClick for solution\n\n\nhist(bundestag$LINKE1)\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## clear concentration of points around 10k. let's try narrower bins for more detail\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=2000))\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## that's a bit more useful\n\n\nstripchart(bundestag$LINKE1,method='stack',pch=16)\n\n\n\n\n\n\n\n## this looks mostly like the rugplot information on the plot above, but even with stacking this doesn't resemble the histogram\n## Why not? The histogram is grouping nearby points into its bars, but the dotplot/stripchart does not and is treating each unique value as a separate point.\n\n\nlibrary(beeswarm)\nbeeswarm(bundestag$LINKE1,horiz=TRUE)\n\n\n\n\n\n\n\n## the shape of the beewswarm more closely resembles the histogram, albeit rather than stacking points up from the bottom it works from the middle out towards the edges\n\n\nA stem and leaf plot is a technique for displaying the data in a similar fashion to a histogram, while preserving the information ofthe individual numerical values. Where the histogram summarises the data by the counts in its various intervals, the stem and leaf plot retains the original data values up to two significant figures.\n\n\n\n\nDraw a stem and leaf plot of the German election support for ‘Die Linke’ data using the stem() function.\nHow does the stem and leaf plot represent these data? You may want to look at the data value to help understand.\nNow draw a histogram, adjusting the histogram to have axis range and bar width to match the stem and leaf plot.\n\n\n\nClick for solution\n\n\nstem(bundestag$LINKE1)\n\n\n  The decimal point is 4 digit(s) to the right of the |\n\n  0 | \n  0 | 55566666667777777777777888888888888888888888888888888888999999999999+14\n  1 | 00000000000000000000000000000000000001111111111111111111111111111111+51\n  1 | 5555566\n  2 | 122234\n  2 | 588889\n  3 | 01223333344444\n  3 | 556666677777788\n  4 | 0000011233444\n  4 | 6677899\n  5 | \n  5 | \n  6 | 0233\n\n## This is effectively a histogram with bins of width 5000 units. The number indicate the actual data values, with the \"stem\" being the leading digit to the left of the | symbol, and the values of the next digits of the data being indicated to the right of the |\n## We can see some structure now *within* the bars, which is a bit more detail than we get from the histogram.\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=5000))\n\n\n\n\n\n\n\n\nAs with the beeswarm plot, the stem and leaf plot is only suitable for relatively modestly sized data sets due to the fact it is literally writing out all of the data values on the screen!\n\n\n\n\n\n\n\n‘Stripplots’ or ‘Stripcharts’ are very similar to the rugplot we applied to our histograms, and display the individual data points along a single axis. They can be used in much the same way as a boxplot, but rather than showing the data summaries they display everything!\nThe built-in faithful data set contains measurements on the waiting times between the eruptions of the Old Faithful geyser.\n\ndata(faithful)\nstripchart(faithful$waiting,ylab='Waiting Time', pch=16) \n\n\n\n\n\n\n\n\nPlotting symbols\nThe symbols used for points in plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is usually pch=1 which is a hollow circle, in the plot above we changed it to 15 which is a filled circle.\nHowever, when we have a lot of data points concentrated in a small interval, the stripplot suffers from problems of ‘overplotting’ where many points with similar values are drawn on top of each other.\nA partial solution to this is to add random noise (known as ‘jittering’) to spread out the points.\n\nMake a stripplot of the movie length data - how does the overplotting problem manifest here? You may want to compare to your histogram.\n\nA better solution is to stack the dots that fall close together, producing an alternative plot to a histogram - sometimes called a ‘dotplot’ or ‘stacked dotplot’\n\nstripchart(faithful$waiting, method='stack',pch=16)\n\n\n\n\n\n\n\n\n\nTry this out with the movies data.\n\n\n\n\nAn evolution of the stripplot is the ‘beeswarm’ plot, available from the beeswarm package. A bee swarm plot is similar to stripplot, but with various methods to separate nearby points such that each point is visible.\n\nlibrary(beeswarm)\nbeeswarm(faithful$waiting)\n\n\n\n\n\n\n\n\nOne limitation of the beeswarm plot is that the computations to arrange all the points do not scale well with large data sets. Do not try this with the movies data, or you will be waiting for a very long time!\n\n\n\nTo see how this works, let’s look at the Old Faithful data, sorted from smallest to largest.\n\nsort(faithful$waiting)\n\n  [1] 43 45 45 45 46 46 46 46 46 47 47 47 47 48 48 48 49 49 49 49 49 50 50 50 50\n [26] 50 51 51 51 51 51 51 52 52 52 52 52 53 53 53 53 53 53 53 54 54 54 54 54 54\n [51] 54 54 54 55 55 55 55 55 55 56 56 56 56 57 57 57 58 58 58 58 59 59 59 59 59\n [76] 59 59 60 60 60 60 60 60 62 62 62 62 63 63 63 64 64 64 64 65 65 65 66 66 67\n[101] 68 69 69 70 70 70 70 71 71 71 71 71 72 73 73 73 73 73 73 73 74 74 74 74 74\n[126] 74 75 75 75 75 75 75 75 75 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77\n[151] 77 77 77 77 77 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79\n[176] 79 79 79 79 79 80 80 80 80 80 80 80 80 81 81 81 81 81 81 81 81 81 81 81 81\n[201] 81 82 82 82 82 82 82 82 82 82 82 82 82 83 83 83 83 83 83 83 83 83 83 83 83\n[226] 83 83 84 84 84 84 84 84 84 84 84 84 85 85 85 85 85 85 86 86 86 86 86 86 87\n[251] 87 88 88 88 88 88 88 89 89 89 90 90 90 90 90 90 91 92 93 93 94 96\n\n\nNote that the smallest value is 43, followed by three values of 45. A stem and leaf plot of these data looks like this\n\nstem(faithful$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 3\n  4 | 55566666777788899999\n  5 | 00000111111222223333333444444444\n  5 | 555555666677788889999999\n  6 | 00000022223334444\n  6 | 555667899\n  7 | 00001111123333333444444\n  7 | 555555556666666667777777777778888888888888889999999999\n  8 | 000000001111111111111222222222222333333333333334444444444\n  8 | 55555566666677888888999\n  9 | 00000012334\n  9 | 6\n\n\nEach row of this plot is called a ‘stem’ and the values to the right of the ‘|’ symbol are the leaves. Be sure to read where R places the decimal point for the output. For this result, the decimal is placed one digit to the right of the vertical bar. Thus, the first row of the table then consists of data values of the form \\(4x\\), and the only leaf is a \\(3\\) corresponding to the value \\(43\\) in the data. The next stem groups the values \\(45-49\\), and we notice the three observations of \\(45\\) are represented by the \\(555\\) at the start of the second stem.\nNotice that each stem part is representing an interval of width 5, much like a histogram. As usual, R figures out how best to increment the stem part unless you specify otherwise. Finally, notice how the shape of the stem and leaf plot mirrors that of a histogram with interval width 5 - the only difference is that here we can see the values inside the bars.\n\n\n\nThe data come from an old survey of 237 students taking their first statistics course. The dataset is called survey in the package MASS.\n\n\n\nLoad the data with data(survey, package='MASS')\nDraw a histogram of student heights - do you see evidence of bimodality?\nExperiment with different binwidths for the histogram. Which choice do you think is the best for conveying the information in the data?\nCompare male and femal heights using separate histograms with a common scale and binwidths.\n\n\n\n\n\n Download data: diamonds\nThe set diamonds includes information on the weight in carats (carat) and price of 53,940 diamonds.\n\n\n\nIs there anything unusual about the distribution of diamond weights? Which plot do you think shows it best? How might you explain the pattern you find?\nWhat about the distribution of prices? With a bit of detective work you ought to be able to uncover at least one unexpected feature. How you discover it, whether with a histogram, a dotplot, or whatever, is unimportant, the important thing is to find it. Having found it, what plot would you draw to present your results to someone else? Can you think of an explanation for the feature?\n\n\n\n\n\n Download data: zuni\nThe zuni dataset seems quite simple. There are three pieces of information about each of 89 school districts in the US State of New Mexico: the name of the district, the average revenue per pupil in dollars, and the number of pupils. The apparent simplicity hides an interesting story. The data were used to determine how to allocate substantial amounts of money and there were intense legal disgreements about how the law should be interpreted and how the data should be used. Gastwirth was heavily involved and has written informatively about the case from a statistical point of view Gastwirth, 2006 and Gastwirth, 2008.\nOne statistical issue was the rule that before determining whether district revenues were sufficiently equal, the largest and smallest 5% of the data should first be deleted.\n\n\n\nAre the lowest and highest 5% of the revenue values extreme? Do you prefer a histogram or boxplot for showing this?\nRemove the lowest and highest 5% of the cases, draw a plot of the remaining data and discuss whether the resulting distribution looks symmetric.\nDraw a Normal quantile plot of the data after removal of the 5% at each end and comment on whether you would regard the remaining distribution as normal.\n\n\n\n\n\n Download data: engine\nThese data record the amounts of three pollutants - carbon monoxide CO, hydrocarbons HC, and nitrogen oxide NO - in grammes emitted per mile by 46 light-duty engines.\n\n\n\nAre the distributions for the three pollutants similar? To make this an easier question to answer, try to produce histograms of the variables, using the same class intervals and range for the horizontal axis in each case.\n\n\n\n\n\n Download data: chlorph\nThese data come from a semi-automated process for measuring the actual amount of chlorpheniramine maleate in tablets which are supposed to contain a 4mg dose.\nThe tablets used for the study were made by two different manufacturers. For each manufacturer, a composite was produced by grinding together a number of tablets. Each composite was split into seven pieces each of the same weight as a tablet and the pieces were sent to seven different laboratories. Each laboratory made 10 separate measurements on each composite.\nThe data contain three variables: * chlorpheniramine - the amount measured * manufacturer - the tablet manufacturer as a factor (A or B) * laboratory - the laboratory which performed the measurement as a factor (1 to 7)\n\n\n\nProduce box-plots of the chlorpheniramine measurements split by laboratory. What, if anything, do they suggest?\nNow produce box-plots by manufacturer. Anything noticeable?\nThe problem here is that we really need a separate box-plot for each combination of manufacturer and laboratory. We can do this by boxplot(chlorpheniramine~laboratory*manufacturer, data=chlorph) Try this (or some abbreviated version of it).\nTry reversing the order of laboratory and manufacturer. Which way round is better? Try colouring the box-plots by manufacturer. Does that help?\nDo the data suggest that the two manufacturers actually put different amounts of the drug into supposed 4mg tablets?\nAre there obvious differences between different laboratories? If so, what kind of differences do you observe?\nIf you had to choose a single laboratory to make some measurements for you, which would you choose and why?"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-1-standard-plots-in-r",
    "href": "Lab1_ExplContVar.html#exercise-1-standard-plots-in-r",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "To begin with, let’s first see how to use R to produce standard plots of a variable, namely histograms, boxplots, and quantile (or QQ) plots.\nFor illustration, let us use the R built-in mtcars data set, which contains information on the characteristics of 23 cars.\n\ndata(mtcars)\n\n\n\nA histogram consists of parallel vertical bars that graphically shows the frequency distribution of a quantitative variable. The area of each bar is proportional to the frequency of items found in each class. A histogram is useful to look at when we want to see more detail on the full distribution of the data, and features relating to its shape.\nTo plot a histogram, we apply the hist function to the data vector. We can extract the mpg (miles-per-gallon) variable from the mtcars data set using the $ operator, and so we can draw a histogram as follows:\n\nhist(mtcars$mpg)\n\n\n\n\n\n\n\n\nThis seems to suggest that we have a peak somewhere between 15 and 20 mpg, and potentially another peak between 30 and 35 mpg - perhaps suggesting groups of ‘fuel efficient’ and ‘fuel inefficient’ cars.\nOne way to assess if the number of bars in the histogram is appropriate is to show the location of the data points on the horizontal axis. We can add a ‘rug plot’ to our histogram, which marks the positions of the data with lines on the axis:\n\nhist(mtcars$mpg)\nrug(mtcars$mpg) ## Note: the 'rug' function draws on top of an existing histogram\n\n\n\n\n\n\n\n\nNow we can also see where the data fall within the bars!\nThe default settings of hist will determine the number of bars to display algorithmically, and in this case it has drawn only 5. In general, this is probably too few to show any detail, but we don’t have many data points here. Fortunately, the display of the histogram can be adjusted by a number of arguments:\n\nbreaks - allows us to control the number of bars in the histogram. breaks can take a variety of different inputs:\n\nIf breaks is set to a single number, this will be used to (suggest) the number of bars in the histogram.\nIf breaks is set to a vector, the values will be used to indicate the endpoints of the bars of the histogram.\n\nfreq - if TRUE the histogram shows the simple frequencies or counts within each bar; if FALSE then the histogram shows probability densities rather than counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the hist function to draw histograms of miles-per-gallon which match those shown above (don’t worry about the labels).\n\n\n\nClick for solution\n\n\nhist(mtcars$mpg,freq=FALSE,main='freq=FALSE')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=10,main='breaks=10')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=50,main='breaks=50')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=c(10,12,20,30,35), main=\"breaks=c(10,12,20,30,35)\")\n\n\n\n\n\n\n\n\n\n\n\n\nA five-number numerical summary can be computed with the fivenum function, which takes a vector of numbers as input. To add a little more information, the summary function includes the mean of the data for a 6-number summary.\n\n\n\nCompute summaries of the mpg data in the mtcars dataset using fivenum and summary. What is your interpretation of the result?\n\n\nClick for solution\n\n\nfivenum(mtcars$mpg)\n\n[1] 10.40 15.35 19.20 22.80 33.90\n\n\nThe values returned are the sample minimum, lower quartile, median, upper quartile and maximum. We can see that the median across all the cars in the dataset is about 20 miles per gallon. This is pretty terrible by modern standards, but the data are from 1974 and the USA.\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nThe inclusion of the mean can be quite helpful. If the data have an approximately symmetric distribution then the mean and median values should be close, which can be used as a quick check for any potential skewness in the data. Given that the mean is fairly close to the median, there doesn’t appear to be a dramatic amount of skewness in the distribution of MPG."
  },
  {
    "objectID": "Lab1_ExplContVar.html#boxplot",
    "href": "Lab1_ExplContVar.html#boxplot",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "A boxplot provides a graphical view of the median, quartiles, maximum, and minimum of a data set. In many ways, it is simply a direct visualisation of the five number summary constructed above. The whiskers (vertical lines) capture roughly 99% of a normal distribution, and observations outside this range are plotted as points representing outliers (see the figure below).\n\nBoxplots are created for single variables using the boxplot function, but can be used to easily compare many variables or groups within the data. To draw a boxplot of a single variable, multiple variables, or all variables in a data frame, we simply pass the data directly to the boxplot function:\n\nboxplot(mtcars$mpg)\n\n\n\n\n\n\n\n\nAs the boxplot is based on the simple 5-number summary, it lacks the detail of a histogram. However, we can inspect it for features such as symmetry or skewness - a symmetric distribution will give a boxplot with a whiskers of equal length, a centrally-positioned box evenly divided by the median line.\nHere, we see the box is slightly off-centre, suggesting some slight skewness. We also note that there are no obvious outliers.\n\n\nDraw a a boxplot of all the variables in mtcars by passing the entire data frame to the boxplot function. Can you see anything useful in the plot?\n\n\nClick for solution\n\n\nboxplot(mtcars)\n\n\n\n\n\n\n\n\nUnfortunately, because of the different scales of variables we can’t see what’s going on with the smaller variables.\n\nThe boxplot is most useful when comparing how a variable behaves in different groups (i.e., the levels of a categorical variable). For example, we can compare the MPG with the number of engine cylinders\n\ncyl &lt;- factor(mtcars$cyl) # make the 'cyl' variable categorical\nboxplot(mtcars$mpg ~ cyl)\n\n\n\n\n\n\n\n\nWhat do you conclude about fuel efficiency in cars with more engine cylinders?\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data."
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-2-data-analysis-of-galtons-heights",
    "href": "Lab1_ExplContVar.html#exercise-2-data-analysis-of-galtons-heights",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Francis Galton famously developed his ideas on correlation and regression using data which included the heights of parents and their children. This galton data set include data on heights for 928 children and their 205 ‘midparents’. Each ‘midparent’ height is the average of the father’s height and 1.08 times the mother’s height (to adjust for the usual gender differences). Similarly, the daughter’s heights have also been multiplied by 1.08. Note that we have one midparent height for each child, so that many midparent heights are repeated.\nThe variables are child and parent for the different heights recorded in inches.\n\n\n\nDownload the galton.Rda data set from the Ultra page and load the file\nDraw a boxplot of both variables in the data set.\nWhat features do you see? How do the heights compare in terms of location and spread?\nDoes this agree with what you expected?\nDraw and compare histograms of the two variables - can you detect any noticeable similarities or differences?\nRedraw your histograms and add a rugplot to each. Does this give you more information?\n\n\n\nClick for solution\n\n\nboxplot(galton)\n\n\n\n\n\n\n\n## parents are less spread than children, both seem to be centred around the \n## same values (about 68.5)\n## both appear symmetric\n## parent displays two outliers\n\n## expectations: similar heights? so yes.\n## not sure would we expect more variability in children than parents, though parents are an average!\n\nhist(galton$child)\nrug(galton$child)\n\n\n\n\n\n\n\nhist(galton$parent)\nrug(galton$parent)\n\n\n\n\n\n\n\n## histograms are clearly symmetric, normal distributions?\n## rugplot shows something strange - all of these data points are lining up on a few values - why?\n## lets look at the first 30 values\ngalton$child[1:30]\n\n [1] 61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 62.2 62.2 63.2 63.2 63.2\n[16] 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2\n\n## is there rounding here? all the values are at integers +0.2...\n## how many unique values are there?\nunique(galton$parent)\n\n [1] 70.5 68.5 65.5 64.5 64.0 67.5 66.5 69.5 71.5 72.5 73.0\n\n## only 11 unique values!\n\n\n\n\n\n\nUsing the par() command, draw histograms of parents and child heights side-by-side on the same plot (you may need to adjust the width of your plot window.)\nDo your plots reveal anything interesting?\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\nhist(galton$parent)\nhist(galton$child)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the difficulties of comparing multiple independent plots is we need to do more work to ensure consistency of presentation. In particular, we should ensure that our histogram intervals and axis ranges are the same for both plots, as the default presentation will change from plot to plot.\nMany high level plotting functions (plot, hist, boxplot, etc.) allow you to include additional options to customise how the plot is drawn (as well as other graphical parameters). We have seen examples of these already with the axis label arguments xlab and ylab, however we can customise the following plot features for finer control of how a plot is drawn.\nAxis limits\nTo control the ranges of the horizontal and vertical axes, we can add the xlim and ylim arguments to our original plotting function To set the horixontal axis limits, we pass a vector of two numbers to represent the lower and upper limits, xlim = c(lower, upper) specifying numerical values for upper and lower, and repeat the same for ylim to customise the vertical axis.\nAxis labels\nTo specify a label for the x- and y-axes we can supply a string to the xlab and ylab arguments. To give a plot a title, we pass the title as a string to the main argument.\nIt is easier to compare the shape of distributions in histograms when they are arranged vertically, they use equal horizontal axis limits, and the same binwidths.\n\n\n\n\nUse par to setup a column of two plots.\nPlot a histogram of child and then parent from galton using:\n\nx-axis limits of 60 to 75\nA bar width of 1 unit\nAn appropriate x-axis label and plot title\n\nDoes the comparison yield any new information that wasn’t conveyed in the boxplot?\nTry reducing your bar widths - what do you find?\n\n\n\nClick for solution\n\n\npar(mfrow=c(2,1))\nhist(galton$parent,xlim=c(60,75),breaks=60:75,xlab='Parent heights', main='')\nhist(galton$child,xlim=c(60,75),breaks=60:75,xlab='Child heights', main='')\n\n\n\n\n\n\n\n## they seem to line up nicely, with the children more spread. Much like the boxplots\n## why are the parents less variable? One set of parents has many children, so \n## there's a lot more variability from many children (between 1 and 15) there\n## due to repetition of the parent values \n## its actually a difference of the order of 1/sqrt(2), which you would expect for\n## the mean of 2 parents!\n\nIn interpreting the data, it is worth noting that:\n\nGalton obtained this data “through the offer of prizes” for the “bext Extracts from their own Family Records”, so the sample is hardly a random one\nthe data are clearly heavily rounded for tabulation\nfamily sizes vary from 1 child up to 15, so that there is a lot of repetition in the midparent heights.\n\nYou might expect that if we had the individual un-adjusted heights and the genders of the parents and children we would find that the height data distributions would be neatly bimodal with one peak for females and one for males. They are not. Apparently, height distributions are rarely like that."
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-3-data-exploration-of-the-movies-dataset",
    "href": "Lab1_ExplContVar.html#exercise-3-data-exploration-of-the-movies-dataset",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Data science inevitably involves working with large data sets. The effort involved in preparing and making a large dataset usable for analysis should not be underestimated, but thankfully we’re going to look at a dataset “prepared earlier”. This movies data set (downloadable from Ultra) is reasonably large(ish), containing 24 different attribues of 28819 movies gathered from IMDB. One of the variables is the movie length in minutes, and it is interesting to look at this variable in some detail.\n\n\n\nLoad the movies data set and draw a histogram of the data.\nPlot a histogram of the length variable.\nWhat features can you see?\nAdd a rugplot to the histogram - what problems does this highlight?\nLet’s try a boxplot of the data - do we learn anything more?\nAre there any obvious outliers?\n\n\n\nClick for solution\n\n\nhist(movies$length)\nrug(movies$length)\n\n\n\n\n\n\n\n## all the data stack up at the left end near zero, but the range of the data is huge!\nboxplot(movies$length,horizontal = TRUE)\n\n\n\n\n\n\n\n## this confirms the same as the rugplot, but there are two ridiculously large outliers\n\nClearly, our data are distorted by some particularly egregious outliers.\n\n\n\n\nLook at the outliers more closely:\n\nWhat are the lengths of the two longest movies?\nDoes that seem sensible?\nExtract the subset of the dataframe containing all the variables for both of these movies\nInspect the variable values:\nThe variables r1 to r10 give the percentage of reviews which rated the movie as a 1 up to a 10 out of 10. Are these movies particularly popular?\nWhat are the names of the movies? Do a quick Google search to see if you can find out more.\n\n\n\nClick for solution\n\nThere are various way you can do this, I chose to sort the data and use the head or tail functions to extract the end/start of the list\n\ntail(sort(movies$length))\n\n[1]  647  773  873 1100 2880 5220\n\nhead(sort(movies$length,decreasing=TRUE))\n\n[1] 5220 2880 1100  873  773  647\n\n\nso 5220 and 2880 minutes - that’s 87 and 48 hours, or 14.5 and 8 days respectively. These are obviously not ‘genuine’ movies. Although it’s tempting to dismiss these as simple errors, it is worth checking if possible.\nTo look at all the variables for these two movies, we must subset the data frame using the square brackets\n\nmovies[movies$length&gt;2000,]\n\n                                                 title year length budget\n11937                           Cure for Insomnia, The 1987   5220     NA\n30574 Longest Most Meaningless Movie in the World, The 1970   2880     NA\n      rating votes   r1  r2  r3  r4 r5 r6 r7  r8  r9  r10 mpaa Action Animation\n11937    3.8    59 44.5 4.5 4.5 4.5  0  0  0 4.5 4.5 44.5           0         0\n30574    6.4    15 44.5 0.0 0.0 0.0  0  0  0 0.0 0.0 64.5           0         0\n      Comedy Drama Documentary Romance Short\n11937      0     0           0       0     0\n30574      0     0           0       0     0\n\n\nThe movies seem to be somewhat polarising, either rated 0 or 10, but only by a small number of reviews! Incidentally, this data set is no longer up-to-date and there are some even longer films now (though it’s a mystery why.)\n\nIn this case, the extreme outliers should be ignored, and for exploring the main distribution of movie lengths it makes sense to set some kind of upper limit. Over 99% of the data are less than three hours in length, so let’s restrict ourselve to those.\n\n\n\n\nExtract the all the movies of length at most three hours.\nDraw a histogram - what do you find?\n\n\n\nClick for solution\n\n\nmovies2 &lt;- movies[movies$length&lt;180,]\nhist(movies2$length)\n\n\n\n\n\n\n\n\nAt last, we see some structure! There are two clear peaks here (bi-modality): the first under 20 minutes, the second in [80,100] minutes. The latter group seems about right for the ‘average’ movie.\n\nUseful context:\n\nThe Oscars define a “short film” as anything under 40 minutes.\nAnimated shorts are typically between 5 and 8 minutes long, and are counted as individual movies (so, e.g., every ‘Tom and Jerry’ cartoon has its own entry).\n\n\n\n\n\nRedraw your histogram using a bin-width of 1 minute (you may need to enlarge your plot window).\nWhat do you see? How does the information above help explain the data?\nIs there any heaping in the data? At what values?\n\n\n\nClick for solution\n\nTo get a 1-minute bin width, I made a sequence of integers from 0 to 180 as my breakpoints. If you’re not familiar with this, look at the help for the ‘seq’ function in creating similar sequences.\n\nhist(movies2$length,breaks=0:180)\n\n\n\n\n\n\n\n\nThere’s a lot going on here! * there are still a fair few longer films over 2hrs, but its a minority * the clump of short movies correspond to the defined ‘short film’ and have a clear peak around 7mins. Coincidentally, most short cartoons are 6-8 mins long. * A big pronounced spike in values at exactly 90 minutes. This probably isn’t rounding, but rather that when making and editing movies they will have aimed to produce a film of that length. Perhaps we might have expected an ever sharper peak? * We also see some stacking around this peak, with certain lengths being favoured at 80, 85, 95, 100, etc"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-4-using-colour",
    "href": "Lab1_ExplContVar.html#exercise-4-using-colour",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Using colour in a plot can be very effective, for example to highlight different groups within the data. Colour is adjusted by setting the col optional arugment to the plotting function, and what R does with that information depends on the value we supply.\n\ncol is assigned a single value: all points on a scatterplot, all bars of a histogram, all boxplots are coloured with the new colour\ncol is a vector:\n\nin a scatterplot, if col is a vector of the same length as the number of data points then each data point is coloured individually\nin a histogram, if col is a vector of the same length as the number of bars then each bar is coloured individually\nin a boxplot, if col is a vector of the same length as the number of boxplots then each boxplot is coloured individually\nif the vector is not of the correct length, it will be replicated until it is and the above rules apply\n\n\nNow that we know how the col argument works, we need to know how to specify colours. Again, there are a number of ways and you can mix and match as appropriate\n\nIntegers: The integers 1:8 are interpreted as colours (black, red, green, blue, …) and can be used as a quick shorthand for a common colour. Type palette() to see the sequence of colours R uses.\nNames: R recognises over 650 named colours is specified as text, e.g.\"steelblue\", \"darkorange\". You can see the list of recognised names by typing colors(), and a document showing the actual colors is available here\nHexadecimal: R can recognise colours specified as hexadecimal RGB codes (as used in HTML etc), so pure red can be specified as \"#ff0000\" and cyan as \"#00ffff\".\nColour functions: R has a number of functions that will generate a number of colours for use in plotting. These functions include rainbow, heat.colors, and terrain.colors and all take the number of desired colours as argument.\n\n\n## Colour example\n## 3 plots in one row\npar(mfrow=c(1,3))\n## colour the cars data by number of gears\nplot(x=mtcars$wt, y=mtcars$mpg, col=mtcars$gear, xlab=\"Weight\", ylab=\"MPG\", \n     main=\"MPG vs Weight\")\n## manually colour boxplots\nboxplot(mpg~cyl, data=mtcars, col=c(\"orange\",\"violet\",\"steelblue3\"),\n        main=\"Car Milage Data\", xlab=\"Number of Cylinders\", \n        ylab=\"Miles Per Gallon\")\n## use a colour function to shade histogram bars\nhist(mtcars$mpg,col=rainbow(5),main='MPG')\n\n\n\n\n\n\n\n\n\n\n\nShow the histograms of length of short movies next to that for ‘non-short’ movies, using a different colour for each histogram.\nExperiment with using the col argument to add colour to your histograms.\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\n## again, we can use subsetting to select a subset of the data. Here we don't need a ',' as we're subsetting a vector instead of a matrix \nhist(movies2$length[movies2$length&lt;=40],col='royalblue',xlab='length',main='Short films')\nhist(movies2$length[movies2$length&gt;40],col='tomato',xlab='length', main='Regular films')"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-5-optional-german-opinion-polls",
    "href": "Lab1_ExplContVar.html#exercise-5-optional-german-opinion-polls",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: bundestag\nThese data contain the results of the 2009 elections for the German Bundestag, the first chamber of the German parliament. The contains the number of votes cast for the various political parties, for each state (“Bundesland”). Amongst the German political parties there are two on the left of the political spectrum, the SPD - similar to the UK’s Labour party - and Die Linke (“The Left”), a party even further to the left. Suppose we’re interested in the support for this “Die Linke” party.\n\n\n\nExplore the election support for ‘Die Linke’ party by examining the LINKE1 variable using. Compare the outputs and explain the difference you find:\n\nA histogram, with a rugplot\nA stacked dotplot\nA beewswarm plot, setting horizontal=TRUE\n\n\n\n\nClick for solution\n\n\nhist(bundestag$LINKE1)\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## clear concentration of points around 10k. let's try narrower bins for more detail\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=2000))\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## that's a bit more useful\n\n\nstripchart(bundestag$LINKE1,method='stack',pch=16)\n\n\n\n\n\n\n\n## this looks mostly like the rugplot information on the plot above, but even with stacking this doesn't resemble the histogram\n## Why not? The histogram is grouping nearby points into its bars, but the dotplot/stripchart does not and is treating each unique value as a separate point.\n\n\nlibrary(beeswarm)\nbeeswarm(bundestag$LINKE1,horiz=TRUE)\n\n\n\n\n\n\n\n## the shape of the beewswarm more closely resembles the histogram, albeit rather than stacking points up from the bottom it works from the middle out towards the edges\n\n\nA stem and leaf plot is a technique for displaying the data in a similar fashion to a histogram, while preserving the information ofthe individual numerical values. Where the histogram summarises the data by the counts in its various intervals, the stem and leaf plot retains the original data values up to two significant figures.\n\n\n\n\nDraw a stem and leaf plot of the German election support for ‘Die Linke’ data using the stem() function.\nHow does the stem and leaf plot represent these data? You may want to look at the data value to help understand.\nNow draw a histogram, adjusting the histogram to have axis range and bar width to match the stem and leaf plot.\n\n\n\nClick for solution\n\n\nstem(bundestag$LINKE1)\n\n\n  The decimal point is 4 digit(s) to the right of the |\n\n  0 | \n  0 | 55566666667777777777777888888888888888888888888888888888999999999999+14\n  1 | 00000000000000000000000000000000000001111111111111111111111111111111+51\n  1 | 5555566\n  2 | 122234\n  2 | 588889\n  3 | 01223333344444\n  3 | 556666677777788\n  4 | 0000011233444\n  4 | 6677899\n  5 | \n  5 | \n  6 | 0233\n\n## This is effectively a histogram with bins of width 5000 units. The number indicate the actual data values, with the \"stem\" being the leading digit to the left of the | symbol, and the values of the next digits of the data being indicated to the right of the |\n## We can see some structure now *within* the bars, which is a bit more detail than we get from the histogram.\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=5000))\n\n\n\n\n\n\n\n\nAs with the beeswarm plot, the stem and leaf plot is only suitable for relatively modestly sized data sets due to the fact it is literally writing out all of the data values on the screen!"
  },
  {
    "objectID": "Lab1_ExplContVar.html#stripplots-or-stripcharts",
    "href": "Lab1_ExplContVar.html#stripplots-or-stripcharts",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "‘Stripplots’ or ‘Stripcharts’ are very similar to the rugplot we applied to our histograms, and display the individual data points along a single axis. They can be used in much the same way as a boxplot, but rather than showing the data summaries they display everything!\nThe built-in faithful data set contains measurements on the waiting times between the eruptions of the Old Faithful geyser.\n\ndata(faithful)\nstripchart(faithful$waiting,ylab='Waiting Time', pch=16) \n\n\n\n\n\n\n\n\nPlotting symbols\nThe symbols used for points in plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is usually pch=1 which is a hollow circle, in the plot above we changed it to 15 which is a filled circle.\nHowever, when we have a lot of data points concentrated in a small interval, the stripplot suffers from problems of ‘overplotting’ where many points with similar values are drawn on top of each other.\nA partial solution to this is to add random noise (known as ‘jittering’) to spread out the points.\n\nMake a stripplot of the movie length data - how does the overplotting problem manifest here? You may want to compare to your histogram.\n\nA better solution is to stack the dots that fall close together, producing an alternative plot to a histogram - sometimes called a ‘dotplot’ or ‘stacked dotplot’\n\nstripchart(faithful$waiting, method='stack',pch=16)\n\n\n\n\n\n\n\n\n\nTry this out with the movies data."
  },
  {
    "objectID": "Lab1_ExplContVar.html#beeswarm-plots",
    "href": "Lab1_ExplContVar.html#beeswarm-plots",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "An evolution of the stripplot is the ‘beeswarm’ plot, available from the beeswarm package. A bee swarm plot is similar to stripplot, but with various methods to separate nearby points such that each point is visible.\n\nlibrary(beeswarm)\nbeeswarm(faithful$waiting)\n\n\n\n\n\n\n\n\nOne limitation of the beeswarm plot is that the computations to arrange all the points do not scale well with large data sets. Do not try this with the movies data, or you will be waiting for a very long time!"
  },
  {
    "objectID": "Lab1_ExplContVar.html#stem-and-leaf-plots",
    "href": "Lab1_ExplContVar.html#stem-and-leaf-plots",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "To see how this works, let’s look at the Old Faithful data, sorted from smallest to largest.\n\nsort(faithful$waiting)\n\n  [1] 43 45 45 45 46 46 46 46 46 47 47 47 47 48 48 48 49 49 49 49 49 50 50 50 50\n [26] 50 51 51 51 51 51 51 52 52 52 52 52 53 53 53 53 53 53 53 54 54 54 54 54 54\n [51] 54 54 54 55 55 55 55 55 55 56 56 56 56 57 57 57 58 58 58 58 59 59 59 59 59\n [76] 59 59 60 60 60 60 60 60 62 62 62 62 63 63 63 64 64 64 64 65 65 65 66 66 67\n[101] 68 69 69 70 70 70 70 71 71 71 71 71 72 73 73 73 73 73 73 73 74 74 74 74 74\n[126] 74 75 75 75 75 75 75 75 75 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77\n[151] 77 77 77 77 77 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79\n[176] 79 79 79 79 79 80 80 80 80 80 80 80 80 81 81 81 81 81 81 81 81 81 81 81 81\n[201] 81 82 82 82 82 82 82 82 82 82 82 82 82 83 83 83 83 83 83 83 83 83 83 83 83\n[226] 83 83 84 84 84 84 84 84 84 84 84 84 85 85 85 85 85 85 86 86 86 86 86 86 87\n[251] 87 88 88 88 88 88 88 89 89 89 90 90 90 90 90 90 91 92 93 93 94 96\n\n\nNote that the smallest value is 43, followed by three values of 45. A stem and leaf plot of these data looks like this\n\nstem(faithful$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 3\n  4 | 55566666777788899999\n  5 | 00000111111222223333333444444444\n  5 | 555555666677788889999999\n  6 | 00000022223334444\n  6 | 555667899\n  7 | 00001111123333333444444\n  7 | 555555556666666667777777777778888888888888889999999999\n  8 | 000000001111111111111222222222222333333333333334444444444\n  8 | 55555566666677888888999\n  9 | 00000012334\n  9 | 6\n\n\nEach row of this plot is called a ‘stem’ and the values to the right of the ‘|’ symbol are the leaves. Be sure to read where R places the decimal point for the output. For this result, the decimal is placed one digit to the right of the vertical bar. Thus, the first row of the table then consists of data values of the form \\(4x\\), and the only leaf is a \\(3\\) corresponding to the value \\(43\\) in the data. The next stem groups the values \\(45-49\\), and we notice the three observations of \\(45\\) are represented by the \\(555\\) at the start of the second stem.\nNotice that each stem part is representing an interval of width 5, much like a histogram. As usual, R figures out how best to increment the stem part unless you specify otherwise. Finally, notice how the shape of the stem and leaf plot mirrors that of a histogram with interval width 5 - the only difference is that here we can see the values inside the bars."
  },
  {
    "objectID": "Lab1_ExplContVar.html#student-survey",
    "href": "Lab1_ExplContVar.html#student-survey",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "The data come from an old survey of 237 students taking their first statistics course. The dataset is called survey in the package MASS.\n\n\n\nLoad the data with data(survey, package='MASS')\nDraw a histogram of student heights - do you see evidence of bimodality?\nExperiment with different binwidths for the histogram. Which choice do you think is the best for conveying the information in the data?\nCompare male and femal heights using separate histograms with a common scale and binwidths."
  },
  {
    "objectID": "Lab1_ExplContVar.html#diamonds",
    "href": "Lab1_ExplContVar.html#diamonds",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: diamonds\nThe set diamonds includes information on the weight in carats (carat) and price of 53,940 diamonds.\n\n\n\nIs there anything unusual about the distribution of diamond weights? Which plot do you think shows it best? How might you explain the pattern you find?\nWhat about the distribution of prices? With a bit of detective work you ought to be able to uncover at least one unexpected feature. How you discover it, whether with a histogram, a dotplot, or whatever, is unimportant, the important thing is to find it. Having found it, what plot would you draw to present your results to someone else? Can you think of an explanation for the feature?"
  },
  {
    "objectID": "Lab1_ExplContVar.html#zuni-educational-funding",
    "href": "Lab1_ExplContVar.html#zuni-educational-funding",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: zuni\nThe zuni dataset seems quite simple. There are three pieces of information about each of 89 school districts in the US State of New Mexico: the name of the district, the average revenue per pupil in dollars, and the number of pupils. The apparent simplicity hides an interesting story. The data were used to determine how to allocate substantial amounts of money and there were intense legal disgreements about how the law should be interpreted and how the data should be used. Gastwirth was heavily involved and has written informatively about the case from a statistical point of view Gastwirth, 2006 and Gastwirth, 2008.\nOne statistical issue was the rule that before determining whether district revenues were sufficiently equal, the largest and smallest 5% of the data should first be deleted.\n\n\n\nAre the lowest and highest 5% of the revenue values extreme? Do you prefer a histogram or boxplot for showing this?\nRemove the lowest and highest 5% of the cases, draw a plot of the remaining data and discuss whether the resulting distribution looks symmetric.\nDraw a Normal quantile plot of the data after removal of the 5% at each end and comment on whether you would regard the remaining distribution as normal."
  },
  {
    "objectID": "Lab1_ExplContVar.html#pollutants-from-engines",
    "href": "Lab1_ExplContVar.html#pollutants-from-engines",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: engine\nThese data record the amounts of three pollutants - carbon monoxide CO, hydrocarbons HC, and nitrogen oxide NO - in grammes emitted per mile by 46 light-duty engines.\n\n\n\nAre the distributions for the three pollutants similar? To make this an easier question to answer, try to produce histograms of the variables, using the same class intervals and range for the horizontal axis in each case."
  },
  {
    "objectID": "Lab1_ExplContVar.html#dosage-of-chlorpheniramine-maleate",
    "href": "Lab1_ExplContVar.html#dosage-of-chlorpheniramine-maleate",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: chlorph\nThese data come from a semi-automated process for measuring the actual amount of chlorpheniramine maleate in tablets which are supposed to contain a 4mg dose.\nThe tablets used for the study were made by two different manufacturers. For each manufacturer, a composite was produced by grinding together a number of tablets. Each composite was split into seven pieces each of the same weight as a tablet and the pieces were sent to seven different laboratories. Each laboratory made 10 separate measurements on each composite.\nThe data contain three variables: * chlorpheniramine - the amount measured * manufacturer - the tablet manufacturer as a factor (A or B) * laboratory - the laboratory which performed the measurement as a factor (1 to 7)\n\n\n\nProduce box-plots of the chlorpheniramine measurements split by laboratory. What, if anything, do they suggest?\nNow produce box-plots by manufacturer. Anything noticeable?\nThe problem here is that we really need a separate box-plot for each combination of manufacturer and laboratory. We can do this by boxplot(chlorpheniramine~laboratory*manufacturer, data=chlorph) Try this (or some abbreviated version of it).\nTry reversing the order of laboratory and manufacturer. Which way round is better? Try colouring the box-plots by manufacturer. Does that help?\nDo the data suggest that the two manufacturers actually put different amounts of the drug into supposed 4mg tablets?\nAre there obvious differences between different laboratories? If so, what kind of differences do you observe?\nIf you had to choose a single laboratory to make some measurements for you, which would you choose and why?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DEVUL 2025/26",
    "section": "",
    "text": "This website contains the HTML version of lecture notes for the module in 2025/26. Use the left bar to navigate through the different lectures and the right bar to navigate through different sections in the same lecture.\nThe content of these notes integrates the material of previous lecturers of this module (Jonathan Cummings, Emmanuel Ogundimu and Hyeyoung Maeng). Without their input, the overall quality of the material would be much worse. It shall be noted however that any mistake in the notes is my responsibility: if you spot one, please let me know at daniele.turchetti@durham.ac.uk.\n\n\n\n\n\nLecture 1a\nLecture 1b\nPractical 1\nWorkshop 1\n\n\n\n\n\nLecture 2a\nLecture 2b\nPractical 2\nWorkshop 2"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "DEVUL 2025/26",
    "section": "",
    "text": "Lecture 1a\nLecture 1b\nPractical 1\nWorkshop 1\n\n\n\n\n\nLecture 2a\nLecture 2b\nPractical 2\nWorkshop 2"
  },
  {
    "objectID": "Workshop1.html",
    "href": "Workshop1.html",
    "title": "Workshop 1 - Challenge 1",
    "section": "",
    "text": "For this challenge you are going to work with the Marriage data set:\nload('Marriage.Rda')\nAmong the categorical variables in this data set, there is one called sign\nMarriage$sign\n\n\nAriesLeoPiscesGeminiSaggitariusPiscesLibraAquariusSaggitariusCancerAquariusScorpioVirgoLibraScorpioSaggitariusAquariusTaurusPiscesTaurusAriesSaggitariusLibraCapricornSaggitariusLeoLibraLibraAriesPiscesPiscesVirgoAriesGeminiAquariusSaggitariusGeminiPiscesGeminiVirgoPiscesPiscesGeminiVirgoPiscesVirgoGeminiCancerSaggitariusPiscesTaurusPiscesTaurusCapricornScorpioSaggitariusVirgoAriesLeoAriesTaurusTaurusAquariusCancerPiscesAriesAriesScorpioCancerVirgoLibraGeminiAriesPiscesLibraLeoCancerSaggitariusScorpioScorpioAriesPiscesCancerPiscesLeoLeoScorpioPiscesAquariusVirgoCancerLeoVirgoGeminiCancerAquariusGeminiVirgo\n\n\n    \n        Levels:\n    \n    \n    'Aquarius''Aries''Cancer''Capricorn''Gemini''Leo''Libra''Pisces''Saggitarius''Scorpio''Taurus''Virgo'\n1. Create a barchart of the counts for the categorical variable sign\n2. Create a piechart of the counts for the categorical variable sign\n3. Use the par() command to visualise the barchart and the piechart side by side"
  },
  {
    "objectID": "Workshop1.html#workshop-1---challenge-2",
    "href": "Workshop1.html#workshop-1---challenge-2",
    "title": "Workshop 1 - Challenge 1",
    "section": "Workshop 1 - Challenge 2",
    "text": "Workshop 1 - Challenge 2\nIn this challenge, you’ll work with the British election survey data set.\n\nload('beps.Rda')\n\nWe want to assess whether the attitude towards Europe changes along main party lines or not. Before doing anything - think a little about what you expect to see and write it down in the following box:\n 1. Extract the data corresponding to those individuals who intend to vote for Labour, and plot their attitudes to Europe. Add a main plot label to indicate the party.\n 2. Do the same for LibDem and Conservatives and show the three plots side by side using different colours."
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html",
    "href": "Workshop1_CategoricalVariables.html",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "",
    "text": "Extracting various types of subsets of a given data set\n\n\nDrawing simple barplots using barplot to show the distribution of a categorical variable\n\n\nUsing pie to draw pie charts to represent proportions"
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html#pie-charts",
    "href": "Workshop1_CategoricalVariables.html#pie-charts",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "Pie charts",
    "text": "Pie charts\nThe many issues with pie charts notwithstanding, generating a pie chart is relatively easy with the function pie. Note that the pie chart emphasises showing the data as proportions of the whole, rather than separate counts.\npie takes the same data input as the barplot, and can be supplied with labels, and given custom colours for the wedges. We’ll stick with the defaults for now, but feel free to experiment.\n\npie(offs)\n\n\n\n\n\n\n\n\nIf your goal is compare each category with the the whole (e.g., what portion of weddings are officiated by a Bishop compared to all participants), and the number of categories is small, then pie charts may work for you. However, the best alternative to a piechart is the barchart, but if you really want to show proportions of a whole via area, then a treemap is a better choice (see below).\n\n\nChallenge\n\nThis is your first data visualisation challenge. To start it simple, we’ll just do a recap of bar charts and pie charts. On your assignment on JupyterHub you will\n\nCreate a barchart of the counts for the categorical variable sign in the Marriage dataset.\nCreate a piechart of the same counts.\nUse the par() command to visualise the barchart and the piechart side by side.\n\n\n\nA refresher on how the par() command works\n\nR makes it easy to combine multiple plots into one overall graph, using either the par or layout functions.\nWith the par function, we specify the argument mfrow=c(nr, nc) to split the plot window into a grid of nr x nc plots that are filled in by row.\nNote: if you don’t want to arrange plots in a simple regular grid and have something more complex in mind, you can use the layout function.\n\n\n\n\nSolution of the Challenge\n\n\ndatacount &lt;- xtabs(~sign, data=Marriage) ## save the table to `offs`\nbarplot(datacount, las=2, col='red')\n\n\n\n\n\n\n\n\n\npie(datacount)\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nbarplot(datacount, las=2, col='red')\npie(datacount)"
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html#data-analysis-british-election-survey-data",
    "href": "Workshop1_CategoricalVariables.html#data-analysis-british-election-survey-data",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "Data analysis: British Election survey data",
    "text": "Data analysis: British Election survey data\nIn many surveys there is a string of questions to which respondents give answers on integer scales from, say, 1 to 5. This is so commonplace that one such scale even has a name - the Likert scale.\nThe beps dataset includes seven questions put to 1525 voters in the British Election Panel Study for 1997-2001. One question was on a scale from 1 to 11, one from 0 to 3, and the rest were from 1 to 5.\nThe leaders of the main political parties at the time were Tony Blair for Labour (the Prime Minister at the time), William Hague for the Conservatives, and Charles Kennedy for the Liberal Democrats. Each surveyed individual assessed each party leader on a scale of 1 to 5 (5=best).\n\nLoad the beps data you can find on Ultra. Use the head function to get a quick look at the first few rows of the data to see what the variables are and how they are represented.\nLet’s begin with the party leader data:\n\nSplit your plot display to show a single row of three plots.\nDraw a barplot of each of the party leader’s ratings as contained in the three variables Blair, Hague and Kennedy. Don’t forget to call the xtabs function to summarise the data before plotting.\nColour your barplots by the corresponding party colours (‘red’, ‘blue’, and ‘orange’ respectively.).\n\nCan you see any similarities in the distributions of the assessments of Blair and Hague? How would you interpret these patterns?\nWhat do you find about the assessments of Kennedy?\nRepeat the plots using pie charts - which plots do you find easier to read and interpret?\n\n\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\npar(mfrow=c(1,3))\nbarplot(xtabs(~Blair,data=beps),col='red')\nbarplot(xtabs(~Hague,data=beps),col='blue')\nbarplot(xtabs(~Kennedy,data=beps),col='orange')\n\n\n\n\n\n\n\n## Blair and Hague are very polarised - either v popular or v not, rarely in the middle at '3'\n## Kennedy is less polarised, more middle/positive\n\n\npar(mfrow=c(1,3))\npie(xtabs(~Blair,data=beps))\npie(xtabs(~Hague,data=beps))\npie(xtabs(~Kennedy,data=beps))\n\n\n\n\n\n\n\n# pie give us information on the breakdown of the ratings, but not super helpful when it comes to comparisons\n\nIn addition to assessing the party leaders, two further variables concerned Europe. The first asked the respondents to quantify their knowledge of the parties’ policies on European integration from low knowledge (0) to high knowledge (3), and the second measures the individuals attitudes to European integration from 1 to 11, where higher values are more Eurosceptic.\n\nFirst, we investigate the respondents knowledge of the parties’ policies on Europe as contained in the political.knowledge variable.\nThen we investigate the Europe variable representing individual attitudes towards Europe. Do any features stand out?\nThe data set also contains a column representing the voting intentions of the individuals surveyed in the vote variable. We are going to plot the voting intentions.\n\n\n## Political knowledge of party's European policy\nbarplot(xtabs(~political.knowledge,data=beps),col='forestgreen')\n\n\n\n\n\n\n\n## many 0s and 2s - again, quite split between 'none' and 'some' knowledge of European policy\n\n\n## Attitudes towards European integration\nbarplot(xtabs(~Europe,data=beps),col=c('thistle'))\n\n\n\n\n\n\n\n## Note: 1=pro europe, 11=europsceptic.\n## BIG spike on 11 so strong Eurosceptic sentiment in the sample, with a lesser spike at a neutral position of 6.\n## Noticeably no strong pro-European sentiment at the time\n\n\n## Voting intentions\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'))\n\n\n\n\n\n\n\n## strong Labour support - not surprising as Blair is during his first term as PM\npie(xtabs(~vote,data=beps),col=c('blue','red','orange'))\n\n\n\n\n\n\n\n## this is probably a good use of a pie chart, with few categories and an easy obvious comparison\n\n\n\nChallenge\n\n\nNow, let’s dig a little deeper and think about how attitudes to Europe may differ between the different party voters:\n\nBefore doing anything - think a little about what you expect to see.\nNow, split the plot window into three, extract the data corresponding to those individuals who intend to vote for Labour, and plot their attitudes to Europe, using party colours as above. Add a main plot label to indicate the party.\nRepeat for Conservative and Liberal Democrat voters.\nWhat features do you see? Are there any surprises, or does this confirm what you expected?\n\n\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,3))\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Labour\",]),col='red')\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Conservative\",]),col='blue')\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Liberal Democrat\",]),col='orange')\n\n\n\n\n\n\n\n## Conservatives are very eurosceptic, and almost never pro-Europe\n## Labour is a mix of either slightly pro/neutral, and strongly against\n## LibDem is also similar to Labour - though surprisingly (given recent years) less pro-European"
  },
  {
    "objectID": "Lecture1b_Variables.html",
    "href": "Lecture1b_Variables.html",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "Data is organised into variables, representing attributes or measurements - e.g. age, weight, income, temperature, time, etc.\nTo begin with, we’ll focus on using standard techniques to explore a single variable at a time. Specifically:\n\nExploring Continuous Variables - using statistical summaries, box plots, histograms, and quantile plots\nExploring Categorical Variables - using bar plots, pie charts, and stacked bars.\n\nThese techniques are quite simple, so our focus is on using them effectively to learn about data features and how to do so using R.\nThere are a number of Graphics packages in R that we could use to visualise our data:\n\nBase R covers most standard statistical visualisations\nggplot2 - GGPlot and related packages provide more modern graphics, but it has unusual syntax that can be more difficult to learn\nplotly - similar to ggplot. However, a bit easier to use\nOther custom packages will provide support for specific visualisations\n\nWe will focus on the base R functions, as those always available. However, you should experiment with the other packages, but be aware they work differently.\n\n\n\nWe focus first on quantitative data that is continuous (i.e. real-valued).\nWe view our data as a sample from an underlying continuous distribution - the goal of our explorations here is to seek some clues about the features of that distribution.\nMany possible ways to explore this - graphically, we will focus on histograms and boxplots.\nWhen we have a particular type of distribution in mind, we can also draw a quantile plot to see how plausible it is.\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. the birthweight of babies isn’t.\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\nA histogram is an approximate representation of the distribution of continuous data where we divide the range of values into bins (or buckets) and draw a bar over each bin with area proportional to the frequency of cases in each bin.\nA histogram is an effective tool for visualising features relating to the shape of the data distribution.\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n\nThis data set contains the various information on housing values and related quantities for the 506 suburban areas in Boston. The main interest is in the `median values of owner-occupied homes’, but there are 14 variables to explore here.\nLooking first at the median housing value, we can produce the histogram below. Some obvious features of note are:\n\nA concentration of housing values around 20-25, then a sudden drop-off – is there an explanation for this, such as changes in tax levels?\nPossible multi-modality, with modes around 25 and 30 – are there two classes or groups of housing?\nA further spike in values occur in the upper tail of the data, at 50 – this seems dubious, and could be some crude rounding or grouping of all values ‘’50 and over’’.\n\n\n\n\n\n\n\n\n\n\nChoosing the width of the bars can substantially affect the detail of the histogram. If we choose too few bins, then we can obscure key features by over-smoothing the data. Alterntaively, if we have too many bins then we can introduce too much noise to the plot that it obscures more general features and patterns. Unfortunately, the only way to find a good compromise is to experiment – R and other software will default to a ‘best guess’, but this invariably needs adjusting. The histograms below show the same data, but using bar widths of 5, 2.5, and 1 unit respectively. We’ll see more sophisticated methods for smoothing the data later.\n\n\n\n\n\n\n\n\n\nWith 14 variables in the data set, we could inspect the histograms of each of the variables. We could do this one-at-a-time, or arrange them in a grid or matrix as below:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe other variables in the data set show a variety of the features discussed above that we may want to investigate further:\n\nThe ‘age’ variable - in position \\((1,1)\\) - shows obvious left-skewness.\nThe ‘dis’ variable - in position \\((1,2)\\) - shows right skewness\nThe ‘rm’ variable - in position \\((3,4)\\) - looks symmetric, and possible approximate Normality\nIn the variables in the bottom two rows, we see a lot of gaps in the data and heaping on particular values.\nThe ‘chas’ variable - in position \\((1,3)\\) - exhibits heaping on only two values, 0 and 1. This is a discrete binary variable, not continuous a continuous one! We should use different methods to explore this variable.\n\nIn a full analysis, we would want to look at the relationships between the variables to determine how the features observed relate to each other. We’ll return to this later…\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe geyser data set contains 272 observations of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The variables are:\n\nduration - Length of eruption in mins\nwaiting - Waiting time to next eruption\n\nTo draw a histogram, we use the hist function:\n\nhist(geyser$waiting)\n\n\n\n\n\n\n\n\nHere we can see the data appear to come in two groups: one group with smaller values (shorter waiting times), and one group of larger (longer waiting times).\nWe can add a rug plot to an existing histogram to add more detail and to show where the individual data values fall inside the bars:\n\nhist(geyser$waiting)\nrug(geyser$waiting)\n\n\n\n\n\n\n\n\nNow, each individual mark on the horizontal axis shows where we have observed a data value.\nHistograms can be customised in many ways, but the most important one is changing the configuration of the bars drawn. This is controlled by the breaks parameter. We can set breaks to a number to indicate an approximate number of bars to draw:\n\nhist(geyser$waiting,breaks=20)\n\n\n\n\n\n\n\n\nOr we can be more specific and state where each individual bar should begin and end by listing the breakpoints of the bars as a vector:\n\nhist(faithful$waiting, breaks=c(30,45,47,50,52,55,60,75,80,85,95,105))\n\n\n\n\n\n\n\n\nAdditionally, almost all plot functions can take the following arguments to customise the plot:\n\nxlab, ylab - sets the x and y axis labels\nmain - sets the main title\nxlim, ylim - set x and y axis limits, e.g. c(0,10)\ncol - sets the plot colour(s)\n\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. zero-inflation\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\n\n\n\nWe have considered the shape of a distribution already, and have seen that histograms are a handy way of exploring shape.\nWe come now to several summary statistics (recall ISDS):\n\nthe average and median summarise the centre or location of the distribution.\nthe standard deviation and inter-quartile range summarise spread around the centre of the distribution.\n\nThe average is the most (over-)used statistic. It is useful especially where distributions are reasonably symmetric.\nFor distributions with long tails, it can give misleading signals, e.g. average household income will include households like Buckingham palace which distort the average.\nThe median is a measure of the midpoint of a distribution, and, unlike the mean, it is quite resistant to extreme values.\nA robust summary of spread is given by the interquartile range (IQR), the difference between the first and third quartiles. Every distribution has three quartiles:\n\nThe first quartile, \\(Q_1\\), is a number such that 25% of the values in the distribution do not exceed this number. This is called a lower quartile.\nThe second quartile, \\(Q_2\\), is a number such that 50% of the values in the distribution do not exceed this number. This is the same as the median.\nThe third quartile, \\(Q_3\\), is a number such that 75% of the values in the distribution do not exceed this number. This is called an upper quartile.\n\nTukey suggested that we combine the three quartiles with the minimum and maximum values in the data to form a five-number summary.\n\nIn R, the fivenum function gives the standard 5-number summary:\n\nfivenum(geyser$waiting)\n\n[1]  43  59  76  83 108\n\n\nThe summary function adds a 6th number (the mean):\n\nsummary(geyser$waiting)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  43.00   59.00   76.00   72.31   83.00  108.00 \n\n\nMin. 1st Qu. Median Mean 3rd Qu. Max. 43.0 58.0 76.0 70.9 82.0 96.0\n\n\n\n\nA boxplot (or box-and-whisker plot) is constructed from the 5-number summary by: * Drawing a box with lower boundary at \\(Q_1\\) and upper boundary \\(Q_3\\). * Drawing a line inside the box at the median (\\(Q_2\\)). * Drawing lines (whiskers) from the edges of the box to the most extreme data point that is within \\(1.5\\times IQR\\) of the edge of the box (often the minimum and maximum values).\n\nWe can draw a boxplot by using the boxplot function on a vector of data values:\n\nboxplot(geyser$waiting)\n\n\n\n\n\n\n\n\nOr we can pass all the columns of a data set to boxplot to draw everything at once:\n\nboxplot(geyser)\n\n\n\n\n\n\n\n\nNow, all variables are shown together on a common axis scale. This can be useful if all the variables take values of a similar size, but - as we see here - when the variables are quite different it can obscure the features of some variables by drawing everything together.\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data. Though usually, you have the same number of data points in each column of the data set so this is not often very helpful.\n\n\n\nTo show the relationship between a histogram and a boxplot, we have a data set comprising the length (mm) of 100 cuckoo eggs. Drawng the histogram and boxplot together, we can see that the histogram gives far more detail on the shape of the distribution and the boxplot is more of a summary visualisation. The median is indicated in red and the upper/lower quartiles in green, which shows how these quantities align between the plots. Note that the smallest value in the data is flagged as an outlier and drawn separately as a circle on the boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValues too far from the centre of the distribution are known as outliers.\nData values further than \\(1.5\\times IQR\\) from a lower/upper quartile are called an outlier, and usually indicated with a circle or point.\nAnything beyond \\(3\\times IQR\\) is an extreme outlier, and usually indicated with a star or asterisk.\nThis definition of outliers is a simple rule-of-thumb. It leads to about 0.7% of (normally distributed) observations being described as extreme for large samples.\nHowever, if you have a million data points then 0.7% of your data is still a lot! This doesn’t necessarily mean those points are all genuinely extreme or outliers.\n\n\n\nOne advantage of the boxplot, is that as it is a simple summary plot it is much easier to use boxplots to compare many variables at once. For example, the data plotted below are boxplots of the heights in inches of the singers in the New York Choral Society in 1979. The data are grouped according to the voice part they play in the choir. The vocal range for each voice part increases in pitch according to the following order: Bass 2, Bass 1, Tenor 2, Tenor 1, Alto 2, Alto 1, Soprano 2, Soprano 1. We can see immediately that the lowest pitch voices are associated with the taller singers and the higher pitches with smaller singers, which makes a lot of intuitive sense. There will also be a strong correspondance with Gender here too, though that information is not recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should examine a single boxplot for the following features:\n\nIs the median line in the centre of the 50% portion of the distribution? If not, some degree of skew or asymmetry is indicated.\nAre the whiskers the same length? If not, some degree of skew or asymmetry is indicated.\nAre there any outliers?\n\nWe should examine several boxplots for the following features:\n\nDo the groups have similar shapes? Some might seem symmetric, some skewed.\nAre the median lines about the same, or do there seem to be differences in location** between groups?\nAre the IQRs about the same, or do there seem to be differences in spread between groups?\nAre there some groups with more outliers  than others? Such inspection can reveal important differences between groups.\n\n\n\n\n\nMany statistical methods require our data be approximately Normally distributed - but how can we tell whether the approximation is reasonable?\nNormal-quantile plots provide a simple and informal way of doing this, without having to do a formal hypothesis test — though that may be a natural next step.\nA Normal Quantile (or Q-Q plot) can be used to informally assess the normality of a data set. We construct the plot as follows:\n\nOrder the \\(n\\) data values, giving the ordered data \\(x_{(1)},x_{(2)},\\dots,x_{(n)}\\).\nCalculate the values \\(\\frac{1}{n+1},\\frac{2}{n+1},\\dots,\\frac{n}{n+1}\\), which gives \\(n+1\\) divisions of the \\((0,1)\\) interval.\nUse Normal tables or computer to find the value \\(z_k\\) such that \\(\\Phi(z_k)=\\frac{k}{n+1}\\) for the values \\(k=1,2,\\dots,n\\). The \\(n\\) values \\(z_1,z_2,\\dots,z_n\\) are the \\(\\frac{k}{n+1}\\) quantiles of the Normal distribution.\nPlot the \\(n\\) pairs of points \\((x_{(1)},z_1), (x_{(2)},z_2), \\dots, (x_{(n)},z_n)\\).\n\nThe basic idea is that these plots have the property that plotted points for Normally distributed data should fall roughly on a straight line.\nThis is because \\(x_{(k)}\\) are the sample quantiles of our data, and \\(Z_k\\) are the theoretical quantiles of a Normal distribution. If our sample distribution is approximately normal, these pairs of values will be in agreement.\nSystematic deviations from the straight line indicate non-normality.\nWe don’t need the points to lie on a perfect straight line – we often use the `fat pen test’, meaning that if the points are covered by placing a fat pen over the top, then that’s enough to conclude approximate normality!\n\n\n\nLet’s inspect the cuckoo egg data for normality. We can draw a histogram first, and compare its shape to a normal curve (blue line). While there looks to be approximate agreement (and we only ever need approximate Normality…), we see some divergence for small values of the data.\nDrawing the Normal quantile plot confirms these observations - things look reasonably close to a straight line, but deviate from Normality for small values. However, this would probably be enough to pass our ‘fat pen test’.\n\n\n\n\n\n\n\n\n\nIn R, we can use the qqnorm function to draw a Normal quantile plot of a single variable. Calling qqline after adds the theoretical straight line for comparison\n\nqqnorm(mtcars$mpg)\nqqline(mtcars$mpg)\n\n\n\n\n\n\n\n\nFor this variable (miles per gallon of various cars) we see strong departure from Normality as the points lie far from the desired straight line.\n\n\n\nTo illustrate what happens when our data are very non-Normal, consider this data set of ‘monthly mean relative sunspot numbers’ recorded from from 1749 to 1983. The data are counts of a quantity that is usually quite small in magnitude, this means the data are usually concentrated on small values with an ‘invisible wall’ at 0 - since we cannot observe negative counts! This gives rise to a heavily skewed distribution, as we see in the histogram and boxplot below. The Normal quantile plot (right) now shows strong curvature - not the straight-line feature we would expect if the data were Normally distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is occasionally possible to transform the data so that the transformed data are more approximately Normal.\nThe kinds of transformation we might employ are simple one-to-one power transformations, such \\(\\ln x\\), \\(\\sqrt{x}\\), \\(1/x\\), etc.\nWe can decide informally whether a transformation is useful by examining a Normal quantile plot of the transformed data.\nGraphical displays can help appraise the effectiveness of the transformation, but the cannot tell you if it makes sense - you should consider the interpretation as well as the statistical properties!\n\n\n\n\n\n\nThe frequency distributions of single continuous variables can exhibit a lot of different features\nHistograms are great at for emphasising features of the raw data, but struggle when comparing multiple variables or groups.\nBoxplots show less detail than histograms, but excel at comparing distributions across variables and for identifying outliers.\nQuantile plots can be used to see if data (approximately) follow a particular distribution.\n\n\n\n\nTests using the summary statistics - tests of means and variances against hypothesised values may be appropriate. Normality - formal tests of Normality, such as the KS test, could be applied if following a Normal distribution is important\nOutliers - formal tests for outliers exist (‘outliers’ package) and could be applied, plus the effect of excluding outliers from any analysis could be explored\nMultimodality - methods for assessing multimodality also exist (e.g. ‘diptest’ package), but care must be taken to avoid interpreting data noise as structure\nSample size - many of the methods can be overly sensitive for very large samples\n\n\n\n\n\nCategorical or qualitative variables can only take one of a limited number of possible values (categories). The possible values a variable can take are known as the categories, levels, or labels.\nCategorical data comes in various forms depending on how the categories relate to each other:\n\nNominal - the categories have no standard order (e.g. eye colour)\nOrdinal - the categories have an intrinsic order (e.g. age recorded as “young”, “middle-aged”, and “old”)\nDiscrete - the categories are numerical, and hence ordered, but can only take a finite number of values (e.g. number of people per household)\n\nFor example, the data set below contains the responses to seven questions put to 1525 voters in an election survey for 1997-2001. All the variables in the data are categorical of different types. vote is the party the voter would vote for is clearly categorical, as is gender. The others are ordinal, but with numerical values. In some cases, where the variable is discrete, numerically valued, with a large number of categories - like age - it does make sense to treat it as continuous. Variables such as age are often thought of as being fundamentally continuous albeit recorded as discrete values.\n\nload(\"beps.Rda\")\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\n\nThe levels of a categorical variables are expressed by a factor coding to represent the different categories. Numerical codings are sometimes used, e.g. for a variable on marital status we could define:\n\nsingle\nmarried\nseparated\ndivorced\n…\n\nWhile this helps abbreviate the categories, it is important to remember that variables expressed this way are not the same as a numerical variable that can take these values. We should never treat these values as if they were continuous - for example, it could be tempting to take an average and, say, get a result of 2.6. But this is meaningless as it doesn’t correspond to any of the possibilities, the value we get depends entirely on how we coded our factors, and the rules of arthimetic make no sense for these variables – 1+3 may equal 4, but that does not mean single+separated=divorced!\nInstead, using text strings to represent the categories is safest and avoids mistakes such as these. However, care must be taken to ensure that there is consistency in how these levels are labelled, e.g. do we treat the labels “Female”, “female”, “F” as the same?\n\n\nCompared to continuous variables, categorical variables are relatively simplistic and usually contain little useful information on their own. As data, they usually reduce to the counts of the number of observations in the various categories.\nWe can obtain summary tables of the frequency of each category by using the table function.\n\ntable(beps$vote)\n\n\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\nThe R output is useful for a quick summary, but will need some manual re-formatting to make it presentable. R output is seldom an acceptable way to present information to others, and almost always should be transformed (e.g. into a summary table of relevant information) or summaries (e.g. by reporting only the relevant information R has given you). Here we can transform the ugly R code into a small data table:\n\n\n\n\n\nConservative\nLabour\nLiberal Democrat\n\n\n\n\n462\n720\n343\n\n\n\n\n\nThe xtabs function does something similar, and will be more useful later on when we have multiple variables at once:\n\nxtabs(~vote,data=beps)\n\nvote\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\n\n\n\nVisualisation of categorical variables usually focuses on plotting the counts of the categories, or the proportions that each category contribute to the total.\nThe range of useful graphics for such data is usually limited to:\n\nBarcharts - depict the counts or proportions by the size of the bar\nPiecharts - display the proportions of categories as fractions of the whole\nVariations of the above (stacked bars, treemaps)\n\nWhat features to look for?\n\nExtremes - the largest (smallest) category is often of particular interest.\nUneven distributions - Observational studies can often observe many more cases of one category than others. Some categories may not be observed\nUnexpected patterns of results - surprisingly large or small numbers for particular categories\nLarge numbers of categories - these may require grouping together or filtering out\nDon’t knows, missing values - Missing, unknown, or unavailable data is common in e.g. surveys and opinion polls.\nErrors in factor codings - e.g. gender could be denoted ‘M’ or ‘F’, but we may find values of ‘m’, or ‘female’.\n\n\n\n\nA bar chart or barplot simply draws the distribution of counts per category. We can use barplot to draw a barplot from a summary table of counts generated by the xtabs function\n\nbarplot(xtabs(~vote,data=beps))\n\n\n\n\n\n\n\n\nbarplot takes a number of additional arguments to customise the plot:\n\nnames - a vector of labels for each bar\nhoriz - set to TRUE to show a horizontal barplot.\nwidth - a vector of values to specify the widths of the bars\nspace - a vector of values to specify the spacing between bars\ncol - a vector of colours for the bars\n\n\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'), horiz=TRUE, names=c('Con','Lab','LibDem'))\n\n\n\n\n\n\n\n\n\n\n\nHere we have the number of eligible voters in each of the 16 Bundesländer (states) in the German Federal elections in 2009.\n\nload('btw9s.Rda')\nhead(btw9s)\n\n   Bundesland  Voters   EW State1\nBW         BW 7633818 West     BW\nBY         BY 9382583 West     BY\nBE         BE 2471665 East     BE\nBB         BB 2128715 East     BB\nHB         HB  487978 West     HB\nHH         HH 1256634 West     HH\n\n\n\n\n\n\n\n\n\n\n\nAs states are categorical the ordering of the bars is arbitrary, which limits what we can interpret - but clearly there are some very large and very small states.\nWe notice wide variation in populations - the largest is ‘NW’ (Nordrhein-Westfalen) which includes many major cities like Cologne and Düsseldorf; the smallest is ‘HB’ which is Bremen, a smaller city-state.\nThe ordering of bars can be used for emphasis - here its alphabetical. Ordering the bars by size gives a much better impression of the relative sizes of the sixteen states:\n\n\n\n\n\n\n\n\n\nUsing problem-specific structure can help give you context to your analysis. For example, we can separate the states belonging to the former East (left) and West Germany (right) and use a little colour for emphasis:\n\n\n\n\n\n\n\n\n\nClearly the West German states are substantially more populous.\n\n\n\n\ntitanic &lt;- data.frame(Titanic)\n\nThe Titanic data contains information on the fate of survivors fatal maiden voyage of the ocean liner Titanic. The data are in the form of counts of survivors (and not) summarised by economic status (class), sex and age. The variables are all categorical and defined as\n\nClass - 1st, 2nd, 3rd, or crew\nSex - Female, Male\nAge - Adult, Child\nSurvived - No, Yes\n\nBefore we analyse the data, think about what you expect the results to show. Do we expect more Male or Female survivors? Do we expect those in 1st class to fare better than those in 3rd class? Would we expect the Crew to fare better or worse than the passengers?\nBy thinking about what we might expect before looking at the data, we allow ourselves to be surprised when we find features we did not expect!\n\npar(mfrow=c(1,4))\nbarplot(xtabs(Freq~Survived,data=titanic), col=\"red\",main=\"Survived\")\nbarplot(xtabs(Freq~Sex,data=titanic), col=\"green\",main=\"Sex\")\nbarplot(xtabs(Freq~Age,data=titanic), col=\"orange\",main=\"Sex\")\nbarplot(xtabs(Freq~Class,data=titanic), col=\"blue\",main=\"Sex\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nMore than twice as many passengers died as survived\nMore Male than Female passengers on board - more than 3x as many!\nVery few Child passengers\nMore people in Crew than any other class; fewest in second class\n\nThe interesting questions arise when we try to think whether Survived is related to the other variables. But we’ll have to come back to this later on.\n\n\n\nProportions are of particular interest in opinion polls, or studies where the composition of a larger population is of interest. In elections, the values of the counts are far less important than the share of the total - in particular the size of the largest proportion.\nVisualisations of proportions are based around the simple idea of dividing the larger whole into pieces which reflect the corresponding fractions.\n\npiecharts - slices of a circle\ncomposite or stacked barcharts - fractions of a bar\ntreemaps - tiles within a square or rectangle\n\nA stacked barplot is a variation of a standard barplot where the individual bars are broken up into portions reflecting the different. When we just have one variable, the effect is to stack the individual bars on top of each other.\n\nbarplot(as.matrix(xtabs(~vote,data=beps)), ## note we have to conver to a matrix here\n  beside=FALSE,\n  horiz=TRUE,\n  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nHere we can see the individual sizes of the bars, as well as a clear indication of their contribution to the overall total. So, it’s easy to read that the red category (Labour) has the highest share of the vote in this poll.\nThe traditional pie chart can be generated using pie:\n\npie(xtabs(~vote,data=beps),  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nWe’re probably all familiar with this, but the basic idea is to use slices of the ‘pie’ to represent the proportion. Unfortunately, pie charts are often not the best visualisation as it can be difficult to detect differences between similarly-sized slices of a circle when compared to similiarly sized rectangles.\nHistorical aside: the invention of the pie chart is often attributed to Florence Nightingale. In one of the earliest conventional uses of data visualisation, she used an early version of the pie chart highlight the poor conditions of soldiers in field hospitals during the Crimean War (1854-6). Florence was an early pioneer of statistics and the first female member of the Royal Statistical Society – this aspect of her life and achivements is often overlooked given her frequent association with nursing.\n\n\n\n\n\n\n\nA stem and leaf plot presents the numerical values of the data in a similar form to a histogram.\n\nstem(geyser$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   4 | 3\n   4 | 577888889999999\n   5 | 00000000000011111222223333333444444444\n   5 | 5556677777777788888999\n   6 | 0000001112222234\n   6 | 5555555668889999\n   7 | 01111122222233333344444444\n   7 | 5555555556666666677777777778888888888888888899999999999\n   8 | 00000000000001111111111112222222233333344444444444\n   8 | 5555555666667777777777777788888889999999\n   9 | 0011222333333334\n   9 | 668\n  10 | \n  10 | 8\n\n\nThis plot resembles a sideways histogram, only it shows extra information on the values within each bar.\n\n\n\nA stripplot is similar again, but displays the data as points rather than the numerical value\n\nstripchart(geyser$waiting, method='stack')\n\n\n\n\n\n\n\n\nBy default, it draws a hollow box for each data point which can make it difficult to read. We can modify the shape of the points by the pch (plot character) argument can help improve readability:\n\nstripchart(geyser$waiting, method='stack', pch=16)\n\n\n\n\n\n\n\n\n\n\n\nA beeswarm plot is similar to stripplot, but uses various techniques to separate nearby points such that each point remains visible. It also draws the plot symmetric about a central axis, rather than stacking up points from a baseline.\n\nlibrary(beeswarm)\nbeeswarm(geyser$waiting,horizontal = TRUE)"
  },
  {
    "objectID": "Lecture1b_Variables.html#exploring-continuous-variables",
    "href": "Lecture1b_Variables.html#exploring-continuous-variables",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "We focus first on quantitative data that is continuous (i.e. real-valued).\nWe view our data as a sample from an underlying continuous distribution - the goal of our explorations here is to seek some clues about the features of that distribution.\nMany possible ways to explore this - graphically, we will focus on histograms and boxplots.\nWhen we have a particular type of distribution in mind, we can also draw a quantile plot to see how plausible it is.\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. the birthweight of babies isn’t.\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\nA histogram is an approximate representation of the distribution of continuous data where we divide the range of values into bins (or buckets) and draw a bar over each bin with area proportional to the frequency of cases in each bin.\nA histogram is an effective tool for visualising features relating to the shape of the data distribution.\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n\nThis data set contains the various information on housing values and related quantities for the 506 suburban areas in Boston. The main interest is in the `median values of owner-occupied homes’, but there are 14 variables to explore here.\nLooking first at the median housing value, we can produce the histogram below. Some obvious features of note are:\n\nA concentration of housing values around 20-25, then a sudden drop-off – is there an explanation for this, such as changes in tax levels?\nPossible multi-modality, with modes around 25 and 30 – are there two classes or groups of housing?\nA further spike in values occur in the upper tail of the data, at 50 – this seems dubious, and could be some crude rounding or grouping of all values ‘’50 and over’’.\n\n\n\n\n\n\n\n\n\n\nChoosing the width of the bars can substantially affect the detail of the histogram. If we choose too few bins, then we can obscure key features by over-smoothing the data. Alterntaively, if we have too many bins then we can introduce too much noise to the plot that it obscures more general features and patterns. Unfortunately, the only way to find a good compromise is to experiment – R and other software will default to a ‘best guess’, but this invariably needs adjusting. The histograms below show the same data, but using bar widths of 5, 2.5, and 1 unit respectively. We’ll see more sophisticated methods for smoothing the data later.\n\n\n\n\n\n\n\n\n\nWith 14 variables in the data set, we could inspect the histograms of each of the variables. We could do this one-at-a-time, or arrange them in a grid or matrix as below:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe other variables in the data set show a variety of the features discussed above that we may want to investigate further:\n\nThe ‘age’ variable - in position \\((1,1)\\) - shows obvious left-skewness.\nThe ‘dis’ variable - in position \\((1,2)\\) - shows right skewness\nThe ‘rm’ variable - in position \\((3,4)\\) - looks symmetric, and possible approximate Normality\nIn the variables in the bottom two rows, we see a lot of gaps in the data and heaping on particular values.\nThe ‘chas’ variable - in position \\((1,3)\\) - exhibits heaping on only two values, 0 and 1. This is a discrete binary variable, not continuous a continuous one! We should use different methods to explore this variable.\n\nIn a full analysis, we would want to look at the relationships between the variables to determine how the features observed relate to each other. We’ll return to this later…\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe geyser data set contains 272 observations of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The variables are:\n\nduration - Length of eruption in mins\nwaiting - Waiting time to next eruption\n\nTo draw a histogram, we use the hist function:\n\nhist(geyser$waiting)\n\n\n\n\n\n\n\n\nHere we can see the data appear to come in two groups: one group with smaller values (shorter waiting times), and one group of larger (longer waiting times).\nWe can add a rug plot to an existing histogram to add more detail and to show where the individual data values fall inside the bars:\n\nhist(geyser$waiting)\nrug(geyser$waiting)\n\n\n\n\n\n\n\n\nNow, each individual mark on the horizontal axis shows where we have observed a data value.\nHistograms can be customised in many ways, but the most important one is changing the configuration of the bars drawn. This is controlled by the breaks parameter. We can set breaks to a number to indicate an approximate number of bars to draw:\n\nhist(geyser$waiting,breaks=20)\n\n\n\n\n\n\n\n\nOr we can be more specific and state where each individual bar should begin and end by listing the breakpoints of the bars as a vector:\n\nhist(faithful$waiting, breaks=c(30,45,47,50,52,55,60,75,80,85,95,105))\n\n\n\n\n\n\n\n\nAdditionally, almost all plot functions can take the following arguments to customise the plot:\n\nxlab, ylab - sets the x and y axis labels\nmain - sets the main title\nxlim, ylim - set x and y axis limits, e.g. c(0,10)\ncol - sets the plot colour(s)\n\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. zero-inflation\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\n\n\n\nWe have considered the shape of a distribution already, and have seen that histograms are a handy way of exploring shape.\nWe come now to several summary statistics (recall ISDS):\n\nthe average and median summarise the centre or location of the distribution.\nthe standard deviation and inter-quartile range summarise spread around the centre of the distribution.\n\nThe average is the most (over-)used statistic. It is useful especially where distributions are reasonably symmetric.\nFor distributions with long tails, it can give misleading signals, e.g. average household income will include households like Buckingham palace which distort the average.\nThe median is a measure of the midpoint of a distribution, and, unlike the mean, it is quite resistant to extreme values.\nA robust summary of spread is given by the interquartile range (IQR), the difference between the first and third quartiles. Every distribution has three quartiles:\n\nThe first quartile, \\(Q_1\\), is a number such that 25% of the values in the distribution do not exceed this number. This is called a lower quartile.\nThe second quartile, \\(Q_2\\), is a number such that 50% of the values in the distribution do not exceed this number. This is the same as the median.\nThe third quartile, \\(Q_3\\), is a number such that 75% of the values in the distribution do not exceed this number. This is called an upper quartile.\n\nTukey suggested that we combine the three quartiles with the minimum and maximum values in the data to form a five-number summary.\n\nIn R, the fivenum function gives the standard 5-number summary:\n\nfivenum(geyser$waiting)\n\n[1]  43  59  76  83 108\n\n\nThe summary function adds a 6th number (the mean):\n\nsummary(geyser$waiting)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  43.00   59.00   76.00   72.31   83.00  108.00 \n\n\nMin. 1st Qu. Median Mean 3rd Qu. Max. 43.0 58.0 76.0 70.9 82.0 96.0\n\n\n\n\nA boxplot (or box-and-whisker plot) is constructed from the 5-number summary by: * Drawing a box with lower boundary at \\(Q_1\\) and upper boundary \\(Q_3\\). * Drawing a line inside the box at the median (\\(Q_2\\)). * Drawing lines (whiskers) from the edges of the box to the most extreme data point that is within \\(1.5\\times IQR\\) of the edge of the box (often the minimum and maximum values).\n\nWe can draw a boxplot by using the boxplot function on a vector of data values:\n\nboxplot(geyser$waiting)\n\n\n\n\n\n\n\n\nOr we can pass all the columns of a data set to boxplot to draw everything at once:\n\nboxplot(geyser)\n\n\n\n\n\n\n\n\nNow, all variables are shown together on a common axis scale. This can be useful if all the variables take values of a similar size, but - as we see here - when the variables are quite different it can obscure the features of some variables by drawing everything together.\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data. Though usually, you have the same number of data points in each column of the data set so this is not often very helpful.\n\n\n\nTo show the relationship between a histogram and a boxplot, we have a data set comprising the length (mm) of 100 cuckoo eggs. Drawng the histogram and boxplot together, we can see that the histogram gives far more detail on the shape of the distribution and the boxplot is more of a summary visualisation. The median is indicated in red and the upper/lower quartiles in green, which shows how these quantities align between the plots. Note that the smallest value in the data is flagged as an outlier and drawn separately as a circle on the boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValues too far from the centre of the distribution are known as outliers.\nData values further than \\(1.5\\times IQR\\) from a lower/upper quartile are called an outlier, and usually indicated with a circle or point.\nAnything beyond \\(3\\times IQR\\) is an extreme outlier, and usually indicated with a star or asterisk.\nThis definition of outliers is a simple rule-of-thumb. It leads to about 0.7% of (normally distributed) observations being described as extreme for large samples.\nHowever, if you have a million data points then 0.7% of your data is still a lot! This doesn’t necessarily mean those points are all genuinely extreme or outliers.\n\n\n\nOne advantage of the boxplot, is that as it is a simple summary plot it is much easier to use boxplots to compare many variables at once. For example, the data plotted below are boxplots of the heights in inches of the singers in the New York Choral Society in 1979. The data are grouped according to the voice part they play in the choir. The vocal range for each voice part increases in pitch according to the following order: Bass 2, Bass 1, Tenor 2, Tenor 1, Alto 2, Alto 1, Soprano 2, Soprano 1. We can see immediately that the lowest pitch voices are associated with the taller singers and the higher pitches with smaller singers, which makes a lot of intuitive sense. There will also be a strong correspondance with Gender here too, though that information is not recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should examine a single boxplot for the following features:\n\nIs the median line in the centre of the 50% portion of the distribution? If not, some degree of skew or asymmetry is indicated.\nAre the whiskers the same length? If not, some degree of skew or asymmetry is indicated.\nAre there any outliers?\n\nWe should examine several boxplots for the following features:\n\nDo the groups have similar shapes? Some might seem symmetric, some skewed.\nAre the median lines about the same, or do there seem to be differences in location** between groups?\nAre the IQRs about the same, or do there seem to be differences in spread between groups?\nAre there some groups with more outliers  than others? Such inspection can reveal important differences between groups.\n\n\n\n\n\nMany statistical methods require our data be approximately Normally distributed - but how can we tell whether the approximation is reasonable?\nNormal-quantile plots provide a simple and informal way of doing this, without having to do a formal hypothesis test — though that may be a natural next step.\nA Normal Quantile (or Q-Q plot) can be used to informally assess the normality of a data set. We construct the plot as follows:\n\nOrder the \\(n\\) data values, giving the ordered data \\(x_{(1)},x_{(2)},\\dots,x_{(n)}\\).\nCalculate the values \\(\\frac{1}{n+1},\\frac{2}{n+1},\\dots,\\frac{n}{n+1}\\), which gives \\(n+1\\) divisions of the \\((0,1)\\) interval.\nUse Normal tables or computer to find the value \\(z_k\\) such that \\(\\Phi(z_k)=\\frac{k}{n+1}\\) for the values \\(k=1,2,\\dots,n\\). The \\(n\\) values \\(z_1,z_2,\\dots,z_n\\) are the \\(\\frac{k}{n+1}\\) quantiles of the Normal distribution.\nPlot the \\(n\\) pairs of points \\((x_{(1)},z_1), (x_{(2)},z_2), \\dots, (x_{(n)},z_n)\\).\n\nThe basic idea is that these plots have the property that plotted points for Normally distributed data should fall roughly on a straight line.\nThis is because \\(x_{(k)}\\) are the sample quantiles of our data, and \\(Z_k\\) are the theoretical quantiles of a Normal distribution. If our sample distribution is approximately normal, these pairs of values will be in agreement.\nSystematic deviations from the straight line indicate non-normality.\nWe don’t need the points to lie on a perfect straight line – we often use the `fat pen test’, meaning that if the points are covered by placing a fat pen over the top, then that’s enough to conclude approximate normality!\n\n\n\nLet’s inspect the cuckoo egg data for normality. We can draw a histogram first, and compare its shape to a normal curve (blue line). While there looks to be approximate agreement (and we only ever need approximate Normality…), we see some divergence for small values of the data.\nDrawing the Normal quantile plot confirms these observations - things look reasonably close to a straight line, but deviate from Normality for small values. However, this would probably be enough to pass our ‘fat pen test’.\n\n\n\n\n\n\n\n\n\nIn R, we can use the qqnorm function to draw a Normal quantile plot of a single variable. Calling qqline after adds the theoretical straight line for comparison\n\nqqnorm(mtcars$mpg)\nqqline(mtcars$mpg)\n\n\n\n\n\n\n\n\nFor this variable (miles per gallon of various cars) we see strong departure from Normality as the points lie far from the desired straight line.\n\n\n\nTo illustrate what happens when our data are very non-Normal, consider this data set of ‘monthly mean relative sunspot numbers’ recorded from from 1749 to 1983. The data are counts of a quantity that is usually quite small in magnitude, this means the data are usually concentrated on small values with an ‘invisible wall’ at 0 - since we cannot observe negative counts! This gives rise to a heavily skewed distribution, as we see in the histogram and boxplot below. The Normal quantile plot (right) now shows strong curvature - not the straight-line feature we would expect if the data were Normally distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is occasionally possible to transform the data so that the transformed data are more approximately Normal.\nThe kinds of transformation we might employ are simple one-to-one power transformations, such \\(\\ln x\\), \\(\\sqrt{x}\\), \\(1/x\\), etc.\nWe can decide informally whether a transformation is useful by examining a Normal quantile plot of the transformed data.\nGraphical displays can help appraise the effectiveness of the transformation, but the cannot tell you if it makes sense - you should consider the interpretation as well as the statistical properties!"
  },
  {
    "objectID": "Lecture1b_Variables.html#summary",
    "href": "Lecture1b_Variables.html#summary",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "The frequency distributions of single continuous variables can exhibit a lot of different features\nHistograms are great at for emphasising features of the raw data, but struggle when comparing multiple variables or groups.\nBoxplots show less detail than histograms, but excel at comparing distributions across variables and for identifying outliers.\nQuantile plots can be used to see if data (approximately) follow a particular distribution.\n\n\n\n\nTests using the summary statistics - tests of means and variances against hypothesised values may be appropriate. Normality - formal tests of Normality, such as the KS test, could be applied if following a Normal distribution is important\nOutliers - formal tests for outliers exist (‘outliers’ package) and could be applied, plus the effect of excluding outliers from any analysis could be explored\nMultimodality - methods for assessing multimodality also exist (e.g. ‘diptest’ package), but care must be taken to avoid interpreting data noise as structure\nSample size - many of the methods can be overly sensitive for very large samples"
  },
  {
    "objectID": "Lecture1b_Variables.html#exploring-categorical-variables",
    "href": "Lecture1b_Variables.html#exploring-categorical-variables",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "Categorical or qualitative variables can only take one of a limited number of possible values (categories). The possible values a variable can take are known as the categories, levels, or labels.\nCategorical data comes in various forms depending on how the categories relate to each other:\n\nNominal - the categories have no standard order (e.g. eye colour)\nOrdinal - the categories have an intrinsic order (e.g. age recorded as “young”, “middle-aged”, and “old”)\nDiscrete - the categories are numerical, and hence ordered, but can only take a finite number of values (e.g. number of people per household)\n\nFor example, the data set below contains the responses to seven questions put to 1525 voters in an election survey for 1997-2001. All the variables in the data are categorical of different types. vote is the party the voter would vote for is clearly categorical, as is gender. The others are ordinal, but with numerical values. In some cases, where the variable is discrete, numerically valued, with a large number of categories - like age - it does make sense to treat it as continuous. Variables such as age are often thought of as being fundamentally continuous albeit recorded as discrete values.\n\nload(\"beps.Rda\")\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\n\nThe levels of a categorical variables are expressed by a factor coding to represent the different categories. Numerical codings are sometimes used, e.g. for a variable on marital status we could define:\n\nsingle\nmarried\nseparated\ndivorced\n…\n\nWhile this helps abbreviate the categories, it is important to remember that variables expressed this way are not the same as a numerical variable that can take these values. We should never treat these values as if they were continuous - for example, it could be tempting to take an average and, say, get a result of 2.6. But this is meaningless as it doesn’t correspond to any of the possibilities, the value we get depends entirely on how we coded our factors, and the rules of arthimetic make no sense for these variables – 1+3 may equal 4, but that does not mean single+separated=divorced!\nInstead, using text strings to represent the categories is safest and avoids mistakes such as these. However, care must be taken to ensure that there is consistency in how these levels are labelled, e.g. do we treat the labels “Female”, “female”, “F” as the same?\n\n\nCompared to continuous variables, categorical variables are relatively simplistic and usually contain little useful information on their own. As data, they usually reduce to the counts of the number of observations in the various categories.\nWe can obtain summary tables of the frequency of each category by using the table function.\n\ntable(beps$vote)\n\n\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\nThe R output is useful for a quick summary, but will need some manual re-formatting to make it presentable. R output is seldom an acceptable way to present information to others, and almost always should be transformed (e.g. into a summary table of relevant information) or summaries (e.g. by reporting only the relevant information R has given you). Here we can transform the ugly R code into a small data table:\n\n\n\n\n\nConservative\nLabour\nLiberal Democrat\n\n\n\n\n462\n720\n343\n\n\n\n\n\nThe xtabs function does something similar, and will be more useful later on when we have multiple variables at once:\n\nxtabs(~vote,data=beps)\n\nvote\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\n\n\n\nVisualisation of categorical variables usually focuses on plotting the counts of the categories, or the proportions that each category contribute to the total.\nThe range of useful graphics for such data is usually limited to:\n\nBarcharts - depict the counts or proportions by the size of the bar\nPiecharts - display the proportions of categories as fractions of the whole\nVariations of the above (stacked bars, treemaps)\n\nWhat features to look for?\n\nExtremes - the largest (smallest) category is often of particular interest.\nUneven distributions - Observational studies can often observe many more cases of one category than others. Some categories may not be observed\nUnexpected patterns of results - surprisingly large or small numbers for particular categories\nLarge numbers of categories - these may require grouping together or filtering out\nDon’t knows, missing values - Missing, unknown, or unavailable data is common in e.g. surveys and opinion polls.\nErrors in factor codings - e.g. gender could be denoted ‘M’ or ‘F’, but we may find values of ‘m’, or ‘female’.\n\n\n\n\nA bar chart or barplot simply draws the distribution of counts per category. We can use barplot to draw a barplot from a summary table of counts generated by the xtabs function\n\nbarplot(xtabs(~vote,data=beps))\n\n\n\n\n\n\n\n\nbarplot takes a number of additional arguments to customise the plot:\n\nnames - a vector of labels for each bar\nhoriz - set to TRUE to show a horizontal barplot.\nwidth - a vector of values to specify the widths of the bars\nspace - a vector of values to specify the spacing between bars\ncol - a vector of colours for the bars\n\n\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'), horiz=TRUE, names=c('Con','Lab','LibDem'))\n\n\n\n\n\n\n\n\n\n\n\nHere we have the number of eligible voters in each of the 16 Bundesländer (states) in the German Federal elections in 2009.\n\nload('btw9s.Rda')\nhead(btw9s)\n\n   Bundesland  Voters   EW State1\nBW         BW 7633818 West     BW\nBY         BY 9382583 West     BY\nBE         BE 2471665 East     BE\nBB         BB 2128715 East     BB\nHB         HB  487978 West     HB\nHH         HH 1256634 West     HH\n\n\n\n\n\n\n\n\n\n\n\nAs states are categorical the ordering of the bars is arbitrary, which limits what we can interpret - but clearly there are some very large and very small states.\nWe notice wide variation in populations - the largest is ‘NW’ (Nordrhein-Westfalen) which includes many major cities like Cologne and Düsseldorf; the smallest is ‘HB’ which is Bremen, a smaller city-state.\nThe ordering of bars can be used for emphasis - here its alphabetical. Ordering the bars by size gives a much better impression of the relative sizes of the sixteen states:\n\n\n\n\n\n\n\n\n\nUsing problem-specific structure can help give you context to your analysis. For example, we can separate the states belonging to the former East (left) and West Germany (right) and use a little colour for emphasis:\n\n\n\n\n\n\n\n\n\nClearly the West German states are substantially more populous.\n\n\n\n\ntitanic &lt;- data.frame(Titanic)\n\nThe Titanic data contains information on the fate of survivors fatal maiden voyage of the ocean liner Titanic. The data are in the form of counts of survivors (and not) summarised by economic status (class), sex and age. The variables are all categorical and defined as\n\nClass - 1st, 2nd, 3rd, or crew\nSex - Female, Male\nAge - Adult, Child\nSurvived - No, Yes\n\nBefore we analyse the data, think about what you expect the results to show. Do we expect more Male or Female survivors? Do we expect those in 1st class to fare better than those in 3rd class? Would we expect the Crew to fare better or worse than the passengers?\nBy thinking about what we might expect before looking at the data, we allow ourselves to be surprised when we find features we did not expect!\n\npar(mfrow=c(1,4))\nbarplot(xtabs(Freq~Survived,data=titanic), col=\"red\",main=\"Survived\")\nbarplot(xtabs(Freq~Sex,data=titanic), col=\"green\",main=\"Sex\")\nbarplot(xtabs(Freq~Age,data=titanic), col=\"orange\",main=\"Sex\")\nbarplot(xtabs(Freq~Class,data=titanic), col=\"blue\",main=\"Sex\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nMore than twice as many passengers died as survived\nMore Male than Female passengers on board - more than 3x as many!\nVery few Child passengers\nMore people in Crew than any other class; fewest in second class\n\nThe interesting questions arise when we try to think whether Survived is related to the other variables. But we’ll have to come back to this later on.\n\n\n\nProportions are of particular interest in opinion polls, or studies where the composition of a larger population is of interest. In elections, the values of the counts are far less important than the share of the total - in particular the size of the largest proportion.\nVisualisations of proportions are based around the simple idea of dividing the larger whole into pieces which reflect the corresponding fractions.\n\npiecharts - slices of a circle\ncomposite or stacked barcharts - fractions of a bar\ntreemaps - tiles within a square or rectangle\n\nA stacked barplot is a variation of a standard barplot where the individual bars are broken up into portions reflecting the different. When we just have one variable, the effect is to stack the individual bars on top of each other.\n\nbarplot(as.matrix(xtabs(~vote,data=beps)), ## note we have to conver to a matrix here\n  beside=FALSE,\n  horiz=TRUE,\n  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nHere we can see the individual sizes of the bars, as well as a clear indication of their contribution to the overall total. So, it’s easy to read that the red category (Labour) has the highest share of the vote in this poll.\nThe traditional pie chart can be generated using pie:\n\npie(xtabs(~vote,data=beps),  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nWe’re probably all familiar with this, but the basic idea is to use slices of the ‘pie’ to represent the proportion. Unfortunately, pie charts are often not the best visualisation as it can be difficult to detect differences between similarly-sized slices of a circle when compared to similiarly sized rectangles.\nHistorical aside: the invention of the pie chart is often attributed to Florence Nightingale. In one of the earliest conventional uses of data visualisation, she used an early version of the pie chart highlight the poor conditions of soldiers in field hospitals during the Crimean War (1854-6). Florence was an early pioneer of statistics and the first female member of the Royal Statistical Society – this aspect of her life and achivements is often overlooked given her frequent association with nursing.\n\n\n\n\n\n\n\nA stem and leaf plot presents the numerical values of the data in a similar form to a histogram.\n\nstem(geyser$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   4 | 3\n   4 | 577888889999999\n   5 | 00000000000011111222223333333444444444\n   5 | 5556677777777788888999\n   6 | 0000001112222234\n   6 | 5555555668889999\n   7 | 01111122222233333344444444\n   7 | 5555555556666666677777777778888888888888888899999999999\n   8 | 00000000000001111111111112222222233333344444444444\n   8 | 5555555666667777777777777788888889999999\n   9 | 0011222333333334\n   9 | 668\n  10 | \n  10 | 8\n\n\nThis plot resembles a sideways histogram, only it shows extra information on the values within each bar.\n\n\n\nA stripplot is similar again, but displays the data as points rather than the numerical value\n\nstripchart(geyser$waiting, method='stack')\n\n\n\n\n\n\n\n\nBy default, it draws a hollow box for each data point which can make it difficult to read. We can modify the shape of the points by the pch (plot character) argument can help improve readability:\n\nstripchart(geyser$waiting, method='stack', pch=16)\n\n\n\n\n\n\n\n\n\n\n\nA beeswarm plot is similar to stripplot, but uses various techniques to separate nearby points such that each point remains visible. It also draws the plot symmetric about a central axis, rather than stacking up points from a baseline.\n\nlibrary(beeswarm)\nbeeswarm(geyser$waiting,horizontal = TRUE)"
  },
  {
    "objectID": "Lecture1a_IntroViz.html",
    "href": "Lecture1a_IntroViz.html",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "In this lecture, we look into the concept of exploratory data analysis and learn what makes good (and bad) data visualisation.\n\n\n\nExploratory data analysis (EDA) should be one of the first steps in analysing any data set and was pioneered as a discipline of its own by John Tukey in the 1960s and 1970s.\n\n\n\n\nExploratory Data Analysis chart\n\n\n\nIn the words of Tukey:\n\n“Exploratory data analysis is detective work — in the purest sense — finding and revealing the clues.”\n“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone — as the first step.”\n\nSo, the definition of EDA is fairly self-explanatory. We seek to explore the data, by asking questions and looking for clues to help us better understand the data. The purpose is generally to gain sufficient information to make reasoned and justifiable hypotheses to explore with more formal methods, or to identify sensible modelling approaches (and exclude others). The features and insights we gather while exploring will suggest what appropriate strategies for our subsequent analysis.\nTherefore, it is simply good practice to try to understand and gather as many insights from the data as possible before even attempting before any modelling or inference. Without a solid understanding of the data, we do not know if the techniques we apply are appropriate, and so we risk our inferences and conclusions being invalid.\nThat said, EDA is not a one-off process. It is often iterative, going back and forth between exploration and modelling/analysis as each part of the process can suggest new questions or hypotheses to investigate.\nExploratory Data Analysis is not a formal process and it does not have strict rules to follow or methods to apply: the overarching goal is simply to develop an understanding of the data set. However, typically we do try and do this without fitting any complex models or making assumptions about our data. We’re looking to see what the data tell us, not what our choice of technique or model says. We have insufficient knowledge of the properties of our data to exploit sophisticated techniques.\nThe “detective work” of EDA is essentially looking for clues in the data that will reveal insights about what is actually going on with the problem from which they come — but this requires looking both in the right places and with the right magnifying glass.\nData sets rarely arrive with a manual, or a shopping list of specific features to investigate. Absent any formal structure, the best approach is to pursue any lead that occurs to you, and ask questions - lots of them — some will lead to insights, others will be dead ends.\nSome obvious ‘clues’ and features to investigate in an unseen data set are: 1 Features and properties of individual variables and collections 2 Identifying important and unimportant variables 3 Identifying structure, patterns and relationships between variables 4 Anomalies, errors, and outliers 5 Variation and distributions 6 Missing values\nAs we’re only exploring the data with minimal assumptions, the tools of EDA must be mathematically quite simple and robust as we should not be relying on assumptions of distributions or structure that may not be justified. We rely primarily on:\n\nStatistical summaries\nGraphics and visualisations\n\nOur focus will be extensively on making a graphical exploration of the data.\n\n\n\n\n\nData visualisation is the creation and study of the visual representation of data.\nLike EDA, there is no complex theory about graphics — in fact, there is not much theory at all! The topics are not usually covered in depth in books or lectures as they build on relatively simple statistical concepts. Once the basic graphical forms have been described, textbooks usually move on to more mathematical ideas such as proving the central limit theorem.\nExploratory Data Analysis through investigation of data graphics is sometimes called Graphical Data Analysis (GDA).\nThere are some standard plots and graphics that are applicable in some fairly generaly situations, but\nA good visualisation reveals the data, and communicates complex ideas with clarity, precision and efficiency. Some features of a good data visualisation would be\n\nShow the data!\nInduce the viewer to think about the substance rather than the methodology, design, etc.\nAvoid distorting what the data have to say\nPresent many numbers in a small space\nMake large data sets coherent\nEncourage comparisons between different pieces of data\n\n\nGood data visualisations can communicate the key features of complex data sets more convincingly and more effectively than the raw data often can achieve.\nTypically, data visualisation is used for one of two purposes:\n1- Analysis - used to find patterns, trends, aid in data description, interpretation * Goal: the “Eureka!” moment * Many images for the analyst only, linked to analysis/modelling * Typically many rough and simple plots used to detect interesting features of the data and suggest directions for future investigation, analysis or modelling\n2- Presentation - used to attract attention, make a point, illustrate a conclusion * Goal: The “Wow!” moment. * A single image suitable for a large audience which tells a clear story * Once the key features and behaviours of the data are known, the best graphic can be produced to show those features in a clear way. Often targetting a less technical audience.\nFor example, the visualisation below shows a presentation of the number of cases of measles per 100,000 for the 50 US states over time. The impact of vaccination on the levels of measles is striking and clear.\n\nPresentation quality graphics can venture into the realm of data art, but this is rather beyond what we could hope to achieve in our short course. These visualisations, often called infographics, try to present data in a non-technical way that can easily be understood by non-experts. For example, the following graph illustrates the scale of the amount of waste plastic from plastic bottle sales over 10 years, relative to New York.\n\n\n\n\nFirst, a little bit of historical context. Data visualisation (and statistics) are relatively new disciplines - relative to the rest of mathematics. For data visualisation, William Playfair (1759-1823) is often credited for pioneering many of the graphical forms we still use today. Slightly later, Florence Nightingale (1820-1910) became one of the first people to persuade the public and influence public policy with data visualisation. Despite being better known for her achievements in nursing, Florence was the first female member of the Royal Statistical Society. Her rose diagrams were an innovative combination of pie chart and time series, and were used to illustrate the terrible conditions suffered by soldiers in the Crimean war.\nThese early visualisations were difficult to produce and required a combination of art and intuition. The 20th century brought computers and the ability to process and visualise increasing amounts of data with ease. Ultimately, a number of standard graphics were developed that exploit our visual perception to interpret complex data - most of which we have seen in the course so far.\nSo, despite having a long history, what is it about data visualisation that is so effective that we continue to do it? The answer is that depicting data graphically can be extremely effective as it takes advantage of the human brain’s natural strengths at quickly and efficiently processing visual information. Understanding this will help you make better visualisations!\nThe human brain has developed many subconscious natural abilities to process visual information and make sense of the world around us. We are constantly processing and interpreting the visual signals from our eyes and much of this happens sub-consciously without any actual effort. The reason for this is that this analysis relies on the visual perception part of our brain, rather than cognitive “thinking” part.\nVisual perception is the ability to see, interpret, and organise our environment. It’s fast and efficient, and happens automatically. Cognition, which is handled by the cerebral cortex, is much slower and requires more effort to process information. So, presenting data visually exploits our quick perception capabilities and helps to reduce cognitive load.\nTo illustrate these difference consider the following table and plots. From which of the three presentations of the data is it easiest to identify which 3 regions have the highest available renewable water resources?\n\n\n\n\n\nregion\nkm3\n\n\n\n\nCentral America and Caribbean\n735\n\n\nCentral Asia\n242\n\n\nEast Asia\n3410\n\n\nEastern Europe\n4448\n\n\nMiddle East\n484\n\n\nNorth America\n6077\n\n\nNorthern Africa\n47\n\n\nOceania\n902\n\n\nSouth America\n12724\n\n\nSouth Asia\n1935\n\n\nSub-Saharan Africa\n3884\n\n\nWestern & Central Europe\n2129\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table takes longer to process, as we must read each row, process that information into numbers, that we then compare. The first plot abstracts this for us by using bigger bars for bigger numbers - this makes it much easier to assess the sizes, but we must compare many sets of bars to decide which are the largest. The final plot simplifies this for us, by sorting the bars by size.\nThe difference in speeds at which our human senses can process information was compared by Danish physicist Tor Nørretranders to standard computer throughputs.\n\nNotice how sight comes out on top as it has the same bandwidth as a computer network. This is followed by touch, and hearing, with taste having the same processing power as a pocket calculator. The small white square in the bottom-right corner is the portion of this processing of which we are cognitively aware.\nNot only do our visual senses dominate our sensory processing, but the amount of data and the speed with which we process are far higher than we are aware of. This is known as pre-attentive processing. Pre-attentive processing is subconscious and fast - it take 50-500ms for the eye and brain to process and react to simple visual stimuli. This is clearly much faster thanour brain could process the data table in the small example above. So, turning our data into visual representations means we can process far more information much more quickly.\n\n\n\nThe key idea of data visualisation is that quantitative and categorical information is encoded by visual variables that can be easily perceived. Visual variables are “the differences in elements of a visual as perceived by the human eye”. Essentially these are the fundamental ways in which graphic symbols can be distinguished. When we view a graph, the goal is to decode the graphical attributes and extract information about the data that was encoded\nA number of authors have proposed sets of visual variables that are easy to detect visually:\n\nPosition\nLength\nDirection\nAngle\nArea\nShape\nColour, Texture\nVolume\n\n\n\nWhen using a common coordinate system, position is the easiest feature to recognise and evaluate with regard to elements in space.\nExample: Scatter plots, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s easy to compare separate scales repeated with the same axis even if they are not aligned.\nExample: Lattice/Grid/Facet plots\n\n\n\n\n\n\n\n\n\n\n\n\nLength can effectively represent quantitative information. The human brain easily recognises proportions and evaluates length, even if the objects are not aligned.\nExample: Bar charts, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nAngles help to make comparisons by providing a sense of proportion. Angles are harder to evaluate than length or position, but pie charts are as efficient with small numbers of categories.\nExample: Pie charts\n\n\n\n\n\n\n\n\n\n\n\n\nThe relative magnitude of areas is harder to compare versus the length of lines. The second direction requires more effort to process and interpret.\nExample: Bubble plots, Treemaps, Mosaic plots, Corrplots\n\n\n\n\n\n\n\n\n\n\n\n\nHue is what we usually mean by ‘colour’. Hue can be used to highlight, identify, and group elements in a visual display.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nColour has many aspects. Saturation is the intensity of a single hue. Increasing intensities of colour can be perceived intuitively as numbers of increasing value.\nExample: Heatmaps\n\n\n\n\n\n\n\n\n\n\n\n\nGroups can be distinguished by different shapes, though comparison requires cognition which makes it less effective than with colour.\nExample: Glyph plots, Scatterplots\n\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen colour is not available, different shadings or fills can be applied where previously we would use hue. Generally, these textures are seldom used in modern visualisations as they are less effective than colour.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nVolume refers to using 3D shapes to represent information. But 3D objects are hard to evaluate in a 2D space, making them particularly difficult to read effectively.\nExample: 3D charts\n To make plots appear 3D in a 2D plot, we must introduce a forced perspective, which distorts the quantitative information that we’re trying to present. For example, consider the following 3D barplot:\n\n\nSome bars get hidden behind others\nThe perspective effect makes bars at the front appear taller than those at the back\nIts difficult to read the numerical values\nThe quantitative data is only 1D -the vertical axis. The 3D chart has added 2 un-needed dimensions to the plot, which has compromised its ability to present the data without distortion.\n\n\n3D pie charts are even worse\n\n\n\nThe different visual variables have different levels of efficiency when visually interpreting values of different size. Different tasks will have different rankings. In general, we should use encodings at the top of the scale where possible (and sensible). For instance, when assessing the magnitude of a quantitative variable, we would rank the encodings something like this:\n\nPosition: Common Scale\nPosition: Non-aligned Scale\nLength\nDirection\nAngle\nArea\nColour: Hue\nColour: Saturation\nShape\nTexture\nVolume\n\n\n\n\nWhen displaying multiple quantities, we can combine encodings:\n\nSome encodings can be combined and visually decoded separately. These are separable encodings.\nOther combinations cannot be easily decoded separately, and are integral encodings.\n\n\nSuppose the red point is of interest - finding it among a low number of points is relatively easy (top left). With only two encodings and low density, it is easy to spot the unusual point.\nIncreasing the number of points makes it a little harder to find, but the contrast between colours helps (top centre). Position and hue are clearly separable encodings.\nIf we repeat the same experiment but changing point shape instead, the task becomes harder (top right). Shape requires more effor to process as an encoding.\nAmong 100 points, the triangle is almost lost. Shape is a more challenging feature to distinguish. (bottom left)\nMixing colour and shape compounds the problem further! (bottom centre)\n\n\n\n\n\n\n\n\n\n\nHere, we are juggling many data encodings at once. Horizontal and vertical position of the points indicate numerical values of two variables Colour and point shape indicating values of two categorical variables. Clearly, some aspects are more separable than others (e.g. x and y postition). Using many encodings with multiple different options to show your data become rapidly uninterpretable (below left), unless your data has a great deal of structure to help make sense of things (below right).\n\nN &lt;- 150\nlibrary(mvtnorm)\nxs &lt;- rmvnorm(150, c(0,0),matrix(c(1,0.5,0.5,1),nr=2))\ncs &lt;- cols[sample(1:4,N,replace=TRUE)]\nps &lt;- c(3,15,16,17)[sample(1:4,N,replace=TRUE)]\npar(mfrow=c(1,2))\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=ps,col=cs);graphics::box()\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=c(3,15,16,17)[cut(xs[,2],4)],col=cols[cut(xs[,1],4)]);graphics::box()\n\n\n\n\n\n\n\n\nThe plot below shows the prevalence of Diabetes and Obesity by county in the USA. Here, multiple inseparable encodings have been used, namely two colour hues with blue indicating obesity, and red indicating Diabetes, with intensity of the colours and their combinations showing the level of prevalance in each county. It is almost impossible to disentangle the obesity information from the diabetes information - these are integral encodings. The only obvious features are dark vs light shades of colour - the saturation.\n\n\n\n\n\nThe human brain is wired to see structure, logic, and patterns. It attempts to simplify what it sees by subconsciously arranging parts of an image into an organised whole, rather than just a series of disparate elements. The Gestalt principles were developed to explain how the brain does this, and can be used to aid (and break) data visualisation.\n\nProximity\nSimilarity\nEnclosure\nConnectedness\nClosure\nContinuity\nFigure and Ground\nFocal Point\n\n\n\nThe Proximity principle says that we perceive visual elements which are close together as belonging to the same group. This is easily seen in scatterplots, where we associatethe proximity in the plot with similarity of the object.\n\n\n\n\n\n\n\n\n\nThe same idea applies more generally and can be applied to other plots, where we can arrange the plot to group items we want to perceive as belonging together. Which of the two plots below best compares the sales per country?\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nSpatial proximity takes precedence over all other principles of grouping.\nUse proximity to focus on the visualisation goal, by keeping the main data points closer together.\n\n\n\n\nWe perceive elements with shared visual properties as being “similar”. Objects of similar colors, similar shapes, similar sizes, and similar orientations are instinctively perceived as a group. For example, in the scatterplots below the use of colour reinforces a sense of commonality with points of the same colour that is stronger than the three loose clusters we observe with proximity alone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nUse colour, shape, or size to group visual objects together.\nThe Similarity Principle can help you more readily identify which groups the displayed data belong to.\nColour can be used effectively when associated with intuitive quantities, e.g. red=financial loss, blue=negative temperature. Beware that association of colour with particular concepts varies around the world. An important example is that in Europe and the Americas coloring an upward trend in finanical markets usually uses green or blue is used to denote an upward trend and red is used to denote a downward trend, but in mainland China, Japan, South Korea, and Taiwan, the reverse is true.\nThe idea can be useful when similar colours, shapes, sizes are used consistently across multiple graphics.\n\n\n\n\nThe enclosure principle addresses the fact that enclosing a group of objects brings our attention to them, they are processed together, and our mind perceives them as connected.\n\nRecommendations to aid effective data visualisation:\n\nEnclose objects that you want to be perceived as grouped in a container.\nEnclosures can be used to highlight regions of a visualisation that can be “zoomed in” to give extra detail.\n\n\n\n\nConnectedness says that objects that are connected in some way, such as by a line, are seen as part of the same group. This supersedes other principles like proximity and similarity in terms of visual grouping perception because putting a direct connection between objects is a strong factor in determining the grouping of objects.\nRecommendations to aid effective data visualisation:\n\nConnecting grouped elements by lines is one of the strongest ways to visualise a grouping in the data\nThis is particularly natural with time series, but generally should be avoided unless the x-axis has a similar meaning\nParallel coordinate plots exploit this principle to connect individual data observations\n\n\n\n\nClosure states that when the brain sees complex arrangements of elements, it organises them into recognisable patterns. While this is usually helpful, it can occasionally cause problems. When the human brain is confronted with an incomplete image, it will fill in the blanks to complete the image and make it make sense.\nConsider the three plots of a time series below. The first plot is incomplete, showing a gap around 2013. Absent any other information, our brains would intuitively connect the lines on either side to give the impression of a behaviour like the plot in the middle. The true data actually followed the right plot. If we ignored the gap entirely and plotted all the data, we would draw a time series like the middle curve which would be highly misleading.\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nBe careful when showing graphs with breaks because the human mind tends to form complete shapes even if the shape is incomplete.\nSimilarly, beware joining all points up with lines when there are large gaps in the data - this is just falling into the same trap!\nIf your data have a lot of gaps, use points not lines.\n\n\n\n\nContinuity states that human brains tend to perceive any line or trend as continuing its established direction. The eye follows lines, curves, or a sequence of shapes to determine a relationship between elements.\nFor example, compare the two barplots below. The plot on the right is more easily readable than the one on the left.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nArrange visual objects in a line to simplify grouping and comparison. This happens naturally on scatterplots with obvious trends. Another example would be using bar chars ordered by y-value.\nUsing lines in time series graphs exploits continuity by joining points into a series\nThis can be paired with using colour saturation to emphasise the continuity along a secondary encoding.\n\n\n\n\nThe Figure and Ground principle says the brain will unconsciously place objects either in the foreground or the background. Background subtraction is a “brain technique” which allows an images foreground shapes to be extracted for further processing. To ensure that we can easily recognise patterns and features in a visualisation, we must ensure that the background and foreground elements are sufficiently different that we can easily identify the data from the background of the plot.\nThe plots below are two examples of doing this badly. The low contrast between the background and the data points makes it difficult to read. In particular, beware using yellow or other pale shades on a white background as they can be rendered nearly invisible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nEnsure there is enough contrast between your foreground and background to make charts and graphs more legible and not misleading.\nChoose colours and contrast levels that make your foreground image stand out.\nTransparency can help push less important features to the background.\nAvoid colour overload with many different and contrasting colours. Stick to a small number of distinct hues, or a scale of different intensities.\n\n\n\n\nA relatively recent addition, the focal point principle says that elements that visually stand out are the first thing we see and process in an image. This is related to the Figure and Ground principle, where we make particular elements stand out prominently from the background.\n\n\nLoading required package: grid\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nDistinctive characteristics (e.g., a different color or a different shape) can be used to highlight and create focal points.\nMosaicplots with \\(\\chi^2\\) shading automatically highlight “interesting” features, which immediately draws the eye\nUsing a substantially different colour or intensity can make the feature ‘pop out’ into the foreground\n\n\n\n\n\nThe idea of ‘graphical excellence’ was developed by Edward Tufte. Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. In particular, he said that good graphical displays of data should:\n\nshow the data,\ninduce the viewer to think about the substance,\navoid distorting what the data says,\npresent many numbers in small space,\nmake large data sets coherent,\nencourage comparison between data,\nreveal the data at several levels of detail,\nhave a clear purpose: description, exploration, tabulation or decoration.\n\nUnfortunately, it’s all too easy (and sometimes tempting) to ignore some of these principles to try and prove a particular point.\n\n\nRecall the Anscombe quartet data were identical when examined using simple summary statistics, but vary considerably when graphed\n\nlibrary(datasets)\ndata(anscombe)\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn ill-specified hypothesis or model cannot be rescued by a graphic. No matter how clever or fancy they are!\n\n\n\n\n\n\n\n\n\nThe correlation between these two series is \\(0.952\\), however there is obviously no genuine relationship here. Beware spurious correlations that lead to spurious conclusions, and remember that correlation does not imply causation!\n\n\n\nGraphs rely on our understanding that a number is represented visually by the magnitude of some graphical element.\n“The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the quantities represented.” — E Tufte\nTufte proposed measuring the violation of this principle by: \\[ \\text{Lie factor} = \\frac{\\text{size of effect in graphic}}{\\text{size of effect in data}}\\]\nA good graph should be between 0.95 and 1.05. Anything outside of this is distorting the numerical effect in the data.\nThe image below is hopelessly distoring the proportions, which don’t even add up to one. The graphical element of six equal sized segments bear no resemblance to the data whatsoever!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelective choice of axis ranges is one of the most common abuses of data graphics by disproportionately exaggerating visual effects. Where possible common axes should be used, and when emphasising relative values the inclusion of the origin (0) is recommended.\nThis plot from the Daily Mail below substantially distorts the data by starting the horizontal axis at 0.55 rather than zero. Notice how this substantially inflates the size of the blue bar relative to the red one.\n\n\n\n\n\n\n\n\n\nA more truthful plot would look like this.\n\n\n\n\n\n\n\n\n\nThough it is questionable as to whether a plot is needed to compare \\(0.6\\) with \\(0.7\\)! We probably don’t need the machinery of data visualisation to assess this difference.\n\n\n\nOmitting axis labels is perhaps even worse than being selective about what ranges you draw. Omitting numerical labels on the axes makes any meaningful comparison or interpretation impossible by removing the connection of the graphic with the numerical quantity it represents.\nPolitics is a common source of badly presented data distorted to prove a point. For instance, this tweet showing a selective part of a data set with no numbers to give any sens of scale:\n\n\n\n\n\n\n\n\n\nHow big is the difference between these time series? 0.1? 1? 100? Accurate interpretation is impossible.\nWhereas this shows a more complete picture, with a longer history:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe rely on our visual perception to interpret the graphical elements in terms of numerical values. Plotting simple data using 3D plots introduce a forced perspective, unnecessarily distorting our perception of the data. In 3D plots, the plot region is no longer a rectangle but is distorted - compressing distances at one side and expanding them at the other.\n\n\n\n\n\n\n\n\n\nThis is only plotting the integers 1 to 4, but it is not easy to identify the sizes of the bars. All of the bars appear to be smaller than their defined values, and it is difficult to assess relative sizes - does bar D really look \\(4\\times\\) larger than bar A? Do we really need a barplot with a fake 3rd dimension to compare four integers?\n\n\n\nChartjunk is defined as content-free decoration of a data graphic that hinders the interpretation. This sort of nuisance decoration becomes problematic as it becomes hard to extract the data in the foreground of the plot when it is cluttered and surrounded by other decorations (see the Figure and Ground principle earlier). In general, avoid unnecessary distraction and focus on the data!\n\n\n\n\n\n\n\n\n\nThere are a lot of unnecessary and confusing elements to this simple plot:\n\nA very heavy 3D projection which seriously distorts the plot region\nA drop shadow that has no value\nRedundant use of bar labels and a plot legend.\nColouring the bars is probably not even necessary, as the bars are already labelled and each bar has a unique colour\nPoor choice of colours - Europe and Oceania are two shades of red, implying a degree of similarity\n\n\n\n\nA plot isn’t always the best way to show simple data - ask yourself if a drawing a plot is necessary. In particular, if the data are simple then keen the plot simple - contriving elaborate plots out of very little information is just confusing.\nThe plot below shows the proportion of students enrolling at a US college for two age groups - below 25, and 25+.\n\n\n\n\n\n\n\n\n\nAccording to Edward Tufte in his book `The Visual Display of Quantitative Information’ (2002):\n“This may well be the worst graphic ever to find its way into print.” — E Tufte\nSince all students will fall into one group or the other, it is clear that we can get one time series by subtracting the other from 100%. So, this is trying to show a single time series of 4 points. However, they do just about everything wrong:\n\nThe two time series being plotted are complementary (i.e. they sum to one), so plotting both series is redundant\nAn exaggerated 3D effect is used for no reason\nEach series is shaded using two different colours\nThe \\(y\\) axes ranges includes neither 0 nor 100, so and skips over two sizeable ranges of values (notice the squiggles)\nThe data are interpolated with smooth lines in a rather strange way\n\nIf we were to just plot the data, we would simply see the following\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have extensively used colour in our graphics to highlight features of interest, distinguish different groups, or even indicate values of quantitative variables. Encoding features with colour can be effective, but colour needs to be used appropriately to be successful.\n\n\nThere are three common patterns of use of colour encodings:\n\nCategorical - distinguish different groups\nSequential - indicate different levels of a quantitative variable\nDiverging - indicate different levels and direction of a quantitative variable\n\nCategorical encodings are used to distinguish multiple different groups that have no intrinsic ordering, e.g. levels of a categorical variable. It is important to use a collection of sufficiently distinct and easily recognisable hues (colours) to easily distinguish multiple groups in your visualisation. The groups have no relationship, the hues should be as different as possible and preferably of a similar intensity. The main challenge with this encoding is that only a small number of categories can be encoded this way before we run out of sufficiently different colours.\n\n\n\n\n\n\n\n\n\nSequential encodings are used to represent values of a quantitative variable. By varying the intensity of the colour we can indicate a quantitivate variable by associating large values with more intense (saturated) colour, and lower values with less intense colours. A common example of this is on heatmaps, or more general maps such as of rainfall levels in weather forecasts. Typically, we restrict the colour to many shades of a single hue, but additional shades can be used if meaningful (e.g. to indicate extreme rainfall). While effective at highlighting major differences by major contrasts in the colour intensity, it is more challenging to detect smaller differences as subtle changes in colour and to decode numerical information from the plot.\n\n\n\n\n\n\n\n\n\nDiverging encodings are used to represent the values and direction of a quantitative variable. Combining the two previous ideas, we use two sequential schemes based on substantially different hues (e.g. red and blue) that meet in the middle. Now the colour intensity indicates the magnitude of the value, and the hue of the colour indicates its direction. For example, we have seen this used already in plots of correlation matrices, where strong colour indicated strong correlations and red/blue indicated negative/positive correlation. Another common example is a maps of temperatures in weather forecasts, where warm temperatures use one hue and cold temperatures use another, and they meet in the middle.\n\n\n\n\n\n\n\n\n\n\n\n\nWhile we may have a particular set of colours in mind to use with our visualisations, it can be difficult to set this up in R. There are many different ways to specify colours, and not all of them are intuitive to use:\n\nIntegers codes: R interprets integer values as particular colours from its default palette. For instance, 1=“black”, 2=“red”, 3=“green”, etc.\nColour names: R will a long list of named colours, e.g. “black”, “red”, “green3”, “skyblue“, “cyan”. The full list of names can be found http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\nRGB and similar: Any colour can be represented as a combination of proportions of red, green and blue. R’s rgb function will convert those red, green and blue amounts to a usable colour: black=rgb(0,0,0); green3=rgb(0, 205, 0, max=255), cyan=rgb(0, 255, 255,max=255). The hsv and hcl provide similar functions using alternative colour specifications.\nHexadecimal: The RGB values can also be expressed as a hexademical code: black=“#000000”; cyan=“#00FFFF”.\n\n\n\nR has a default palette of colours used for the default colours in graphs. Integer colour codes are the corresponding colour in this default list. The default palette contains the following eight elements.\n\npalette()\n\n[1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n[8] \"gray\"   \n\npie(rep(1, 8), labels = sprintf(\"%d (%s)\", 1:8, palette()), col = 1:8)\n\n\n\n\n\n\n\n\nWe can replace the standard palette with a vector of our own colours.\n\npalette(c(\"black\",\"#3366cc\",\"#61D04F\",\"#C45560\", \"#F5C710\", \"#CD0BBC\"))\npie(rep(1, 6), labels = sprintf(\"%d (%s)\", 1:6, palette()), col = 1:6)\n\n\n\n\n\n\n\n\nR also has a number of functions that will generate a list of n colours according to a particular scheme. These can be used to replace the default palette, or as input for a particular plot.\n\n\n\n\n\n\n\n\n\nWhich of these are better for: * categorical? * sequential? * diverging?\nIt is also worth noting that we’ve used these to colour area. Their effectiveness may vary when colouring points or lines.\n\n\n\n\n\n\nThe human brain can read and process visual information far faster than any other form - this is why data visualisation is so effective.\nData visualisation uses visual variables to encode the data in a graphical way.\nSome encodings are more effective than others. Some encodings work well together, and others less so.\nThe Gestalt principles describe how the brain sees patterns - we can use this to create better/worse visualisations.\nColour is an effective encoding, but we should carefully choose the colour palette to get the most out of it"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#exploratory-data-analysis",
    "href": "Lecture1a_IntroViz.html#exploratory-data-analysis",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "Exploratory data analysis (EDA) should be one of the first steps in analysing any data set and was pioneered as a discipline of its own by John Tukey in the 1960s and 1970s.\n\n\n\n\nExploratory Data Analysis chart\n\n\n\nIn the words of Tukey:\n\n“Exploratory data analysis is detective work — in the purest sense — finding and revealing the clues.”\n“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone — as the first step.”\n\nSo, the definition of EDA is fairly self-explanatory. We seek to explore the data, by asking questions and looking for clues to help us better understand the data. The purpose is generally to gain sufficient information to make reasoned and justifiable hypotheses to explore with more formal methods, or to identify sensible modelling approaches (and exclude others). The features and insights we gather while exploring will suggest what appropriate strategies for our subsequent analysis.\nTherefore, it is simply good practice to try to understand and gather as many insights from the data as possible before even attempting before any modelling or inference. Without a solid understanding of the data, we do not know if the techniques we apply are appropriate, and so we risk our inferences and conclusions being invalid.\nThat said, EDA is not a one-off process. It is often iterative, going back and forth between exploration and modelling/analysis as each part of the process can suggest new questions or hypotheses to investigate.\nExploratory Data Analysis is not a formal process and it does not have strict rules to follow or methods to apply: the overarching goal is simply to develop an understanding of the data set. However, typically we do try and do this without fitting any complex models or making assumptions about our data. We’re looking to see what the data tell us, not what our choice of technique or model says. We have insufficient knowledge of the properties of our data to exploit sophisticated techniques.\nThe “detective work” of EDA is essentially looking for clues in the data that will reveal insights about what is actually going on with the problem from which they come — but this requires looking both in the right places and with the right magnifying glass.\nData sets rarely arrive with a manual, or a shopping list of specific features to investigate. Absent any formal structure, the best approach is to pursue any lead that occurs to you, and ask questions - lots of them — some will lead to insights, others will be dead ends.\nSome obvious ‘clues’ and features to investigate in an unseen data set are: 1 Features and properties of individual variables and collections 2 Identifying important and unimportant variables 3 Identifying structure, patterns and relationships between variables 4 Anomalies, errors, and outliers 5 Variation and distributions 6 Missing values\nAs we’re only exploring the data with minimal assumptions, the tools of EDA must be mathematically quite simple and robust as we should not be relying on assumptions of distributions or structure that may not be justified. We rely primarily on:\n\nStatistical summaries\nGraphics and visualisations\n\nOur focus will be extensively on making a graphical exploration of the data."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#data-visualisation",
    "href": "Lecture1a_IntroViz.html#data-visualisation",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "Data visualisation is the creation and study of the visual representation of data.\nLike EDA, there is no complex theory about graphics — in fact, there is not much theory at all! The topics are not usually covered in depth in books or lectures as they build on relatively simple statistical concepts. Once the basic graphical forms have been described, textbooks usually move on to more mathematical ideas such as proving the central limit theorem.\nExploratory Data Analysis through investigation of data graphics is sometimes called Graphical Data Analysis (GDA).\nThere are some standard plots and graphics that are applicable in some fairly generaly situations, but\nA good visualisation reveals the data, and communicates complex ideas with clarity, precision and efficiency. Some features of a good data visualisation would be\n\nShow the data!\nInduce the viewer to think about the substance rather than the methodology, design, etc.\nAvoid distorting what the data have to say\nPresent many numbers in a small space\nMake large data sets coherent\nEncourage comparisons between different pieces of data\n\n\nGood data visualisations can communicate the key features of complex data sets more convincingly and more effectively than the raw data often can achieve.\nTypically, data visualisation is used for one of two purposes:\n1- Analysis - used to find patterns, trends, aid in data description, interpretation * Goal: the “Eureka!” moment * Many images for the analyst only, linked to analysis/modelling * Typically many rough and simple plots used to detect interesting features of the data and suggest directions for future investigation, analysis or modelling\n2- Presentation - used to attract attention, make a point, illustrate a conclusion * Goal: The “Wow!” moment. * A single image suitable for a large audience which tells a clear story * Once the key features and behaviours of the data are known, the best graphic can be produced to show those features in a clear way. Often targetting a less technical audience.\nFor example, the visualisation below shows a presentation of the number of cases of measles per 100,000 for the 50 US states over time. The impact of vaccination on the levels of measles is striking and clear.\n\nPresentation quality graphics can venture into the realm of data art, but this is rather beyond what we could hope to achieve in our short course. These visualisations, often called infographics, try to present data in a non-technical way that can easily be understood by non-experts. For example, the following graph illustrates the scale of the amount of waste plastic from plastic bottle sales over 10 years, relative to New York."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#making-effective-data-visualisation",
    "href": "Lecture1a_IntroViz.html#making-effective-data-visualisation",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "First, a little bit of historical context. Data visualisation (and statistics) are relatively new disciplines - relative to the rest of mathematics. For data visualisation, William Playfair (1759-1823) is often credited for pioneering many of the graphical forms we still use today. Slightly later, Florence Nightingale (1820-1910) became one of the first people to persuade the public and influence public policy with data visualisation. Despite being better known for her achievements in nursing, Florence was the first female member of the Royal Statistical Society. Her rose diagrams were an innovative combination of pie chart and time series, and were used to illustrate the terrible conditions suffered by soldiers in the Crimean war.\nThese early visualisations were difficult to produce and required a combination of art and intuition. The 20th century brought computers and the ability to process and visualise increasing amounts of data with ease. Ultimately, a number of standard graphics were developed that exploit our visual perception to interpret complex data - most of which we have seen in the course so far.\nSo, despite having a long history, what is it about data visualisation that is so effective that we continue to do it? The answer is that depicting data graphically can be extremely effective as it takes advantage of the human brain’s natural strengths at quickly and efficiently processing visual information. Understanding this will help you make better visualisations!\nThe human brain has developed many subconscious natural abilities to process visual information and make sense of the world around us. We are constantly processing and interpreting the visual signals from our eyes and much of this happens sub-consciously without any actual effort. The reason for this is that this analysis relies on the visual perception part of our brain, rather than cognitive “thinking” part.\nVisual perception is the ability to see, interpret, and organise our environment. It’s fast and efficient, and happens automatically. Cognition, which is handled by the cerebral cortex, is much slower and requires more effort to process information. So, presenting data visually exploits our quick perception capabilities and helps to reduce cognitive load.\nTo illustrate these difference consider the following table and plots. From which of the three presentations of the data is it easiest to identify which 3 regions have the highest available renewable water resources?\n\n\n\n\n\nregion\nkm3\n\n\n\n\nCentral America and Caribbean\n735\n\n\nCentral Asia\n242\n\n\nEast Asia\n3410\n\n\nEastern Europe\n4448\n\n\nMiddle East\n484\n\n\nNorth America\n6077\n\n\nNorthern Africa\n47\n\n\nOceania\n902\n\n\nSouth America\n12724\n\n\nSouth Asia\n1935\n\n\nSub-Saharan Africa\n3884\n\n\nWestern & Central Europe\n2129\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table takes longer to process, as we must read each row, process that information into numbers, that we then compare. The first plot abstracts this for us by using bigger bars for bigger numbers - this makes it much easier to assess the sizes, but we must compare many sets of bars to decide which are the largest. The final plot simplifies this for us, by sorting the bars by size.\nThe difference in speeds at which our human senses can process information was compared by Danish physicist Tor Nørretranders to standard computer throughputs.\n\nNotice how sight comes out on top as it has the same bandwidth as a computer network. This is followed by touch, and hearing, with taste having the same processing power as a pocket calculator. The small white square in the bottom-right corner is the portion of this processing of which we are cognitively aware.\nNot only do our visual senses dominate our sensory processing, but the amount of data and the speed with which we process are far higher than we are aware of. This is known as pre-attentive processing. Pre-attentive processing is subconscious and fast - it take 50-500ms for the eye and brain to process and react to simple visual stimuli. This is clearly much faster thanour brain could process the data table in the small example above. So, turning our data into visual representations means we can process far more information much more quickly."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#encoding-data",
    "href": "Lecture1a_IntroViz.html#encoding-data",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The key idea of data visualisation is that quantitative and categorical information is encoded by visual variables that can be easily perceived. Visual variables are “the differences in elements of a visual as perceived by the human eye”. Essentially these are the fundamental ways in which graphic symbols can be distinguished. When we view a graph, the goal is to decode the graphical attributes and extract information about the data that was encoded\nA number of authors have proposed sets of visual variables that are easy to detect visually:\n\nPosition\nLength\nDirection\nAngle\nArea\nShape\nColour, Texture\nVolume\n\n\n\nWhen using a common coordinate system, position is the easiest feature to recognise and evaluate with regard to elements in space.\nExample: Scatter plots, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s easy to compare separate scales repeated with the same axis even if they are not aligned.\nExample: Lattice/Grid/Facet plots\n\n\n\n\n\n\n\n\n\n\n\n\nLength can effectively represent quantitative information. The human brain easily recognises proportions and evaluates length, even if the objects are not aligned.\nExample: Bar charts, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nAngles help to make comparisons by providing a sense of proportion. Angles are harder to evaluate than length or position, but pie charts are as efficient with small numbers of categories.\nExample: Pie charts\n\n\n\n\n\n\n\n\n\n\n\n\nThe relative magnitude of areas is harder to compare versus the length of lines. The second direction requires more effort to process and interpret.\nExample: Bubble plots, Treemaps, Mosaic plots, Corrplots\n\n\n\n\n\n\n\n\n\n\n\n\nHue is what we usually mean by ‘colour’. Hue can be used to highlight, identify, and group elements in a visual display.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nColour has many aspects. Saturation is the intensity of a single hue. Increasing intensities of colour can be perceived intuitively as numbers of increasing value.\nExample: Heatmaps\n\n\n\n\n\n\n\n\n\n\n\n\nGroups can be distinguished by different shapes, though comparison requires cognition which makes it less effective than with colour.\nExample: Glyph plots, Scatterplots\n\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen colour is not available, different shadings or fills can be applied where previously we would use hue. Generally, these textures are seldom used in modern visualisations as they are less effective than colour.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nVolume refers to using 3D shapes to represent information. But 3D objects are hard to evaluate in a 2D space, making them particularly difficult to read effectively.\nExample: 3D charts\n To make plots appear 3D in a 2D plot, we must introduce a forced perspective, which distorts the quantitative information that we’re trying to present. For example, consider the following 3D barplot:\n\n\nSome bars get hidden behind others\nThe perspective effect makes bars at the front appear taller than those at the back\nIts difficult to read the numerical values\nThe quantitative data is only 1D -the vertical axis. The 3D chart has added 2 un-needed dimensions to the plot, which has compromised its ability to present the data without distortion.\n\n\n3D pie charts are even worse\n\n\n\nThe different visual variables have different levels of efficiency when visually interpreting values of different size. Different tasks will have different rankings. In general, we should use encodings at the top of the scale where possible (and sensible). For instance, when assessing the magnitude of a quantitative variable, we would rank the encodings something like this:\n\nPosition: Common Scale\nPosition: Non-aligned Scale\nLength\nDirection\nAngle\nArea\nColour: Hue\nColour: Saturation\nShape\nTexture\nVolume\n\n\n\n\nWhen displaying multiple quantities, we can combine encodings:\n\nSome encodings can be combined and visually decoded separately. These are separable encodings.\nOther combinations cannot be easily decoded separately, and are integral encodings.\n\n\nSuppose the red point is of interest - finding it among a low number of points is relatively easy (top left). With only two encodings and low density, it is easy to spot the unusual point.\nIncreasing the number of points makes it a little harder to find, but the contrast between colours helps (top centre). Position and hue are clearly separable encodings.\nIf we repeat the same experiment but changing point shape instead, the task becomes harder (top right). Shape requires more effor to process as an encoding.\nAmong 100 points, the triangle is almost lost. Shape is a more challenging feature to distinguish. (bottom left)\nMixing colour and shape compounds the problem further! (bottom centre)\n\n\n\n\n\n\n\n\n\n\nHere, we are juggling many data encodings at once. Horizontal and vertical position of the points indicate numerical values of two variables Colour and point shape indicating values of two categorical variables. Clearly, some aspects are more separable than others (e.g. x and y postition). Using many encodings with multiple different options to show your data become rapidly uninterpretable (below left), unless your data has a great deal of structure to help make sense of things (below right).\n\nN &lt;- 150\nlibrary(mvtnorm)\nxs &lt;- rmvnorm(150, c(0,0),matrix(c(1,0.5,0.5,1),nr=2))\ncs &lt;- cols[sample(1:4,N,replace=TRUE)]\nps &lt;- c(3,15,16,17)[sample(1:4,N,replace=TRUE)]\npar(mfrow=c(1,2))\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=ps,col=cs);graphics::box()\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=c(3,15,16,17)[cut(xs[,2],4)],col=cols[cut(xs[,1],4)]);graphics::box()\n\n\n\n\n\n\n\n\nThe plot below shows the prevalence of Diabetes and Obesity by county in the USA. Here, multiple inseparable encodings have been used, namely two colour hues with blue indicating obesity, and red indicating Diabetes, with intensity of the colours and their combinations showing the level of prevalance in each county. It is almost impossible to disentangle the obesity information from the diabetes information - these are integral encodings. The only obvious features are dark vs light shades of colour - the saturation."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#gestalt-principles",
    "href": "Lecture1a_IntroViz.html#gestalt-principles",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The human brain is wired to see structure, logic, and patterns. It attempts to simplify what it sees by subconsciously arranging parts of an image into an organised whole, rather than just a series of disparate elements. The Gestalt principles were developed to explain how the brain does this, and can be used to aid (and break) data visualisation.\n\nProximity\nSimilarity\nEnclosure\nConnectedness\nClosure\nContinuity\nFigure and Ground\nFocal Point\n\n\n\nThe Proximity principle says that we perceive visual elements which are close together as belonging to the same group. This is easily seen in scatterplots, where we associatethe proximity in the plot with similarity of the object.\n\n\n\n\n\n\n\n\n\nThe same idea applies more generally and can be applied to other plots, where we can arrange the plot to group items we want to perceive as belonging together. Which of the two plots below best compares the sales per country?\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nSpatial proximity takes precedence over all other principles of grouping.\nUse proximity to focus on the visualisation goal, by keeping the main data points closer together.\n\n\n\n\nWe perceive elements with shared visual properties as being “similar”. Objects of similar colors, similar shapes, similar sizes, and similar orientations are instinctively perceived as a group. For example, in the scatterplots below the use of colour reinforces a sense of commonality with points of the same colour that is stronger than the three loose clusters we observe with proximity alone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nUse colour, shape, or size to group visual objects together.\nThe Similarity Principle can help you more readily identify which groups the displayed data belong to.\nColour can be used effectively when associated with intuitive quantities, e.g. red=financial loss, blue=negative temperature. Beware that association of colour with particular concepts varies around the world. An important example is that in Europe and the Americas coloring an upward trend in finanical markets usually uses green or blue is used to denote an upward trend and red is used to denote a downward trend, but in mainland China, Japan, South Korea, and Taiwan, the reverse is true.\nThe idea can be useful when similar colours, shapes, sizes are used consistently across multiple graphics.\n\n\n\n\nThe enclosure principle addresses the fact that enclosing a group of objects brings our attention to them, they are processed together, and our mind perceives them as connected.\n\nRecommendations to aid effective data visualisation:\n\nEnclose objects that you want to be perceived as grouped in a container.\nEnclosures can be used to highlight regions of a visualisation that can be “zoomed in” to give extra detail.\n\n\n\n\nConnectedness says that objects that are connected in some way, such as by a line, are seen as part of the same group. This supersedes other principles like proximity and similarity in terms of visual grouping perception because putting a direct connection between objects is a strong factor in determining the grouping of objects.\nRecommendations to aid effective data visualisation:\n\nConnecting grouped elements by lines is one of the strongest ways to visualise a grouping in the data\nThis is particularly natural with time series, but generally should be avoided unless the x-axis has a similar meaning\nParallel coordinate plots exploit this principle to connect individual data observations\n\n\n\n\nClosure states that when the brain sees complex arrangements of elements, it organises them into recognisable patterns. While this is usually helpful, it can occasionally cause problems. When the human brain is confronted with an incomplete image, it will fill in the blanks to complete the image and make it make sense.\nConsider the three plots of a time series below. The first plot is incomplete, showing a gap around 2013. Absent any other information, our brains would intuitively connect the lines on either side to give the impression of a behaviour like the plot in the middle. The true data actually followed the right plot. If we ignored the gap entirely and plotted all the data, we would draw a time series like the middle curve which would be highly misleading.\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nBe careful when showing graphs with breaks because the human mind tends to form complete shapes even if the shape is incomplete.\nSimilarly, beware joining all points up with lines when there are large gaps in the data - this is just falling into the same trap!\nIf your data have a lot of gaps, use points not lines.\n\n\n\n\nContinuity states that human brains tend to perceive any line or trend as continuing its established direction. The eye follows lines, curves, or a sequence of shapes to determine a relationship between elements.\nFor example, compare the two barplots below. The plot on the right is more easily readable than the one on the left.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nArrange visual objects in a line to simplify grouping and comparison. This happens naturally on scatterplots with obvious trends. Another example would be using bar chars ordered by y-value.\nUsing lines in time series graphs exploits continuity by joining points into a series\nThis can be paired with using colour saturation to emphasise the continuity along a secondary encoding.\n\n\n\n\nThe Figure and Ground principle says the brain will unconsciously place objects either in the foreground or the background. Background subtraction is a “brain technique” which allows an images foreground shapes to be extracted for further processing. To ensure that we can easily recognise patterns and features in a visualisation, we must ensure that the background and foreground elements are sufficiently different that we can easily identify the data from the background of the plot.\nThe plots below are two examples of doing this badly. The low contrast between the background and the data points makes it difficult to read. In particular, beware using yellow or other pale shades on a white background as they can be rendered nearly invisible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nEnsure there is enough contrast between your foreground and background to make charts and graphs more legible and not misleading.\nChoose colours and contrast levels that make your foreground image stand out.\nTransparency can help push less important features to the background.\nAvoid colour overload with many different and contrasting colours. Stick to a small number of distinct hues, or a scale of different intensities.\n\n\n\n\nA relatively recent addition, the focal point principle says that elements that visually stand out are the first thing we see and process in an image. This is related to the Figure and Ground principle, where we make particular elements stand out prominently from the background.\n\n\nLoading required package: grid\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nDistinctive characteristics (e.g., a different color or a different shape) can be used to highlight and create focal points.\nMosaicplots with \\(\\chi^2\\) shading automatically highlight “interesting” features, which immediately draws the eye\nUsing a substantially different colour or intensity can make the feature ‘pop out’ into the foreground"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#what-to-avoid",
    "href": "Lecture1a_IntroViz.html#what-to-avoid",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The idea of ‘graphical excellence’ was developed by Edward Tufte. Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. In particular, he said that good graphical displays of data should:\n\nshow the data,\ninduce the viewer to think about the substance,\navoid distorting what the data says,\npresent many numbers in small space,\nmake large data sets coherent,\nencourage comparison between data,\nreveal the data at several levels of detail,\nhave a clear purpose: description, exploration, tabulation or decoration.\n\nUnfortunately, it’s all too easy (and sometimes tempting) to ignore some of these principles to try and prove a particular point.\n\n\nRecall the Anscombe quartet data were identical when examined using simple summary statistics, but vary considerably when graphed\n\nlibrary(datasets)\ndata(anscombe)\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn ill-specified hypothesis or model cannot be rescued by a graphic. No matter how clever or fancy they are!\n\n\n\n\n\n\n\n\n\nThe correlation between these two series is \\(0.952\\), however there is obviously no genuine relationship here. Beware spurious correlations that lead to spurious conclusions, and remember that correlation does not imply causation!\n\n\n\nGraphs rely on our understanding that a number is represented visually by the magnitude of some graphical element.\n“The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the quantities represented.” — E Tufte\nTufte proposed measuring the violation of this principle by: \\[ \\text{Lie factor} = \\frac{\\text{size of effect in graphic}}{\\text{size of effect in data}}\\]\nA good graph should be between 0.95 and 1.05. Anything outside of this is distorting the numerical effect in the data.\nThe image below is hopelessly distoring the proportions, which don’t even add up to one. The graphical element of six equal sized segments bear no resemblance to the data whatsoever!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelective choice of axis ranges is one of the most common abuses of data graphics by disproportionately exaggerating visual effects. Where possible common axes should be used, and when emphasising relative values the inclusion of the origin (0) is recommended.\nThis plot from the Daily Mail below substantially distorts the data by starting the horizontal axis at 0.55 rather than zero. Notice how this substantially inflates the size of the blue bar relative to the red one.\n\n\n\n\n\n\n\n\n\nA more truthful plot would look like this.\n\n\n\n\n\n\n\n\n\nThough it is questionable as to whether a plot is needed to compare \\(0.6\\) with \\(0.7\\)! We probably don’t need the machinery of data visualisation to assess this difference.\n\n\n\nOmitting axis labels is perhaps even worse than being selective about what ranges you draw. Omitting numerical labels on the axes makes any meaningful comparison or interpretation impossible by removing the connection of the graphic with the numerical quantity it represents.\nPolitics is a common source of badly presented data distorted to prove a point. For instance, this tweet showing a selective part of a data set with no numbers to give any sens of scale:\n\n\n\n\n\n\n\n\n\nHow big is the difference between these time series? 0.1? 1? 100? Accurate interpretation is impossible.\nWhereas this shows a more complete picture, with a longer history:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe rely on our visual perception to interpret the graphical elements in terms of numerical values. Plotting simple data using 3D plots introduce a forced perspective, unnecessarily distorting our perception of the data. In 3D plots, the plot region is no longer a rectangle but is distorted - compressing distances at one side and expanding them at the other.\n\n\n\n\n\n\n\n\n\nThis is only plotting the integers 1 to 4, but it is not easy to identify the sizes of the bars. All of the bars appear to be smaller than their defined values, and it is difficult to assess relative sizes - does bar D really look \\(4\\times\\) larger than bar A? Do we really need a barplot with a fake 3rd dimension to compare four integers?\n\n\n\nChartjunk is defined as content-free decoration of a data graphic that hinders the interpretation. This sort of nuisance decoration becomes problematic as it becomes hard to extract the data in the foreground of the plot when it is cluttered and surrounded by other decorations (see the Figure and Ground principle earlier). In general, avoid unnecessary distraction and focus on the data!\n\n\n\n\n\n\n\n\n\nThere are a lot of unnecessary and confusing elements to this simple plot:\n\nA very heavy 3D projection which seriously distorts the plot region\nA drop shadow that has no value\nRedundant use of bar labels and a plot legend.\nColouring the bars is probably not even necessary, as the bars are already labelled and each bar has a unique colour\nPoor choice of colours - Europe and Oceania are two shades of red, implying a degree of similarity\n\n\n\n\nA plot isn’t always the best way to show simple data - ask yourself if a drawing a plot is necessary. In particular, if the data are simple then keen the plot simple - contriving elaborate plots out of very little information is just confusing.\nThe plot below shows the proportion of students enrolling at a US college for two age groups - below 25, and 25+.\n\n\n\n\n\n\n\n\n\nAccording to Edward Tufte in his book `The Visual Display of Quantitative Information’ (2002):\n“This may well be the worst graphic ever to find its way into print.” — E Tufte\nSince all students will fall into one group or the other, it is clear that we can get one time series by subtracting the other from 100%. So, this is trying to show a single time series of 4 points. However, they do just about everything wrong:\n\nThe two time series being plotted are complementary (i.e. they sum to one), so plotting both series is redundant\nAn exaggerated 3D effect is used for no reason\nEach series is shaded using two different colours\nThe \\(y\\) axes ranges includes neither 0 nor 100, so and skips over two sizeable ranges of values (notice the squiggles)\nThe data are interpolated with smooth lines in a rather strange way\n\nIf we were to just plot the data, we would simply see the following"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#bonus-content-using-colour-effectively",
    "href": "Lecture1a_IntroViz.html#bonus-content-using-colour-effectively",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "We have extensively used colour in our graphics to highlight features of interest, distinguish different groups, or even indicate values of quantitative variables. Encoding features with colour can be effective, but colour needs to be used appropriately to be successful.\n\n\nThere are three common patterns of use of colour encodings:\n\nCategorical - distinguish different groups\nSequential - indicate different levels of a quantitative variable\nDiverging - indicate different levels and direction of a quantitative variable\n\nCategorical encodings are used to distinguish multiple different groups that have no intrinsic ordering, e.g. levels of a categorical variable. It is important to use a collection of sufficiently distinct and easily recognisable hues (colours) to easily distinguish multiple groups in your visualisation. The groups have no relationship, the hues should be as different as possible and preferably of a similar intensity. The main challenge with this encoding is that only a small number of categories can be encoded this way before we run out of sufficiently different colours.\n\n\n\n\n\n\n\n\n\nSequential encodings are used to represent values of a quantitative variable. By varying the intensity of the colour we can indicate a quantitivate variable by associating large values with more intense (saturated) colour, and lower values with less intense colours. A common example of this is on heatmaps, or more general maps such as of rainfall levels in weather forecasts. Typically, we restrict the colour to many shades of a single hue, but additional shades can be used if meaningful (e.g. to indicate extreme rainfall). While effective at highlighting major differences by major contrasts in the colour intensity, it is more challenging to detect smaller differences as subtle changes in colour and to decode numerical information from the plot.\n\n\n\n\n\n\n\n\n\nDiverging encodings are used to represent the values and direction of a quantitative variable. Combining the two previous ideas, we use two sequential schemes based on substantially different hues (e.g. red and blue) that meet in the middle. Now the colour intensity indicates the magnitude of the value, and the hue of the colour indicates its direction. For example, we have seen this used already in plots of correlation matrices, where strong colour indicated strong correlations and red/blue indicated negative/positive correlation. Another common example is a maps of temperatures in weather forecasts, where warm temperatures use one hue and cold temperatures use another, and they meet in the middle.\n\n\n\n\n\n\n\n\n\n\n\n\nWhile we may have a particular set of colours in mind to use with our visualisations, it can be difficult to set this up in R. There are many different ways to specify colours, and not all of them are intuitive to use:\n\nIntegers codes: R interprets integer values as particular colours from its default palette. For instance, 1=“black”, 2=“red”, 3=“green”, etc.\nColour names: R will a long list of named colours, e.g. “black”, “red”, “green3”, “skyblue“, “cyan”. The full list of names can be found http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\nRGB and similar: Any colour can be represented as a combination of proportions of red, green and blue. R’s rgb function will convert those red, green and blue amounts to a usable colour: black=rgb(0,0,0); green3=rgb(0, 205, 0, max=255), cyan=rgb(0, 255, 255,max=255). The hsv and hcl provide similar functions using alternative colour specifications.\nHexadecimal: The RGB values can also be expressed as a hexademical code: black=“#000000”; cyan=“#00FFFF”.\n\n\n\nR has a default palette of colours used for the default colours in graphs. Integer colour codes are the corresponding colour in this default list. The default palette contains the following eight elements.\n\npalette()\n\n[1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n[8] \"gray\"   \n\npie(rep(1, 8), labels = sprintf(\"%d (%s)\", 1:8, palette()), col = 1:8)\n\n\n\n\n\n\n\n\nWe can replace the standard palette with a vector of our own colours.\n\npalette(c(\"black\",\"#3366cc\",\"#61D04F\",\"#C45560\", \"#F5C710\", \"#CD0BBC\"))\npie(rep(1, 6), labels = sprintf(\"%d (%s)\", 1:6, palette()), col = 1:6)\n\n\n\n\n\n\n\n\nR also has a number of functions that will generate a list of n colours according to a particular scheme. These can be used to replace the default palette, or as input for a particular plot.\n\n\n\n\n\n\n\n\n\nWhich of these are better for: * categorical? * sequential? * diverging?\nIt is also worth noting that we’ve used these to colour area. Their effectiveness may vary when colouring points or lines."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#summary",
    "href": "Lecture1a_IntroViz.html#summary",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The human brain can read and process visual information far faster than any other form - this is why data visualisation is so effective.\nData visualisation uses visual variables to encode the data in a graphical way.\nSome encodings are more effective than others. Some encodings work well together, and others less so.\nThe Gestalt principles describe how the brain sees patterns - we can use this to create better/worse visualisations.\nColour is an effective encoding, but we should carefully choose the colour palette to get the most out of it"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Workshop2_DepRelAss.html",
    "href": "Workshop2_DepRelAss.html",
    "title": "Workshop 2 - Dependency, Relationships and Associations",
    "section": "",
    "text": "In this workshop, we will learn how to:\n\n\nDraw scatterplots of pairs of continuous variables\n\n\nUse transparency in our visualisations\n\n\nAssess dependency, relationships and associations\n\n\nVisualise correlation through heatmaps and scatterplot matrices\n\n\n\nYou will need to install the following packages for today’s workshop:\n\nscales for the alpha() function for transparency in plots\npsych for making improved version of scatterplot matrices\n\nCustom packages are exceptionally useful in R, as they provide more specialised functionality beyond what is included in the base version of R. We will typically make use of custom packages to access specialised visualisation functions or particular data sets that are included in the package.\nThere are many ways to install a new package. Here are a few of the easiest:\n\nIn RStudio, open the Tools menu and select Install Packages.... Type the name of the package, and click install.\nIn the console, type install.packages(\"pkgname\") where you replace pkgname with the name of the package you want to install.\nNote that you can’t install your own packages on the NCC server, a set of advanced packages (including the above) are already installed, so you don’t have to worry about this step in that case.\n\n\ninstall.packages(\"scales\")\n\n\n\nScatterplots are a rather simple plot, but incredibly effective at showing structure in the data. By simply plotting points at the coordinates of two numerical variables, we can easily detect any patterns that may appear.\nThe plot function produces a scatterplot of its two arguments. For illustration, let us use the mtcars data set again, containing information on the characteristics of 23 cars. We can plot miles per gallon against weight with the command\n\ndata(mtcars)\nplot(x=mtcars$wt, y=mtcars$mpg)\n\n\n\n\n\n\n\n\nUnsurprisingly, heavier cars do fewer miles per gallon and are less efficient. The relationship here, while clear and negative, is far from exact with a lot of noise and variation.\nIf the argument labels x and y are not supplied to plot, R will assume the first argument is x and the second is y. If only one vector of data is supplied, this will be taken as the \\(y\\) value and will be plotted against the integers 1:length(y), i.e. in the sequence in which they appear in the data.\n\n\nAnother useful optional argument is type, which can substantially change how plot draws the data. The type argument can take a number of different values to produce different types of plot:\n\ntype=\"p\" - draws a standard scatterplot with a point for every \\((x,y)\\) pair\ntype=\"l\" - connects adjacent \\((x,y)\\) pairs with straight lines, does not draw points. Note this is a lowercase L, not a number 1.\ntype=\"b\" - draws both points and connecting line segments\ntype=\"s\" - connects points with ‘steps’ rather than straight lines\n\n\npar(mfrow=c(2,2))\no &lt;- order(mtcars$wt)\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"p\"', ty='p')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"l\"', ty='l')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"b\"', ty='b')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"s\"', ty='s')\n\n\n\n\n\n\n\n\n\n\n\nThe symbols used for points in scatter plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is pch=1 which is a hollow circle. The possible values of pch are shown in the plot below:\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"2\"', pch=2)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"3\"', pch=3)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"7\"', pch=7)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"10\"', pch=10)\n\n\n\n\n\n\n\n\n\n\n\n\nTo deal with issues of overplotting - where dense areas of points are drawn ontop of each other - we can use transparency to make the plot symbols. For example, here are 500 points randomly generated from a 2-D normal distribution. Notice how the middle of the plot is a solid lump of black\n\nplot(x=rnorm(5000),y=rnorm(5000),pch=16)\n\n\n\n\n\n\n\n\nTo ‘fix’ this, we can specify a transparent colour in the col argument by using the alpha function from the scales package:\n\nlibrary(scales)\nplot(x=rnorm(5000),y=rnorm(5000),pch=16,col=alpha('black',0.2))\n\n\n\n\n\n\n\n\nThe alpha function takes two arguments - a colour first, and then the alpha level itself. This should be a number in \\([0,1]\\) with smaller values being more transparent. Finding a good value for alpha is usually a case of trial-and-error, but in general it will be smaller than you might first expect!\nNow with the transparency we can see a bit more structure in the data, and the darker areas now highlight regions of high data density.\n\n\n Download data: engine\nThis rather simple data set contains three numerical variables, each representing different amounts of pollutants emitted by 46 light-duty engines. The pollutants recorded are Carbon monoxide (CO), Hydrocarbons (HC), and Nitrogen oxide (NOX), all recorded as grammes emitted per mile.\n\n\n\nDownload the engine data set and load it into your workspace\nConstruct three scatterplots to investigate the relationships between every pair of pollutants:\n\nCO versus HC\nCO versus NOX\nHC versus NOX\n\nWhat are the relationships between the amounts of different pollutants emitted by the various engines? In particular, which pollutants are positively associated and which are negatively associated?\n\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,3))\nplot(x=engine$CO, y=engine$HC, pch=16) \nplot(x=engine$CO, y=engine$NOX, pch=16)  \nplot(x=engine$HC, y=engine$NOX, pch=16) \n\n\n\n\n\n\n\n\nThe first plot shows a strong positive linear relationship; The second plot a fairly strong, decreasing non-linear relationship; The third plot a weaker linear relationship.\n\nThe correlation between these variables is checked using the cor function.\n\n\n\nCorrelation Matrix\n\n\n\nCO\nHC\nNOX\n\n\n\n\nCO\n1.00\n0.90\n-0.69\n\n\nHC\n0.90\n1.00\n-0.56\n\n\nNOX\n-0.69\n-0.56\n1.00\n\n\n\n\n\nThe following is code to produce a nice correlation matrix\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncor_matrix &lt;- cor(engine)\n\ncorrplot(cor_matrix, method = \"color\", addCoef.col = \"white\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nand a scatterplot matrix can be plotted using the pairs() command.\n\npairs(engine, panel = panel.smooth, main = \"Scatterplot Matrix\")\n\n\n\n\n\n\n\n\nIf you want include correlation coefficients in a scatterplot matrix, you might want to look into the pairs.panels command. You need to have the psych package installed for this.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:scales':\n\n    alpha, rescale\n\npairs.panels(engine, scale=TRUE, ellipses=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplots are useful at identifying outliers and other distributional features.\nData points can be outliers in two dimensions without being outliers in the separate dimensions! For example taller people are generally heavier, but a particularly heavy person of average height stands out more.\nScatterplots help you determine which points are outliers and if they are outliers in more than one variable.\nThe scatterplots suggest maybe one large outlier on NOX, and two on CO.\n\nboxplot(engine)\n\n\n\n\n\n\n\n\nThe boxplots suggests a few outliers at the top end of each scale.\n\nengine[engine$NOX&gt;2.5,]\n\n     CO   HC  NOX\n39 4.29 0.52 2.94\n\nengine[engine$CO&gt;20,]\n\n      CO   HC  NOX\n34 23.53 1.02 0.86\n35 22.92 1.10 0.57\n\nengine[engine$HC&gt;1,]\n\n      CO   HC  NOX\n34 23.53 1.02 0.86\n35 22.92 1.10 0.57\n\n\nOutliers are identified as cases 39 (for NOX) and 34, 35 (for CO).\nWe are now ready to create a vector of colours to assign\n\ncolours &lt;- scales::alpha(rep('black', length=46 ),0.5) ## transparent black for all points\ncolours[39] &lt;- 'red' ## NOX outlier will be solid red\ncolours[c(34,35)] &lt;- 'blue' ## CO outliers will be solid blue\n\n\npar(mfrow=c(1,3)) ## redraw plots with new colours\nplot(x=engine$CO, y=engine$HC, col=colours, pch=16)\nplot(x=engine$CO, y=engine$NOX, col=colours, pch=16)\nplot(x=engine$HC, y=engine$NOX, col=colours, pch=16)\n\n\n\n\n\n\n\n## now we can see that the outliers on CO are also the two largest values on HC\n\n\n\n\nType data(airquality) to load the built-in airquality data set\nProduce a boxplot of the variables Ozone and Wind. How many outliers are there?\nCheck which observations correspond to these outliers and answer the question again: how many outliers? (caution! The variable Ozone has some missing data and the code above might need to be modified)\nProduce a scatterplot of the two variables where the outliers are highlighted (by colour, as above or in other ways)\n\n\n\n\nClick for solution\n\n\ndata(airquality)\n\npar(mfrow=c(1,2))\nboxplot(airquality$Ozone, col = \"lightblue\")\n\nboxplot(airquality$Wind, col = \"lightgreen\")\n\n\n\n\n\n\n\n\n\nairquality[!is.na(airquality$Ozone) & airquality$Ozone &gt; 120, ]\n\n    Ozone Solar.R Wind Temp Month Day\n62    135     269  4.1   84     7   1\n99    122     255  4.0   89     8   7\n117   168     238  3.4   81     8  25\n\nairquality[airquality$Wind&gt;20,]\n\n   Ozone Solar.R Wind Temp Month Day\n9      8      19 20.1   61     5   9\n48    37     284 20.7   72     6  17\n\n\n\ncolours &lt;- scales::alpha(rep('black',length=153),0.5) ## transparent black for all points\ncolours[c(62, 99, 117)] &lt;- 'red' ## NOX outlier will be solid red\ncolours[c(9,48)] &lt;- 'blue' ## CO outliers will be solid blue\n\n\nplot(x=airquality$Ozone, y=airquality$Wind, col=colours, pch=16)"
  },
  {
    "objectID": "Workshop2_DepRelAss.html#using-the-plot-function",
    "href": "Workshop2_DepRelAss.html#using-the-plot-function",
    "title": "Workshop 2 - Dependency, Relationships and Associations",
    "section": "",
    "text": "Scatterplots are a rather simple plot, but incredibly effective at showing structure in the data. By simply plotting points at the coordinates of two numerical variables, we can easily detect any patterns that may appear.\nThe plot function produces a scatterplot of its two arguments. For illustration, let us use the mtcars data set again, containing information on the characteristics of 23 cars. We can plot miles per gallon against weight with the command\n\ndata(mtcars)\nplot(x=mtcars$wt, y=mtcars$mpg)\n\n\n\n\n\n\n\n\nUnsurprisingly, heavier cars do fewer miles per gallon and are less efficient. The relationship here, while clear and negative, is far from exact with a lot of noise and variation.\nIf the argument labels x and y are not supplied to plot, R will assume the first argument is x and the second is y. If only one vector of data is supplied, this will be taken as the \\(y\\) value and will be plotted against the integers 1:length(y), i.e. in the sequence in which they appear in the data.\n\n\nAnother useful optional argument is type, which can substantially change how plot draws the data. The type argument can take a number of different values to produce different types of plot:\n\ntype=\"p\" - draws a standard scatterplot with a point for every \\((x,y)\\) pair\ntype=\"l\" - connects adjacent \\((x,y)\\) pairs with straight lines, does not draw points. Note this is a lowercase L, not a number 1.\ntype=\"b\" - draws both points and connecting line segments\ntype=\"s\" - connects points with ‘steps’ rather than straight lines\n\n\npar(mfrow=c(2,2))\no &lt;- order(mtcars$wt)\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"p\"', ty='p')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"l\"', ty='l')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"b\"', ty='b')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"s\"', ty='s')\n\n\n\n\n\n\n\n\n\n\n\nThe symbols used for points in scatter plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is pch=1 which is a hollow circle. The possible values of pch are shown in the plot below:\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"2\"', pch=2)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"3\"', pch=3)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"7\"', pch=7)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"10\"', pch=10)"
  },
  {
    "objectID": "Workshop2_DepRelAss.html#overplotting-and-transparency",
    "href": "Workshop2_DepRelAss.html#overplotting-and-transparency",
    "title": "Workshop 2 - Dependency, Relationships and Associations",
    "section": "",
    "text": "To deal with issues of overplotting - where dense areas of points are drawn ontop of each other - we can use transparency to make the plot symbols. For example, here are 500 points randomly generated from a 2-D normal distribution. Notice how the middle of the plot is a solid lump of black\n\nplot(x=rnorm(5000),y=rnorm(5000),pch=16)\n\n\n\n\n\n\n\n\nTo ‘fix’ this, we can specify a transparent colour in the col argument by using the alpha function from the scales package:\n\nlibrary(scales)\nplot(x=rnorm(5000),y=rnorm(5000),pch=16,col=alpha('black',0.2))\n\n\n\n\n\n\n\n\nThe alpha function takes two arguments - a colour first, and then the alpha level itself. This should be a number in \\([0,1]\\) with smaller values being more transparent. Finding a good value for alpha is usually a case of trial-and-error, but in general it will be smaller than you might first expect!\nNow with the transparency we can see a bit more structure in the data, and the darker areas now highlight regions of high data density.\n\n\n Download data: engine\nThis rather simple data set contains three numerical variables, each representing different amounts of pollutants emitted by 46 light-duty engines. The pollutants recorded are Carbon monoxide (CO), Hydrocarbons (HC), and Nitrogen oxide (NOX), all recorded as grammes emitted per mile.\n\n\n\nDownload the engine data set and load it into your workspace\nConstruct three scatterplots to investigate the relationships between every pair of pollutants:\n\nCO versus HC\nCO versus NOX\nHC versus NOX\n\nWhat are the relationships between the amounts of different pollutants emitted by the various engines? In particular, which pollutants are positively associated and which are negatively associated?\n\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,3))\nplot(x=engine$CO, y=engine$HC, pch=16) \nplot(x=engine$CO, y=engine$NOX, pch=16)  \nplot(x=engine$HC, y=engine$NOX, pch=16) \n\n\n\n\n\n\n\n\nThe first plot shows a strong positive linear relationship; The second plot a fairly strong, decreasing non-linear relationship; The third plot a weaker linear relationship.\n\nThe correlation between these variables is checked using the cor function.\n\n\n\nCorrelation Matrix\n\n\n\nCO\nHC\nNOX\n\n\n\n\nCO\n1.00\n0.90\n-0.69\n\n\nHC\n0.90\n1.00\n-0.56\n\n\nNOX\n-0.69\n-0.56\n1.00\n\n\n\n\n\nThe following is code to produce a nice correlation matrix\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncor_matrix &lt;- cor(engine)\n\ncorrplot(cor_matrix, method = \"color\", addCoef.col = \"white\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nand a scatterplot matrix can be plotted using the pairs() command.\n\npairs(engine, panel = panel.smooth, main = \"Scatterplot Matrix\")\n\n\n\n\n\n\n\n\nIf you want include correlation coefficients in a scatterplot matrix, you might want to look into the pairs.panels command. You need to have the psych package installed for this.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:scales':\n\n    alpha, rescale\n\npairs.panels(engine, scale=TRUE, ellipses=FALSE)"
  },
  {
    "objectID": "Workshop2_DepRelAss.html#visualizing-outliers",
    "href": "Workshop2_DepRelAss.html#visualizing-outliers",
    "title": "Workshop 2 - Dependency, Relationships and Associations",
    "section": "",
    "text": "Scatterplots are useful at identifying outliers and other distributional features.\nData points can be outliers in two dimensions without being outliers in the separate dimensions! For example taller people are generally heavier, but a particularly heavy person of average height stands out more.\nScatterplots help you determine which points are outliers and if they are outliers in more than one variable.\nThe scatterplots suggest maybe one large outlier on NOX, and two on CO.\n\nboxplot(engine)\n\n\n\n\n\n\n\n\nThe boxplots suggests a few outliers at the top end of each scale.\n\nengine[engine$NOX&gt;2.5,]\n\n     CO   HC  NOX\n39 4.29 0.52 2.94\n\nengine[engine$CO&gt;20,]\n\n      CO   HC  NOX\n34 23.53 1.02 0.86\n35 22.92 1.10 0.57\n\nengine[engine$HC&gt;1,]\n\n      CO   HC  NOX\n34 23.53 1.02 0.86\n35 22.92 1.10 0.57\n\n\nOutliers are identified as cases 39 (for NOX) and 34, 35 (for CO).\nWe are now ready to create a vector of colours to assign\n\ncolours &lt;- scales::alpha(rep('black', length=46 ),0.5) ## transparent black for all points\ncolours[39] &lt;- 'red' ## NOX outlier will be solid red\ncolours[c(34,35)] &lt;- 'blue' ## CO outliers will be solid blue\n\n\npar(mfrow=c(1,3)) ## redraw plots with new colours\nplot(x=engine$CO, y=engine$HC, col=colours, pch=16)\nplot(x=engine$CO, y=engine$NOX, col=colours, pch=16)\nplot(x=engine$HC, y=engine$NOX, col=colours, pch=16)\n\n\n\n\n\n\n\n## now we can see that the outliers on CO are also the two largest values on HC\n\n\n\n\nType data(airquality) to load the built-in airquality data set\nProduce a boxplot of the variables Ozone and Wind. How many outliers are there?\nCheck which observations correspond to these outliers and answer the question again: how many outliers? (caution! The variable Ozone has some missing data and the code above might need to be modified)\nProduce a scatterplot of the two variables where the outliers are highlighted (by colour, as above or in other ways)\n\n\n\n\nClick for solution\n\n\ndata(airquality)\n\npar(mfrow=c(1,2))\nboxplot(airquality$Ozone, col = \"lightblue\")\n\nboxplot(airquality$Wind, col = \"lightgreen\")\n\n\n\n\n\n\n\n\n\nairquality[!is.na(airquality$Ozone) & airquality$Ozone &gt; 120, ]\n\n    Ozone Solar.R Wind Temp Month Day\n62    135     269  4.1   84     7   1\n99    122     255  4.0   89     8   7\n117   168     238  3.4   81     8  25\n\nairquality[airquality$Wind&gt;20,]\n\n   Ozone Solar.R Wind Temp Month Day\n9      8      19 20.1   61     5   9\n48    37     284 20.7   72     6  17\n\n\n\ncolours &lt;- scales::alpha(rep('black',length=153),0.5) ## transparent black for all points\ncolours[c(62, 99, 117)] &lt;- 'red' ## NOX outlier will be solid red\ncolours[c(9,48)] &lt;- 'blue' ## CO outliers will be solid blue\n\n\nplot(x=airquality$Ozone, y=airquality$Wind, col=colours, pch=16)"
  },
  {
    "objectID": "Lecture2b_ManyContVar.html",
    "href": "Lecture2b_ManyContVar.html",
    "title": "Lecture 2b - Exploring Many Categorical Variables",
    "section": "",
    "text": "In this lecture, we explore data sets with more than one continuous variable. The main aspects we are concerned with are:\n\nWhen studying the relationship between two numerical variables, the simplest and most powerful visualisation is the scatterplot\nWhen a third categorical variable is studied on top of the two numerical ones, one can either use colours (for fewer groups) or a grid or trellis of individual scatterplots (for larger groups)\nCorrelation is the appropriate summary statistic for assessing association, but it is limited to quantifying linear associations.\n\nAs a motivation, we start by looking at the example of Fisher’s Iris data. We choose one continuous variable, say Petal.Length, and start our exploration with a simple histogram:\n\ndata(iris)\nhist(iris$Petal.Length, breaks = 30)\n\n\n\n\n\n\n\n\nThere is a clear separation in two distinct groups, but without further exploration we can’t conclude much. Luckily for us, we have other variables in the same data set: let us plot Petal.Length against Petal.Width:\n\nlibrary(ggplot2)\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"Petal Width vs Petal Length\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFrom this simple plot, we reinforce our idea that at least two distinct group of observations are in our data set. Moreover, we can see a strong association, as flowers with longer petals show also a greater width of petals.\nLet’s now exploit a third variable, species. This is a categorical variable, containing only three groups. A good solution is then to use colour:\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  scale_color_manual(\n    values = c(\n      setosa = \"black\",\n      versicolor = \"yellow3\",\n      virginica = \"lightblue3\"\n    )\n  ) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"Petal Width vs Petal Length by Species\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe discover that the two groups of observations correspond to\n\nthe irises of species setosa\nthe irises of the two other species.\n\nMoreover, we see that inside each group the correlation between petal length and petal width is much lower: the variable species determines a big proportion of the petal size, and once one knows that two given irises belong to a certain species, Petal.Length is less of a good predictor of Petal.Width.\nThe takeout message of this example is that assessing more variables allows for the emergence of complex features of a data set.\nWe are now going to see other examples of features of a data set with many continuous variables.\n\n\nLet’s start our exploration with scatterplots:\n\nDrawing scatterplots is one of the first things statisticians do when looking at data.\nA scatterplot displays two quantitative variables against each other data by plotting each data points values as \\((x,y)\\) coordinates.\nScatterplots can reveal structure not readily apparent from summaries, and are both easy to present and interpret.\nThe major role of scatterplots is in exposing associations between variables - not just linear associations, but any kind of association.\nScatterplots are also useful at identifying outliers and other distributional features.\nHowever, marginal distributions cannot always be easily seen from a scatterplot.\n\n\n\nFor example, let’s consider the Weight and Height of the 10,384 athletes competing in the London 2012 Olympics:\n\nlibrary(VGAMdata)\ndata(oly12)\n\nWe can draw a scatterplot of these variables using the plot function:\n\nplot(x=oly12$Height, y=oly12$Weight)\n\n\n\n\n\n\n\n\nNote the choice of which variable is drawn as the horizontal coordinate and which is the vertical.\nThe plot function accepts the usual arguments to customise the graphic.\n\nxlab, ylab, main - axis labels and main title\nxlim, ylim - axis ranges, a vector of length two\npch - changes the plot character, integer\ncol - changes the point colour, either specifying one colour for all points or one colour for each point\ncex - relative point size, defaults to 1\n\nIn particular, the plot character (pch) can be changed to something more solid to give a clearer picture.\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=20)\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships - associations between variables will manifest as trends (either linear or nonlinear) in a scatterplot. Here, we’re interested in the nature, direction, and strength of any discovered relationship\nCausal relationships - Great care should be taken to distinguish association from possible causation.\nOutliers or groups of outliers - Cases can be outliers in two dimensions without being outliers in the separate dimensions. Taller people are generally heavier, but a particularly heavy person of average height stands out more.\nClusters - Groups of cases occurring separately from the rest of the data, such as the iris species in Fisher’s iris data.\nGranularity - Values may line up in regular columns or rows indicating some form of rounding or grouping of the data has occurred before analysis.\nBarriers - Some combinations of values may be impossible. Age cannot be negative, and years of employment cannot be more than Age.\nGaps and holes - Some combinations of values may be theoretically possible, but do not occur in the data, e.g. very tall but very light people.\nConditional relationships - sometimes the relationship can change fundamentally given another variable, e.g. income vs age will look quite different for working age and retired people, and iris flowers look different for different species.\n\nSo, what do features do we see in this plot:\n\nThere is a fairly strong positive relationship - taller athletes are heavier. This makes rather obvious sense. Sometimes part of statistical exploration is to confirm common-sense inutuitions and to reassure ourselves that the data are correctly recorded (and that any pre-conceptions we have are correct!)\nSome outliers break this pattern - the single athlete with a weight over 200 is a rather heavy Judo player\nNote how some points arrange themselves into parallel vertical and horizontal lines - this is because the data values have been rounded, which forces the points onto a grid of regular values and leaving gaps in between.\nWe have 10384 athletes in the data, but there area far fewer visible points here - this is a problem of over plotting\n\n\n\n\nOverplotting is a problem in a scatterplot where the drawn data points overlap one another. This typically occurs when there are a large number of data points and/or a small number of unique values in the dataset.\nAs multiple stacked points look the same as a single point, this makes it difficult to identify areas of high density. In the scatterplot above, we cannot tell if there is one person with a weight over 200 or one hundred.\nPossible solutions include:\n\nUsing transparency - higher density is then evident by darker regions, where the degree of darkness is caused by the multiple overlapping points\nJittering - add random noise to the points to turn the stacks into point clouds\nUsing smaller points - only feasible for modestly sized problems.\n\nWe can apply transparency to the scatterplot of the Olympic athletes as follows:\n\nlibrary(scales)\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=20,\n    col=alpha('black',0.2)) ## this tells R to use 20% transparent black for each point\n\n\n\n\n\n\n\n\nNow the areas of high density in the main cloud of data stand out as darker, and the more unusual values fade out.\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe Old Faithful geyser in Yellowstone National Park, Wyoming, USA which a very regular pattern of eruption.\nConsider the duration of the eruptions and the waiting time until the next eruption.\n\nplot(y=geyser$waiting,x=geyser$duration, pch=16, xlab='duration',ylab='waiting')\n\n\n\n\n\n\n\n\nWhat can we see?\n\nPossible evidence of 2 or 3 clusters of points in the data\nOne cluster of shorter duration eruptions associated with a longer waiting time.\nA further cluster (or two) of longer eruptions with a short or long waiting time.\nClear signs of rounding producing a line of data points with a duration of exactly 4. Definitely suspicious!\n\n\n\n\n Download data: movies\nMost of the data sets we’ve seen so far are not very big. Consider instead the movies data set, which contained 24 variables and 58 788 cases corresponding to various attributes of different movies taken from IMDB. Let’s focus on two of the variables: the average IMDB user rating, and the number of users who rated that movie on IMDB.\n\nplot(x=movies$votes, y=movies$rating, ylab=\"Rating\", xlab=\"Votes\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nThere are no films with many votes and very low average rating.\nFor films with more than a small number of votes, the average rating increases with number of votes.\nNo film with lots of votes has an average rating close to the maximum possible. There is almost an invisible barrier preventing very high scores.\nA few films with a high number of votes (over 50 000) appear far from the rest of the data with unusually low ratings compared to others.\nFilms with a low number of votes may have any average ratin from the worst to the best.\nThe only films with high average ratings are films with relatively few votes.\n\nWe can extract a lot of information from a scatterplot!\n\n\n\nPatterns and trends observed within a scatterplot can often be explained by the action of other variables.\nIn particular, other categorical variables may induce different behaviours for each group (e.g. male and female).\n\nThere are a variety of ways to explore and compare possible groupings:\nIndicate the groups within the plot via colour or plot symbol\nSplit the data and draw separate plots for each potential group\n\n\n\nConsider the average values of fertility (number of children per mother) versus percentage of contraceptors among women of childbearing age in developing nations around 1990.\n\nlibrary(carData)\ndata(Robey)\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=20,\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\n\n\n\n\n\n\n\n\n\nWe find a rough straight line relationship for most of the points - suggests a negative linear association between fertility and contraceptor percentage\nStill a lot of spread about the rough line, i.e. the relationship is clear but far from perfect.\nPragmatically, this seems to agree with the common-sense view that fertility declines on average as the percentage of contraceptors in a country rise.\nSometimes part of the job of exploring the dat is confirming the data agrees with what might be considered obvious - finding the opposite effect in the data would be far more surprising!\n\nThe fertility data set also contains a third variable representing the region of the world, which we can indicate in colour.\n\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=20, col=Robey$region,\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\nlegend(x='topright',pch=20,lwd=NA, col=1:nlevels(Robey$region), legend=levels(Robey$region))\n\n\n\n\n\n\n\n\n\nThis doesn’t seem to affect the overall conclusion, in that the pattern seems to be the same, and linear, in each region.\nSometimes, not finding anything is a useful thing to learn!\n\nWe could have used a different symbol for each region, but this is generally less effective than colour:\n\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=as.numeric(Robey$region),\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\nlegend(x='topright',lwd=NA, pch=1:nlevels(Robey$region), legend=levels(Robey$region))\n\n\n\n\n\n\n\n\n\nColour creates more of a contrast, making it much easier to distinguish the groups\nIn general, stick to a single symbol - unless you must use black & white, or you have multiple different grouping variables!\n\n\n\n\n\nThe Olympic athletes data set also could be investigated by using colour to represent Gender. This shows an expected differentiation between the two:\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=16,\n     col=alpha(c('#000000','#ff0000'),0.2)[oly12$Sex])\nlegend(x='topleft', legend=levels(oly12$Sex), pch=16, col=c(1,2))\n\n\n\n\n\n\n\n\nColouring the 42 sport categories is less helpful.\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=16,\n     col=oly12$Sport)\n\n\n\n\n\n\n\n\nUnfortunately, with many categories the differences between 42 colours become too subtle to detect easily. Colour is only useful when indicating a small number of groups.\nIn this case, a better solution is to break the data up and draw separate mini scatterplots for each of the 42 sports. This is called a lattice, grid or trellis of plots:\n\n\n\n\n\n\n\n\n\n\nThough the plots are small, we can still see the key features.\nThe increasing trend is common across all sports.\nFor some sports, the relationship looks nonlinear.\nThe picture is less clear for Athletics, which groups a lot of very different events together.\nSome sports seem to have very few or no athletes - we’re missing data on 1346 of the athletes.\n\n\n\n\n\nThe standard measure of the strength of linear relationship between two variables is the correlation coefficient . Assuming that we have \\(n\\) pairs of observations \\((x_1,y_1)\\), …,\\((x_n,y_n)\\), the correlation coefficient is defined to be: [ r=_{i=1}^n()() ] In R, we can use the cor function to compute this.\nThe correlation can take any value between \\(-1\\) and \\(+1\\), inclusive, with positive values representing positive correlation (as one variable increases, so does the other), and negative values representing negative correlation (as one variable increases, the other decreases). A correlation of \\(0\\) means uncorrelated: no association (as one variable increases, the other doesn’t consistently increase or decrease).\nThus the value of \\(r\\) can range from \\(r=-1\\) (perfect negative correlation) through weaker negative correlation until \\(r=0\\) (no correlation) through weak positive correlation to \\(r=1\\) (perfect +ve correlation).\nThe correlation tells us two things:\n\nthe strength of association (strong, weak or zero);\nthe direction of association (negative or positive).\n\nFor the fertility data:\n\ncor(y=Robey$tfr, x=Robey$contraceptors)\n\n[1] -0.9203109\n\n\nThe value here is negative reflecting the negative aassociation - the increase in contraception corresponds to a reduction in numbers of children. The value itself is quite large (close to -1) which indicates a strong association - this is reflected by the fact the data are roughly organised along a straight line.\nDifficulties with correlation:\n\nCan be difficult to interpret numerically: a value of \\(r = 0.7\\) can mean different strengths of association for different numbers of pairs of points. Generally, the more points there are, the lower the correlation has to be to indicate association.\nStrong correlations (near \\(\\pm 1\\)) are quite rare, especially if the size of the distributions is large. Weak correlations (near 0) are common. Quite small correlations can indicate a linear association if the size of distributions is large, though it is difficult to set a hard and fast rule.\n\\(r\\) doesn’t tell us by how much a change in one variable affects change in the other - it just tells us the rough direction and degree of change. For that, you need a regression.\nThe correlation coefficient measures linear association, and so can be misleading when there is nonlinear association, or when their are outliers present.\nFor two separate scatter plots, the correlation coefficient might be numerically the same, but the \\(x,y\\) relationship between the variables may be very different.\n\nRecall, for example, Anscombe’s quartet of data points:\n\nlibrary(datasets)\ndata(anscombe)\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\n\nTheir correlations are almost the same (up to 2 decimal places), but the actual patterns of the points are quite different:\n\npar(mfrow=c(2,2),mai = c(0.3, 0.3, 0.3, 0.3))\nplot(x=anscombe$x1, y=anscombe$y1,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x2, y=anscombe$y2,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x3, y=anscombe$y3,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x4, y=anscombe$y4,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\n\n\n\n\n\n\n\n\n\n\n\nVisualising large numbers of continuous variables has the potential to uncover enven more features, but also presents more challenges.\n\n\n\ndata(iris)\nboxplot(iris[,4:1])\n\n\n\n\n\n\n\n\nBoxplots are effective for getting an overview of major differences between variables - particularly in terms of their location (position vertically) and scale (length of the boxplot). However, boxplots are too simple to show whether a variable splits into different modes or groups. Boxplots are also fundamentally a 1-dimensional graphic - they cannot detect or display whether the variables are related.\n\npairs(iris[,4:1], pch=16)\n\n\n\n\n\n\n\n\nConversely, the scatterplot matrix exposes a great deal of the structure of the data, and makes relationships clear. Patterns, trends, and groups emerge quite easily and can be easily spotted by eye. But it is not as effective for comparing scales and locations - we still need the boxplots, even if they are limited!\n\n\n\nWhat if we need to include a third numerical variable into a scatter plot? One way to achieve this is a bubble plot, where we use the value of a third variable to control the size of the points we draw on a scatterplot:\n\nlibrary(plotly)\nplot_ly(iris, x=~Petal.Width, y=~Petal.Length, size=~Sepal.Width, color=~Species,\n    sizes = c(1,50), type='scatter', mode='markers', marker = list(opacity = 0.7, sizemode = \"diameter\"))\n\n\nHere we have positioned the points of the iris data according to the Petal measurements and used the size of the point to indicate the Sepal.Width. We can note that the orange points (versicolor) appear to have more small bubbles than the others, and the green points (setosa) look to have larger values.\nThis technique can be quite effective when the variable corresponding to point size corresponds to some measure of importance, scale, or size (such as population per country, number of samples in a survey, number of patients in a medical trial). While it can be effectively used in three-variable problems, for more variables than this we require other methods.\nDealing with more continuous variables will require scaling up a lot of the familiar techniques from earlier. Individual numerical summaries can still be computed, though it is even more difficult to easily compare. Some of the standard visualisations can be easily applied to many variables, such as histograms and box plots.\nOne particularly useful technique is to take scatterplots, and draw many of them in a matrix to effectively compare multiple variables at once. Scatterplot matrices are a matrix of scatterplots with each variable plotted against all of the others.\nLike the grid or trellis scatterplot we produce an array of plots, but now we plot all variables simultaneously!\nThese give excellent initial overviews of the relationship between continuous variables in data sets with relatively small numbers of variables.\n\n\n\nLet’s use a data set on Swiss banknotes to illustrate variations we can make to a scatterplot matrix.\nThe data are six measurements on each of 100 genuine and 100 forged Swiss banknotes. The idea is to see if we can distinguish the real notes from the fakes on the basis of the notes dimensions only:\n\nlibrary(car)\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:VGAM':\n\n    logit\n\ndata(bank,package=\"gclus\")\nscatterplotMatrix(bank[,-1], smooth=FALSE, regLine=FALSE, diagonal=TRUE, groups=bank$Status,\n                  col=c(5,7), pch=c(16,16))\n\n\n\n\n\n\n\n\n\nBlue=Genuine, Yellow=Fake.\nWe can use the diagonal elements to show histograms (or smoothed histograms)\nCan we see any variables for which the two groups are well separated? “Diagonal” looks like a good candidate. Looking at the (smoothed) histograms in the Diagonal panel, we can see the two distributions barely overlap indicating a useful variable for separating the groups.\nWe can see some assocations and relationships here “Left”/“Right” are positively correlated, but “Top”/“Diagonal” are negatively associated.\nThere also seem to be a few possible outliers, not all are forgeries!\n\n\n\n\nParallel coordinate plots (PCP) have become a popular tool for highly-multivariate data.\nRather than using perpendicular axes for pairs of variables and points for data, we draw all variables on parallel vertical axes and connect data with lines.\nThe variables are standardised, so that a common vertical axis makes sense.\n\npar(mfrow=c(1,2))\npairs(iris[,4:1],pch=16,col=c('#f8766d','#00ba38','#619cff')[iris$Species])\n\n\n\n\n\n\n\nlibrary(lattice)\nparallelplot(iris[,1:4], col=c('#f8766d','#00ba38','#619cff')[iris$Species], horizontal=FALSE)\n\n\n\n\n\n\n\n\nIn general:\n\nEach line represents one data point.\nThe height of each line above the variable label indicates the (relative) size of the value for that data point. The variables are transformed to a common scale to make comparison possible.\nAll the values for one observation are connected across the plot, so we can see how things change for individuals as we move from one variable to another. Note that this is sensitive to the ordering we choose for the variables in the plot – some orderings of the variables may give clearer pictures than others.\n\nFor these data:\n\nWe can see how the setosa species (red) is separated from the others on the Petal measurements in the PCP by the separation between the group of red lines and the rest.\nSetosa (red) is also generally smaller than the others, except on Sepal Width where it is larger than the other species.\nWe can also pick out outliers, for instance one setosa iris has a particularly small value of Sepal Width compared to all the others.\n\nUsing parallel coordinate plots:\n\nReading and interpreting a parallel coordinate plot (PCP) is a bit more difficult than a scatterplot, but can be just as effective for bigger problems.\nWhen we identify interesting features, we can then investigate them using more familiar graphics.\nA PCP gives a quick overview of the univariate distribution for each variable, and so we can identify skewness, outliers, gaps and concentrations.\nHowever, for pairwise properties and associations then it’s best to draw a scatterplot.\n\n\n\nThe Guardian newspaper in the UK publishes a ranking of British universities each year and it reported these data in May, 2012 as a guide for 2013. 120 Universities are ranked using a combination of 8 criteria, combined into an ‘Average Teaching Score’ used to form the ranking.\n\nlibrary(GDAdata)\ndata(uniranks)\n## a little bit of data tidying\nnames(uniranks)[c(5,6,8,10,11,13)] &lt;- c('AvTeach','NSSTeach','SpendPerSt','Careers',\n                                        'VAddScore','NSSFeedb')\nuniranks1 &lt;- within(uniranks, StaffStu &lt;- 1/(StudentStaffRatio))\n## draw the scatterplot matrix\npairs(uniranks1[,c(5:8,10:14)])\n\n\n\n\n\n\n\n\nWe can see some obvious patterns and dependencies here. What about the parallel coordinate plot?\n\nparallelplot(uniranks1[,c(5:8,10:14)], col='black',  horizontal=FALSE)\n\n\n\n\n\n\n\n\nCan we learn anything from this crazy mess of spaghetti?\nPCPs are most effective if we colour the lines to represent subgroups of the data. We can colour the Russell Group universities, which unsurprisingly are usually at the top (except on NSSFeedback!)\n\n## create a new variable to represent the group we want\nuniranks2 &lt;- within(uniranks1,\n                    Rus &lt;- factor(ifelse(UniGroup==\"Russell\", \"Russell\", \"not\")))\n\nparallelplot(uniranks2[,c(5:8,10:14)], col=c(\"#2297E6\",\"#DF536B\")[uniranks2$Rus],  horizontal=FALSE)\n\n\n\n\n\n\n\n\nUsing colour helps us to diffentiate the lines from the different data points more clearly. Now we can start to explore for features and patterns:\n\nAvTeach is the overall ranking, so notice how high/low values here are connected to high/low values elsewhere.\nThere are some exceptions - the 3rd ranked university on AvTeach has a surprisingly low NSSTeaching score (note the rapidly descending red line)\nNotice how the data ‘pinch’ at certain values on NSS Teach and NSSOverall leaving gaps - this suggests a granularity effect due to limited options of values on the NSS survey scores, e.g. integer scores out of 10. This creates heaping and gaps in the data values as the data is really ordinal.\nAlso, notice how most of the lines are moving upwards when connecting these NSSTeach and NSSOverall - this would suggest a positive correlation.\nThe extremes of these two NSS scores are rarely observed, and they also correspond strongly to extremes on the overall score and ranking.\nThe heavy concentration on low values on EntryTariff (entry requirements) and StaffStu (staff:student ratio) suggests strong skewness in the distributions\n\nA scatterplot matrix corroborates most of these features, though we’re probably near the limit of what we can read and extract from such a plot!\n\npairs(uniranks2[,c(5:8,10:14)],col=c(\"#2297E6\",\"#DF536B\")[uniranks2$Rus], pch=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot matrix is easily overloaded with many variables and cases.\nCorrelations provide a useful numerical summary, and the corrplot is a good way to visualise that information.\nNote that the standard correlation coefficient is not a good measure of non-linear relationships or suitable for categorical variables! Alternative correlations exist for such variables, but these are not computed by default.\n\n\nround(cor(Boston),2) ## round to 2dp\n\n         crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio\ncrim     1.00 -0.20  0.41 -0.06  0.42 -0.22  0.35 -0.38  0.63  0.58    0.29\nzn      -0.20  1.00 -0.53 -0.04 -0.52  0.31 -0.57  0.66 -0.31 -0.31   -0.39\nindus    0.41 -0.53  1.00  0.06  0.76 -0.39  0.64 -0.71  0.60  0.72    0.38\nchas    -0.06 -0.04  0.06  1.00  0.09  0.09  0.09 -0.10 -0.01 -0.04   -0.12\nnox      0.42 -0.52  0.76  0.09  1.00 -0.30  0.73 -0.77  0.61  0.67    0.19\nrm      -0.22  0.31 -0.39  0.09 -0.30  1.00 -0.24  0.21 -0.21 -0.29   -0.36\nage      0.35 -0.57  0.64  0.09  0.73 -0.24  1.00 -0.75  0.46  0.51    0.26\ndis     -0.38  0.66 -0.71 -0.10 -0.77  0.21 -0.75  1.00 -0.49 -0.53   -0.23\nrad      0.63 -0.31  0.60 -0.01  0.61 -0.21  0.46 -0.49  1.00  0.91    0.46\ntax      0.58 -0.31  0.72 -0.04  0.67 -0.29  0.51 -0.53  0.91  1.00    0.46\nptratio  0.29 -0.39  0.38 -0.12  0.19 -0.36  0.26 -0.23  0.46  0.46    1.00\nblack   -0.39  0.18 -0.36  0.05 -0.38  0.13 -0.27  0.29 -0.44 -0.44   -0.18\nlstat    0.46 -0.41  0.60 -0.05  0.59 -0.61  0.60 -0.50  0.49  0.54    0.37\nmedv    -0.39  0.36 -0.48  0.18 -0.43  0.70 -0.38  0.25 -0.38 -0.47   -0.51\n        black lstat  medv\ncrim    -0.39  0.46 -0.39\nzn       0.18 -0.41  0.36\nindus   -0.36  0.60 -0.48\nchas     0.05 -0.05  0.18\nnox     -0.38  0.59 -0.43\nrm       0.13 -0.61  0.70\nage     -0.27  0.60 -0.38\ndis      0.29 -0.50  0.25\nrad     -0.44  0.49 -0.38\ntax     -0.44  0.54 -0.47\nptratio -0.18  0.37 -0.51\nblack    1.00 -0.37  0.33\nlstat   -0.37  1.00 -0.74\nmedv     0.33 -0.74  1.00\n\n\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncorrplot(cor(Boston))\n\n\n\n\n\n\n\n\nThere are clearly strong associations between age and dis, indus and dis, and lstat and medv that may be worth studying closer. chas appears almost uncorrelated to all the other variables – but remember chas was a binary variable, and so the correlation coefficient is meaningless here and we should not draw conclusions from this feature!"
  },
  {
    "objectID": "Lecture2b_ManyContVar.html#scatterplots",
    "href": "Lecture2b_ManyContVar.html#scatterplots",
    "title": "Lecture 2b - Exploring Many Categorical Variables",
    "section": "",
    "text": "Let’s start our exploration with scatterplots:\n\nDrawing scatterplots is one of the first things statisticians do when looking at data.\nA scatterplot displays two quantitative variables against each other data by plotting each data points values as \\((x,y)\\) coordinates.\nScatterplots can reveal structure not readily apparent from summaries, and are both easy to present and interpret.\nThe major role of scatterplots is in exposing associations between variables - not just linear associations, but any kind of association.\nScatterplots are also useful at identifying outliers and other distributional features.\nHowever, marginal distributions cannot always be easily seen from a scatterplot.\n\n\n\nFor example, let’s consider the Weight and Height of the 10,384 athletes competing in the London 2012 Olympics:\n\nlibrary(VGAMdata)\ndata(oly12)\n\nWe can draw a scatterplot of these variables using the plot function:\n\nplot(x=oly12$Height, y=oly12$Weight)\n\n\n\n\n\n\n\n\nNote the choice of which variable is drawn as the horizontal coordinate and which is the vertical.\nThe plot function accepts the usual arguments to customise the graphic.\n\nxlab, ylab, main - axis labels and main title\nxlim, ylim - axis ranges, a vector of length two\npch - changes the plot character, integer\ncol - changes the point colour, either specifying one colour for all points or one colour for each point\ncex - relative point size, defaults to 1\n\nIn particular, the plot character (pch) can be changed to something more solid to give a clearer picture.\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=20)\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships - associations between variables will manifest as trends (either linear or nonlinear) in a scatterplot. Here, we’re interested in the nature, direction, and strength of any discovered relationship\nCausal relationships - Great care should be taken to distinguish association from possible causation.\nOutliers or groups of outliers - Cases can be outliers in two dimensions without being outliers in the separate dimensions. Taller people are generally heavier, but a particularly heavy person of average height stands out more.\nClusters - Groups of cases occurring separately from the rest of the data, such as the iris species in Fisher’s iris data.\nGranularity - Values may line up in regular columns or rows indicating some form of rounding or grouping of the data has occurred before analysis.\nBarriers - Some combinations of values may be impossible. Age cannot be negative, and years of employment cannot be more than Age.\nGaps and holes - Some combinations of values may be theoretically possible, but do not occur in the data, e.g. very tall but very light people.\nConditional relationships - sometimes the relationship can change fundamentally given another variable, e.g. income vs age will look quite different for working age and retired people, and iris flowers look different for different species.\n\nSo, what do features do we see in this plot:\n\nThere is a fairly strong positive relationship - taller athletes are heavier. This makes rather obvious sense. Sometimes part of statistical exploration is to confirm common-sense inutuitions and to reassure ourselves that the data are correctly recorded (and that any pre-conceptions we have are correct!)\nSome outliers break this pattern - the single athlete with a weight over 200 is a rather heavy Judo player\nNote how some points arrange themselves into parallel vertical and horizontal lines - this is because the data values have been rounded, which forces the points onto a grid of regular values and leaving gaps in between.\nWe have 10384 athletes in the data, but there area far fewer visible points here - this is a problem of over plotting\n\n\n\n\nOverplotting is a problem in a scatterplot where the drawn data points overlap one another. This typically occurs when there are a large number of data points and/or a small number of unique values in the dataset.\nAs multiple stacked points look the same as a single point, this makes it difficult to identify areas of high density. In the scatterplot above, we cannot tell if there is one person with a weight over 200 or one hundred.\nPossible solutions include:\n\nUsing transparency - higher density is then evident by darker regions, where the degree of darkness is caused by the multiple overlapping points\nJittering - add random noise to the points to turn the stacks into point clouds\nUsing smaller points - only feasible for modestly sized problems.\n\nWe can apply transparency to the scatterplot of the Olympic athletes as follows:\n\nlibrary(scales)\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=20,\n    col=alpha('black',0.2)) ## this tells R to use 20% transparent black for each point\n\n\n\n\n\n\n\n\nNow the areas of high density in the main cloud of data stand out as darker, and the more unusual values fade out.\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe Old Faithful geyser in Yellowstone National Park, Wyoming, USA which a very regular pattern of eruption.\nConsider the duration of the eruptions and the waiting time until the next eruption.\n\nplot(y=geyser$waiting,x=geyser$duration, pch=16, xlab='duration',ylab='waiting')\n\n\n\n\n\n\n\n\nWhat can we see?\n\nPossible evidence of 2 or 3 clusters of points in the data\nOne cluster of shorter duration eruptions associated with a longer waiting time.\nA further cluster (or two) of longer eruptions with a short or long waiting time.\nClear signs of rounding producing a line of data points with a duration of exactly 4. Definitely suspicious!\n\n\n\n\n Download data: movies\nMost of the data sets we’ve seen so far are not very big. Consider instead the movies data set, which contained 24 variables and 58 788 cases corresponding to various attributes of different movies taken from IMDB. Let’s focus on two of the variables: the average IMDB user rating, and the number of users who rated that movie on IMDB.\n\nplot(x=movies$votes, y=movies$rating, ylab=\"Rating\", xlab=\"Votes\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nThere are no films with many votes and very low average rating.\nFor films with more than a small number of votes, the average rating increases with number of votes.\nNo film with lots of votes has an average rating close to the maximum possible. There is almost an invisible barrier preventing very high scores.\nA few films with a high number of votes (over 50 000) appear far from the rest of the data with unusually low ratings compared to others.\nFilms with a low number of votes may have any average ratin from the worst to the best.\nThe only films with high average ratings are films with relatively few votes.\n\nWe can extract a lot of information from a scatterplot!\n\n\n\nPatterns and trends observed within a scatterplot can often be explained by the action of other variables.\nIn particular, other categorical variables may induce different behaviours for each group (e.g. male and female).\n\nThere are a variety of ways to explore and compare possible groupings:\nIndicate the groups within the plot via colour or plot symbol\nSplit the data and draw separate plots for each potential group\n\n\n\nConsider the average values of fertility (number of children per mother) versus percentage of contraceptors among women of childbearing age in developing nations around 1990.\n\nlibrary(carData)\ndata(Robey)\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=20,\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\n\n\n\n\n\n\n\n\n\nWe find a rough straight line relationship for most of the points - suggests a negative linear association between fertility and contraceptor percentage\nStill a lot of spread about the rough line, i.e. the relationship is clear but far from perfect.\nPragmatically, this seems to agree with the common-sense view that fertility declines on average as the percentage of contraceptors in a country rise.\nSometimes part of the job of exploring the dat is confirming the data agrees with what might be considered obvious - finding the opposite effect in the data would be far more surprising!\n\nThe fertility data set also contains a third variable representing the region of the world, which we can indicate in colour.\n\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=20, col=Robey$region,\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\nlegend(x='topright',pch=20,lwd=NA, col=1:nlevels(Robey$region), legend=levels(Robey$region))\n\n\n\n\n\n\n\n\n\nThis doesn’t seem to affect the overall conclusion, in that the pattern seems to be the same, and linear, in each region.\nSometimes, not finding anything is a useful thing to learn!\n\nWe could have used a different symbol for each region, but this is generally less effective than colour:\n\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=as.numeric(Robey$region),\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\nlegend(x='topright',lwd=NA, pch=1:nlevels(Robey$region), legend=levels(Robey$region))\n\n\n\n\n\n\n\n\n\nColour creates more of a contrast, making it much easier to distinguish the groups\nIn general, stick to a single symbol - unless you must use black & white, or you have multiple different grouping variables!\n\n\n\n\n\nThe Olympic athletes data set also could be investigated by using colour to represent Gender. This shows an expected differentiation between the two:\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=16,\n     col=alpha(c('#000000','#ff0000'),0.2)[oly12$Sex])\nlegend(x='topleft', legend=levels(oly12$Sex), pch=16, col=c(1,2))\n\n\n\n\n\n\n\n\nColouring the 42 sport categories is less helpful.\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=16,\n     col=oly12$Sport)\n\n\n\n\n\n\n\n\nUnfortunately, with many categories the differences between 42 colours become too subtle to detect easily. Colour is only useful when indicating a small number of groups.\nIn this case, a better solution is to break the data up and draw separate mini scatterplots for each of the 42 sports. This is called a lattice, grid or trellis of plots:\n\n\n\n\n\n\n\n\n\n\nThough the plots are small, we can still see the key features.\nThe increasing trend is common across all sports.\nFor some sports, the relationship looks nonlinear.\nThe picture is less clear for Athletics, which groups a lot of very different events together.\nSome sports seem to have very few or no athletes - we’re missing data on 1346 of the athletes."
  },
  {
    "objectID": "Lecture2b_ManyContVar.html#correlation",
    "href": "Lecture2b_ManyContVar.html#correlation",
    "title": "Lecture 2b - Exploring Many Categorical Variables",
    "section": "",
    "text": "The standard measure of the strength of linear relationship between two variables is the correlation coefficient . Assuming that we have \\(n\\) pairs of observations \\((x_1,y_1)\\), …,\\((x_n,y_n)\\), the correlation coefficient is defined to be: [ r=_{i=1}^n()() ] In R, we can use the cor function to compute this.\nThe correlation can take any value between \\(-1\\) and \\(+1\\), inclusive, with positive values representing positive correlation (as one variable increases, so does the other), and negative values representing negative correlation (as one variable increases, the other decreases). A correlation of \\(0\\) means uncorrelated: no association (as one variable increases, the other doesn’t consistently increase or decrease).\nThus the value of \\(r\\) can range from \\(r=-1\\) (perfect negative correlation) through weaker negative correlation until \\(r=0\\) (no correlation) through weak positive correlation to \\(r=1\\) (perfect +ve correlation).\nThe correlation tells us two things:\n\nthe strength of association (strong, weak or zero);\nthe direction of association (negative or positive).\n\nFor the fertility data:\n\ncor(y=Robey$tfr, x=Robey$contraceptors)\n\n[1] -0.9203109\n\n\nThe value here is negative reflecting the negative aassociation - the increase in contraception corresponds to a reduction in numbers of children. The value itself is quite large (close to -1) which indicates a strong association - this is reflected by the fact the data are roughly organised along a straight line.\nDifficulties with correlation:\n\nCan be difficult to interpret numerically: a value of \\(r = 0.7\\) can mean different strengths of association for different numbers of pairs of points. Generally, the more points there are, the lower the correlation has to be to indicate association.\nStrong correlations (near \\(\\pm 1\\)) are quite rare, especially if the size of the distributions is large. Weak correlations (near 0) are common. Quite small correlations can indicate a linear association if the size of distributions is large, though it is difficult to set a hard and fast rule.\n\\(r\\) doesn’t tell us by how much a change in one variable affects change in the other - it just tells us the rough direction and degree of change. For that, you need a regression.\nThe correlation coefficient measures linear association, and so can be misleading when there is nonlinear association, or when their are outliers present.\nFor two separate scatter plots, the correlation coefficient might be numerically the same, but the \\(x,y\\) relationship between the variables may be very different.\n\nRecall, for example, Anscombe’s quartet of data points:\n\nlibrary(datasets)\ndata(anscombe)\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\n\nTheir correlations are almost the same (up to 2 decimal places), but the actual patterns of the points are quite different:\n\npar(mfrow=c(2,2),mai = c(0.3, 0.3, 0.3, 0.3))\nplot(x=anscombe$x1, y=anscombe$y1,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x2, y=anscombe$y2,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x3, y=anscombe$y3,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x4, y=anscombe$y4,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))"
  },
  {
    "objectID": "Lecture2b_ManyContVar.html#many-continuous-variables",
    "href": "Lecture2b_ManyContVar.html#many-continuous-variables",
    "title": "Lecture 2b - Exploring Many Categorical Variables",
    "section": "",
    "text": "Visualising large numbers of continuous variables has the potential to uncover enven more features, but also presents more challenges.\n\n\n\ndata(iris)\nboxplot(iris[,4:1])\n\n\n\n\n\n\n\n\nBoxplots are effective for getting an overview of major differences between variables - particularly in terms of their location (position vertically) and scale (length of the boxplot). However, boxplots are too simple to show whether a variable splits into different modes or groups. Boxplots are also fundamentally a 1-dimensional graphic - they cannot detect or display whether the variables are related.\n\npairs(iris[,4:1], pch=16)\n\n\n\n\n\n\n\n\nConversely, the scatterplot matrix exposes a great deal of the structure of the data, and makes relationships clear. Patterns, trends, and groups emerge quite easily and can be easily spotted by eye. But it is not as effective for comparing scales and locations - we still need the boxplots, even if they are limited!\n\n\n\nWhat if we need to include a third numerical variable into a scatter plot? One way to achieve this is a bubble plot, where we use the value of a third variable to control the size of the points we draw on a scatterplot:\n\nlibrary(plotly)\nplot_ly(iris, x=~Petal.Width, y=~Petal.Length, size=~Sepal.Width, color=~Species,\n    sizes = c(1,50), type='scatter', mode='markers', marker = list(opacity = 0.7, sizemode = \"diameter\"))\n\n\nHere we have positioned the points of the iris data according to the Petal measurements and used the size of the point to indicate the Sepal.Width. We can note that the orange points (versicolor) appear to have more small bubbles than the others, and the green points (setosa) look to have larger values.\nThis technique can be quite effective when the variable corresponding to point size corresponds to some measure of importance, scale, or size (such as population per country, number of samples in a survey, number of patients in a medical trial). While it can be effectively used in three-variable problems, for more variables than this we require other methods.\nDealing with more continuous variables will require scaling up a lot of the familiar techniques from earlier. Individual numerical summaries can still be computed, though it is even more difficult to easily compare. Some of the standard visualisations can be easily applied to many variables, such as histograms and box plots.\nOne particularly useful technique is to take scatterplots, and draw many of them in a matrix to effectively compare multiple variables at once. Scatterplot matrices are a matrix of scatterplots with each variable plotted against all of the others.\nLike the grid or trellis scatterplot we produce an array of plots, but now we plot all variables simultaneously!\nThese give excellent initial overviews of the relationship between continuous variables in data sets with relatively small numbers of variables.\n\n\n\nLet’s use a data set on Swiss banknotes to illustrate variations we can make to a scatterplot matrix.\nThe data are six measurements on each of 100 genuine and 100 forged Swiss banknotes. The idea is to see if we can distinguish the real notes from the fakes on the basis of the notes dimensions only:\n\nlibrary(car)\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:VGAM':\n\n    logit\n\ndata(bank,package=\"gclus\")\nscatterplotMatrix(bank[,-1], smooth=FALSE, regLine=FALSE, diagonal=TRUE, groups=bank$Status,\n                  col=c(5,7), pch=c(16,16))\n\n\n\n\n\n\n\n\n\nBlue=Genuine, Yellow=Fake.\nWe can use the diagonal elements to show histograms (or smoothed histograms)\nCan we see any variables for which the two groups are well separated? “Diagonal” looks like a good candidate. Looking at the (smoothed) histograms in the Diagonal panel, we can see the two distributions barely overlap indicating a useful variable for separating the groups.\nWe can see some assocations and relationships here “Left”/“Right” are positively correlated, but “Top”/“Diagonal” are negatively associated.\nThere also seem to be a few possible outliers, not all are forgeries!\n\n\n\n\nParallel coordinate plots (PCP) have become a popular tool for highly-multivariate data.\nRather than using perpendicular axes for pairs of variables and points for data, we draw all variables on parallel vertical axes and connect data with lines.\nThe variables are standardised, so that a common vertical axis makes sense.\n\npar(mfrow=c(1,2))\npairs(iris[,4:1],pch=16,col=c('#f8766d','#00ba38','#619cff')[iris$Species])\n\n\n\n\n\n\n\nlibrary(lattice)\nparallelplot(iris[,1:4], col=c('#f8766d','#00ba38','#619cff')[iris$Species], horizontal=FALSE)\n\n\n\n\n\n\n\n\nIn general:\n\nEach line represents one data point.\nThe height of each line above the variable label indicates the (relative) size of the value for that data point. The variables are transformed to a common scale to make comparison possible.\nAll the values for one observation are connected across the plot, so we can see how things change for individuals as we move from one variable to another. Note that this is sensitive to the ordering we choose for the variables in the plot – some orderings of the variables may give clearer pictures than others.\n\nFor these data:\n\nWe can see how the setosa species (red) is separated from the others on the Petal measurements in the PCP by the separation between the group of red lines and the rest.\nSetosa (red) is also generally smaller than the others, except on Sepal Width where it is larger than the other species.\nWe can also pick out outliers, for instance one setosa iris has a particularly small value of Sepal Width compared to all the others.\n\nUsing parallel coordinate plots:\n\nReading and interpreting a parallel coordinate plot (PCP) is a bit more difficult than a scatterplot, but can be just as effective for bigger problems.\nWhen we identify interesting features, we can then investigate them using more familiar graphics.\nA PCP gives a quick overview of the univariate distribution for each variable, and so we can identify skewness, outliers, gaps and concentrations.\nHowever, for pairwise properties and associations then it’s best to draw a scatterplot.\n\n\n\nThe Guardian newspaper in the UK publishes a ranking of British universities each year and it reported these data in May, 2012 as a guide for 2013. 120 Universities are ranked using a combination of 8 criteria, combined into an ‘Average Teaching Score’ used to form the ranking.\n\nlibrary(GDAdata)\ndata(uniranks)\n## a little bit of data tidying\nnames(uniranks)[c(5,6,8,10,11,13)] &lt;- c('AvTeach','NSSTeach','SpendPerSt','Careers',\n                                        'VAddScore','NSSFeedb')\nuniranks1 &lt;- within(uniranks, StaffStu &lt;- 1/(StudentStaffRatio))\n## draw the scatterplot matrix\npairs(uniranks1[,c(5:8,10:14)])\n\n\n\n\n\n\n\n\nWe can see some obvious patterns and dependencies here. What about the parallel coordinate plot?\n\nparallelplot(uniranks1[,c(5:8,10:14)], col='black',  horizontal=FALSE)\n\n\n\n\n\n\n\n\nCan we learn anything from this crazy mess of spaghetti?\nPCPs are most effective if we colour the lines to represent subgroups of the data. We can colour the Russell Group universities, which unsurprisingly are usually at the top (except on NSSFeedback!)\n\n## create a new variable to represent the group we want\nuniranks2 &lt;- within(uniranks1,\n                    Rus &lt;- factor(ifelse(UniGroup==\"Russell\", \"Russell\", \"not\")))\n\nparallelplot(uniranks2[,c(5:8,10:14)], col=c(\"#2297E6\",\"#DF536B\")[uniranks2$Rus],  horizontal=FALSE)\n\n\n\n\n\n\n\n\nUsing colour helps us to diffentiate the lines from the different data points more clearly. Now we can start to explore for features and patterns:\n\nAvTeach is the overall ranking, so notice how high/low values here are connected to high/low values elsewhere.\nThere are some exceptions - the 3rd ranked university on AvTeach has a surprisingly low NSSTeaching score (note the rapidly descending red line)\nNotice how the data ‘pinch’ at certain values on NSS Teach and NSSOverall leaving gaps - this suggests a granularity effect due to limited options of values on the NSS survey scores, e.g. integer scores out of 10. This creates heaping and gaps in the data values as the data is really ordinal.\nAlso, notice how most of the lines are moving upwards when connecting these NSSTeach and NSSOverall - this would suggest a positive correlation.\nThe extremes of these two NSS scores are rarely observed, and they also correspond strongly to extremes on the overall score and ranking.\nThe heavy concentration on low values on EntryTariff (entry requirements) and StaffStu (staff:student ratio) suggests strong skewness in the distributions\n\nA scatterplot matrix corroborates most of these features, though we’re probably near the limit of what we can read and extract from such a plot!\n\npairs(uniranks2[,c(5:8,10:14)],col=c(\"#2297E6\",\"#DF536B\")[uniranks2$Rus], pch=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot matrix is easily overloaded with many variables and cases.\nCorrelations provide a useful numerical summary, and the corrplot is a good way to visualise that information.\nNote that the standard correlation coefficient is not a good measure of non-linear relationships or suitable for categorical variables! Alternative correlations exist for such variables, but these are not computed by default.\n\n\nround(cor(Boston),2) ## round to 2dp\n\n         crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio\ncrim     1.00 -0.20  0.41 -0.06  0.42 -0.22  0.35 -0.38  0.63  0.58    0.29\nzn      -0.20  1.00 -0.53 -0.04 -0.52  0.31 -0.57  0.66 -0.31 -0.31   -0.39\nindus    0.41 -0.53  1.00  0.06  0.76 -0.39  0.64 -0.71  0.60  0.72    0.38\nchas    -0.06 -0.04  0.06  1.00  0.09  0.09  0.09 -0.10 -0.01 -0.04   -0.12\nnox      0.42 -0.52  0.76  0.09  1.00 -0.30  0.73 -0.77  0.61  0.67    0.19\nrm      -0.22  0.31 -0.39  0.09 -0.30  1.00 -0.24  0.21 -0.21 -0.29   -0.36\nage      0.35 -0.57  0.64  0.09  0.73 -0.24  1.00 -0.75  0.46  0.51    0.26\ndis     -0.38  0.66 -0.71 -0.10 -0.77  0.21 -0.75  1.00 -0.49 -0.53   -0.23\nrad      0.63 -0.31  0.60 -0.01  0.61 -0.21  0.46 -0.49  1.00  0.91    0.46\ntax      0.58 -0.31  0.72 -0.04  0.67 -0.29  0.51 -0.53  0.91  1.00    0.46\nptratio  0.29 -0.39  0.38 -0.12  0.19 -0.36  0.26 -0.23  0.46  0.46    1.00\nblack   -0.39  0.18 -0.36  0.05 -0.38  0.13 -0.27  0.29 -0.44 -0.44   -0.18\nlstat    0.46 -0.41  0.60 -0.05  0.59 -0.61  0.60 -0.50  0.49  0.54    0.37\nmedv    -0.39  0.36 -0.48  0.18 -0.43  0.70 -0.38  0.25 -0.38 -0.47   -0.51\n        black lstat  medv\ncrim    -0.39  0.46 -0.39\nzn       0.18 -0.41  0.36\nindus   -0.36  0.60 -0.48\nchas     0.05 -0.05  0.18\nnox     -0.38  0.59 -0.43\nrm       0.13 -0.61  0.70\nage     -0.27  0.60 -0.38\ndis      0.29 -0.50  0.25\nrad     -0.44  0.49 -0.38\ntax     -0.44  0.54 -0.47\nptratio -0.18  0.37 -0.51\nblack    1.00 -0.37  0.33\nlstat   -0.37  1.00 -0.74\nmedv     0.33 -0.74  1.00\n\n\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncorrplot(cor(Boston))\n\n\n\n\n\n\n\n\nThere are clearly strong associations between age and dis, indus and dis, and lstat and medv that may be worth studying closer. chas appears almost uncorrelated to all the other variables – but remember chas was a binary variable, and so the correlation coefficient is meaningless here and we should not draw conclusions from this feature!"
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html",
    "href": "Lecture2a_ManyCatVar.html",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "Categorical data poses a number of problems when we have multiple variables. Suppose we have \\(J\\) categorical variables \\(X_1,\\dots,X_J\\), and each categorical variable has \\(c_j\\) possible categories.\nFor example, \\(X_1\\) could be Gender with levels Male/Female and \\(c_1=2\\); then \\(X_2\\) could be Eye Colour with levels Blue/Green/Brown and \\(c_2=3\\); \\(X_3\\) could be Hair colour with levels Brown/Blond/Black/Red/Grey/White and \\(c_3=6\\), etc.\nEach observed data point is then one combination of the possible categories from each variable - Male + Green Eyes + Red hair, or Female + Brown Eyes + Black hair. In total, there are \\(C^*=c_1\\times c_2\\times \\dots\\times c_J= \\prod_j c_j\\) possible combinations of categories! In our example, this would be \\(2\\times3\\times6=36\\). As the number of variables \\(J\\) and number of categories for the variables \\(c_j\\) get bigger, then \\(C^*\\) can grow very large very quickly and can rapidly become challenging to deal with. It can quickly become possible for there to be more possible combinations of categories than you have data observations - a problem known as sparsity.\n\n\nMultivariate categorical data can be summarised by the counts of the number of observations in each possible combination of levels of the categorical variables. This collection of counts forms a contingency table. In general, the contingency table can be represented as\n\na \\(J\\)-dimensional array,\ncontaining a total of \\(C^*\\) cells\neach containing the observed count of each possible combination of categories.\n\nSparsity manifests as many of the table entries being zero.\nFor example, the Alligator data set in the vcdExtra package contains data on 219 observations from a study of the primary food choices of alligators in four Florida lakes. The data set has \\(J=4\\) variables, all categorical:\n\nSex - male, or female\nSize - large (&gt;2.3m) small (&lt;=2.3m)\nLake - one of four possible lakes\nFood - primary food choice: bird, fish, invertebrate, reptile, other\n\nThis seems like a relatively modest data set - how many different combinations of categorical variables are there? The answer is \\(2\\times2\\times4\\times 5=80\\).\n\n\n\nTo explore multivariate categorical data, we start with some low-dimensional plots, for 2 or 3 variables. The most effective visualisations are:\n\nMosaicplots use area to indicate the counts within categories and combinations of categories.\nGrids of barplots can also be effective to visualise how a distribution can change across categories.\nGrouped barplots or stacked barplots\n\nThese plots have the advantage of highlighting some important association features. One shall look at the following questions:\nWhich categories appear most/least often?\nDo the counts and their distributions differ across subgroups?\nIs there evidence of dependence or independence between categories?\n\n\n\n\ntitanic &lt;- data.frame(Titanic)\n\nData on the 2201 people on board the Titanic at the time of its sinking:\n\nClass - 1st, 2nd, 3rd, or crew\nSex - male, or female\nAge - child, or adult\nSurvived - survived or died\n\nThere are 32 combinations of factors here. Interest in these data centres on whether factors like Class or Sex affected survival. We’ll come to this soon, but first let’s just focus on the individual variables.\nThe raw data for categorical variables are not terribly easy to interpret:\n\nprint(titanic[1:10,])\n\n   Class    Sex   Age Survived Freq\n1    1st   Male Child       No    0\n2    2nd   Male Child       No    0\n3    3rd   Male Child       No   35\n4   Crew   Male Child       No    0\n5    1st Female Child       No    0\n6    2nd Female Child       No    0\n7    3rd Female Child       No   17\n8   Crew Female Child       No    0\n9    1st   Male Adult       No  118\n10   2nd   Male Adult       No  154\n\n\nSimple barplots of the individual factors show some general features of the marginal distributions and are a good place to start:\n\n\n\n\n\n\n\n\n\nHere we see that:\n\nTwice as many people died than survived\nMost people were crew rather than passengers\nConsiderably more people onboard were male than female (crew were mostly male)\nFar more adults onboard than children\n\n\n\nIt can be helpful to focus on a single response variable of primary interest to investigate in relation to the others. In the case of the Titanic data, the Survived variable is most of interest. We want to explore how the distribution of the number of survivors is affected by the other variables.\nA simple variation of the barplot is to break apart each bar into the pieces according to combinations with other variables. This gives a stacked barplot. For example, we can decompose the number of survivors by Class:\n\nbarplot(xtabs(Freq~Survived+Class, titanic),  beside=FALSE, col=c('#D9344A','green3'))\n\n\n\n\n\n\n\n\nNow we can see the variations in the nunmber of survivors within each of the bars. This can be helpful to compare the proportions of the sub-groups of Survivors within each bar. For instance, we see that a greater share of passengers in 1st class survived, compared to the rest. However, as the heights of each bar differ it can be difficult to directly compare the numbers in the subgroups for the different bars.\nAlternatively, we can group the bars side-by-side rather than stacking them. This can help if we want to compare the total amounts across classes, rather than proportions.\n\nbarplot(xtabs(Freq~Survived+Class, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\n\n\n\n\n\n\n\n\nIt’s now far clearer that more 1st class passengers survived than did not, and this situation was dramatically reversed for the other passengers. Only a small proportion of the Crew and those in 3rd class surivived.\nWe can repeat this process to look at the effects of the other variables:\n\npar(mfrow=c(2,2))\nbarplot(xtabs(Freq~Survived+Sex, titanic),  beside=FALSE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Sex, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Age, titanic),  beside=FALSE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Age, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\n\n\n\n\n\n\n\n\nThese plots show that a greater proportion of Female passengers survived than Male, but there were far fewer Female passengers overall. For Age, the number of Children appears very (surprisingly?) small and seem to be roughly equally likely to survive or not. The majority of Adults did not survive.\nFeatures of composite barplots:\n\nStacked barplots can be useful to indicate something about the relative composition of each bar in terms of any subgroups within each bar\nGrouped barplots are more effective to compare sizes of subgroups across different categories and bars\nWhen the totals within the bars differ substantially, they become difficult to read\nTo adequately compare proportions, we would have to rescale each bar to have the same overall height.\n\n\n\n\nA mosaic plot is a modification of a stacked barplot which rescales the bars to be the same height so we can focus on the proportions of the subgroups. It also allows the width of the bar to vary. For example, the mosaic plot of Survived by Sex looks like this:\n\nmosaicplot(xtabs(Freq~Sex+Survived, titanic), col=c('#D9344A','#1DB100'), main='')\n\n\n\n\n\n\n\n\nBefore we try and interpret the plot, it is helpful to understand a little about how it was constructed. The general algorithm is as follows:\n\nBegin with an empty rectangle to represent the data set\nTake the first variable, and divide the horizontal axis into sections proportional to the sizes of its categories.\nTake each column, and divide it into rectangles vertically according to the size of the second variable categories.\nContinue as needed, splitting each tile alternately horizontally and vertically\n\nSo, the plot we have generated has columns with widths proportional to the numbers of the two Sexes of passengers. As there were more Male passengers than Female, the Male column is wider. The columns are then split into tiles according to the proportion of each Sex that Survived or did not, with the Green area of each representing the proportion which Survived. If the two Sexes had the same rate of survival, then the two columns would split into similarly sized rows. What we see here is a substantial imbalance in the heights of the rows which is indicative of an association. Female survival rates were much better than the Male survival rate, so there’s an association between Sex and Survived.\nIn general, this functions just like a stacked barplot where we stretch each bar to have the same height. The main advantage of these plots comes when we have more than two variables to explore.\nDid Class affect Survival?\n\nmosaicplot(xtabs(Freq~Class+Survived, titanic), col=c('#D9344A','#1DB100'), main='')\n\n\n\n\n\n\n\n\nAs we said above, if there was no effect of Class then we would expect the four bars would divide equally for each Class as the same proportion of passengers would have survived or not irrespective of Class. Clearly, the bars do not divide equally and again we have signs of an association. We can see that 1st class passengers had far better survival rates, followed by 2nd class, and 3rd class passengers didn’t fare much better than the Crew.\n\n\n\n\n\nlibrary(vcd)\n\nLoading required package: grid\n\ndata(Arthritis)\n\nThe arthritis data contains the results from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. The data set contains observations on 84 patients with variables:\n\nID - patient ID.\nTreatment - factor indicating treatment (Placebo, Treated).\nSex - factor indicating sex (Female, Male).\nAge - age of patient.\nImproved - ordered factor indicating treatment outcome (None, Some, Marked).\n\n\nhead(Arthritis)\n\n  ID Treatment  Sex Age Improved\n1 57   Treated Male  27     Some\n2 46   Treated Male  29     None\n3 77   Treated Male  30     None\n4 17   Treated Male  32   Marked\n5 36   Treated Male  46   Marked\n6 23   Treated Male  58   Marked\n\n\nTreatment, Sex, and Improved are all nominal categorical variables. Improved is ordinal, since the category levels can be placed in a meaningful order. The question is whether the patient Improvement depends on Treatment and/or Sex.\nA mosaic plot of a single variable is basically a simple stacked barplot with only one bar. Looking at the patient improvement only gives:\n\nmosaicplot(xtabs(~Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nSo, no improvement is most common, but of the two groups which do have an improvement the improvement is more likely to be Marked than Some. Of course, we could just get this from a simple summary table:\n\nxtabs(~Improved, Arthritis)\n\nImproved\n  None   Some Marked \n    42     14     28 \n\n\nWe can now start splitting things up by Treatment type. The data used to construct the plot is the 2-way contingency table, obtained by summarising the data:\n\nxtabs(~Treatment+Improved, Arthritis)\n\n         Improved\nTreatment None Some Marked\n  Placebo   29    7      7\n  Treated   13    7     21\n\n\n\nmosaicplot(xtabs(~Treatment+Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nThere are a number of things to note here:\n\nThe number of patients in the Placebo and Treated group appears equal, as the two columns are equal in size\nThe main outcome from the Placebo group is No Improvement - perhaps no surprise! But some patients improve anyway.\nIn the Treatment group, the most likely outcome is a Marked Improvement. An Improvement of Some is the least common outcome.\nClearly there is an association between Treatment and Improvement.\n\nFor comparison, a mosaic plot with no association between Treatment and Improvement would look like this:\n\n\n\n\n\n\n\n\n\nAs we can see, in the event of no association the bars decompose into regular tiles. A loose test of this is whether we can draw a straight line from one side of the plot to the other without going through any of the tiles.\nThe order in which we introduce the variables into the mosaic plot also affects the plot we draw. Here, we have split on Treatment first and then on Improved. We could do this the other way around:\n\nmosaicplot(xtabs(~Improved+Treatment, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nNow we split first by Improved, which creates three columns that are then each split into Treatment type. This plot is most useful for showing how Improvement types decompose into Treatment groups, which is less helpful! Usually, we would split on the dependent or response variable last.\n\n\nWith more than two variables, we can still apply the same techniques to compare proportions, but it leads to two slightly different visualisations\n\nthe mosaicplot plot - for more than two variables draws the plots in a (nested) grid\nfor small numbers of variables, we can look at how barplots for different levels of other variables\nthe doubledecker plot - draws a row of simple mosaic plots to compare the different levels of a third (or fourth, …) variable\n\nIf we were to introduce Sex as a third variable to the mosaic plot of the Arthritis data, we will sub-divide each of the four tiles in the plots above into Male and Female halves.\n\nmosaicplot(xtabs(~Treatment+Sex+Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nNow the data are first split into columns by Improved, then each column split into rows by Treatment, and now further we split these tiles into smaller columns by Sex. This is clearly getting more complicated, but the same ideas apply. Substantial differences in sizes of tiles would suggest something is potentially going on. Possible observations we could make:\n\nThe Female tiles (red) are usually wider than the Male tiles (green) - we have overall more Female patients\nFemale patients generally seem to improve more than Males, regardless of treatment\nTreatment appears to increase the proportion of Improved patients\nFemale patients appear to improve more than Male.\n\n\n\n\nA doubledecker plot is a particular type of mosaic plot that splits all of the tiles vertically, except for the last one. The doubledecker plot is a lot like a sequence of stacked barplots for combinations of the categorical variables.\n\nlibrary(vcd)\ndoubledecker(Improved~Treatment+Sex,data=Arthritis)\n\n\n\n\n\n\n\n\nWe interpret this plot in much the same way. Here the columns are divided into the Treatment groups first, then each Treatment group divided into the two Sexes. This creates four columns, that are split into proportions according to Improved. So, we can observe similar features to before:\n\nThe Female columns are wider than the Male, suggesting more Female patients\nThe Marked improvement tiles in the Female columns are larger than those in the Male, suggesting Female patients response better\nThere was nobody with the combination Male + Placebo + Some Improvement. This is denoted by the hollow circle where this tile should be.\n\nIf we have a particular response variable in mind, the double-decker plot is often more useful than the general mosaic as we can split the independent variables (Sex, Treatment) into columns, and split the columns by the dependent variable (Improved). Overall, we can identify the same features from both types of plots, but the information is presented differently.\n\n\n\nLet’s return to the Titanic data and explore whether Survived depends on combinations of Sex or Class. In fact, since the scale of this problem is relatively modest. we can actually visualise the data as a matrix of barplots.\n\n\n\n\n\n\n\n\n\nWhile they show the shape of the distribution, making detailed comparisons is not terribly easy.\nHow did the combination of Class and Sex affect Survival? We can incorporate more variables into the doubledecker plot which will highlight differences in proportions rather than counts.\nThe columns are now grouped by combinations of Class & Sex.\n\n\n\n\n\n\n\n\n\n\nFemale survival was still higher overall, though declined rapidly with Class.\nAlmost all Female 1st class passengers survived, though those in 3rd class were not so fortunate\nMale survival rates don’t decline so much with Class, and Males in 2nd class have the lowest survival.\n\nAn alternative presentation of the same information is a mosaic plot:\n\n\n\n\n\n\n\n\n\n\nA mosaic plot show the data in a grid, rather than row.\nThe doubledecker plot uses a fixed height for its bars, but the mosaic plot varies the height and width of the panels according to the proportions of the factor variable.\nThis can make it a bit easier to distinguish the relative sizes of the combinations.\n\nThe mosaic plot algorithm is sensitive to the ordering of variables, so changing this will radically affect the plot drawn:\n\n\n\n\n\n\n\n\n\nHowever all of these methods start to struggle with more than a few variables. Unfortunately, too many combinations make the plots difficult to read and introduce a lot of ‘0’ counts into the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe visualisation of Titanic data provided above suggests that there is some association here to explore. The rigorous way to assess association is through \\(\\chi^2\\) tests. Here’s how they work.\n\n\nFirst, we need to set up a general 2-way contingency table problem. Suppose that:\n\nWe have observed \\(n\\) individuals\nWe have recorded that value of two categorical variables for each:\n\nVariable 1, which has \\(C\\) distinct levels\nVariable 2, which has \\(R\\) distinct levels\n\nThe number of individuals who are observed in level \\(i\\) for variable 1 and level \\(j\\) for variable 2 is called \\(o_{ij}\\).\n\nWe can then summarise this in the 2-way table below, where the levels of variable 1 are the rows of the table, and the levels of variable 2 are the columns:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVar 1\\ Var 2\n1\n2\n…\n\\(j\\)\n…\n\\(C\\)\nTotal\n\n\n\n\n1\n\\(o_{11}\\)\n\\(o_{12}\\)\n…\n\\(o_{1j}\\)\n…\n\\(o_{1C}\\)\n\\(r_1\\)\n\n\n2\n\\(o_{21}\\)\n\\(o_{22}\\)\n…\n\\(o_{2j}\\)\n…\n\\(o_{2C}\\)\n\\(r_2\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\ni\n\\(o_{i1}\\)\n\\(o_{i2}\\)\n…\n\\(o_{ij}\\)\n…\n\\(o_{iC}\\)\n\\(r_i\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nR\n\\(o_{R1}\\)\n\\(o_{R2}\\)\n…\n\\(o_{Rj}\\)\n…\n\\(o_{RC}\\)\n\\(r_R\\)\n\n\nTotal\n\\(c_1\\)\n\\(c_2\\)\n…\n\\(c_j\\)\n…\n\\(c_C\\)\n\\(n\\)\n\n\n\nHere we have also introduce notation for the row and columns sums:\n\n\\(r_i\\) is the sum of counts in the \\(i\\)th row. This corresponds to the number of observations of level \\(i\\) of Variable 1. The collection of row counts \\(r_1,\\dots,r_R\\) corresponds to the counts for all the different levels of Variable 1.\n\\(c_j\\) is the sum of counts in the \\(j\\)th row. This corresponds to the number of observations of level \\(j\\) of Variable 2. The collection of row counts \\(c_1,\\dots,c_C\\) corresponds to the counts for all the different levels of Variable 2.\n\n\n\nWe can use the xtabs function from Lecture 1 to construct these contingency tables from data. There are two different usages, depending on whether the data has been pre-summarised as counts (like the Titanic data), or if the data is not summarised and each row corresponds to a single observation.\nIf the data is summarised already, and contains the totals in column called counts, then the summary of counts for the levels of variable1 is obtained by:\n\nxtabs(counts~variable1, data=dataset)\n\nFor example, to summarise survival:\n\nxtabs(Freq~Survived,data=titanic)\n\nSurvived\n  No  Yes \n1490  711 \n\n\nFor 2-way tables, we simply add another variable to the right of the ~ symbol:\n\nxtabs(counts~variable1+variable2, data=dataset)\n\nFor example, to summarise combinations of survival with class of passenger:\n\nxtabs(Freq~Survived+Class,data=titanic)\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  122 167 528  673\n     Yes 203 118 178  212\n\n\n\n\n\nWhen we have \\(n\\) categorical variables, we could create an \\(n\\)-way contingency table. The table would summarise the counts of every possible combination of every possible level of the \\(n\\) variables. This obviously gets more complicated with larger \\(n\\), but we’ll come back to this later.\nFor instance, we could summarise the combinations of Titanic passenger survival, passenger class and passenger sex:\n\nxtabs(Freq~Survived+Class+Sex, data=titanic)\n\n, , Sex = Male\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  118 154 422  670\n     Yes  62  25  88  192\n\n, , Sex = Female\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No    4  13 106    3\n     Yes 141  93  90   20\n\n\nNow we should have a 3-dimensional table with \\(2\\times 4\\times 2\\) cells! But R can only print at most 2-dimensional tables. Instead, it has given us two separate 2-way table for Survived+Class, one for each of Sex=Male (top) and Sex=Female (bottom). We can note here that Female passengers and crew had better survivale than their male counterparts (with a notable exception being those in 3rd class).\n\n\n\n\nThe key idea behind using independence as the property to verify a \\(\\chi^2\\)-test is that if two variables \\(X\\) and \\(Y\\) are independent, then we can write \\[P[X=x,Y=y]=P[X=x]P[Y=y],\\] or equivalently in terms of conditional probabilitiy: \\[P[X=x | Y=y]=P[X=x].\\] The second version tells us that even if I were to tell you the value of Y, that would not cause you to change your probability distribution for \\(X\\). In other words, knowing \\(Y\\) has no influence on how we thinkg \\(X\\) will behave - in other words, the two variables are completely unrelated.\nThe \\(\\chi^2\\)-test is a relatively simple test for assessing independence. As a running example, let’s continue with the Titanic data and investigate possible relationships between Survived and Class. The idea behind our test is as follows:\n\nUsing the data, we can estimate the probabilities of the different levels of Survived (\\(p_i\\)) and Class (\\(p_j\\)) separately using the proportions of the data observed in each category.\nUnder a hypothesis of independence, I can use the first equation above to tell me that the probability of any combination of Survived and Class is just the product of individual probabilities from the previous step, \\(p_{ij}=p_i p_j\\). Applying this to all combinations of categories gives me the probability for every possible combination of the two variables, \\(p_{ij}\\).\nThe expected number of observations in each combination of categories is obtained by multiplying the probabilities from step 2 by the sample size \\(n\\). This gives the expected counts for every combination, \\(E_{ij}=np_{ij}\\).\nWe can then compare our expected counts to the observed counts in the data, and construct a test statistic.\n\nLet’s see how this goes with our data:\nObserved counts: We observe the following counts of the different categories of Survived and Class\n\n\n\n\n\nNo\nYes\n\n\n\n\n1490\n711\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\n325\n285\n706\n885\n\n\n\n\n\nObserved proportions: Dividing the observed counts by the sample size (\\(n=2201\\)) gives the proportions of the data observed in each category:\n\n\n\n\n\nNo\nYes\n\n\n\n\n0.676965\n0.323035\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\n0.1476602\n0.1294866\n0.3207633\n0.40209\n\n\n\n\n\nUnsuprisingly, the more common categories are now associated with larger proportions.\nProbabilities of combinations under independence We assume the null hypothesis to be \\(H_0\\): survival is independent from class. Then we can say, for example:\n\\[P[Survived=Yes,Class=1st] = P[Survived=Yes]\\times P[Class=1st],\\]\nand the same for all the other combinations. By multiplying our probabilities from the previous step, we can make the following probability table for the joint distribution of Survived and Class under our null hypothesis of independence:\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n0.0999608\n0.0876579\n0.2171455\n0.2722008\n\n\nYes\n0.0476994\n0.0418287\n0.1036178\n0.1298891\n\n\n\n\n\nExpected counts under independence Multiplying these probabilities by the number of passengers will give us the expected number of passengers (\\(E\\)) in each group under independence:\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n220.0136\n192.93503\n477.9373\n599.114\n\n\nYes\n104.9864\n92.06497\n228.0627\n285.886\n\n\n\n\n\n\n\n\nWe can now compare these expected counts with the ones we actually observed. If we find big differences, then we would conclude our hypothesis of independence must be wrong and that there is evidence of some form of relationship.\nBefore we do that, however, we’ll just set this up mathematically so we can obtain a formula for our test statistic.\nFor the Titanic data problem, we can construct the test statistic using the method above and get a test statistic of \\(X^2=190.46\\) (check!). Comparing to the \\(\\chi^2\\) distribution - this has a \\(p\\)-value of approximately 0! So, we strongly reject the hypothesis of independence and would do so at any of the usual levels of significance. Survival of the Titanic disaser and passenger class appear to be related!\nHaving discovered an interesting relationship, a good question to ask now is: why? Why did we reject this hypothesis? What about the data seemed to be at odds with the variables being independent?\nTo answer this, we look at the Pearson residuals - the components inside the double sum in our test statistic. For each combination of categories, we get a value of\\(\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\). We can inspect those values and see what we find. In particular, large values of the residuals indicate substantial departures from the null hypothesis. The sign of the residuals indicates whether there are \\(\\color{red}{\\text{fewer}}\\) (&lt;0), or \\(\\color{red}{\\text{more}}\\) (&gt;0) values than expected under independence.\n\n\n\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n\\(\\color{blue}{\\text{-6.608}}\\)\n\\(\\color{blue}{\\text{-1.867}}\\)\n\\(\\color{red}{\\text{2.290}}\\)\n\\(\\color{red}{\\text{3.019}}\\)\n\n\nYes\n\\(\\color{red}{\\text{9.566}}\\)\n\\(\\color{red}{\\text{2.703}}\\)\n\\(\\color{blue}{\\text{-3.315}}\\)\n\\(\\color{blue}{\\text{-4.370}}\\)\n\n\n\nHere we notice the very large values in the 1st class column - we see far \\(\\color{red}{\\text{more}}\\) survivors than expected and far \\(\\color{red}{\\text{fewer}}\\) fatalities. The pattern appears to be reversed for those in 3rd class and the ship’s crew. Clearly, first class passengers had far better outcomes than would be expected if the chances were equal for everyone on board.\nFinally, there’s no real need to do this entire calculation by hand as R easily perform an independence test for a contingency table. All we need to supply is the table of observed values obtained from xtabs:\n\nchisq.test( xtabs(Freq~Survived+Class,data=titanic) )\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(Freq ~ Survived + Class, data = titanic)\nX-squared = 190.4, df = 3, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#contingency-tables",
    "href": "Lecture2a_ManyCatVar.html#contingency-tables",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "Multivariate categorical data can be summarised by the counts of the number of observations in each possible combination of levels of the categorical variables. This collection of counts forms a contingency table. In general, the contingency table can be represented as\n\na \\(J\\)-dimensional array,\ncontaining a total of \\(C^*\\) cells\neach containing the observed count of each possible combination of categories.\n\nSparsity manifests as many of the table entries being zero.\nFor example, the Alligator data set in the vcdExtra package contains data on 219 observations from a study of the primary food choices of alligators in four Florida lakes. The data set has \\(J=4\\) variables, all categorical:\n\nSex - male, or female\nSize - large (&gt;2.3m) small (&lt;=2.3m)\nLake - one of four possible lakes\nFood - primary food choice: bird, fish, invertebrate, reptile, other\n\nThis seems like a relatively modest data set - how many different combinations of categorical variables are there? The answer is \\(2\\times2\\times4\\times 5=80\\)."
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#visualising-many-categorical-variables",
    "href": "Lecture2a_ManyCatVar.html#visualising-many-categorical-variables",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "To explore multivariate categorical data, we start with some low-dimensional plots, for 2 or 3 variables. The most effective visualisations are:\n\nMosaicplots use area to indicate the counts within categories and combinations of categories.\nGrids of barplots can also be effective to visualise how a distribution can change across categories.\nGrouped barplots or stacked barplots\n\nThese plots have the advantage of highlighting some important association features. One shall look at the following questions:\nWhich categories appear most/least often?\nDo the counts and their distributions differ across subgroups?\nIs there evidence of dependence or independence between categories?"
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#example-sinking-of-the-titanic",
    "href": "Lecture2a_ManyCatVar.html#example-sinking-of-the-titanic",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "titanic &lt;- data.frame(Titanic)\n\nData on the 2201 people on board the Titanic at the time of its sinking:\n\nClass - 1st, 2nd, 3rd, or crew\nSex - male, or female\nAge - child, or adult\nSurvived - survived or died\n\nThere are 32 combinations of factors here. Interest in these data centres on whether factors like Class or Sex affected survival. We’ll come to this soon, but first let’s just focus on the individual variables.\nThe raw data for categorical variables are not terribly easy to interpret:\n\nprint(titanic[1:10,])\n\n   Class    Sex   Age Survived Freq\n1    1st   Male Child       No    0\n2    2nd   Male Child       No    0\n3    3rd   Male Child       No   35\n4   Crew   Male Child       No    0\n5    1st Female Child       No    0\n6    2nd Female Child       No    0\n7    3rd Female Child       No   17\n8   Crew Female Child       No    0\n9    1st   Male Adult       No  118\n10   2nd   Male Adult       No  154\n\n\nSimple barplots of the individual factors show some general features of the marginal distributions and are a good place to start:\n\n\n\n\n\n\n\n\n\nHere we see that:\n\nTwice as many people died than survived\nMost people were crew rather than passengers\nConsiderably more people onboard were male than female (crew were mostly male)\nFar more adults onboard than children\n\n\n\nIt can be helpful to focus on a single response variable of primary interest to investigate in relation to the others. In the case of the Titanic data, the Survived variable is most of interest. We want to explore how the distribution of the number of survivors is affected by the other variables.\nA simple variation of the barplot is to break apart each bar into the pieces according to combinations with other variables. This gives a stacked barplot. For example, we can decompose the number of survivors by Class:\n\nbarplot(xtabs(Freq~Survived+Class, titanic),  beside=FALSE, col=c('#D9344A','green3'))\n\n\n\n\n\n\n\n\nNow we can see the variations in the nunmber of survivors within each of the bars. This can be helpful to compare the proportions of the sub-groups of Survivors within each bar. For instance, we see that a greater share of passengers in 1st class survived, compared to the rest. However, as the heights of each bar differ it can be difficult to directly compare the numbers in the subgroups for the different bars.\nAlternatively, we can group the bars side-by-side rather than stacking them. This can help if we want to compare the total amounts across classes, rather than proportions.\n\nbarplot(xtabs(Freq~Survived+Class, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\n\n\n\n\n\n\n\n\nIt’s now far clearer that more 1st class passengers survived than did not, and this situation was dramatically reversed for the other passengers. Only a small proportion of the Crew and those in 3rd class surivived.\nWe can repeat this process to look at the effects of the other variables:\n\npar(mfrow=c(2,2))\nbarplot(xtabs(Freq~Survived+Sex, titanic),  beside=FALSE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Sex, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Age, titanic),  beside=FALSE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Age, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\n\n\n\n\n\n\n\n\nThese plots show that a greater proportion of Female passengers survived than Male, but there were far fewer Female passengers overall. For Age, the number of Children appears very (surprisingly?) small and seem to be roughly equally likely to survive or not. The majority of Adults did not survive.\nFeatures of composite barplots:\n\nStacked barplots can be useful to indicate something about the relative composition of each bar in terms of any subgroups within each bar\nGrouped barplots are more effective to compare sizes of subgroups across different categories and bars\nWhen the totals within the bars differ substantially, they become difficult to read\nTo adequately compare proportions, we would have to rescale each bar to have the same overall height.\n\n\n\n\nA mosaic plot is a modification of a stacked barplot which rescales the bars to be the same height so we can focus on the proportions of the subgroups. It also allows the width of the bar to vary. For example, the mosaic plot of Survived by Sex looks like this:\n\nmosaicplot(xtabs(Freq~Sex+Survived, titanic), col=c('#D9344A','#1DB100'), main='')\n\n\n\n\n\n\n\n\nBefore we try and interpret the plot, it is helpful to understand a little about how it was constructed. The general algorithm is as follows:\n\nBegin with an empty rectangle to represent the data set\nTake the first variable, and divide the horizontal axis into sections proportional to the sizes of its categories.\nTake each column, and divide it into rectangles vertically according to the size of the second variable categories.\nContinue as needed, splitting each tile alternately horizontally and vertically\n\nSo, the plot we have generated has columns with widths proportional to the numbers of the two Sexes of passengers. As there were more Male passengers than Female, the Male column is wider. The columns are then split into tiles according to the proportion of each Sex that Survived or did not, with the Green area of each representing the proportion which Survived. If the two Sexes had the same rate of survival, then the two columns would split into similarly sized rows. What we see here is a substantial imbalance in the heights of the rows which is indicative of an association. Female survival rates were much better than the Male survival rate, so there’s an association between Sex and Survived.\nIn general, this functions just like a stacked barplot where we stretch each bar to have the same height. The main advantage of these plots comes when we have more than two variables to explore.\nDid Class affect Survival?\n\nmosaicplot(xtabs(Freq~Class+Survived, titanic), col=c('#D9344A','#1DB100'), main='')\n\n\n\n\n\n\n\n\nAs we said above, if there was no effect of Class then we would expect the four bars would divide equally for each Class as the same proportion of passengers would have survived or not irrespective of Class. Clearly, the bars do not divide equally and again we have signs of an association. We can see that 1st class passengers had far better survival rates, followed by 2nd class, and 3rd class passengers didn’t fare much better than the Crew."
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#example-arthritis-treatment",
    "href": "Lecture2a_ManyCatVar.html#example-arthritis-treatment",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "library(vcd)\n\nLoading required package: grid\n\ndata(Arthritis)\n\nThe arthritis data contains the results from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. The data set contains observations on 84 patients with variables:\n\nID - patient ID.\nTreatment - factor indicating treatment (Placebo, Treated).\nSex - factor indicating sex (Female, Male).\nAge - age of patient.\nImproved - ordered factor indicating treatment outcome (None, Some, Marked).\n\n\nhead(Arthritis)\n\n  ID Treatment  Sex Age Improved\n1 57   Treated Male  27     Some\n2 46   Treated Male  29     None\n3 77   Treated Male  30     None\n4 17   Treated Male  32   Marked\n5 36   Treated Male  46   Marked\n6 23   Treated Male  58   Marked\n\n\nTreatment, Sex, and Improved are all nominal categorical variables. Improved is ordinal, since the category levels can be placed in a meaningful order. The question is whether the patient Improvement depends on Treatment and/or Sex.\nA mosaic plot of a single variable is basically a simple stacked barplot with only one bar. Looking at the patient improvement only gives:\n\nmosaicplot(xtabs(~Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nSo, no improvement is most common, but of the two groups which do have an improvement the improvement is more likely to be Marked than Some. Of course, we could just get this from a simple summary table:\n\nxtabs(~Improved, Arthritis)\n\nImproved\n  None   Some Marked \n    42     14     28 \n\n\nWe can now start splitting things up by Treatment type. The data used to construct the plot is the 2-way contingency table, obtained by summarising the data:\n\nxtabs(~Treatment+Improved, Arthritis)\n\n         Improved\nTreatment None Some Marked\n  Placebo   29    7      7\n  Treated   13    7     21\n\n\n\nmosaicplot(xtabs(~Treatment+Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nThere are a number of things to note here:\n\nThe number of patients in the Placebo and Treated group appears equal, as the two columns are equal in size\nThe main outcome from the Placebo group is No Improvement - perhaps no surprise! But some patients improve anyway.\nIn the Treatment group, the most likely outcome is a Marked Improvement. An Improvement of Some is the least common outcome.\nClearly there is an association between Treatment and Improvement.\n\nFor comparison, a mosaic plot with no association between Treatment and Improvement would look like this:\n\n\n\n\n\n\n\n\n\nAs we can see, in the event of no association the bars decompose into regular tiles. A loose test of this is whether we can draw a straight line from one side of the plot to the other without going through any of the tiles.\nThe order in which we introduce the variables into the mosaic plot also affects the plot we draw. Here, we have split on Treatment first and then on Improved. We could do this the other way around:\n\nmosaicplot(xtabs(~Improved+Treatment, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nNow we split first by Improved, which creates three columns that are then each split into Treatment type. This plot is most useful for showing how Improvement types decompose into Treatment groups, which is less helpful! Usually, we would split on the dependent or response variable last.\n\n\nWith more than two variables, we can still apply the same techniques to compare proportions, but it leads to two slightly different visualisations\n\nthe mosaicplot plot - for more than two variables draws the plots in a (nested) grid\nfor small numbers of variables, we can look at how barplots for different levels of other variables\nthe doubledecker plot - draws a row of simple mosaic plots to compare the different levels of a third (or fourth, …) variable\n\nIf we were to introduce Sex as a third variable to the mosaic plot of the Arthritis data, we will sub-divide each of the four tiles in the plots above into Male and Female halves.\n\nmosaicplot(xtabs(~Treatment+Sex+Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nNow the data are first split into columns by Improved, then each column split into rows by Treatment, and now further we split these tiles into smaller columns by Sex. This is clearly getting more complicated, but the same ideas apply. Substantial differences in sizes of tiles would suggest something is potentially going on. Possible observations we could make:\n\nThe Female tiles (red) are usually wider than the Male tiles (green) - we have overall more Female patients\nFemale patients generally seem to improve more than Males, regardless of treatment\nTreatment appears to increase the proportion of Improved patients\nFemale patients appear to improve more than Male.\n\n\n\n\nA doubledecker plot is a particular type of mosaic plot that splits all of the tiles vertically, except for the last one. The doubledecker plot is a lot like a sequence of stacked barplots for combinations of the categorical variables.\n\nlibrary(vcd)\ndoubledecker(Improved~Treatment+Sex,data=Arthritis)\n\n\n\n\n\n\n\n\nWe interpret this plot in much the same way. Here the columns are divided into the Treatment groups first, then each Treatment group divided into the two Sexes. This creates four columns, that are split into proportions according to Improved. So, we can observe similar features to before:\n\nThe Female columns are wider than the Male, suggesting more Female patients\nThe Marked improvement tiles in the Female columns are larger than those in the Male, suggesting Female patients response better\nThere was nobody with the combination Male + Placebo + Some Improvement. This is denoted by the hollow circle where this tile should be.\n\nIf we have a particular response variable in mind, the double-decker plot is often more useful than the general mosaic as we can split the independent variables (Sex, Treatment) into columns, and split the columns by the dependent variable (Improved). Overall, we can identify the same features from both types of plots, but the information is presented differently.\n\n\n\nLet’s return to the Titanic data and explore whether Survived depends on combinations of Sex or Class. In fact, since the scale of this problem is relatively modest. we can actually visualise the data as a matrix of barplots.\n\n\n\n\n\n\n\n\n\nWhile they show the shape of the distribution, making detailed comparisons is not terribly easy.\nHow did the combination of Class and Sex affect Survival? We can incorporate more variables into the doubledecker plot which will highlight differences in proportions rather than counts.\nThe columns are now grouped by combinations of Class & Sex.\n\n\n\n\n\n\n\n\n\n\nFemale survival was still higher overall, though declined rapidly with Class.\nAlmost all Female 1st class passengers survived, though those in 3rd class were not so fortunate\nMale survival rates don’t decline so much with Class, and Males in 2nd class have the lowest survival.\n\nAn alternative presentation of the same information is a mosaic plot:\n\n\n\n\n\n\n\n\n\n\nA mosaic plot show the data in a grid, rather than row.\nThe doubledecker plot uses a fixed height for its bars, but the mosaic plot varies the height and width of the panels according to the proportions of the factor variable.\nThis can make it a bit easier to distinguish the relative sizes of the combinations.\n\nThe mosaic plot algorithm is sensitive to the ordering of variables, so changing this will radically affect the plot drawn:\n\n\n\n\n\n\n\n\n\nHowever all of these methods start to struggle with more than a few variables. Unfortunately, too many combinations make the plots difficult to read and introduce a lot of ‘0’ counts into the data."
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#connection-with-chi2-tests",
    "href": "Lecture2a_ManyCatVar.html#connection-with-chi2-tests",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "The visualisation of Titanic data provided above suggests that there is some association here to explore. The rigorous way to assess association is through \\(\\chi^2\\) tests. Here’s how they work.\n\n\nFirst, we need to set up a general 2-way contingency table problem. Suppose that:\n\nWe have observed \\(n\\) individuals\nWe have recorded that value of two categorical variables for each:\n\nVariable 1, which has \\(C\\) distinct levels\nVariable 2, which has \\(R\\) distinct levels\n\nThe number of individuals who are observed in level \\(i\\) for variable 1 and level \\(j\\) for variable 2 is called \\(o_{ij}\\).\n\nWe can then summarise this in the 2-way table below, where the levels of variable 1 are the rows of the table, and the levels of variable 2 are the columns:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVar 1\\ Var 2\n1\n2\n…\n\\(j\\)\n…\n\\(C\\)\nTotal\n\n\n\n\n1\n\\(o_{11}\\)\n\\(o_{12}\\)\n…\n\\(o_{1j}\\)\n…\n\\(o_{1C}\\)\n\\(r_1\\)\n\n\n2\n\\(o_{21}\\)\n\\(o_{22}\\)\n…\n\\(o_{2j}\\)\n…\n\\(o_{2C}\\)\n\\(r_2\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\ni\n\\(o_{i1}\\)\n\\(o_{i2}\\)\n…\n\\(o_{ij}\\)\n…\n\\(o_{iC}\\)\n\\(r_i\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nR\n\\(o_{R1}\\)\n\\(o_{R2}\\)\n…\n\\(o_{Rj}\\)\n…\n\\(o_{RC}\\)\n\\(r_R\\)\n\n\nTotal\n\\(c_1\\)\n\\(c_2\\)\n…\n\\(c_j\\)\n…\n\\(c_C\\)\n\\(n\\)\n\n\n\nHere we have also introduce notation for the row and columns sums:\n\n\\(r_i\\) is the sum of counts in the \\(i\\)th row. This corresponds to the number of observations of level \\(i\\) of Variable 1. The collection of row counts \\(r_1,\\dots,r_R\\) corresponds to the counts for all the different levels of Variable 1.\n\\(c_j\\) is the sum of counts in the \\(j\\)th row. This corresponds to the number of observations of level \\(j\\) of Variable 2. The collection of row counts \\(c_1,\\dots,c_C\\) corresponds to the counts for all the different levels of Variable 2.\n\n\n\nWe can use the xtabs function from Lecture 1 to construct these contingency tables from data. There are two different usages, depending on whether the data has been pre-summarised as counts (like the Titanic data), or if the data is not summarised and each row corresponds to a single observation.\nIf the data is summarised already, and contains the totals in column called counts, then the summary of counts for the levels of variable1 is obtained by:\n\nxtabs(counts~variable1, data=dataset)\n\nFor example, to summarise survival:\n\nxtabs(Freq~Survived,data=titanic)\n\nSurvived\n  No  Yes \n1490  711 \n\n\nFor 2-way tables, we simply add another variable to the right of the ~ symbol:\n\nxtabs(counts~variable1+variable2, data=dataset)\n\nFor example, to summarise combinations of survival with class of passenger:\n\nxtabs(Freq~Survived+Class,data=titanic)\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  122 167 528  673\n     Yes 203 118 178  212\n\n\n\n\n\nWhen we have \\(n\\) categorical variables, we could create an \\(n\\)-way contingency table. The table would summarise the counts of every possible combination of every possible level of the \\(n\\) variables. This obviously gets more complicated with larger \\(n\\), but we’ll come back to this later.\nFor instance, we could summarise the combinations of Titanic passenger survival, passenger class and passenger sex:\n\nxtabs(Freq~Survived+Class+Sex, data=titanic)\n\n, , Sex = Male\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  118 154 422  670\n     Yes  62  25  88  192\n\n, , Sex = Female\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No    4  13 106    3\n     Yes 141  93  90   20\n\n\nNow we should have a 3-dimensional table with \\(2\\times 4\\times 2\\) cells! But R can only print at most 2-dimensional tables. Instead, it has given us two separate 2-way table for Survived+Class, one for each of Sex=Male (top) and Sex=Female (bottom). We can note here that Female passengers and crew had better survivale than their male counterparts (with a notable exception being those in 3rd class).\n\n\n\n\nThe key idea behind using independence as the property to verify a \\(\\chi^2\\)-test is that if two variables \\(X\\) and \\(Y\\) are independent, then we can write \\[P[X=x,Y=y]=P[X=x]P[Y=y],\\] or equivalently in terms of conditional probabilitiy: \\[P[X=x | Y=y]=P[X=x].\\] The second version tells us that even if I were to tell you the value of Y, that would not cause you to change your probability distribution for \\(X\\). In other words, knowing \\(Y\\) has no influence on how we thinkg \\(X\\) will behave - in other words, the two variables are completely unrelated.\nThe \\(\\chi^2\\)-test is a relatively simple test for assessing independence. As a running example, let’s continue with the Titanic data and investigate possible relationships between Survived and Class. The idea behind our test is as follows:\n\nUsing the data, we can estimate the probabilities of the different levels of Survived (\\(p_i\\)) and Class (\\(p_j\\)) separately using the proportions of the data observed in each category.\nUnder a hypothesis of independence, I can use the first equation above to tell me that the probability of any combination of Survived and Class is just the product of individual probabilities from the previous step, \\(p_{ij}=p_i p_j\\). Applying this to all combinations of categories gives me the probability for every possible combination of the two variables, \\(p_{ij}\\).\nThe expected number of observations in each combination of categories is obtained by multiplying the probabilities from step 2 by the sample size \\(n\\). This gives the expected counts for every combination, \\(E_{ij}=np_{ij}\\).\nWe can then compare our expected counts to the observed counts in the data, and construct a test statistic.\n\nLet’s see how this goes with our data:\nObserved counts: We observe the following counts of the different categories of Survived and Class\n\n\n\n\n\nNo\nYes\n\n\n\n\n1490\n711\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\n325\n285\n706\n885\n\n\n\n\n\nObserved proportions: Dividing the observed counts by the sample size (\\(n=2201\\)) gives the proportions of the data observed in each category:\n\n\n\n\n\nNo\nYes\n\n\n\n\n0.676965\n0.323035\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\n0.1476602\n0.1294866\n0.3207633\n0.40209\n\n\n\n\n\nUnsuprisingly, the more common categories are now associated with larger proportions.\nProbabilities of combinations under independence We assume the null hypothesis to be \\(H_0\\): survival is independent from class. Then we can say, for example:\n\\[P[Survived=Yes,Class=1st] = P[Survived=Yes]\\times P[Class=1st],\\]\nand the same for all the other combinations. By multiplying our probabilities from the previous step, we can make the following probability table for the joint distribution of Survived and Class under our null hypothesis of independence:\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n0.0999608\n0.0876579\n0.2171455\n0.2722008\n\n\nYes\n0.0476994\n0.0418287\n0.1036178\n0.1298891\n\n\n\n\n\nExpected counts under independence Multiplying these probabilities by the number of passengers will give us the expected number of passengers (\\(E\\)) in each group under independence:\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n220.0136\n192.93503\n477.9373\n599.114\n\n\nYes\n104.9864\n92.06497\n228.0627\n285.886\n\n\n\n\n\n\n\n\nWe can now compare these expected counts with the ones we actually observed. If we find big differences, then we would conclude our hypothesis of independence must be wrong and that there is evidence of some form of relationship.\nBefore we do that, however, we’ll just set this up mathematically so we can obtain a formula for our test statistic.\nFor the Titanic data problem, we can construct the test statistic using the method above and get a test statistic of \\(X^2=190.46\\) (check!). Comparing to the \\(\\chi^2\\) distribution - this has a \\(p\\)-value of approximately 0! So, we strongly reject the hypothesis of independence and would do so at any of the usual levels of significance. Survival of the Titanic disaser and passenger class appear to be related!\nHaving discovered an interesting relationship, a good question to ask now is: why? Why did we reject this hypothesis? What about the data seemed to be at odds with the variables being independent?\nTo answer this, we look at the Pearson residuals - the components inside the double sum in our test statistic. For each combination of categories, we get a value of\\(\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\). We can inspect those values and see what we find. In particular, large values of the residuals indicate substantial departures from the null hypothesis. The sign of the residuals indicates whether there are \\(\\color{red}{\\text{fewer}}\\) (&lt;0), or \\(\\color{red}{\\text{more}}\\) (&gt;0) values than expected under independence.\n\n\n\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n\\(\\color{blue}{\\text{-6.608}}\\)\n\\(\\color{blue}{\\text{-1.867}}\\)\n\\(\\color{red}{\\text{2.290}}\\)\n\\(\\color{red}{\\text{3.019}}\\)\n\n\nYes\n\\(\\color{red}{\\text{9.566}}\\)\n\\(\\color{red}{\\text{2.703}}\\)\n\\(\\color{blue}{\\text{-3.315}}\\)\n\\(\\color{blue}{\\text{-4.370}}\\)\n\n\n\nHere we notice the very large values in the 1st class column - we see far \\(\\color{red}{\\text{more}}\\) survivors than expected and far \\(\\color{red}{\\text{fewer}}\\) fatalities. The pattern appears to be reversed for those in 3rd class and the ship’s crew. Clearly, first class passengers had far better outcomes than would be expected if the chances were equal for everyone on board.\nFinally, there’s no real need to do this entire calculation by hand as R easily perform an independence test for a contingency table. All we need to supply is the table of observed values obtained from xtabs:\n\nchisq.test( xtabs(Freq~Survived+Class,data=titanic) )\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(Freq ~ Survived + Class, data = titanic)\nX-squared = 190.4, df = 3, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#modelling-testing",
    "href": "Lecture2a_ManyCatVar.html#modelling-testing",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "Modelling & Testing",
    "text": "Modelling & Testing\n\nContingency tables - the standard for checking the association of two categorical variables is the \\(\\chi^2\\) test based on the summary table of counts.\nAssociations between categorical variables - if there are a small number of variables each with only a few categories, then log linear models could be considered.\nBinary dependent variables - for a binary (two-category) dependent variable, logistic regression is a good approach."
  },
  {
    "objectID": "Lab2_ManyCatVariables.html",
    "href": "Lab2_ManyCatVariables.html",
    "title": "Lab 2 - Exploring Many Categorical Variables with R",
    "section": "",
    "text": "This practical will lead you into producing some high quality visualisations of multivariate categorical data.\n\n\n\nBy the end of the lab, you will have acquired the following skills:\n\n\nPlotting stacked barplots for simple data sets\n\n\nPlotting doubledecker and mosaicplot plots for more complex data sets"
  },
  {
    "objectID": "Lab2_ManyCatVariables.html#exercise-1-mosaicplots",
    "href": "Lab2_ManyCatVariables.html#exercise-1-mosaicplots",
    "title": "Lab 2 - Exploring Many Categorical Variables with R",
    "section": "Exercise 1 (Mosaicplots)",
    "text": "Exercise 1 (Mosaicplots)\nDisplaying combinations of categorical variables can be quite difficult, as we can’t represent our data as simple points in a space. Instead, we summarise a categorical variable (e.g. eye colour) with multiple levels (blue, green, brown, …) by the frequencies of each of the levels in the data. When we have multiple categorical variables, we work instead with the counts of the combinations of the levels (e.g. red hair + green eyes).\nAll of our plots are some sort of visualisation of these counts and so are (in one way or another) variations and manipulations of stacked barplots. Multivariate categorical data is a little more complex to work with, so generally it is recommended to start with single or pairs of variables and progressively add more, rather than visualising everything all at once like a scatterplot matrix.\n Download data: arthritis\nThe arthritis data contains the results from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. The variables are:\n\nID - patient ID.\nTreatment - factor indicating treatment (Placebo, Treated).\nSex - factor indicating sex (Female, Male).\nAge - age of patient.\nImproved - ordered factor indicating treatment outcome (None, Some, Marked).\n\nTreatment, Sex, and Improved are all categorical variables. Improved is also ordinal, since the category levels can be ordered. The question is whether the patient improvement depends on the Treatment and Sex.\nMosaic plots can display the relationship between categorical variables using rectangular tiles, whose areas represent the proportion of cases for any given combination of levels. A mosaic plot of a single variable is basically a simple stacked barplot with only one bar. Looking at the patient improvement only, we see the following\n\nmosaicplot(~Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nSince the None box is largest, we see that this appears to be the most common patient outcome. But taking the Some and Marked improvements together, it’s actually an even split. Let’s see how this depends on what Treatment the patient received - we would hope to see more improvement from those in the Treated group:\n\nmosaicplot(~Treatment+Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nNote that the plot now has two splits: the first split is horizontal into bars for the “Treated” and “Placebo” groups, while the second set of splits divides each bar up into the different Improved categories. If Treatment had no effect on the Improved state, then we would see a regular grid where the two sets of bars would have splits of approximately the same size and we could draw lines from the left of the plot to the right without cutting through any of the tiles.\nHowever, we can see clearly that a greater proportion of patients improved on treatment than in the placebo group and so the distribution of Improved is different for different values of Treatment, which may point towards an association between these variables and maybe hints at treatment being effective!\nIf we reverse the positions of Improved and Treatment in the function, it change the order in which the bars are split:\n\n\n\n\n\n\n\n\n\n\nTask 1b\n\nUsing the mosaicplot command, try to reproduce the mosaicplot above\nWhat are the new features that become apparent when you reverse the positions of the variables?\n\n\n\nClick for solution\n\n\nmosaicplot(~Improved+Treatment, data=arthritis, col=2:4,main='')\n\n\n\n\n\n\n\n\nNow we have three vertical bars for Improved, each sub-divided into the Treatment groups. This plot now shows how the patients with different Improved levels break down into the Treatment groups. So, we would read this as saying for those patients with a Marked improvement, the majority were Treated rather than given Placebo. Usually, it is best to split on the response variable at the end, as we had done in the previous plot.\n\n\n\nMore variables\nWe can continue to add variables to the plot and break down the results into more groups. For instance, we can introduce Sex as a variable\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nHere we can see:\n\nThe Treatment groups look to be roughly equal in size\nThere are fewer men than women in the study (the Male rows are narrower than the Female)\nThere are no Male patients in the Placebo group who only had Some improvement - this is indicated by the dashed line where this bar should be.\nTreatment appears to have a positive effect\nFemale patients seemed to improve the most in general, and particularly under Treatment\n\nIt is often worth reordering the variables in the mosaic plot formula to see if a different sequence of splits is a more effective visualisation for your problem. Generally, our dependent variable of interest is split last following the ~ in the function call.\n\n\nTask 1b\n\nProduce mosaicplots of combinations of one, two and three variables for the built-in R data set HairEyeColor\nExperiment with the ordering of the variables in the call to the mosaicplot function to see how different orderings produce different presentations of the data.\nWhich visualisation best highlights the interesting patterns in the data?\n\n\n\nClick for solution\n\n\ndata(\"HairEyeColor\")\n\nmosaicplot(~Hair, data=HairEyeColor,col=2:5,main='')\n\n\n\n\n\n\n\nmosaicplot(~Hair+Sex, data=HairEyeColor,col=2:5,main='')\n\n\n\n\n\n\n\n## With three variables there are 6 different orderings - here are a couple (with a different coloring option)\nmosaicplot(~Hair+Sex+Eye, data=HairEyeColor,col=3:6,main='')\n\n\n\n\n\n\n\nmosaicplot(~Eye+Hair+Sex, data=HairEyeColor,col=3:6,main='')\n\n\n\n\n\n\n\n\nIn the case of three variables, the mosaicplot with Hair highlighted by colours is more readable. With the second one, comparisons are hard to make.\n\n\n\nDirections of the splits\nA doubledecker plot is a version of a mosiac plot where all of the splits in the data are vertical, except for the last one. This effectively produces a type of stacked barplot, which we can achieve by setting the direction argument as follows:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, dir = c(\"v\", \"v\", \"h\"),col=2:4)\n\n\n\n\n\n\n\n\nOr, using the doubledecker function directly gives an almost identical plot:\n\ndoubledecker(Improved~Treatment+Sex, data=arthritis)\n\n\n\n\n\n\n\n\n\n\nAdding some colour\nThe default plots are somewhat dull and monochrome. Note that the different shadings used in the plot correspond to the different levels of the last cut variable, i.e. the dependent variable which is Improved here. So, if we supply one colour for each level of that variable we get the following plot:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, main='', col=c(\"wheat\", \"cornflowerblue\", \"tomato1\"))\n\n\n\n\n\n\n\n\nNote: In general, setting colours on mosaic plots can be quite fiddly. We won’t make much use of it, apart from an automatic coloring technique that we describe below."
  },
  {
    "objectID": "Lab2_ManyCatVariables.html#using-colour-to-highlight-unexpected-patterns",
    "href": "Lab2_ManyCatVariables.html#using-colour-to-highlight-unexpected-patterns",
    "title": "Lab 2 - Exploring Many Categorical Variables with R",
    "section": "Using colour to highlight unexpected patterns",
    "text": "Using colour to highlight unexpected patterns\nA different but helpful use of colour is the shade=TRUE option. The shade=TRUE option works by colouring any tiles in the mosaic that are in disagreement with the independence hypothesis of a chi-square test. This can help us identify any combinations that are unusually common or rate:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis,shade=TRUE,main='')\n\n\n\n\n\n\n\n\nTiles are shaded blue when more cases are observed than expected given independence, and shaded red when there are fewer cases than expected under independence. The strength of colour indicates how “surprising” those values are. The plot here is showing that most of the variation is not significant (coloured white), but in the Female and Treated group there is a surprisingly high number of patients who display a Marked improvement (blue-ish) - and consequently, fewer than expected (red-ish) whose improvement was None.\n\nTask 1c\n\nUse the mosaicplot function with shade and the dir arguments to create a “doubledecker” version of the mosaic plot above.\n\n\n\nClick for solution\n\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, shade=TRUE, main='',dir = c(\"v\", \"v\", \"h\"))"
  },
  {
    "objectID": "Lab2_ManyCatVariables.html#exercise-2-mosaicplots-on-a-data-set-containing-counts",
    "href": "Lab2_ManyCatVariables.html#exercise-2-mosaicplots-on-a-data-set-containing-counts",
    "title": "Lab 2 - Exploring Many Categorical Variables with R",
    "section": "Exercise 2: Mosaicplots on a data set containing counts",
    "text": "Exercise 2: Mosaicplots on a data set containing counts\n Download data: alligator\nThe alligator data, from Agresti (2002), comes from a study of the primary food choices of alligators in four Florida lakes. The goal is to try and learn something about the food choice of the different alligators. The variables are:\n\nlake - one of four lakes: George, Hancock, Oklawaha, and Trafford\nsex - male or female\nsize - small or large\nfood - the food preferences of the alligators in five categories: fish, invertebrates, reptile, bird and other.\n\nAs usual, we begin with a quick look at the data to see what we’re dealing with:\n\nhead(alligator)\n\n\n\n\n\n\nlake\nsex\nsize\nfood\ncount\n\n\n\n\nHancock\nmale\nsmall\nfish\n7\n\n\nHancock\nmale\nsmall\ninvert\n1\n\n\nHancock\nmale\nsmall\nreptile\n0\n\n\nHancock\nmale\nsmall\nbird\n0\n\n\nHancock\nmale\nsmall\nother\n5\n\n\nHancock\nmale\nlarge\nfish\n4\n\n\n\n\n\nHere, unlike the arthritis data, each row does not represent an individual alligator but all of the alligators found with the given combinations of categorical variables. So, for example, we have seen 7 alligators with attributes (Hancock, male, small, fish). This is a slightly different format than we saw above, so we’ll need to deal with it slightly differently.\nTo produce the counts needed for our plots, we need to use the cross-tabulation function xtabs that we used with our barplots. So, to generate the counts of alligators in each lake, we first compute\n\nxtabs(count~lake, data=alligator)\n\nlake\n  George  Hancock Oklawaha Trafford \n      63       55       48       53 \n\n\nand then pass this to our mosaic function for plotting:\n\nmosaicplot(xtabs(count~lake, data=alligator),col=2:5)\n\n\n\n\n\n\n\n\nAlternatively, with a single variable we could just draw a barplot, which is probably a little easier to read!\n\nbarplot(xtabs(count~lake, data=alligator),col=2:5)\n\n\n\n\n\n\n\n\nNote that the mosaicplot is showing the proportions in the different lakes by the width of the bars, whereas the barplot uses the height. We see there are slight differences between the numbers of alligators observed in the different lakes, but they don’t appear to be substantial.\nNote that the only difference with working with these data (which include the counts as a variable) and the previous data set (which did not include the counts) is that we must do the aggregating of the data in the xtabs function first, instead of directly in mosaicplot. The syntax and formula for splitting the data is the same.\n\nTask 2a\n\nInvestigate the distributions of the other categorical variables individually: sex, size, and food. You can use whatever plot you prefer. Try and answer the following questions:\n\nAre the sexes of alligators evenly distributed?\nWhat about the different sizes?\nWhich food type is most popular?\n\n\n\n\nClick for solution\n\n\nmosaicplot(xtabs(count~sex, data=alligator))\n\n\n\n\n\n\n\n\nMore males than female\n\nmosaicplot(xtabs(count~size, data=alligator))\n\n\n\n\n\n\n\n\nslightly more small than large\n\nmosaicplot(xtabs(count~food, data=alligator))\n\n\n\n\n\n\n\n\nmostly eat fish, then invertebrates.\n\n\n\nMore than two variables\nThe strength of mosaic plots is when considering the combination of multiple categorical variables at once. To keep things manageable, let’s looks at some potentially interesting pairs of variables first:\n\n\nTask 2b\n\nUse mosaicplot to visualise the size and sex variables together. Remember, if there is no association here then we would expect a regular grid. What associations do you find?\nWhat about size and food?\nDraw the doubledecker plots of the same variables - how do they compare to the mosaic plot?\n\n\n\nClick for solution\n\n\nmosaicplot(xtabs(count~size+sex, data=alligator))\n\n\n\n\n\n\n\ndoubledecker(xtabs(count~size+sex, data=alligator))\n\n\n\n\n\n\n\n\nmore males are larger, females are smaller\n\nmosaicplot(xtabs(count~size+food, data=alligator))\n\n\n\n\n\n\n\ndoubledecker(xtabs(count~size+food, data=alligator))\n\n\n\n\n\n\n\n\nbig alligators eat more fish+reptile, and less invertebrates.\n\nWe can even make a matrix of all the two-way mosaic plots in the style of a scatterplot matrix by the following command:\n\npairs(xtabs(count~.,data=alligator))\n\nUsing a . on the right side of the formula is a shorthand for “include everything”.\n\n\nTask 2c\n\nCan you locate the plots of size and sex, and size and food within the matrix?\nDo you see any other potential associations (or lack of associations) here? Remember, “no association” will mean the mosaic is divided into an approximately regular grid.\nMake a doubledecker plot of food, size and sex - order the variables so that each bar is split into sections according to the food.\nNow try the mosaic plot and use the shade=TRUE option. Try and achieve the same ordering so that food is the final split. What combinations have been highlighted, and how would you interpret them?\nDo you see any other potentially interesting features here?\n\n\n\nClick for solution\n\nSize:sex plot is in position (1,3), size:food is in (3,4). Seem to be a fair few associations here, except for food:sex Let’s take a closer look:\n\nmosaicplot(xtabs(count~sex+food, data=alligator))\n\n\n\n\n\n\n\n\nThis looks pretty regular, no obvious surprisingly large/small blocks. Let’s try shading the big plot for more emphasis\n\npairs(xtabs(count~.,data=alligator), shade=TRUE)\n\n\n\n\n\n\n\n\nCombining more that two variables in a mosaic can get a bit crazy. Let’s look at size, sex and food all together.\n\ndoubledecker(xtabs(count~size+sex+food, data=alligator))\n\n\n\n\n\n\n\nmosaicplot(xtabs(count~size+sex+food, data=alligator),shade=TRUE)\n\n\n\n\n\n\n\n\nLarge males eat more reptiles, small males eat surprisingly few (are they eating each other…?) Small females eat more invertebrates, large females eat few\nThe four-way table goes a bit mad\n\nmosaicplot(xtabs(count~size+sex+food+lake, data=alligator),shade=TRUE)\n\n\n\n\n\n\n\n\nbut does reveal some features that may be worth digging into, though it has become very hard to digest."
  },
  {
    "objectID": "Lab2_ManyCatVariables.html#exercise-3-for-extra-practice-grouped-and-stacked-barplots",
    "href": "Lab2_ManyCatVariables.html#exercise-3-for-extra-practice-grouped-and-stacked-barplots",
    "title": "Lab 2 - Exploring Many Categorical Variables with R",
    "section": "Exercise 3 (for extra practice): Grouped and stacked barplots",
    "text": "Exercise 3 (for extra practice): Grouped and stacked barplots\nBarplots can be used effectively to display combinations of categorical variables. However, they require a little more setup to provide the data in the correct format.\nFirst, a grouped barplot displays a numeric value (e.g. counts) split in groups and subgroups. A few explanation about the code below: * the input dataset must be a numeric matrix. Each group is a column. Each subgroup is a row. So we can only deal with two variables at once. * the barplot function will recognize this format, and automatically perform the grouping for you. * the beside option allows to toggle between the grouped and the stacked barchart\n\n## make a table of counts by Treatment and Improved from the arthritis data\ntab &lt;- xtabs(~Improved+Treatment,data=arthritis)\ntab\n\n        Treatment\nImproved Placebo Treated\n  None        29      13\n  Some         7       7\n  Marked       7      21\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\nAnd to stack the bars, set beside=FALSE\n\nbarplot(tab, beside=FALSE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\nStacked bars are often used to display the proportions of the respective columns attributable to each sub-group. Thankfully, we can easily convert tables of counts to proportions with the prop.table function. If we want the proportions computed within a column, set the margin=2 argument:\n\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\n\nData set: Airline arrivals\nWe are now going to compare all the techniques seen so far on a new dataset.\n Download data: airlineArrival\nThe airlineArrival data contains 11000 observations of 3 categorical variables: * Airport - a factor with levels LosAngeles, Phoenix, SanDiego, SanFrancisco, Seattle * Result - a factor with levels Delayed,OnTime * Airline - a factor with levels Alaska, AmericaWest\n\n\nTask 3a\n\nCreate a grouped barplot of the counts of delayed flights versus ontime flights for both levels of the Airline variable.\nCreate a stacked barplot of the same data above\nIs there much difference in the amount of delayed flights between the two airlines? Which plot is better to assess this?\n\n\n\nClick for solution\n\n\ntab &lt;- xtabs(~Result+Airline,data=airlineArrival)\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n# Stacked barplot\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n\nIn the second plot we can see much better that the proportion is roughly the same. In the first plot, it is easier to see the actual counts of delayed and on time flights.\n\n\n\nTask 3b\n\nProduce grouped and stacked barcharts of counts of Airport against Result. What is the response variable here?\n\nWhich airports look best for flights being on time? Which look worst?\n\n\n\n\nClick for solution\n\n\ntab &lt;- xtabs(~Result+Airport,data=airlineArrival)\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n# Stacked barplot\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n\nOnce again, stacked plots are more useful: Result is clearly the response variable here and hence we need to produce five different bars.\nSan Francisco is the worse airport and Phoenix the best in terms of flights being on time.\n\n\n\nTask 3c\n\nNow look at whether both Airport and Airline are associated with delays by producing a mosaicplot and a doubledecker plot of all three variables.\nWhat do you find? Try turning on shade=TRUE.\nIn which plot are the associations most pronounced?\n\n\n\nClick for solution\n\nHere is the mosaicplot\n\nmosaicplot(~Airport+Result+Airline, data=airlineArrival,col=c(2,7),main='')\n\n\n\n\n\n\n\n\nHere is the doubledecker plot\n\ndoubledecker(xtabs(~Airport+Airline+Result, data=airlineArrival))\n\n\n\n\n\n\n\n\nThe doubledecker is better at showing that Alaska airlines is doing a good job getting flights from Phoenix to be ontime.\nThe shaded mosaicplot confirms this finding, but gives much more information about unusual associations:\n\nmosaicplot(~Airport+Result+Airline, data=airlineArrival, col=c(2,7), shade=TRUE, main='')"
  }
]