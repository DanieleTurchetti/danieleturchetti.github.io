[
  {
    "objectID": "Lab1_ExplContVar.html",
    "href": "Lab1_ExplContVar.html",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "This practical will lead you into producing some high quality standard visualisations of single continuous data variables, as seen in lectures.\n\n\nBy the end of the lab, you will have acquired the following skills:\n\n\nPlotting a histogram with hist\n\n\nPlotting a boxplot with boxplot\n\n\nExtracting simple numerical summaries with summary and fivenum\n\n\nCustomising plots with labels, titles, colour, etc.\n\n\nProducing multiple variations of the plots above\n\n\n\n\n\nTo begin with, let’s first see how to use R to produce standard plots of a variable, namely histograms, boxplots, and quantile (or QQ) plots.\nFor illustration, let us use the R built-in mtcars data set, which contains information on the characteristics of 23 cars.\n\ndata(mtcars)\n\n\n\nA histogram consists of parallel vertical bars that graphically shows the frequency distribution of a quantitative variable. The area of each bar is proportional to the frequency of items found in each class. A histogram is useful to look at when we want to see more detail on the full distribution of the data, and features relating to its shape.\nTo plot a histogram, we apply the hist function to the data vector. We can extract the mpg (miles-per-gallon) variable from the mtcars data set using the $ operator, and so we can draw a histogram as follows:\n\nhist(mtcars$mpg)\n\n\n\n\n\n\n\n\nThis seems to suggest that we have a peak somewhere between 15 and 20 mpg, and potentially another peak between 30 and 35 mpg - perhaps suggesting groups of ‘fuel efficient’ and ‘fuel inefficient’ cars.\nOne way to assess if the number of bars in the histogram is appropriate is to show the location of the data points on the horizontal axis. We can add a ‘rug plot’ to our histogram, which marks the positions of the data with lines on the axis:\n\nhist(mtcars$mpg)\nrug(mtcars$mpg) ## Note: the 'rug' function draws on top of an existing histogram\n\n\n\n\n\n\n\n\nNow we can also see where the data fall within the bars!\nThe default settings of hist will determine the number of bars to display algorithmically, and in this case it has drawn only 5. In general, this is probably too few to show any detail, but we don’t have many data points here. Fortunately, the display of the histogram can be adjusted by a number of arguments:\n\nbreaks - allows us to control the number of bars in the histogram. breaks can take a variety of different inputs:\n\nIf breaks is set to a single number, this will be used to (suggest) the number of bars in the histogram.\nIf breaks is set to a vector, the values will be used to indicate the endpoints of the bars of the histogram.\n\nfreq - if TRUE the histogram shows the simple frequencies or counts within each bar; if FALSE then the histogram shows probability densities rather than counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the hist function to draw histograms of miles-per-gallon which match those shown above (don’t worry about the labels).\n\n\n\nClick for solution\n\n\nhist(mtcars$mpg,freq=FALSE,main='freq=FALSE')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=10,main='breaks=10')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=50,main='breaks=50')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=c(10,12,20,30,35), main=\"breaks=c(10,12,20,30,35)\")\n\n\n\n\n\n\n\n\n\n\n\n\nA five-number numerical summary can be computed with the fivenum function, which takes a vector of numbers as input. To add a little more information, the summary function includes the mean of the data for a 6-number summary.\n\n\n\nCompute summaries of the mpg data in the mtcars dataset using fivenum and summary. What is your interpretation of the result?\n\n\nClick for solution\n\n\nfivenum(mtcars$mpg)\n\n[1] 10.40 15.35 19.20 22.80 33.90\n\n\nThe values returned are the sample minimum, lower quartile, median, upper quartile and maximum. We can see that the median across all the cars in the dataset is about 20 miles per gallon. This is pretty terrible by modern standards, but the data are from 1974 and the USA.\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nThe inclusion of the mean can be quite helpful. If the data have an approximately symmetric distribution then the mean and median values should be close, which can be used as a quick check for any potential skewness in the data. Given that the mean is fairly close to the median, there doesn’t appear to be a dramatic amount of skewness in the distribution of MPG.\n\n\n\n\n\nA boxplot provides a graphical view of the median, quartiles, maximum, and minimum of a data set. In many ways, it is simply a direct visualisation of the five number summary constructed above. The whiskers (vertical lines) capture roughly 99% of a normal distribution, and observations outside this range are plotted as points representing outliers (see the figure below).\n\nBoxplots are created for single variables using the boxplot function, but can be used to easily compare many variables or groups within the data. To draw a boxplot of a single variable, multiple variables, or all variables in a data frame, we simply pass the data directly to the boxplot function:\n\nboxplot(mtcars$mpg)\n\n\n\n\n\n\n\n\nAs the boxplot is based on the simple 5-number summary, it lacks the detail of a histogram. However, we can inspect it for features such as symmetry or skewness - a symmetric distribution will give a boxplot with a whiskers of equal length, a centrally-positioned box evenly divided by the median line.\nHere, we see the box is slightly off-centre, suggesting some slight skewness. We also note that there are no obvious outliers.\n\n\nDraw a a boxplot of all the variables in mtcars by passing the entire data frame to the boxplot function. Can you see anything useful in the plot?\n\n\nClick for solution\n\n\nboxplot(mtcars)\n\n\n\n\n\n\n\n\nUnfortunately, because of the different scales of variables we can’t see what’s going on with the smaller variables.\n\nThe boxplot is most useful when comparing how a variable behaves in different groups (i.e., the levels of a categorical variable). For example, we can compare the MPG with the number of engine cylinders\n\ncyl &lt;- factor(mtcars$cyl) # make the 'cyl' variable categorical\nboxplot(mtcars$mpg ~ cyl)\n\n\n\n\n\n\n\n\nWhat do you conclude about fuel efficiency in cars with more engine cylinders?\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data.\n\n\n\n\n\nFrancis Galton famously developed his ideas on correlation and regression using data which included the heights of parents and their children. This galton data set include data on heights for 928 children and their 205 ‘midparents’. Each ‘midparent’ height is the average of the father’s height and 1.08 times the mother’s height (to adjust for the usual gender differences). Similarly, the daughter’s heights have also been multiplied by 1.08. Note that we have one midparent height for each child, so that many midparent heights are repeated.\nThe variables are child and parent for the different heights recorded in inches.\n\n\n\nDownload the galton.Rda data set from the Ultra page and load the file\nDraw a boxplot of both variables in the data set.\nWhat features do you see? How do the heights compare in terms of location and spread?\nDoes this agree with what you expected?\nDraw and compare histograms of the two variables - can you detect any noticeable similarities or differences?\nRedraw your histograms and add a rugplot to each. Does this give you more information?\n\n\n\nClick for solution\n\n\nboxplot(galton)\n\n\n\n\n\n\n\n## parents are less spread than children, both seem to be centred around the \n## same values (about 68.5)\n## both appear symmetric\n## parent displays two outliers\n\n## expectations: similar heights? so yes.\n## not sure would we expect more variability in children than parents, though parents are an average!\n\nhist(galton$child)\nrug(galton$child)\n\n\n\n\n\n\n\nhist(galton$parent)\nrug(galton$parent)\n\n\n\n\n\n\n\n## histograms are clearly symmetric, normal distributions?\n## rugplot shows something strange - all of these data points are lining up on a few values - why?\n## lets look at the first 30 values\ngalton$child[1:30]\n\n [1] 61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 62.2 62.2 63.2 63.2 63.2\n[16] 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2\n\n## is there rounding here? all the values are at integers +0.2...\n## how many unique values are there?\nunique(galton$parent)\n\n [1] 70.5 68.5 65.5 64.5 64.0 67.5 66.5 69.5 71.5 72.5 73.0\n\n## only 11 unique values!\n\n\n\n\n\n\nUsing the par() command, draw histograms of parents and child heights side-by-side on the same plot (you may need to adjust the width of your plot window.)\nDo your plots reveal anything interesting?\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\nhist(galton$parent)\nhist(galton$child)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the difficulties of comparing multiple independent plots is we need to do more work to ensure consistency of presentation. In particular, we should ensure that our histogram intervals and axis ranges are the same for both plots, as the default presentation will change from plot to plot.\nMany high level plotting functions (plot, hist, boxplot, etc.) allow you to include additional options to customise how the plot is drawn (as well as other graphical parameters). We have seen examples of these already with the axis label arguments xlab and ylab, however we can customise the following plot features for finer control of how a plot is drawn.\nAxis limits\nTo control the ranges of the horizontal and vertical axes, we can add the xlim and ylim arguments to our original plotting function To set the horixontal axis limits, we pass a vector of two numbers to represent the lower and upper limits, xlim = c(lower, upper) specifying numerical values for upper and lower, and repeat the same for ylim to customise the vertical axis.\nAxis labels\nTo specify a label for the x- and y-axes we can supply a string to the xlab and ylab arguments. To give a plot a title, we pass the title as a string to the main argument.\nIt is easier to compare the shape of distributions in histograms when they are arranged vertically, they use equal horizontal axis limits, and the same binwidths.\n\n\n\n\nUse par to setup a column of two plots.\nPlot a histogram of child and then parent from galton using:\n\nx-axis limits of 60 to 75\nA bar width of 1 unit\nAn appropriate x-axis label and plot title\n\nDoes the comparison yield any new information that wasn’t conveyed in the boxplot?\nTry reducing your bar widths - what do you find?\n\n\n\nClick for solution\n\n\npar(mfrow=c(2,1))\nhist(galton$parent,xlim=c(60,75),breaks=60:75,xlab='Parent heights', main='')\nhist(galton$child,xlim=c(60,75),breaks=60:75,xlab='Child heights', main='')\n\n\n\n\n\n\n\n## they seem to line up nicely, with the children more spread. Much like the boxplots\n## why are the parents less variable? One set of parents has many children, so \n## there's a lot more variability from many children (between 1 and 15) there\n## due to repetition of the parent values \n## its actually a difference of the order of 1/sqrt(2), which you would expect for\n## the mean of 2 parents!\n\nIn interpreting the data, it is worth noting that:\n\nGalton obtained this data “through the offer of prizes” for the “bext Extracts from their own Family Records”, so the sample is hardly a random one\nthe data are clearly heavily rounded for tabulation\nfamily sizes vary from 1 child up to 15, so that there is a lot of repetition in the midparent heights.\n\nYou might expect that if we had the individual un-adjusted heights and the genders of the parents and children we would find that the height data distributions would be neatly bimodal with one peak for females and one for males. They are not. Apparently, height distributions are rarely like that.\n\n\n\n\n\nData science inevitably involves working with large data sets. The effort involved in preparing and making a large dataset usable for analysis should not be underestimated, but thankfully we’re going to look at a dataset “prepared earlier”. This movies data set (downloadable from Ultra) is reasonably large(ish), containing 24 different attribues of 28819 movies gathered from IMDB. One of the variables is the movie length in minutes, and it is interesting to look at this variable in some detail.\n\n\n\nLoad the movies data set and draw a histogram of the data.\nPlot a histogram of the length variable.\nWhat features can you see?\nAdd a rugplot to the histogram - what problems does this highlight?\nLet’s try a boxplot of the data - do we learn anything more?\nAre there any obvious outliers?\n\n\n\nClick for solution\n\n\nhist(movies$length)\nrug(movies$length)\n\n\n\n\n\n\n\n## all the data stack up at the left end near zero, but the range of the data is huge!\nboxplot(movies$length,horizontal = TRUE)\n\n\n\n\n\n\n\n## this confirms the same as the rugplot, but there are two ridiculously large outliers\n\nClearly, our data are distorted by some particularly egregious outliers.\n\n\n\n\nLook at the outliers more closely:\n\nWhat are the lengths of the two longest movies?\nDoes that seem sensible?\nExtract the subset of the dataframe containing all the variables for both of these movies\nInspect the variable values:\nThe variables r1 to r10 give the percentage of reviews which rated the movie as a 1 up to a 10 out of 10. Are these movies particularly popular?\nWhat are the names of the movies? Do a quick Google search to see if you can find out more.\n\n\n\nClick for solution\n\nThere are various way you can do this, I chose to sort the data and use the head or tail functions to extract the end/start of the list\n\ntail(sort(movies$length))\n\n[1]  647  773  873 1100 2880 5220\n\nhead(sort(movies$length,decreasing=TRUE))\n\n[1] 5220 2880 1100  873  773  647\n\n\nso 5220 and 2880 minutes - that’s 87 and 48 hours, or 14.5 and 8 days respectively. These are obviously not ‘genuine’ movies. Although it’s tempting to dismiss these as simple errors, it is worth checking if possible.\nTo look at all the variables for these two movies, we must subset the data frame using the square brackets\n\nmovies[movies$length&gt;2000,]\n\n                                                 title year length budget\n11937                           Cure for Insomnia, The 1987   5220     NA\n30574 Longest Most Meaningless Movie in the World, The 1970   2880     NA\n      rating votes   r1  r2  r3  r4 r5 r6 r7  r8  r9  r10 mpaa Action Animation\n11937    3.8    59 44.5 4.5 4.5 4.5  0  0  0 4.5 4.5 44.5           0         0\n30574    6.4    15 44.5 0.0 0.0 0.0  0  0  0 0.0 0.0 64.5           0         0\n      Comedy Drama Documentary Romance Short\n11937      0     0           0       0     0\n30574      0     0           0       0     0\n\n\nThe movies seem to be somewhat polarising, either rated 0 or 10, but only by a small number of reviews! Incidentally, this data set is no longer up-to-date and there are some even longer films now (though it’s a mystery why.)\n\nIn this case, the extreme outliers should be ignored, and for exploring the main distribution of movie lengths it makes sense to set some kind of upper limit. Over 99% of the data are less than three hours in length, so let’s restrict ourselve to those.\n\n\n\n\nExtract the all the movies of length at most three hours.\nDraw a histogram - what do you find?\n\n\n\nClick for solution\n\n\nmovies2 &lt;- movies[movies$length&lt;180,]\nhist(movies2$length)\n\n\n\n\n\n\n\n\nAt last, we see some structure! There are two clear peaks here (bi-modality): the first under 20 minutes, the second in [80,100] minutes. The latter group seems about right for the ‘average’ movie.\n\nUseful context:\n\nThe Oscars define a “short film” as anything under 40 minutes.\nAnimated shorts are typically between 5 and 8 minutes long, and are counted as individual movies (so, e.g., every ‘Tom and Jerry’ cartoon has its own entry).\n\n\n\n\n\nRedraw your histogram using a bin-width of 1 minute (you may need to enlarge your plot window).\nWhat do you see? How does the information above help explain the data?\nIs there any heaping in the data? At what values?\n\n\n\nClick for solution\n\nTo get a 1-minute bin width, I made a sequence of integers from 0 to 180 as my breakpoints. If you’re not familiar with this, look at the help for the ‘seq’ function in creating similar sequences.\n\nhist(movies2$length,breaks=0:180)\n\n\n\n\n\n\n\n\nThere’s a lot going on here! * there are still a fair few longer films over 2hrs, but its a minority * the clump of short movies correspond to the defined ‘short film’ and have a clear peak around 7mins. Coincidentally, most short cartoons are 6-8 mins long. * A big pronounced spike in values at exactly 90 minutes. This probably isn’t rounding, but rather that when making and editing movies they will have aimed to produce a film of that length. Perhaps we might have expected an ever sharper peak? * We also see some stacking around this peak, with certain lengths being favoured at 80, 85, 95, 100, etc\n\n\n\n\n\nUsing colour in a plot can be very effective, for example to highlight different groups within the data. Colour is adjusted by setting the col optional arugment to the plotting function, and what R does with that information depends on the value we supply.\n\ncol is assigned a single value: all points on a scatterplot, all bars of a histogram, all boxplots are coloured with the new colour\ncol is a vector:\n\nin a scatterplot, if col is a vector of the same length as the number of data points then each data point is coloured individually\nin a histogram, if col is a vector of the same length as the number of bars then each bar is coloured individually\nin a boxplot, if col is a vector of the same length as the number of boxplots then each boxplot is coloured individually\nif the vector is not of the correct length, it will be replicated until it is and the above rules apply\n\n\nNow that we know how the col argument works, we need to know how to specify colours. Again, there are a number of ways and you can mix and match as appropriate\n\nIntegers: The integers 1:8 are interpreted as colours (black, red, green, blue, …) and can be used as a quick shorthand for a common colour. Type palette() to see the sequence of colours R uses.\nNames: R recognises over 650 named colours is specified as text, e.g.\"steelblue\", \"darkorange\". You can see the list of recognised names by typing colors(), and a document showing the actual colors is available here\nHexadecimal: R can recognise colours specified as hexadecimal RGB codes (as used in HTML etc), so pure red can be specified as \"#ff0000\" and cyan as \"#00ffff\".\nColour functions: R has a number of functions that will generate a number of colours for use in plotting. These functions include rainbow, heat.colors, and terrain.colors and all take the number of desired colours as argument.\n\n\n## Colour example\n## 3 plots in one row\npar(mfrow=c(1,3))\n## colour the cars data by number of gears\nplot(x=mtcars$wt, y=mtcars$mpg, col=mtcars$gear, xlab=\"Weight\", ylab=\"MPG\", \n     main=\"MPG vs Weight\")\n## manually colour boxplots\nboxplot(mpg~cyl, data=mtcars, col=c(\"orange\",\"violet\",\"steelblue3\"),\n        main=\"Car Milage Data\", xlab=\"Number of Cylinders\", \n        ylab=\"Miles Per Gallon\")\n## use a colour function to shade histogram bars\nhist(mtcars$mpg,col=rainbow(5),main='MPG')\n\n\n\n\n\n\n\n\n\n\n\nShow the histograms of length of short movies next to that for ‘non-short’ movies, using a different colour for each histogram.\nExperiment with using the col argument to add colour to your histograms.\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\n## again, we can use subsetting to select a subset of the data. Here we don't need a ',' as we're subsetting a vector instead of a matrix \nhist(movies2$length[movies2$length&lt;=40],col='royalblue',xlab='length',main='Short films')\nhist(movies2$length[movies2$length&gt;40],col='tomato',xlab='length', main='Regular films')\n\n\n\n\n\n\n\n\n\n\n\n\n\n Download data: bundestag\nThese data contain the results of the 2009 elections for the German Bundestag, the first chamber of the German parliament. The contains the number of votes cast for the various political parties, for each state (“Bundesland”). Amongst the German political parties there are two on the left of the political spectrum, the SPD - similar to the UK’s Labour party - and Die Linke (“The Left”), a party even further to the left. Suppose we’re interested in the support for this “Die Linke” party.\n\n\n\nExplore the election support for ‘Die Linke’ party by examining the LINKE1 variable using. Compare the outputs and explain the difference you find:\n\nA histogram, with a rugplot\nA stacked dotplot\nA beewswarm plot, setting horizontal=TRUE\n\n\n\n\nClick for solution\n\n\nhist(bundestag$LINKE1)\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## clear concentration of points around 10k. let's try narrower bins for more detail\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=2000))\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## that's a bit more useful\n\n\nstripchart(bundestag$LINKE1,method='stack',pch=16)\n\n\n\n\n\n\n\n## this looks mostly like the rugplot information on the plot above, but even with stacking this doesn't resemble the histogram\n## Why not? The histogram is grouping nearby points into its bars, but the dotplot/stripchart does not and is treating each unique value as a separate point.\n\n\nlibrary(beeswarm)\nbeeswarm(bundestag$LINKE1,horiz=TRUE)\n\n\n\n\n\n\n\n## the shape of the beewswarm more closely resembles the histogram, albeit rather than stacking points up from the bottom it works from the middle out towards the edges\n\n\nA stem and leaf plot is a technique for displaying the data in a similar fashion to a histogram, while preserving the information ofthe individual numerical values. Where the histogram summarises the data by the counts in its various intervals, the stem and leaf plot retains the original data values up to two significant figures.\n\n\n\n\nDraw a stem and leaf plot of the German election support for ‘Die Linke’ data using the stem() function.\nHow does the stem and leaf plot represent these data? You may want to look at the data value to help understand.\nNow draw a histogram, adjusting the histogram to have axis range and bar width to match the stem and leaf plot.\n\n\n\nClick for solution\n\n\nstem(bundestag$LINKE1)\n\n\n  The decimal point is 4 digit(s) to the right of the |\n\n  0 | \n  0 | 55566666667777777777777888888888888888888888888888888888999999999999+14\n  1 | 00000000000000000000000000000000000001111111111111111111111111111111+51\n  1 | 5555566\n  2 | 122234\n  2 | 588889\n  3 | 01223333344444\n  3 | 556666677777788\n  4 | 0000011233444\n  4 | 6677899\n  5 | \n  5 | \n  6 | 0233\n\n## This is effectively a histogram with bins of width 5000 units. The number indicate the actual data values, with the \"stem\" being the leading digit to the left of the | symbol, and the values of the next digits of the data being indicated to the right of the |\n## We can see some structure now *within* the bars, which is a bit more detail than we get from the histogram.\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=5000))\n\n\n\n\n\n\n\n\nAs with the beeswarm plot, the stem and leaf plot is only suitable for relatively modestly sized data sets due to the fact it is literally writing out all of the data values on the screen!\n\n\n\n\n\n\n\n‘Stripplots’ or ‘Stripcharts’ are very similar to the rugplot we applied to our histograms, and display the individual data points along a single axis. They can be used in much the same way as a boxplot, but rather than showing the data summaries they display everything!\nThe built-in faithful data set contains measurements on the waiting times between the eruptions of the Old Faithful geyser.\n\ndata(faithful)\nstripchart(faithful$waiting,ylab='Waiting Time', pch=16) \n\n\n\n\n\n\n\n\nPlotting symbols\nThe symbols used for points in plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is usually pch=1 which is a hollow circle, in the plot above we changed it to 15 which is a filled circle.\nHowever, when we have a lot of data points concentrated in a small interval, the stripplot suffers from problems of ‘overplotting’ where many points with similar values are drawn on top of each other.\nA partial solution to this is to add random noise (known as ‘jittering’) to spread out the points.\n\nMake a stripplot of the movie length data - how does the overplotting problem manifest here? You may want to compare to your histogram.\n\nA better solution is to stack the dots that fall close together, producing an alternative plot to a histogram - sometimes called a ‘dotplot’ or ‘stacked dotplot’\n\nstripchart(faithful$waiting, method='stack',pch=16)\n\n\n\n\n\n\n\n\n\nTry this out with the movies data.\n\n\n\n\nAn evolution of the stripplot is the ‘beeswarm’ plot, available from the beeswarm package. A bee swarm plot is similar to stripplot, but with various methods to separate nearby points such that each point is visible.\n\nlibrary(beeswarm)\nbeeswarm(faithful$waiting)\n\n\n\n\n\n\n\n\nOne limitation of the beeswarm plot is that the computations to arrange all the points do not scale well with large data sets. Do not try this with the movies data, or you will be waiting for a very long time!\n\n\n\nTo see how this works, let’s look at the Old Faithful data, sorted from smallest to largest.\n\nsort(faithful$waiting)\n\n  [1] 43 45 45 45 46 46 46 46 46 47 47 47 47 48 48 48 49 49 49 49 49 50 50 50 50\n [26] 50 51 51 51 51 51 51 52 52 52 52 52 53 53 53 53 53 53 53 54 54 54 54 54 54\n [51] 54 54 54 55 55 55 55 55 55 56 56 56 56 57 57 57 58 58 58 58 59 59 59 59 59\n [76] 59 59 60 60 60 60 60 60 62 62 62 62 63 63 63 64 64 64 64 65 65 65 66 66 67\n[101] 68 69 69 70 70 70 70 71 71 71 71 71 72 73 73 73 73 73 73 73 74 74 74 74 74\n[126] 74 75 75 75 75 75 75 75 75 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77\n[151] 77 77 77 77 77 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79\n[176] 79 79 79 79 79 80 80 80 80 80 80 80 80 81 81 81 81 81 81 81 81 81 81 81 81\n[201] 81 82 82 82 82 82 82 82 82 82 82 82 82 83 83 83 83 83 83 83 83 83 83 83 83\n[226] 83 83 84 84 84 84 84 84 84 84 84 84 85 85 85 85 85 85 86 86 86 86 86 86 87\n[251] 87 88 88 88 88 88 88 89 89 89 90 90 90 90 90 90 91 92 93 93 94 96\n\n\nNote that the smallest value is 43, followed by three values of 45. A stem and leaf plot of these data looks like this\n\nstem(faithful$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 3\n  4 | 55566666777788899999\n  5 | 00000111111222223333333444444444\n  5 | 555555666677788889999999\n  6 | 00000022223334444\n  6 | 555667899\n  7 | 00001111123333333444444\n  7 | 555555556666666667777777777778888888888888889999999999\n  8 | 000000001111111111111222222222222333333333333334444444444\n  8 | 55555566666677888888999\n  9 | 00000012334\n  9 | 6\n\n\nEach row of this plot is called a ‘stem’ and the values to the right of the ‘|’ symbol are the leaves. Be sure to read where R places the decimal point for the output. For this result, the decimal is placed one digit to the right of the vertical bar. Thus, the first row of the table then consists of data values of the form \\(4x\\), and the only leaf is a \\(3\\) corresponding to the value \\(43\\) in the data. The next stem groups the values \\(45-49\\), and we notice the three observations of \\(45\\) are represented by the \\(555\\) at the start of the second stem.\nNotice that each stem part is representing an interval of width 5, much like a histogram. As usual, R figures out how best to increment the stem part unless you specify otherwise. Finally, notice how the shape of the stem and leaf plot mirrors that of a histogram with interval width 5 - the only difference is that here we can see the values inside the bars.\n\n\n\nThe data come from an old survey of 237 students taking their first statistics course. The dataset is called survey in the package MASS.\n\n\n\nLoad the data with data(survey, package='MASS')\nDraw a histogram of student heights - do you see evidence of bimodality?\nExperiment with different binwidths for the histogram. Which choice do you think is the best for conveying the information in the data?\nCompare male and femal heights using separate histograms with a common scale and binwidths.\n\n\n\n\n\n Download data: diamonds\nThe set diamonds includes information on the weight in carats (carat) and price of 53,940 diamonds.\n\n\n\nIs there anything unusual about the distribution of diamond weights? Which plot do you think shows it best? How might you explain the pattern you find?\nWhat about the distribution of prices? With a bit of detective work you ought to be able to uncover at least one unexpected feature. How you discover it, whether with a histogram, a dotplot, or whatever, is unimportant, the important thing is to find it. Having found it, what plot would you draw to present your results to someone else? Can you think of an explanation for the feature?\n\n\n\n\n\n Download data: zuni\nThe zuni dataset seems quite simple. There are three pieces of information about each of 89 school districts in the US State of New Mexico: the name of the district, the average revenue per pupil in dollars, and the number of pupils. The apparent simplicity hides an interesting story. The data were used to determine how to allocate substantial amounts of money and there were intense legal disgreements about how the law should be interpreted and how the data should be used. Gastwirth was heavily involved and has written informatively about the case from a statistical point of view Gastwirth, 2006 and Gastwirth, 2008.\nOne statistical issue was the rule that before determining whether district revenues were sufficiently equal, the largest and smallest 5% of the data should first be deleted.\n\n\n\nAre the lowest and highest 5% of the revenue values extreme? Do you prefer a histogram or boxplot for showing this?\nRemove the lowest and highest 5% of the cases, draw a plot of the remaining data and discuss whether the resulting distribution looks symmetric.\nDraw a Normal quantile plot of the data after removal of the 5% at each end and comment on whether you would regard the remaining distribution as normal.\n\n\n\n\n\n Download data: engine\nThese data record the amounts of three pollutants - carbon monoxide CO, hydrocarbons HC, and nitrogen oxide NO - in grammes emitted per mile by 46 light-duty engines.\n\n\n\nAre the distributions for the three pollutants similar? To make this an easier question to answer, try to produce histograms of the variables, using the same class intervals and range for the horizontal axis in each case.\n\n\n\n\n\n Download data: chlorph\nThese data come from a semi-automated process for measuring the actual amount of chlorpheniramine maleate in tablets which are supposed to contain a 4mg dose.\nThe tablets used for the study were made by two different manufacturers. For each manufacturer, a composite was produced by grinding together a number of tablets. Each composite was split into seven pieces each of the same weight as a tablet and the pieces were sent to seven different laboratories. Each laboratory made 10 separate measurements on each composite.\nThe data contain three variables: * chlorpheniramine - the amount measured * manufacturer - the tablet manufacturer as a factor (A or B) * laboratory - the laboratory which performed the measurement as a factor (1 to 7)\n\n\n\nProduce box-plots of the chlorpheniramine measurements split by laboratory. What, if anything, do they suggest?\nNow produce box-plots by manufacturer. Anything noticeable?\nThe problem here is that we really need a separate box-plot for each combination of manufacturer and laboratory. We can do this by boxplot(chlorpheniramine~laboratory*manufacturer, data=chlorph) Try this (or some abbreviated version of it).\nTry reversing the order of laboratory and manufacturer. Which way round is better? Try colouring the box-plots by manufacturer. Does that help?\nDo the data suggest that the two manufacturers actually put different amounts of the drug into supposed 4mg tablets?\nAre there obvious differences between different laboratories? If so, what kind of differences do you observe?\nIf you had to choose a single laboratory to make some measurements for you, which would you choose and why?"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-1-standard-plots-in-r",
    "href": "Lab1_ExplContVar.html#exercise-1-standard-plots-in-r",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "To begin with, let’s first see how to use R to produce standard plots of a variable, namely histograms, boxplots, and quantile (or QQ) plots.\nFor illustration, let us use the R built-in mtcars data set, which contains information on the characteristics of 23 cars.\n\ndata(mtcars)\n\n\n\nA histogram consists of parallel vertical bars that graphically shows the frequency distribution of a quantitative variable. The area of each bar is proportional to the frequency of items found in each class. A histogram is useful to look at when we want to see more detail on the full distribution of the data, and features relating to its shape.\nTo plot a histogram, we apply the hist function to the data vector. We can extract the mpg (miles-per-gallon) variable from the mtcars data set using the $ operator, and so we can draw a histogram as follows:\n\nhist(mtcars$mpg)\n\n\n\n\n\n\n\n\nThis seems to suggest that we have a peak somewhere between 15 and 20 mpg, and potentially another peak between 30 and 35 mpg - perhaps suggesting groups of ‘fuel efficient’ and ‘fuel inefficient’ cars.\nOne way to assess if the number of bars in the histogram is appropriate is to show the location of the data points on the horizontal axis. We can add a ‘rug plot’ to our histogram, which marks the positions of the data with lines on the axis:\n\nhist(mtcars$mpg)\nrug(mtcars$mpg) ## Note: the 'rug' function draws on top of an existing histogram\n\n\n\n\n\n\n\n\nNow we can also see where the data fall within the bars!\nThe default settings of hist will determine the number of bars to display algorithmically, and in this case it has drawn only 5. In general, this is probably too few to show any detail, but we don’t have many data points here. Fortunately, the display of the histogram can be adjusted by a number of arguments:\n\nbreaks - allows us to control the number of bars in the histogram. breaks can take a variety of different inputs:\n\nIf breaks is set to a single number, this will be used to (suggest) the number of bars in the histogram.\nIf breaks is set to a vector, the values will be used to indicate the endpoints of the bars of the histogram.\n\nfreq - if TRUE the histogram shows the simple frequencies or counts within each bar; if FALSE then the histogram shows probability densities rather than counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the hist function to draw histograms of miles-per-gallon which match those shown above (don’t worry about the labels).\n\n\n\nClick for solution\n\n\nhist(mtcars$mpg,freq=FALSE,main='freq=FALSE')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=10,main='breaks=10')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=50,main='breaks=50')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=c(10,12,20,30,35), main=\"breaks=c(10,12,20,30,35)\")\n\n\n\n\n\n\n\n\n\n\n\n\nA five-number numerical summary can be computed with the fivenum function, which takes a vector of numbers as input. To add a little more information, the summary function includes the mean of the data for a 6-number summary.\n\n\n\nCompute summaries of the mpg data in the mtcars dataset using fivenum and summary. What is your interpretation of the result?\n\n\nClick for solution\n\n\nfivenum(mtcars$mpg)\n\n[1] 10.40 15.35 19.20 22.80 33.90\n\n\nThe values returned are the sample minimum, lower quartile, median, upper quartile and maximum. We can see that the median across all the cars in the dataset is about 20 miles per gallon. This is pretty terrible by modern standards, but the data are from 1974 and the USA.\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nThe inclusion of the mean can be quite helpful. If the data have an approximately symmetric distribution then the mean and median values should be close, which can be used as a quick check for any potential skewness in the data. Given that the mean is fairly close to the median, there doesn’t appear to be a dramatic amount of skewness in the distribution of MPG."
  },
  {
    "objectID": "Lab1_ExplContVar.html#boxplot",
    "href": "Lab1_ExplContVar.html#boxplot",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "A boxplot provides a graphical view of the median, quartiles, maximum, and minimum of a data set. In many ways, it is simply a direct visualisation of the five number summary constructed above. The whiskers (vertical lines) capture roughly 99% of a normal distribution, and observations outside this range are plotted as points representing outliers (see the figure below).\n\nBoxplots are created for single variables using the boxplot function, but can be used to easily compare many variables or groups within the data. To draw a boxplot of a single variable, multiple variables, or all variables in a data frame, we simply pass the data directly to the boxplot function:\n\nboxplot(mtcars$mpg)\n\n\n\n\n\n\n\n\nAs the boxplot is based on the simple 5-number summary, it lacks the detail of a histogram. However, we can inspect it for features such as symmetry or skewness - a symmetric distribution will give a boxplot with a whiskers of equal length, a centrally-positioned box evenly divided by the median line.\nHere, we see the box is slightly off-centre, suggesting some slight skewness. We also note that there are no obvious outliers.\n\n\nDraw a a boxplot of all the variables in mtcars by passing the entire data frame to the boxplot function. Can you see anything useful in the plot?\n\n\nClick for solution\n\n\nboxplot(mtcars)\n\n\n\n\n\n\n\n\nUnfortunately, because of the different scales of variables we can’t see what’s going on with the smaller variables.\n\nThe boxplot is most useful when comparing how a variable behaves in different groups (i.e., the levels of a categorical variable). For example, we can compare the MPG with the number of engine cylinders\n\ncyl &lt;- factor(mtcars$cyl) # make the 'cyl' variable categorical\nboxplot(mtcars$mpg ~ cyl)\n\n\n\n\n\n\n\n\nWhat do you conclude about fuel efficiency in cars with more engine cylinders?\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data."
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-2-data-analysis-of-galtons-heights",
    "href": "Lab1_ExplContVar.html#exercise-2-data-analysis-of-galtons-heights",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Francis Galton famously developed his ideas on correlation and regression using data which included the heights of parents and their children. This galton data set include data on heights for 928 children and their 205 ‘midparents’. Each ‘midparent’ height is the average of the father’s height and 1.08 times the mother’s height (to adjust for the usual gender differences). Similarly, the daughter’s heights have also been multiplied by 1.08. Note that we have one midparent height for each child, so that many midparent heights are repeated.\nThe variables are child and parent for the different heights recorded in inches.\n\n\n\nDownload the galton.Rda data set from the Ultra page and load the file\nDraw a boxplot of both variables in the data set.\nWhat features do you see? How do the heights compare in terms of location and spread?\nDoes this agree with what you expected?\nDraw and compare histograms of the two variables - can you detect any noticeable similarities or differences?\nRedraw your histograms and add a rugplot to each. Does this give you more information?\n\n\n\nClick for solution\n\n\nboxplot(galton)\n\n\n\n\n\n\n\n## parents are less spread than children, both seem to be centred around the \n## same values (about 68.5)\n## both appear symmetric\n## parent displays two outliers\n\n## expectations: similar heights? so yes.\n## not sure would we expect more variability in children than parents, though parents are an average!\n\nhist(galton$child)\nrug(galton$child)\n\n\n\n\n\n\n\nhist(galton$parent)\nrug(galton$parent)\n\n\n\n\n\n\n\n## histograms are clearly symmetric, normal distributions?\n## rugplot shows something strange - all of these data points are lining up on a few values - why?\n## lets look at the first 30 values\ngalton$child[1:30]\n\n [1] 61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 62.2 62.2 63.2 63.2 63.2\n[16] 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2\n\n## is there rounding here? all the values are at integers +0.2...\n## how many unique values are there?\nunique(galton$parent)\n\n [1] 70.5 68.5 65.5 64.5 64.0 67.5 66.5 69.5 71.5 72.5 73.0\n\n## only 11 unique values!\n\n\n\n\n\n\nUsing the par() command, draw histograms of parents and child heights side-by-side on the same plot (you may need to adjust the width of your plot window.)\nDo your plots reveal anything interesting?\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\nhist(galton$parent)\nhist(galton$child)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the difficulties of comparing multiple independent plots is we need to do more work to ensure consistency of presentation. In particular, we should ensure that our histogram intervals and axis ranges are the same for both plots, as the default presentation will change from plot to plot.\nMany high level plotting functions (plot, hist, boxplot, etc.) allow you to include additional options to customise how the plot is drawn (as well as other graphical parameters). We have seen examples of these already with the axis label arguments xlab and ylab, however we can customise the following plot features for finer control of how a plot is drawn.\nAxis limits\nTo control the ranges of the horizontal and vertical axes, we can add the xlim and ylim arguments to our original plotting function To set the horixontal axis limits, we pass a vector of two numbers to represent the lower and upper limits, xlim = c(lower, upper) specifying numerical values for upper and lower, and repeat the same for ylim to customise the vertical axis.\nAxis labels\nTo specify a label for the x- and y-axes we can supply a string to the xlab and ylab arguments. To give a plot a title, we pass the title as a string to the main argument.\nIt is easier to compare the shape of distributions in histograms when they are arranged vertically, they use equal horizontal axis limits, and the same binwidths.\n\n\n\n\nUse par to setup a column of two plots.\nPlot a histogram of child and then parent from galton using:\n\nx-axis limits of 60 to 75\nA bar width of 1 unit\nAn appropriate x-axis label and plot title\n\nDoes the comparison yield any new information that wasn’t conveyed in the boxplot?\nTry reducing your bar widths - what do you find?\n\n\n\nClick for solution\n\n\npar(mfrow=c(2,1))\nhist(galton$parent,xlim=c(60,75),breaks=60:75,xlab='Parent heights', main='')\nhist(galton$child,xlim=c(60,75),breaks=60:75,xlab='Child heights', main='')\n\n\n\n\n\n\n\n## they seem to line up nicely, with the children more spread. Much like the boxplots\n## why are the parents less variable? One set of parents has many children, so \n## there's a lot more variability from many children (between 1 and 15) there\n## due to repetition of the parent values \n## its actually a difference of the order of 1/sqrt(2), which you would expect for\n## the mean of 2 parents!\n\nIn interpreting the data, it is worth noting that:\n\nGalton obtained this data “through the offer of prizes” for the “bext Extracts from their own Family Records”, so the sample is hardly a random one\nthe data are clearly heavily rounded for tabulation\nfamily sizes vary from 1 child up to 15, so that there is a lot of repetition in the midparent heights.\n\nYou might expect that if we had the individual un-adjusted heights and the genders of the parents and children we would find that the height data distributions would be neatly bimodal with one peak for females and one for males. They are not. Apparently, height distributions are rarely like that."
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-3-data-exploration-of-the-movies-dataset",
    "href": "Lab1_ExplContVar.html#exercise-3-data-exploration-of-the-movies-dataset",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Data science inevitably involves working with large data sets. The effort involved in preparing and making a large dataset usable for analysis should not be underestimated, but thankfully we’re going to look at a dataset “prepared earlier”. This movies data set (downloadable from Ultra) is reasonably large(ish), containing 24 different attribues of 28819 movies gathered from IMDB. One of the variables is the movie length in minutes, and it is interesting to look at this variable in some detail.\n\n\n\nLoad the movies data set and draw a histogram of the data.\nPlot a histogram of the length variable.\nWhat features can you see?\nAdd a rugplot to the histogram - what problems does this highlight?\nLet’s try a boxplot of the data - do we learn anything more?\nAre there any obvious outliers?\n\n\n\nClick for solution\n\n\nhist(movies$length)\nrug(movies$length)\n\n\n\n\n\n\n\n## all the data stack up at the left end near zero, but the range of the data is huge!\nboxplot(movies$length,horizontal = TRUE)\n\n\n\n\n\n\n\n## this confirms the same as the rugplot, but there are two ridiculously large outliers\n\nClearly, our data are distorted by some particularly egregious outliers.\n\n\n\n\nLook at the outliers more closely:\n\nWhat are the lengths of the two longest movies?\nDoes that seem sensible?\nExtract the subset of the dataframe containing all the variables for both of these movies\nInspect the variable values:\nThe variables r1 to r10 give the percentage of reviews which rated the movie as a 1 up to a 10 out of 10. Are these movies particularly popular?\nWhat are the names of the movies? Do a quick Google search to see if you can find out more.\n\n\n\nClick for solution\n\nThere are various way you can do this, I chose to sort the data and use the head or tail functions to extract the end/start of the list\n\ntail(sort(movies$length))\n\n[1]  647  773  873 1100 2880 5220\n\nhead(sort(movies$length,decreasing=TRUE))\n\n[1] 5220 2880 1100  873  773  647\n\n\nso 5220 and 2880 minutes - that’s 87 and 48 hours, or 14.5 and 8 days respectively. These are obviously not ‘genuine’ movies. Although it’s tempting to dismiss these as simple errors, it is worth checking if possible.\nTo look at all the variables for these two movies, we must subset the data frame using the square brackets\n\nmovies[movies$length&gt;2000,]\n\n                                                 title year length budget\n11937                           Cure for Insomnia, The 1987   5220     NA\n30574 Longest Most Meaningless Movie in the World, The 1970   2880     NA\n      rating votes   r1  r2  r3  r4 r5 r6 r7  r8  r9  r10 mpaa Action Animation\n11937    3.8    59 44.5 4.5 4.5 4.5  0  0  0 4.5 4.5 44.5           0         0\n30574    6.4    15 44.5 0.0 0.0 0.0  0  0  0 0.0 0.0 64.5           0         0\n      Comedy Drama Documentary Romance Short\n11937      0     0           0       0     0\n30574      0     0           0       0     0\n\n\nThe movies seem to be somewhat polarising, either rated 0 or 10, but only by a small number of reviews! Incidentally, this data set is no longer up-to-date and there are some even longer films now (though it’s a mystery why.)\n\nIn this case, the extreme outliers should be ignored, and for exploring the main distribution of movie lengths it makes sense to set some kind of upper limit. Over 99% of the data are less than three hours in length, so let’s restrict ourselve to those.\n\n\n\n\nExtract the all the movies of length at most three hours.\nDraw a histogram - what do you find?\n\n\n\nClick for solution\n\n\nmovies2 &lt;- movies[movies$length&lt;180,]\nhist(movies2$length)\n\n\n\n\n\n\n\n\nAt last, we see some structure! There are two clear peaks here (bi-modality): the first under 20 minutes, the second in [80,100] minutes. The latter group seems about right for the ‘average’ movie.\n\nUseful context:\n\nThe Oscars define a “short film” as anything under 40 minutes.\nAnimated shorts are typically between 5 and 8 minutes long, and are counted as individual movies (so, e.g., every ‘Tom and Jerry’ cartoon has its own entry).\n\n\n\n\n\nRedraw your histogram using a bin-width of 1 minute (you may need to enlarge your plot window).\nWhat do you see? How does the information above help explain the data?\nIs there any heaping in the data? At what values?\n\n\n\nClick for solution\n\nTo get a 1-minute bin width, I made a sequence of integers from 0 to 180 as my breakpoints. If you’re not familiar with this, look at the help for the ‘seq’ function in creating similar sequences.\n\nhist(movies2$length,breaks=0:180)\n\n\n\n\n\n\n\n\nThere’s a lot going on here! * there are still a fair few longer films over 2hrs, but its a minority * the clump of short movies correspond to the defined ‘short film’ and have a clear peak around 7mins. Coincidentally, most short cartoons are 6-8 mins long. * A big pronounced spike in values at exactly 90 minutes. This probably isn’t rounding, but rather that when making and editing movies they will have aimed to produce a film of that length. Perhaps we might have expected an ever sharper peak? * We also see some stacking around this peak, with certain lengths being favoured at 80, 85, 95, 100, etc"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-4-using-colour",
    "href": "Lab1_ExplContVar.html#exercise-4-using-colour",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Using colour in a plot can be very effective, for example to highlight different groups within the data. Colour is adjusted by setting the col optional arugment to the plotting function, and what R does with that information depends on the value we supply.\n\ncol is assigned a single value: all points on a scatterplot, all bars of a histogram, all boxplots are coloured with the new colour\ncol is a vector:\n\nin a scatterplot, if col is a vector of the same length as the number of data points then each data point is coloured individually\nin a histogram, if col is a vector of the same length as the number of bars then each bar is coloured individually\nin a boxplot, if col is a vector of the same length as the number of boxplots then each boxplot is coloured individually\nif the vector is not of the correct length, it will be replicated until it is and the above rules apply\n\n\nNow that we know how the col argument works, we need to know how to specify colours. Again, there are a number of ways and you can mix and match as appropriate\n\nIntegers: The integers 1:8 are interpreted as colours (black, red, green, blue, …) and can be used as a quick shorthand for a common colour. Type palette() to see the sequence of colours R uses.\nNames: R recognises over 650 named colours is specified as text, e.g.\"steelblue\", \"darkorange\". You can see the list of recognised names by typing colors(), and a document showing the actual colors is available here\nHexadecimal: R can recognise colours specified as hexadecimal RGB codes (as used in HTML etc), so pure red can be specified as \"#ff0000\" and cyan as \"#00ffff\".\nColour functions: R has a number of functions that will generate a number of colours for use in plotting. These functions include rainbow, heat.colors, and terrain.colors and all take the number of desired colours as argument.\n\n\n## Colour example\n## 3 plots in one row\npar(mfrow=c(1,3))\n## colour the cars data by number of gears\nplot(x=mtcars$wt, y=mtcars$mpg, col=mtcars$gear, xlab=\"Weight\", ylab=\"MPG\", \n     main=\"MPG vs Weight\")\n## manually colour boxplots\nboxplot(mpg~cyl, data=mtcars, col=c(\"orange\",\"violet\",\"steelblue3\"),\n        main=\"Car Milage Data\", xlab=\"Number of Cylinders\", \n        ylab=\"Miles Per Gallon\")\n## use a colour function to shade histogram bars\nhist(mtcars$mpg,col=rainbow(5),main='MPG')\n\n\n\n\n\n\n\n\n\n\n\nShow the histograms of length of short movies next to that for ‘non-short’ movies, using a different colour for each histogram.\nExperiment with using the col argument to add colour to your histograms.\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\n## again, we can use subsetting to select a subset of the data. Here we don't need a ',' as we're subsetting a vector instead of a matrix \nhist(movies2$length[movies2$length&lt;=40],col='royalblue',xlab='length',main='Short films')\nhist(movies2$length[movies2$length&gt;40],col='tomato',xlab='length', main='Regular films')"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-5-optional-german-opinion-polls",
    "href": "Lab1_ExplContVar.html#exercise-5-optional-german-opinion-polls",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: bundestag\nThese data contain the results of the 2009 elections for the German Bundestag, the first chamber of the German parliament. The contains the number of votes cast for the various political parties, for each state (“Bundesland”). Amongst the German political parties there are two on the left of the political spectrum, the SPD - similar to the UK’s Labour party - and Die Linke (“The Left”), a party even further to the left. Suppose we’re interested in the support for this “Die Linke” party.\n\n\n\nExplore the election support for ‘Die Linke’ party by examining the LINKE1 variable using. Compare the outputs and explain the difference you find:\n\nA histogram, with a rugplot\nA stacked dotplot\nA beewswarm plot, setting horizontal=TRUE\n\n\n\n\nClick for solution\n\n\nhist(bundestag$LINKE1)\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## clear concentration of points around 10k. let's try narrower bins for more detail\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=2000))\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## that's a bit more useful\n\n\nstripchart(bundestag$LINKE1,method='stack',pch=16)\n\n\n\n\n\n\n\n## this looks mostly like the rugplot information on the plot above, but even with stacking this doesn't resemble the histogram\n## Why not? The histogram is grouping nearby points into its bars, but the dotplot/stripchart does not and is treating each unique value as a separate point.\n\n\nlibrary(beeswarm)\nbeeswarm(bundestag$LINKE1,horiz=TRUE)\n\n\n\n\n\n\n\n## the shape of the beewswarm more closely resembles the histogram, albeit rather than stacking points up from the bottom it works from the middle out towards the edges\n\n\nA stem and leaf plot is a technique for displaying the data in a similar fashion to a histogram, while preserving the information ofthe individual numerical values. Where the histogram summarises the data by the counts in its various intervals, the stem and leaf plot retains the original data values up to two significant figures.\n\n\n\n\nDraw a stem and leaf plot of the German election support for ‘Die Linke’ data using the stem() function.\nHow does the stem and leaf plot represent these data? You may want to look at the data value to help understand.\nNow draw a histogram, adjusting the histogram to have axis range and bar width to match the stem and leaf plot.\n\n\n\nClick for solution\n\n\nstem(bundestag$LINKE1)\n\n\n  The decimal point is 4 digit(s) to the right of the |\n\n  0 | \n  0 | 55566666667777777777777888888888888888888888888888888888999999999999+14\n  1 | 00000000000000000000000000000000000001111111111111111111111111111111+51\n  1 | 5555566\n  2 | 122234\n  2 | 588889\n  3 | 01223333344444\n  3 | 556666677777788\n  4 | 0000011233444\n  4 | 6677899\n  5 | \n  5 | \n  6 | 0233\n\n## This is effectively a histogram with bins of width 5000 units. The number indicate the actual data values, with the \"stem\" being the leading digit to the left of the | symbol, and the values of the next digits of the data being indicated to the right of the |\n## We can see some structure now *within* the bars, which is a bit more detail than we get from the histogram.\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=5000))\n\n\n\n\n\n\n\n\nAs with the beeswarm plot, the stem and leaf plot is only suitable for relatively modestly sized data sets due to the fact it is literally writing out all of the data values on the screen!"
  },
  {
    "objectID": "Lab1_ExplContVar.html#stripplots-or-stripcharts",
    "href": "Lab1_ExplContVar.html#stripplots-or-stripcharts",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "‘Stripplots’ or ‘Stripcharts’ are very similar to the rugplot we applied to our histograms, and display the individual data points along a single axis. They can be used in much the same way as a boxplot, but rather than showing the data summaries they display everything!\nThe built-in faithful data set contains measurements on the waiting times between the eruptions of the Old Faithful geyser.\n\ndata(faithful)\nstripchart(faithful$waiting,ylab='Waiting Time', pch=16) \n\n\n\n\n\n\n\n\nPlotting symbols\nThe symbols used for points in plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is usually pch=1 which is a hollow circle, in the plot above we changed it to 15 which is a filled circle.\nHowever, when we have a lot of data points concentrated in a small interval, the stripplot suffers from problems of ‘overplotting’ where many points with similar values are drawn on top of each other.\nA partial solution to this is to add random noise (known as ‘jittering’) to spread out the points.\n\nMake a stripplot of the movie length data - how does the overplotting problem manifest here? You may want to compare to your histogram.\n\nA better solution is to stack the dots that fall close together, producing an alternative plot to a histogram - sometimes called a ‘dotplot’ or ‘stacked dotplot’\n\nstripchart(faithful$waiting, method='stack',pch=16)\n\n\n\n\n\n\n\n\n\nTry this out with the movies data."
  },
  {
    "objectID": "Lab1_ExplContVar.html#beeswarm-plots",
    "href": "Lab1_ExplContVar.html#beeswarm-plots",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "An evolution of the stripplot is the ‘beeswarm’ plot, available from the beeswarm package. A bee swarm plot is similar to stripplot, but with various methods to separate nearby points such that each point is visible.\n\nlibrary(beeswarm)\nbeeswarm(faithful$waiting)\n\n\n\n\n\n\n\n\nOne limitation of the beeswarm plot is that the computations to arrange all the points do not scale well with large data sets. Do not try this with the movies data, or you will be waiting for a very long time!"
  },
  {
    "objectID": "Lab1_ExplContVar.html#stem-and-leaf-plots",
    "href": "Lab1_ExplContVar.html#stem-and-leaf-plots",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "To see how this works, let’s look at the Old Faithful data, sorted from smallest to largest.\n\nsort(faithful$waiting)\n\n  [1] 43 45 45 45 46 46 46 46 46 47 47 47 47 48 48 48 49 49 49 49 49 50 50 50 50\n [26] 50 51 51 51 51 51 51 52 52 52 52 52 53 53 53 53 53 53 53 54 54 54 54 54 54\n [51] 54 54 54 55 55 55 55 55 55 56 56 56 56 57 57 57 58 58 58 58 59 59 59 59 59\n [76] 59 59 60 60 60 60 60 60 62 62 62 62 63 63 63 64 64 64 64 65 65 65 66 66 67\n[101] 68 69 69 70 70 70 70 71 71 71 71 71 72 73 73 73 73 73 73 73 74 74 74 74 74\n[126] 74 75 75 75 75 75 75 75 75 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77\n[151] 77 77 77 77 77 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79\n[176] 79 79 79 79 79 80 80 80 80 80 80 80 80 81 81 81 81 81 81 81 81 81 81 81 81\n[201] 81 82 82 82 82 82 82 82 82 82 82 82 82 83 83 83 83 83 83 83 83 83 83 83 83\n[226] 83 83 84 84 84 84 84 84 84 84 84 84 85 85 85 85 85 85 86 86 86 86 86 86 87\n[251] 87 88 88 88 88 88 88 89 89 89 90 90 90 90 90 90 91 92 93 93 94 96\n\n\nNote that the smallest value is 43, followed by three values of 45. A stem and leaf plot of these data looks like this\n\nstem(faithful$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 3\n  4 | 55566666777788899999\n  5 | 00000111111222223333333444444444\n  5 | 555555666677788889999999\n  6 | 00000022223334444\n  6 | 555667899\n  7 | 00001111123333333444444\n  7 | 555555556666666667777777777778888888888888889999999999\n  8 | 000000001111111111111222222222222333333333333334444444444\n  8 | 55555566666677888888999\n  9 | 00000012334\n  9 | 6\n\n\nEach row of this plot is called a ‘stem’ and the values to the right of the ‘|’ symbol are the leaves. Be sure to read where R places the decimal point for the output. For this result, the decimal is placed one digit to the right of the vertical bar. Thus, the first row of the table then consists of data values of the form \\(4x\\), and the only leaf is a \\(3\\) corresponding to the value \\(43\\) in the data. The next stem groups the values \\(45-49\\), and we notice the three observations of \\(45\\) are represented by the \\(555\\) at the start of the second stem.\nNotice that each stem part is representing an interval of width 5, much like a histogram. As usual, R figures out how best to increment the stem part unless you specify otherwise. Finally, notice how the shape of the stem and leaf plot mirrors that of a histogram with interval width 5 - the only difference is that here we can see the values inside the bars."
  },
  {
    "objectID": "Lab1_ExplContVar.html#student-survey",
    "href": "Lab1_ExplContVar.html#student-survey",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "The data come from an old survey of 237 students taking their first statistics course. The dataset is called survey in the package MASS.\n\n\n\nLoad the data with data(survey, package='MASS')\nDraw a histogram of student heights - do you see evidence of bimodality?\nExperiment with different binwidths for the histogram. Which choice do you think is the best for conveying the information in the data?\nCompare male and femal heights using separate histograms with a common scale and binwidths."
  },
  {
    "objectID": "Lab1_ExplContVar.html#diamonds",
    "href": "Lab1_ExplContVar.html#diamonds",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: diamonds\nThe set diamonds includes information on the weight in carats (carat) and price of 53,940 diamonds.\n\n\n\nIs there anything unusual about the distribution of diamond weights? Which plot do you think shows it best? How might you explain the pattern you find?\nWhat about the distribution of prices? With a bit of detective work you ought to be able to uncover at least one unexpected feature. How you discover it, whether with a histogram, a dotplot, or whatever, is unimportant, the important thing is to find it. Having found it, what plot would you draw to present your results to someone else? Can you think of an explanation for the feature?"
  },
  {
    "objectID": "Lab1_ExplContVar.html#zuni-educational-funding",
    "href": "Lab1_ExplContVar.html#zuni-educational-funding",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: zuni\nThe zuni dataset seems quite simple. There are three pieces of information about each of 89 school districts in the US State of New Mexico: the name of the district, the average revenue per pupil in dollars, and the number of pupils. The apparent simplicity hides an interesting story. The data were used to determine how to allocate substantial amounts of money and there were intense legal disgreements about how the law should be interpreted and how the data should be used. Gastwirth was heavily involved and has written informatively about the case from a statistical point of view Gastwirth, 2006 and Gastwirth, 2008.\nOne statistical issue was the rule that before determining whether district revenues were sufficiently equal, the largest and smallest 5% of the data should first be deleted.\n\n\n\nAre the lowest and highest 5% of the revenue values extreme? Do you prefer a histogram or boxplot for showing this?\nRemove the lowest and highest 5% of the cases, draw a plot of the remaining data and discuss whether the resulting distribution looks symmetric.\nDraw a Normal quantile plot of the data after removal of the 5% at each end and comment on whether you would regard the remaining distribution as normal."
  },
  {
    "objectID": "Lab1_ExplContVar.html#pollutants-from-engines",
    "href": "Lab1_ExplContVar.html#pollutants-from-engines",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: engine\nThese data record the amounts of three pollutants - carbon monoxide CO, hydrocarbons HC, and nitrogen oxide NO - in grammes emitted per mile by 46 light-duty engines.\n\n\n\nAre the distributions for the three pollutants similar? To make this an easier question to answer, try to produce histograms of the variables, using the same class intervals and range for the horizontal axis in each case."
  },
  {
    "objectID": "Lab1_ExplContVar.html#dosage-of-chlorpheniramine-maleate",
    "href": "Lab1_ExplContVar.html#dosage-of-chlorpheniramine-maleate",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: chlorph\nThese data come from a semi-automated process for measuring the actual amount of chlorpheniramine maleate in tablets which are supposed to contain a 4mg dose.\nThe tablets used for the study were made by two different manufacturers. For each manufacturer, a composite was produced by grinding together a number of tablets. Each composite was split into seven pieces each of the same weight as a tablet and the pieces were sent to seven different laboratories. Each laboratory made 10 separate measurements on each composite.\nThe data contain three variables: * chlorpheniramine - the amount measured * manufacturer - the tablet manufacturer as a factor (A or B) * laboratory - the laboratory which performed the measurement as a factor (1 to 7)\n\n\n\nProduce box-plots of the chlorpheniramine measurements split by laboratory. What, if anything, do they suggest?\nNow produce box-plots by manufacturer. Anything noticeable?\nThe problem here is that we really need a separate box-plot for each combination of manufacturer and laboratory. We can do this by boxplot(chlorpheniramine~laboratory*manufacturer, data=chlorph) Try this (or some abbreviated version of it).\nTry reversing the order of laboratory and manufacturer. Which way round is better? Try colouring the box-plots by manufacturer. Does that help?\nDo the data suggest that the two manufacturers actually put different amounts of the drug into supposed 4mg tablets?\nAre there obvious differences between different laboratories? If so, what kind of differences do you observe?\nIf you had to choose a single laboratory to make some measurements for you, which would you choose and why?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DEVUL 2025/26",
    "section": "",
    "text": "This website contains the HTML version of lecture notes for the module in 2025/26. Use the left bar to navigate through the different lectures and the right bar to navigate through different sections in the same lecture.\nThe content of these notes integrates the material of previous lecturers of this module (Jonathan Cummings, Emmanuel Ogundimu and Hyeyoung Maeng). Without their input, the overall quality of the material would be much worse. It shall be noted however that any mistake in the notes is my responsibility: if you spot one, please let me know at daniele.turchetti@durham.ac.uk.\n\n\n\n\n\nLecture 1a\nLecture 1b\nPractical 1\nWorkshop 1\n\n\n\n\n\nLecture 2a\nLecture 2b\nPractical 2\nWorkshop 2\n\n\n\n\n\nLecture 3a\nLecture 3b\nPractical 3\nWorkshop 3\n\n\n\n\n\nLecture 4a\nLecture 4b\nPractical 4\nWorkshop 4\n\n\n\n\n\nLecture 5a\nLecture 5b\nPractical 5\nWorkshop 5"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "DEVUL 2025/26",
    "section": "",
    "text": "Lecture 1a\nLecture 1b\nPractical 1\nWorkshop 1\n\n\n\n\n\nLecture 2a\nLecture 2b\nPractical 2\nWorkshop 2\n\n\n\n\n\nLecture 3a\nLecture 3b\nPractical 3\nWorkshop 3\n\n\n\n\n\nLecture 4a\nLecture 4b\nPractical 4\nWorkshop 4\n\n\n\n\n\nLecture 5a\nLecture 5b\nPractical 5\nWorkshop 5"
  },
  {
    "objectID": "Workshop1.html",
    "href": "Workshop1.html",
    "title": "Workshop 1 - Challenge 1",
    "section": "",
    "text": "For this challenge you are going to work with the Marriage data set:\nload('Marriage.Rda')\nAmong the categorical variables in this data set, there is one called sign\nMarriage$sign\n\n\nAriesLeoPiscesGeminiSaggitariusPiscesLibraAquariusSaggitariusCancerAquariusScorpioVirgoLibraScorpioSaggitariusAquariusTaurusPiscesTaurusAriesSaggitariusLibraCapricornSaggitariusLeoLibraLibraAriesPiscesPiscesVirgoAriesGeminiAquariusSaggitariusGeminiPiscesGeminiVirgoPiscesPiscesGeminiVirgoPiscesVirgoGeminiCancerSaggitariusPiscesTaurusPiscesTaurusCapricornScorpioSaggitariusVirgoAriesLeoAriesTaurusTaurusAquariusCancerPiscesAriesAriesScorpioCancerVirgoLibraGeminiAriesPiscesLibraLeoCancerSaggitariusScorpioScorpioAriesPiscesCancerPiscesLeoLeoScorpioPiscesAquariusVirgoCancerLeoVirgoGeminiCancerAquariusGeminiVirgo\n\n\n    \n        Levels:\n    \n    \n    'Aquarius''Aries''Cancer''Capricorn''Gemini''Leo''Libra''Pisces''Saggitarius''Scorpio''Taurus''Virgo'\n1. Create a barchart of the counts for the categorical variable sign\n2. Create a piechart of the counts for the categorical variable sign\n3. Use the par() command to visualise the barchart and the piechart side by side"
  },
  {
    "objectID": "Workshop1.html#workshop-1---challenge-2",
    "href": "Workshop1.html#workshop-1---challenge-2",
    "title": "Workshop 1 - Challenge 1",
    "section": "Workshop 1 - Challenge 2",
    "text": "Workshop 1 - Challenge 2\nIn this challenge, you’ll work with the British election survey data set.\n\nload('beps.Rda')\n\nWe want to assess whether the attitude towards Europe changes along main party lines or not. Before doing anything - think a little about what you expect to see and write it down in the following box:\n 1. Extract the data corresponding to those individuals who intend to vote for Labour, and plot their attitudes to Europe. Add a main plot label to indicate the party.\n 2. Do the same for LibDem and Conservatives and show the three plots side by side using different colours."
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html",
    "href": "Workshop1_CategoricalVariables.html",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "",
    "text": "Extracting various types of subsets of a given data set\n\n\nDrawing simple barplots using barplot to show the distribution of a categorical variable\n\n\nUsing pie to draw pie charts to represent proportions"
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html#pie-charts",
    "href": "Workshop1_CategoricalVariables.html#pie-charts",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "Pie charts",
    "text": "Pie charts\nThe many issues with pie charts notwithstanding, generating a pie chart is relatively easy with the function pie. Note that the pie chart emphasises showing the data as proportions of the whole, rather than separate counts.\npie takes the same data input as the barplot, and can be supplied with labels, and given custom colours for the wedges. We’ll stick with the defaults for now, but feel free to experiment.\n\npie(offs)\n\n\n\n\n\n\n\n\nIf your goal is compare each category with the the whole (e.g., what portion of weddings are officiated by a Bishop compared to all participants), and the number of categories is small, then pie charts may work for you. However, the best alternative to a piechart is the barchart, but if you really want to show proportions of a whole via area, then a treemap is a better choice (see below).\n\n\nChallenge\n\nThis is your first data visualisation challenge. To start it simple, we’ll just do a recap of bar charts and pie charts. On your assignment on JupyterHub you will\n\nCreate a barchart of the counts for the categorical variable sign in the Marriage dataset.\nCreate a piechart of the same counts.\nUse the par() command to visualise the barchart and the piechart side by side.\n\n\n\nA refresher on how the par() command works\n\nR makes it easy to combine multiple plots into one overall graph, using either the par or layout functions.\nWith the par function, we specify the argument mfrow=c(nr, nc) to split the plot window into a grid of nr x nc plots that are filled in by row.\nNote: if you don’t want to arrange plots in a simple regular grid and have something more complex in mind, you can use the layout function.\n\n\n\n\nSolution of the Challenge\n\n\ndatacount &lt;- xtabs(~sign, data=Marriage) ## save the table to `offs`\nbarplot(datacount, las=2, col='red')\n\n\n\n\n\n\n\n\n\npie(datacount)\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nbarplot(datacount, las=2, col='red')\npie(datacount)"
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html#data-analysis-british-election-survey-data",
    "href": "Workshop1_CategoricalVariables.html#data-analysis-british-election-survey-data",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "Data analysis: British Election survey data",
    "text": "Data analysis: British Election survey data\nIn many surveys there is a string of questions to which respondents give answers on integer scales from, say, 1 to 5. This is so commonplace that one such scale even has a name - the Likert scale.\nThe beps dataset includes seven questions put to 1525 voters in the British Election Panel Study for 1997-2001. One question was on a scale from 1 to 11, one from 0 to 3, and the rest were from 1 to 5.\nThe leaders of the main political parties at the time were Tony Blair for Labour (the Prime Minister at the time), William Hague for the Conservatives, and Charles Kennedy for the Liberal Democrats. Each surveyed individual assessed each party leader on a scale of 1 to 5 (5=best).\n\nLoad the beps data you can find on Ultra. Use the head function to get a quick look at the first few rows of the data to see what the variables are and how they are represented.\nLet’s begin with the party leader data:\n\nSplit your plot display to show a single row of three plots.\nDraw a barplot of each of the party leader’s ratings as contained in the three variables Blair, Hague and Kennedy. Don’t forget to call the xtabs function to summarise the data before plotting.\nColour your barplots by the corresponding party colours (‘red’, ‘blue’, and ‘orange’ respectively.).\n\nCan you see any similarities in the distributions of the assessments of Blair and Hague? How would you interpret these patterns?\nWhat do you find about the assessments of Kennedy?\nRepeat the plots using pie charts - which plots do you find easier to read and interpret?\n\n\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\npar(mfrow=c(1,3))\nbarplot(xtabs(~Blair,data=beps),col='red')\nbarplot(xtabs(~Hague,data=beps),col='blue')\nbarplot(xtabs(~Kennedy,data=beps),col='orange')\n\n\n\n\n\n\n\n## Blair and Hague are very polarised - either v popular or v not, rarely in the middle at '3'\n## Kennedy is less polarised, more middle/positive\n\n\npar(mfrow=c(1,3))\npie(xtabs(~Blair,data=beps))\npie(xtabs(~Hague,data=beps))\npie(xtabs(~Kennedy,data=beps))\n\n\n\n\n\n\n\n# pie give us information on the breakdown of the ratings, but not super helpful when it comes to comparisons\n\nIn addition to assessing the party leaders, two further variables concerned Europe. The first asked the respondents to quantify their knowledge of the parties’ policies on European integration from low knowledge (0) to high knowledge (3), and the second measures the individuals attitudes to European integration from 1 to 11, where higher values are more Eurosceptic.\n\nFirst, we investigate the respondents knowledge of the parties’ policies on Europe as contained in the political.knowledge variable.\nThen we investigate the Europe variable representing individual attitudes towards Europe. Do any features stand out?\nThe data set also contains a column representing the voting intentions of the individuals surveyed in the vote variable. We are going to plot the voting intentions.\n\n\n## Political knowledge of party's European policy\nbarplot(xtabs(~political.knowledge,data=beps),col='forestgreen')\n\n\n\n\n\n\n\n## many 0s and 2s - again, quite split between 'none' and 'some' knowledge of European policy\n\n\n## Attitudes towards European integration\nbarplot(xtabs(~Europe,data=beps),col=c('thistle'))\n\n\n\n\n\n\n\n## Note: 1=pro europe, 11=europsceptic.\n## BIG spike on 11 so strong Eurosceptic sentiment in the sample, with a lesser spike at a neutral position of 6.\n## Noticeably no strong pro-European sentiment at the time\n\n\n## Voting intentions\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'))\n\n\n\n\n\n\n\n## strong Labour support - not surprising as Blair is during his first term as PM\npie(xtabs(~vote,data=beps),col=c('blue','red','orange'))\n\n\n\n\n\n\n\n## this is probably a good use of a pie chart, with few categories and an easy obvious comparison\n\n\n\nChallenge\n\n\nNow, let’s dig a little deeper and think about how attitudes to Europe may differ between the different party voters:\n\nBefore doing anything - think a little about what you expect to see.\nNow, split the plot window into three, extract the data corresponding to those individuals who intend to vote for Labour, and plot their attitudes to Europe, using party colours as above. Add a main plot label to indicate the party.\nRepeat for Conservative and Liberal Democrat voters.\nWhat features do you see? Are there any surprises, or does this confirm what you expected?\n\n\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,3))\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Labour\",]),col='red')\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Conservative\",]),col='blue')\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Liberal Democrat\",]),col='orange')\n\n\n\n\n\n\n\n## Conservatives are very eurosceptic, and almost never pro-Europe\n## Labour is a mix of either slightly pro/neutral, and strongly against\n## LibDem is also similar to Labour - though surprisingly (given recent years) less pro-European"
  },
  {
    "objectID": "Lecture1b_Variables.html",
    "href": "Lecture1b_Variables.html",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "Data is organised into variables, representing attributes or measurements - e.g. age, weight, income, temperature, time, etc.\nTo begin with, we’ll focus on using standard techniques to explore a single variable at a time. Specifically:\n\nExploring Continuous Variables - using statistical summaries, box plots, histograms, and quantile plots\nExploring Categorical Variables - using bar plots, pie charts, and stacked bars.\n\nThese techniques are quite simple, so our focus is on using them effectively to learn about data features and how to do so using R.\nThere are a number of Graphics packages in R that we could use to visualise our data:\n\nBase R covers most standard statistical visualisations\nggplot2 - GGPlot and related packages provide more modern graphics, but it has unusual syntax that can be more difficult to learn\nplotly - similar to ggplot. However, a bit easier to use\nOther custom packages will provide support for specific visualisations\n\nWe will focus on the base R functions, as those always available. However, you should experiment with the other packages, but be aware they work differently.\n\n\n\nWe focus first on quantitative data that is continuous (i.e. real-valued).\nWe view our data as a sample from an underlying continuous distribution - the goal of our explorations here is to seek some clues about the features of that distribution.\nMany possible ways to explore this - graphically, we will focus on histograms and boxplots.\nWhen we have a particular type of distribution in mind, we can also draw a quantile plot to see how plausible it is.\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. the birthweight of babies isn’t.\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\nA histogram is an approximate representation of the distribution of continuous data where we divide the range of values into bins (or buckets) and draw a bar over each bin with area proportional to the frequency of cases in each bin.\nA histogram is an effective tool for visualising features relating to the shape of the data distribution.\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n\nThis data set contains the various information on housing values and related quantities for the 506 suburban areas in Boston. The main interest is in the `median values of owner-occupied homes’, but there are 14 variables to explore here.\nLooking first at the median housing value, we can produce the histogram below. Some obvious features of note are:\n\nA concentration of housing values around 20-25, then a sudden drop-off – is there an explanation for this, such as changes in tax levels?\nPossible multi-modality, with modes around 25 and 30 – are there two classes or groups of housing?\nA further spike in values occur in the upper tail of the data, at 50 – this seems dubious, and could be some crude rounding or grouping of all values ‘’50 and over’’.\n\n\n\n\n\n\n\n\n\n\nChoosing the width of the bars can substantially affect the detail of the histogram. If we choose too few bins, then we can obscure key features by over-smoothing the data. Alterntaively, if we have too many bins then we can introduce too much noise to the plot that it obscures more general features and patterns. Unfortunately, the only way to find a good compromise is to experiment – R and other software will default to a ‘best guess’, but this invariably needs adjusting. The histograms below show the same data, but using bar widths of 5, 2.5, and 1 unit respectively. We’ll see more sophisticated methods for smoothing the data later.\n\n\n\n\n\n\n\n\n\nWith 14 variables in the data set, we could inspect the histograms of each of the variables. We could do this one-at-a-time, or arrange them in a grid or matrix as below:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe other variables in the data set show a variety of the features discussed above that we may want to investigate further:\n\nThe ‘age’ variable - in position \\((1,1)\\) - shows obvious left-skewness.\nThe ‘dis’ variable - in position \\((1,2)\\) - shows right skewness\nThe ‘rm’ variable - in position \\((3,4)\\) - looks symmetric, and possible approximate Normality\nIn the variables in the bottom two rows, we see a lot of gaps in the data and heaping on particular values.\nThe ‘chas’ variable - in position \\((1,3)\\) - exhibits heaping on only two values, 0 and 1. This is a discrete binary variable, not continuous a continuous one! We should use different methods to explore this variable.\n\nIn a full analysis, we would want to look at the relationships between the variables to determine how the features observed relate to each other. We’ll return to this later…\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe geyser data set contains 272 observations of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The variables are:\n\nduration - Length of eruption in mins\nwaiting - Waiting time to next eruption\n\nTo draw a histogram, we use the hist function:\n\nhist(geyser$waiting)\n\n\n\n\n\n\n\n\nHere we can see the data appear to come in two groups: one group with smaller values (shorter waiting times), and one group of larger (longer waiting times).\nWe can add a rug plot to an existing histogram to add more detail and to show where the individual data values fall inside the bars:\n\nhist(geyser$waiting)\nrug(geyser$waiting)\n\n\n\n\n\n\n\n\nNow, each individual mark on the horizontal axis shows where we have observed a data value.\nHistograms can be customised in many ways, but the most important one is changing the configuration of the bars drawn. This is controlled by the breaks parameter. We can set breaks to a number to indicate an approximate number of bars to draw:\n\nhist(geyser$waiting,breaks=20)\n\n\n\n\n\n\n\n\nOr we can be more specific and state where each individual bar should begin and end by listing the breakpoints of the bars as a vector:\n\nhist(faithful$waiting, breaks=c(30,45,47,50,52,55,60,75,80,85,95,105))\n\n\n\n\n\n\n\n\nAdditionally, almost all plot functions can take the following arguments to customise the plot:\n\nxlab, ylab - sets the x and y axis labels\nmain - sets the main title\nxlim, ylim - set x and y axis limits, e.g. c(0,10)\ncol - sets the plot colour(s)\n\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. zero-inflation\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\n\n\n\nWe have considered the shape of a distribution already, and have seen that histograms are a handy way of exploring shape.\nWe come now to several summary statistics (recall ISDS):\n\nthe average and median summarise the centre or location of the distribution.\nthe standard deviation and inter-quartile range summarise spread around the centre of the distribution.\n\nThe average is the most (over-)used statistic. It is useful especially where distributions are reasonably symmetric.\nFor distributions with long tails, it can give misleading signals, e.g. average household income will include households like Buckingham palace which distort the average.\nThe median is a measure of the midpoint of a distribution, and, unlike the mean, it is quite resistant to extreme values.\nA robust summary of spread is given by the interquartile range (IQR), the difference between the first and third quartiles. Every distribution has three quartiles:\n\nThe first quartile, \\(Q_1\\), is a number such that 25% of the values in the distribution do not exceed this number. This is called a lower quartile.\nThe second quartile, \\(Q_2\\), is a number such that 50% of the values in the distribution do not exceed this number. This is the same as the median.\nThe third quartile, \\(Q_3\\), is a number such that 75% of the values in the distribution do not exceed this number. This is called an upper quartile.\n\nTukey suggested that we combine the three quartiles with the minimum and maximum values in the data to form a five-number summary.\n\nIn R, the fivenum function gives the standard 5-number summary:\n\nfivenum(geyser$waiting)\n\n[1]  43  59  76  83 108\n\n\nThe summary function adds a 6th number (the mean):\n\nsummary(geyser$waiting)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  43.00   59.00   76.00   72.31   83.00  108.00 \n\n\nMin. 1st Qu. Median Mean 3rd Qu. Max. 43.0 58.0 76.0 70.9 82.0 96.0\n\n\n\n\nA boxplot (or box-and-whisker plot) is constructed from the 5-number summary by: * Drawing a box with lower boundary at \\(Q_1\\) and upper boundary \\(Q_3\\). * Drawing a line inside the box at the median (\\(Q_2\\)). * Drawing lines (whiskers) from the edges of the box to the most extreme data point that is within \\(1.5\\times IQR\\) of the edge of the box (often the minimum and maximum values).\n\nWe can draw a boxplot by using the boxplot function on a vector of data values:\n\nboxplot(geyser$waiting)\n\n\n\n\n\n\n\n\nOr we can pass all the columns of a data set to boxplot to draw everything at once:\n\nboxplot(geyser)\n\n\n\n\n\n\n\n\nNow, all variables are shown together on a common axis scale. This can be useful if all the variables take values of a similar size, but - as we see here - when the variables are quite different it can obscure the features of some variables by drawing everything together.\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data. Though usually, you have the same number of data points in each column of the data set so this is not often very helpful.\n\n\n\nTo show the relationship between a histogram and a boxplot, we have a data set comprising the length (mm) of 100 cuckoo eggs. Drawng the histogram and boxplot together, we can see that the histogram gives far more detail on the shape of the distribution and the boxplot is more of a summary visualisation. The median is indicated in red and the upper/lower quartiles in green, which shows how these quantities align between the plots. Note that the smallest value in the data is flagged as an outlier and drawn separately as a circle on the boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValues too far from the centre of the distribution are known as outliers.\nData values further than \\(1.5\\times IQR\\) from a lower/upper quartile are called an outlier, and usually indicated with a circle or point.\nAnything beyond \\(3\\times IQR\\) is an extreme outlier, and usually indicated with a star or asterisk.\nThis definition of outliers is a simple rule-of-thumb. It leads to about 0.7% of (normally distributed) observations being described as extreme for large samples.\nHowever, if you have a million data points then 0.7% of your data is still a lot! This doesn’t necessarily mean those points are all genuinely extreme or outliers.\n\n\n\nOne advantage of the boxplot, is that as it is a simple summary plot it is much easier to use boxplots to compare many variables at once. For example, the data plotted below are boxplots of the heights in inches of the singers in the New York Choral Society in 1979. The data are grouped according to the voice part they play in the choir. The vocal range for each voice part increases in pitch according to the following order: Bass 2, Bass 1, Tenor 2, Tenor 1, Alto 2, Alto 1, Soprano 2, Soprano 1. We can see immediately that the lowest pitch voices are associated with the taller singers and the higher pitches with smaller singers, which makes a lot of intuitive sense. There will also be a strong correspondance with Gender here too, though that information is not recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should examine a single boxplot for the following features:\n\nIs the median line in the centre of the 50% portion of the distribution? If not, some degree of skew or asymmetry is indicated.\nAre the whiskers the same length? If not, some degree of skew or asymmetry is indicated.\nAre there any outliers?\n\nWe should examine several boxplots for the following features:\n\nDo the groups have similar shapes? Some might seem symmetric, some skewed.\nAre the median lines about the same, or do there seem to be differences in location** between groups?\nAre the IQRs about the same, or do there seem to be differences in spread between groups?\nAre there some groups with more outliers  than others? Such inspection can reveal important differences between groups.\n\n\n\n\n\nMany statistical methods require our data be approximately Normally distributed - but how can we tell whether the approximation is reasonable?\nNormal-quantile plots provide a simple and informal way of doing this, without having to do a formal hypothesis test — though that may be a natural next step.\nA Normal Quantile (or Q-Q plot) can be used to informally assess the normality of a data set. We construct the plot as follows:\n\nOrder the \\(n\\) data values, giving the ordered data \\(x_{(1)},x_{(2)},\\dots,x_{(n)}\\).\nCalculate the values \\(\\frac{1}{n+1},\\frac{2}{n+1},\\dots,\\frac{n}{n+1}\\), which gives \\(n+1\\) divisions of the \\((0,1)\\) interval.\nUse Normal tables or computer to find the value \\(z_k\\) such that \\(\\Phi(z_k)=\\frac{k}{n+1}\\) for the values \\(k=1,2,\\dots,n\\). The \\(n\\) values \\(z_1,z_2,\\dots,z_n\\) are the \\(\\frac{k}{n+1}\\) quantiles of the Normal distribution.\nPlot the \\(n\\) pairs of points \\((x_{(1)},z_1), (x_{(2)},z_2), \\dots, (x_{(n)},z_n)\\).\n\nThe basic idea is that these plots have the property that plotted points for Normally distributed data should fall roughly on a straight line.\nThis is because \\(x_{(k)}\\) are the sample quantiles of our data, and \\(Z_k\\) are the theoretical quantiles of a Normal distribution. If our sample distribution is approximately normal, these pairs of values will be in agreement.\nSystematic deviations from the straight line indicate non-normality.\nWe don’t need the points to lie on a perfect straight line – we often use the `fat pen test’, meaning that if the points are covered by placing a fat pen over the top, then that’s enough to conclude approximate normality!\n\n\n\nLet’s inspect the cuckoo egg data for normality. We can draw a histogram first, and compare its shape to a normal curve (blue line). While there looks to be approximate agreement (and we only ever need approximate Normality…), we see some divergence for small values of the data.\nDrawing the Normal quantile plot confirms these observations - things look reasonably close to a straight line, but deviate from Normality for small values. However, this would probably be enough to pass our ‘fat pen test’.\n\n\n\n\n\n\n\n\n\nIn R, we can use the qqnorm function to draw a Normal quantile plot of a single variable. Calling qqline after adds the theoretical straight line for comparison\n\nqqnorm(mtcars$mpg)\nqqline(mtcars$mpg)\n\n\n\n\n\n\n\n\nFor this variable (miles per gallon of various cars) we see strong departure from Normality as the points lie far from the desired straight line.\n\n\n\nTo illustrate what happens when our data are very non-Normal, consider this data set of ‘monthly mean relative sunspot numbers’ recorded from from 1749 to 1983. The data are counts of a quantity that is usually quite small in magnitude, this means the data are usually concentrated on small values with an ‘invisible wall’ at 0 - since we cannot observe negative counts! This gives rise to a heavily skewed distribution, as we see in the histogram and boxplot below. The Normal quantile plot (right) now shows strong curvature - not the straight-line feature we would expect if the data were Normally distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is occasionally possible to transform the data so that the transformed data are more approximately Normal.\nThe kinds of transformation we might employ are simple one-to-one power transformations, such \\(\\ln x\\), \\(\\sqrt{x}\\), \\(1/x\\), etc.\nWe can decide informally whether a transformation is useful by examining a Normal quantile plot of the transformed data.\nGraphical displays can help appraise the effectiveness of the transformation, but the cannot tell you if it makes sense - you should consider the interpretation as well as the statistical properties!\n\n\n\n\n\n\nThe frequency distributions of single continuous variables can exhibit a lot of different features\nHistograms are great at for emphasising features of the raw data, but struggle when comparing multiple variables or groups.\nBoxplots show less detail than histograms, but excel at comparing distributions across variables and for identifying outliers.\nQuantile plots can be used to see if data (approximately) follow a particular distribution.\n\n\n\n\nTests using the summary statistics - tests of means and variances against hypothesised values may be appropriate. Normality - formal tests of Normality, such as the KS test, could be applied if following a Normal distribution is important\nOutliers - formal tests for outliers exist (‘outliers’ package) and could be applied, plus the effect of excluding outliers from any analysis could be explored\nMultimodality - methods for assessing multimodality also exist (e.g. ‘diptest’ package), but care must be taken to avoid interpreting data noise as structure\nSample size - many of the methods can be overly sensitive for very large samples\n\n\n\n\n\nCategorical or qualitative variables can only take one of a limited number of possible values (categories). The possible values a variable can take are known as the categories, levels, or labels.\nCategorical data comes in various forms depending on how the categories relate to each other:\n\nNominal - the categories have no standard order (e.g. eye colour)\nOrdinal - the categories have an intrinsic order (e.g. age recorded as “young”, “middle-aged”, and “old”)\nDiscrete - the categories are numerical, and hence ordered, but can only take a finite number of values (e.g. number of people per household)\n\nFor example, the data set below contains the responses to seven questions put to 1525 voters in an election survey for 1997-2001. All the variables in the data are categorical of different types. vote is the party the voter would vote for is clearly categorical, as is gender. The others are ordinal, but with numerical values. In some cases, where the variable is discrete, numerically valued, with a large number of categories - like age - it does make sense to treat it as continuous. Variables such as age are often thought of as being fundamentally continuous albeit recorded as discrete values.\n\nload(\"beps.Rda\")\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\n\nThe levels of a categorical variables are expressed by a factor coding to represent the different categories. Numerical codings are sometimes used, e.g. for a variable on marital status we could define:\n\nsingle\nmarried\nseparated\ndivorced\n…\n\nWhile this helps abbreviate the categories, it is important to remember that variables expressed this way are not the same as a numerical variable that can take these values. We should never treat these values as if they were continuous - for example, it could be tempting to take an average and, say, get a result of 2.6. But this is meaningless as it doesn’t correspond to any of the possibilities, the value we get depends entirely on how we coded our factors, and the rules of arthimetic make no sense for these variables – 1+3 may equal 4, but that does not mean single+separated=divorced!\nInstead, using text strings to represent the categories is safest and avoids mistakes such as these. However, care must be taken to ensure that there is consistency in how these levels are labelled, e.g. do we treat the labels “Female”, “female”, “F” as the same?\n\n\nCompared to continuous variables, categorical variables are relatively simplistic and usually contain little useful information on their own. As data, they usually reduce to the counts of the number of observations in the various categories.\nWe can obtain summary tables of the frequency of each category by using the table function.\n\ntable(beps$vote)\n\n\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\nThe R output is useful for a quick summary, but will need some manual re-formatting to make it presentable. R output is seldom an acceptable way to present information to others, and almost always should be transformed (e.g. into a summary table of relevant information) or summaries (e.g. by reporting only the relevant information R has given you). Here we can transform the ugly R code into a small data table:\n\n\n\n\n\nConservative\nLabour\nLiberal Democrat\n\n\n\n\n462\n720\n343\n\n\n\n\n\nThe xtabs function does something similar, and will be more useful later on when we have multiple variables at once:\n\nxtabs(~vote,data=beps)\n\nvote\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\n\n\n\nVisualisation of categorical variables usually focuses on plotting the counts of the categories, or the proportions that each category contribute to the total.\nThe range of useful graphics for such data is usually limited to:\n\nBarcharts - depict the counts or proportions by the size of the bar\nPiecharts - display the proportions of categories as fractions of the whole\nVariations of the above (stacked bars, treemaps)\n\nWhat features to look for?\n\nExtremes - the largest (smallest) category is often of particular interest.\nUneven distributions - Observational studies can often observe many more cases of one category than others. Some categories may not be observed\nUnexpected patterns of results - surprisingly large or small numbers for particular categories\nLarge numbers of categories - these may require grouping together or filtering out\nDon’t knows, missing values - Missing, unknown, or unavailable data is common in e.g. surveys and opinion polls.\nErrors in factor codings - e.g. gender could be denoted ‘M’ or ‘F’, but we may find values of ‘m’, or ‘female’.\n\n\n\n\nA bar chart or barplot simply draws the distribution of counts per category. We can use barplot to draw a barplot from a summary table of counts generated by the xtabs function\n\nbarplot(xtabs(~vote,data=beps))\n\n\n\n\n\n\n\n\nbarplot takes a number of additional arguments to customise the plot:\n\nnames - a vector of labels for each bar\nhoriz - set to TRUE to show a horizontal barplot.\nwidth - a vector of values to specify the widths of the bars\nspace - a vector of values to specify the spacing between bars\ncol - a vector of colours for the bars\n\n\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'), horiz=TRUE, names=c('Con','Lab','LibDem'))\n\n\n\n\n\n\n\n\n\n\n\nHere we have the number of eligible voters in each of the 16 Bundesländer (states) in the German Federal elections in 2009.\n\nload('btw9s.Rda')\nhead(btw9s)\n\n   Bundesland  Voters   EW State1\nBW         BW 7633818 West     BW\nBY         BY 9382583 West     BY\nBE         BE 2471665 East     BE\nBB         BB 2128715 East     BB\nHB         HB  487978 West     HB\nHH         HH 1256634 West     HH\n\n\n\n\n\n\n\n\n\n\n\nAs states are categorical the ordering of the bars is arbitrary, which limits what we can interpret - but clearly there are some very large and very small states.\nWe notice wide variation in populations - the largest is ‘NW’ (Nordrhein-Westfalen) which includes many major cities like Cologne and Düsseldorf; the smallest is ‘HB’ which is Bremen, a smaller city-state.\nThe ordering of bars can be used for emphasis - here its alphabetical. Ordering the bars by size gives a much better impression of the relative sizes of the sixteen states:\n\n\n\n\n\n\n\n\n\nUsing problem-specific structure can help give you context to your analysis. For example, we can separate the states belonging to the former East (left) and West Germany (right) and use a little colour for emphasis:\n\n\n\n\n\n\n\n\n\nClearly the West German states are substantially more populous.\n\n\n\n\ntitanic &lt;- data.frame(Titanic)\n\nThe Titanic data contains information on the fate of survivors fatal maiden voyage of the ocean liner Titanic. The data are in the form of counts of survivors (and not) summarised by economic status (class), sex and age. The variables are all categorical and defined as\n\nClass - 1st, 2nd, 3rd, or crew\nSex - Female, Male\nAge - Adult, Child\nSurvived - No, Yes\n\nBefore we analyse the data, think about what you expect the results to show. Do we expect more Male or Female survivors? Do we expect those in 1st class to fare better than those in 3rd class? Would we expect the Crew to fare better or worse than the passengers?\nBy thinking about what we might expect before looking at the data, we allow ourselves to be surprised when we find features we did not expect!\n\npar(mfrow=c(1,4))\nbarplot(xtabs(Freq~Survived,data=titanic), col=\"red\",main=\"Survived\")\nbarplot(xtabs(Freq~Sex,data=titanic), col=\"green\",main=\"Sex\")\nbarplot(xtabs(Freq~Age,data=titanic), col=\"orange\",main=\"Sex\")\nbarplot(xtabs(Freq~Class,data=titanic), col=\"blue\",main=\"Sex\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nMore than twice as many passengers died as survived\nMore Male than Female passengers on board - more than 3x as many!\nVery few Child passengers\nMore people in Crew than any other class; fewest in second class\n\nThe interesting questions arise when we try to think whether Survived is related to the other variables. But we’ll have to come back to this later on.\n\n\n\nProportions are of particular interest in opinion polls, or studies where the composition of a larger population is of interest. In elections, the values of the counts are far less important than the share of the total - in particular the size of the largest proportion.\nVisualisations of proportions are based around the simple idea of dividing the larger whole into pieces which reflect the corresponding fractions.\n\npiecharts - slices of a circle\ncomposite or stacked barcharts - fractions of a bar\ntreemaps - tiles within a square or rectangle\n\nA stacked barplot is a variation of a standard barplot where the individual bars are broken up into portions reflecting the different. When we just have one variable, the effect is to stack the individual bars on top of each other.\n\nbarplot(as.matrix(xtabs(~vote,data=beps)), ## note we have to conver to a matrix here\n  beside=FALSE,\n  horiz=TRUE,\n  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nHere we can see the individual sizes of the bars, as well as a clear indication of their contribution to the overall total. So, it’s easy to read that the red category (Labour) has the highest share of the vote in this poll.\nThe traditional pie chart can be generated using pie:\n\npie(xtabs(~vote,data=beps),  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nWe’re probably all familiar with this, but the basic idea is to use slices of the ‘pie’ to represent the proportion. Unfortunately, pie charts are often not the best visualisation as it can be difficult to detect differences between similarly-sized slices of a circle when compared to similiarly sized rectangles.\nHistorical aside: the invention of the pie chart is often attributed to Florence Nightingale. In one of the earliest conventional uses of data visualisation, she used an early version of the pie chart highlight the poor conditions of soldiers in field hospitals during the Crimean War (1854-6). Florence was an early pioneer of statistics and the first female member of the Royal Statistical Society – this aspect of her life and achivements is often overlooked given her frequent association with nursing.\n\n\n\n\n\n\n\nA stem and leaf plot presents the numerical values of the data in a similar form to a histogram.\n\nstem(geyser$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   4 | 3\n   4 | 577888889999999\n   5 | 00000000000011111222223333333444444444\n   5 | 5556677777777788888999\n   6 | 0000001112222234\n   6 | 5555555668889999\n   7 | 01111122222233333344444444\n   7 | 5555555556666666677777777778888888888888888899999999999\n   8 | 00000000000001111111111112222222233333344444444444\n   8 | 5555555666667777777777777788888889999999\n   9 | 0011222333333334\n   9 | 668\n  10 | \n  10 | 8\n\n\nThis plot resembles a sideways histogram, only it shows extra information on the values within each bar.\n\n\n\nA stripplot is similar again, but displays the data as points rather than the numerical value\n\nstripchart(geyser$waiting, method='stack')\n\n\n\n\n\n\n\n\nBy default, it draws a hollow box for each data point which can make it difficult to read. We can modify the shape of the points by the pch (plot character) argument can help improve readability:\n\nstripchart(geyser$waiting, method='stack', pch=16)\n\n\n\n\n\n\n\n\n\n\n\nA beeswarm plot is similar to stripplot, but uses various techniques to separate nearby points such that each point remains visible. It also draws the plot symmetric about a central axis, rather than stacking up points from a baseline.\n\nlibrary(beeswarm)\nbeeswarm(geyser$waiting,horizontal = TRUE)"
  },
  {
    "objectID": "Lecture1b_Variables.html#exploring-continuous-variables",
    "href": "Lecture1b_Variables.html#exploring-continuous-variables",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "We focus first on quantitative data that is continuous (i.e. real-valued).\nWe view our data as a sample from an underlying continuous distribution - the goal of our explorations here is to seek some clues about the features of that distribution.\nMany possible ways to explore this - graphically, we will focus on histograms and boxplots.\nWhen we have a particular type of distribution in mind, we can also draw a quantile plot to see how plausible it is.\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. the birthweight of babies isn’t.\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\nA histogram is an approximate representation of the distribution of continuous data where we divide the range of values into bins (or buckets) and draw a bar over each bin with area proportional to the frequency of cases in each bin.\nA histogram is an effective tool for visualising features relating to the shape of the data distribution.\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n\nThis data set contains the various information on housing values and related quantities for the 506 suburban areas in Boston. The main interest is in the `median values of owner-occupied homes’, but there are 14 variables to explore here.\nLooking first at the median housing value, we can produce the histogram below. Some obvious features of note are:\n\nA concentration of housing values around 20-25, then a sudden drop-off – is there an explanation for this, such as changes in tax levels?\nPossible multi-modality, with modes around 25 and 30 – are there two classes or groups of housing?\nA further spike in values occur in the upper tail of the data, at 50 – this seems dubious, and could be some crude rounding or grouping of all values ‘’50 and over’’.\n\n\n\n\n\n\n\n\n\n\nChoosing the width of the bars can substantially affect the detail of the histogram. If we choose too few bins, then we can obscure key features by over-smoothing the data. Alterntaively, if we have too many bins then we can introduce too much noise to the plot that it obscures more general features and patterns. Unfortunately, the only way to find a good compromise is to experiment – R and other software will default to a ‘best guess’, but this invariably needs adjusting. The histograms below show the same data, but using bar widths of 5, 2.5, and 1 unit respectively. We’ll see more sophisticated methods for smoothing the data later.\n\n\n\n\n\n\n\n\n\nWith 14 variables in the data set, we could inspect the histograms of each of the variables. We could do this one-at-a-time, or arrange them in a grid or matrix as below:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe other variables in the data set show a variety of the features discussed above that we may want to investigate further:\n\nThe ‘age’ variable - in position \\((1,1)\\) - shows obvious left-skewness.\nThe ‘dis’ variable - in position \\((1,2)\\) - shows right skewness\nThe ‘rm’ variable - in position \\((3,4)\\) - looks symmetric, and possible approximate Normality\nIn the variables in the bottom two rows, we see a lot of gaps in the data and heaping on particular values.\nThe ‘chas’ variable - in position \\((1,3)\\) - exhibits heaping on only two values, 0 and 1. This is a discrete binary variable, not continuous a continuous one! We should use different methods to explore this variable.\n\nIn a full analysis, we would want to look at the relationships between the variables to determine how the features observed relate to each other. We’ll return to this later…\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe geyser data set contains 272 observations of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The variables are:\n\nduration - Length of eruption in mins\nwaiting - Waiting time to next eruption\n\nTo draw a histogram, we use the hist function:\n\nhist(geyser$waiting)\n\n\n\n\n\n\n\n\nHere we can see the data appear to come in two groups: one group with smaller values (shorter waiting times), and one group of larger (longer waiting times).\nWe can add a rug plot to an existing histogram to add more detail and to show where the individual data values fall inside the bars:\n\nhist(geyser$waiting)\nrug(geyser$waiting)\n\n\n\n\n\n\n\n\nNow, each individual mark on the horizontal axis shows where we have observed a data value.\nHistograms can be customised in many ways, but the most important one is changing the configuration of the bars drawn. This is controlled by the breaks parameter. We can set breaks to a number to indicate an approximate number of bars to draw:\n\nhist(geyser$waiting,breaks=20)\n\n\n\n\n\n\n\n\nOr we can be more specific and state where each individual bar should begin and end by listing the breakpoints of the bars as a vector:\n\nhist(faithful$waiting, breaks=c(30,45,47,50,52,55,60,75,80,85,95,105))\n\n\n\n\n\n\n\n\nAdditionally, almost all plot functions can take the following arguments to customise the plot:\n\nxlab, ylab - sets the x and y axis labels\nmain - sets the main title\nxlim, ylim - set x and y axis limits, e.g. c(0,10)\ncol - sets the plot colour(s)\n\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. zero-inflation\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\n\n\n\nWe have considered the shape of a distribution already, and have seen that histograms are a handy way of exploring shape.\nWe come now to several summary statistics (recall ISDS):\n\nthe average and median summarise the centre or location of the distribution.\nthe standard deviation and inter-quartile range summarise spread around the centre of the distribution.\n\nThe average is the most (over-)used statistic. It is useful especially where distributions are reasonably symmetric.\nFor distributions with long tails, it can give misleading signals, e.g. average household income will include households like Buckingham palace which distort the average.\nThe median is a measure of the midpoint of a distribution, and, unlike the mean, it is quite resistant to extreme values.\nA robust summary of spread is given by the interquartile range (IQR), the difference between the first and third quartiles. Every distribution has three quartiles:\n\nThe first quartile, \\(Q_1\\), is a number such that 25% of the values in the distribution do not exceed this number. This is called a lower quartile.\nThe second quartile, \\(Q_2\\), is a number such that 50% of the values in the distribution do not exceed this number. This is the same as the median.\nThe third quartile, \\(Q_3\\), is a number such that 75% of the values in the distribution do not exceed this number. This is called an upper quartile.\n\nTukey suggested that we combine the three quartiles with the minimum and maximum values in the data to form a five-number summary.\n\nIn R, the fivenum function gives the standard 5-number summary:\n\nfivenum(geyser$waiting)\n\n[1]  43  59  76  83 108\n\n\nThe summary function adds a 6th number (the mean):\n\nsummary(geyser$waiting)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  43.00   59.00   76.00   72.31   83.00  108.00 \n\n\nMin. 1st Qu. Median Mean 3rd Qu. Max. 43.0 58.0 76.0 70.9 82.0 96.0\n\n\n\n\nA boxplot (or box-and-whisker plot) is constructed from the 5-number summary by: * Drawing a box with lower boundary at \\(Q_1\\) and upper boundary \\(Q_3\\). * Drawing a line inside the box at the median (\\(Q_2\\)). * Drawing lines (whiskers) from the edges of the box to the most extreme data point that is within \\(1.5\\times IQR\\) of the edge of the box (often the minimum and maximum values).\n\nWe can draw a boxplot by using the boxplot function on a vector of data values:\n\nboxplot(geyser$waiting)\n\n\n\n\n\n\n\n\nOr we can pass all the columns of a data set to boxplot to draw everything at once:\n\nboxplot(geyser)\n\n\n\n\n\n\n\n\nNow, all variables are shown together on a common axis scale. This can be useful if all the variables take values of a similar size, but - as we see here - when the variables are quite different it can obscure the features of some variables by drawing everything together.\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data. Though usually, you have the same number of data points in each column of the data set so this is not often very helpful.\n\n\n\nTo show the relationship between a histogram and a boxplot, we have a data set comprising the length (mm) of 100 cuckoo eggs. Drawng the histogram and boxplot together, we can see that the histogram gives far more detail on the shape of the distribution and the boxplot is more of a summary visualisation. The median is indicated in red and the upper/lower quartiles in green, which shows how these quantities align between the plots. Note that the smallest value in the data is flagged as an outlier and drawn separately as a circle on the boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValues too far from the centre of the distribution are known as outliers.\nData values further than \\(1.5\\times IQR\\) from a lower/upper quartile are called an outlier, and usually indicated with a circle or point.\nAnything beyond \\(3\\times IQR\\) is an extreme outlier, and usually indicated with a star or asterisk.\nThis definition of outliers is a simple rule-of-thumb. It leads to about 0.7% of (normally distributed) observations being described as extreme for large samples.\nHowever, if you have a million data points then 0.7% of your data is still a lot! This doesn’t necessarily mean those points are all genuinely extreme or outliers.\n\n\n\nOne advantage of the boxplot, is that as it is a simple summary plot it is much easier to use boxplots to compare many variables at once. For example, the data plotted below are boxplots of the heights in inches of the singers in the New York Choral Society in 1979. The data are grouped according to the voice part they play in the choir. The vocal range for each voice part increases in pitch according to the following order: Bass 2, Bass 1, Tenor 2, Tenor 1, Alto 2, Alto 1, Soprano 2, Soprano 1. We can see immediately that the lowest pitch voices are associated with the taller singers and the higher pitches with smaller singers, which makes a lot of intuitive sense. There will also be a strong correspondance with Gender here too, though that information is not recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should examine a single boxplot for the following features:\n\nIs the median line in the centre of the 50% portion of the distribution? If not, some degree of skew or asymmetry is indicated.\nAre the whiskers the same length? If not, some degree of skew or asymmetry is indicated.\nAre there any outliers?\n\nWe should examine several boxplots for the following features:\n\nDo the groups have similar shapes? Some might seem symmetric, some skewed.\nAre the median lines about the same, or do there seem to be differences in location** between groups?\nAre the IQRs about the same, or do there seem to be differences in spread between groups?\nAre there some groups with more outliers  than others? Such inspection can reveal important differences between groups.\n\n\n\n\n\nMany statistical methods require our data be approximately Normally distributed - but how can we tell whether the approximation is reasonable?\nNormal-quantile plots provide a simple and informal way of doing this, without having to do a formal hypothesis test — though that may be a natural next step.\nA Normal Quantile (or Q-Q plot) can be used to informally assess the normality of a data set. We construct the plot as follows:\n\nOrder the \\(n\\) data values, giving the ordered data \\(x_{(1)},x_{(2)},\\dots,x_{(n)}\\).\nCalculate the values \\(\\frac{1}{n+1},\\frac{2}{n+1},\\dots,\\frac{n}{n+1}\\), which gives \\(n+1\\) divisions of the \\((0,1)\\) interval.\nUse Normal tables or computer to find the value \\(z_k\\) such that \\(\\Phi(z_k)=\\frac{k}{n+1}\\) for the values \\(k=1,2,\\dots,n\\). The \\(n\\) values \\(z_1,z_2,\\dots,z_n\\) are the \\(\\frac{k}{n+1}\\) quantiles of the Normal distribution.\nPlot the \\(n\\) pairs of points \\((x_{(1)},z_1), (x_{(2)},z_2), \\dots, (x_{(n)},z_n)\\).\n\nThe basic idea is that these plots have the property that plotted points for Normally distributed data should fall roughly on a straight line.\nThis is because \\(x_{(k)}\\) are the sample quantiles of our data, and \\(Z_k\\) are the theoretical quantiles of a Normal distribution. If our sample distribution is approximately normal, these pairs of values will be in agreement.\nSystematic deviations from the straight line indicate non-normality.\nWe don’t need the points to lie on a perfect straight line – we often use the `fat pen test’, meaning that if the points are covered by placing a fat pen over the top, then that’s enough to conclude approximate normality!\n\n\n\nLet’s inspect the cuckoo egg data for normality. We can draw a histogram first, and compare its shape to a normal curve (blue line). While there looks to be approximate agreement (and we only ever need approximate Normality…), we see some divergence for small values of the data.\nDrawing the Normal quantile plot confirms these observations - things look reasonably close to a straight line, but deviate from Normality for small values. However, this would probably be enough to pass our ‘fat pen test’.\n\n\n\n\n\n\n\n\n\nIn R, we can use the qqnorm function to draw a Normal quantile plot of a single variable. Calling qqline after adds the theoretical straight line for comparison\n\nqqnorm(mtcars$mpg)\nqqline(mtcars$mpg)\n\n\n\n\n\n\n\n\nFor this variable (miles per gallon of various cars) we see strong departure from Normality as the points lie far from the desired straight line.\n\n\n\nTo illustrate what happens when our data are very non-Normal, consider this data set of ‘monthly mean relative sunspot numbers’ recorded from from 1749 to 1983. The data are counts of a quantity that is usually quite small in magnitude, this means the data are usually concentrated on small values with an ‘invisible wall’ at 0 - since we cannot observe negative counts! This gives rise to a heavily skewed distribution, as we see in the histogram and boxplot below. The Normal quantile plot (right) now shows strong curvature - not the straight-line feature we would expect if the data were Normally distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is occasionally possible to transform the data so that the transformed data are more approximately Normal.\nThe kinds of transformation we might employ are simple one-to-one power transformations, such \\(\\ln x\\), \\(\\sqrt{x}\\), \\(1/x\\), etc.\nWe can decide informally whether a transformation is useful by examining a Normal quantile plot of the transformed data.\nGraphical displays can help appraise the effectiveness of the transformation, but the cannot tell you if it makes sense - you should consider the interpretation as well as the statistical properties!"
  },
  {
    "objectID": "Lecture1b_Variables.html#summary",
    "href": "Lecture1b_Variables.html#summary",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "The frequency distributions of single continuous variables can exhibit a lot of different features\nHistograms are great at for emphasising features of the raw data, but struggle when comparing multiple variables or groups.\nBoxplots show less detail than histograms, but excel at comparing distributions across variables and for identifying outliers.\nQuantile plots can be used to see if data (approximately) follow a particular distribution.\n\n\n\n\nTests using the summary statistics - tests of means and variances against hypothesised values may be appropriate. Normality - formal tests of Normality, such as the KS test, could be applied if following a Normal distribution is important\nOutliers - formal tests for outliers exist (‘outliers’ package) and could be applied, plus the effect of excluding outliers from any analysis could be explored\nMultimodality - methods for assessing multimodality also exist (e.g. ‘diptest’ package), but care must be taken to avoid interpreting data noise as structure\nSample size - many of the methods can be overly sensitive for very large samples"
  },
  {
    "objectID": "Lecture1b_Variables.html#exploring-categorical-variables",
    "href": "Lecture1b_Variables.html#exploring-categorical-variables",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "Categorical or qualitative variables can only take one of a limited number of possible values (categories). The possible values a variable can take are known as the categories, levels, or labels.\nCategorical data comes in various forms depending on how the categories relate to each other:\n\nNominal - the categories have no standard order (e.g. eye colour)\nOrdinal - the categories have an intrinsic order (e.g. age recorded as “young”, “middle-aged”, and “old”)\nDiscrete - the categories are numerical, and hence ordered, but can only take a finite number of values (e.g. number of people per household)\n\nFor example, the data set below contains the responses to seven questions put to 1525 voters in an election survey for 1997-2001. All the variables in the data are categorical of different types. vote is the party the voter would vote for is clearly categorical, as is gender. The others are ordinal, but with numerical values. In some cases, where the variable is discrete, numerically valued, with a large number of categories - like age - it does make sense to treat it as continuous. Variables such as age are often thought of as being fundamentally continuous albeit recorded as discrete values.\n\nload(\"beps.Rda\")\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\n\nThe levels of a categorical variables are expressed by a factor coding to represent the different categories. Numerical codings are sometimes used, e.g. for a variable on marital status we could define:\n\nsingle\nmarried\nseparated\ndivorced\n…\n\nWhile this helps abbreviate the categories, it is important to remember that variables expressed this way are not the same as a numerical variable that can take these values. We should never treat these values as if they were continuous - for example, it could be tempting to take an average and, say, get a result of 2.6. But this is meaningless as it doesn’t correspond to any of the possibilities, the value we get depends entirely on how we coded our factors, and the rules of arthimetic make no sense for these variables – 1+3 may equal 4, but that does not mean single+separated=divorced!\nInstead, using text strings to represent the categories is safest and avoids mistakes such as these. However, care must be taken to ensure that there is consistency in how these levels are labelled, e.g. do we treat the labels “Female”, “female”, “F” as the same?\n\n\nCompared to continuous variables, categorical variables are relatively simplistic and usually contain little useful information on their own. As data, they usually reduce to the counts of the number of observations in the various categories.\nWe can obtain summary tables of the frequency of each category by using the table function.\n\ntable(beps$vote)\n\n\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\nThe R output is useful for a quick summary, but will need some manual re-formatting to make it presentable. R output is seldom an acceptable way to present information to others, and almost always should be transformed (e.g. into a summary table of relevant information) or summaries (e.g. by reporting only the relevant information R has given you). Here we can transform the ugly R code into a small data table:\n\n\n\n\n\nConservative\nLabour\nLiberal Democrat\n\n\n\n\n462\n720\n343\n\n\n\n\n\nThe xtabs function does something similar, and will be more useful later on when we have multiple variables at once:\n\nxtabs(~vote,data=beps)\n\nvote\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\n\n\n\nVisualisation of categorical variables usually focuses on plotting the counts of the categories, or the proportions that each category contribute to the total.\nThe range of useful graphics for such data is usually limited to:\n\nBarcharts - depict the counts or proportions by the size of the bar\nPiecharts - display the proportions of categories as fractions of the whole\nVariations of the above (stacked bars, treemaps)\n\nWhat features to look for?\n\nExtremes - the largest (smallest) category is often of particular interest.\nUneven distributions - Observational studies can often observe many more cases of one category than others. Some categories may not be observed\nUnexpected patterns of results - surprisingly large or small numbers for particular categories\nLarge numbers of categories - these may require grouping together or filtering out\nDon’t knows, missing values - Missing, unknown, or unavailable data is common in e.g. surveys and opinion polls.\nErrors in factor codings - e.g. gender could be denoted ‘M’ or ‘F’, but we may find values of ‘m’, or ‘female’.\n\n\n\n\nA bar chart or barplot simply draws the distribution of counts per category. We can use barplot to draw a barplot from a summary table of counts generated by the xtabs function\n\nbarplot(xtabs(~vote,data=beps))\n\n\n\n\n\n\n\n\nbarplot takes a number of additional arguments to customise the plot:\n\nnames - a vector of labels for each bar\nhoriz - set to TRUE to show a horizontal barplot.\nwidth - a vector of values to specify the widths of the bars\nspace - a vector of values to specify the spacing between bars\ncol - a vector of colours for the bars\n\n\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'), horiz=TRUE, names=c('Con','Lab','LibDem'))\n\n\n\n\n\n\n\n\n\n\n\nHere we have the number of eligible voters in each of the 16 Bundesländer (states) in the German Federal elections in 2009.\n\nload('btw9s.Rda')\nhead(btw9s)\n\n   Bundesland  Voters   EW State1\nBW         BW 7633818 West     BW\nBY         BY 9382583 West     BY\nBE         BE 2471665 East     BE\nBB         BB 2128715 East     BB\nHB         HB  487978 West     HB\nHH         HH 1256634 West     HH\n\n\n\n\n\n\n\n\n\n\n\nAs states are categorical the ordering of the bars is arbitrary, which limits what we can interpret - but clearly there are some very large and very small states.\nWe notice wide variation in populations - the largest is ‘NW’ (Nordrhein-Westfalen) which includes many major cities like Cologne and Düsseldorf; the smallest is ‘HB’ which is Bremen, a smaller city-state.\nThe ordering of bars can be used for emphasis - here its alphabetical. Ordering the bars by size gives a much better impression of the relative sizes of the sixteen states:\n\n\n\n\n\n\n\n\n\nUsing problem-specific structure can help give you context to your analysis. For example, we can separate the states belonging to the former East (left) and West Germany (right) and use a little colour for emphasis:\n\n\n\n\n\n\n\n\n\nClearly the West German states are substantially more populous.\n\n\n\n\ntitanic &lt;- data.frame(Titanic)\n\nThe Titanic data contains information on the fate of survivors fatal maiden voyage of the ocean liner Titanic. The data are in the form of counts of survivors (and not) summarised by economic status (class), sex and age. The variables are all categorical and defined as\n\nClass - 1st, 2nd, 3rd, or crew\nSex - Female, Male\nAge - Adult, Child\nSurvived - No, Yes\n\nBefore we analyse the data, think about what you expect the results to show. Do we expect more Male or Female survivors? Do we expect those in 1st class to fare better than those in 3rd class? Would we expect the Crew to fare better or worse than the passengers?\nBy thinking about what we might expect before looking at the data, we allow ourselves to be surprised when we find features we did not expect!\n\npar(mfrow=c(1,4))\nbarplot(xtabs(Freq~Survived,data=titanic), col=\"red\",main=\"Survived\")\nbarplot(xtabs(Freq~Sex,data=titanic), col=\"green\",main=\"Sex\")\nbarplot(xtabs(Freq~Age,data=titanic), col=\"orange\",main=\"Sex\")\nbarplot(xtabs(Freq~Class,data=titanic), col=\"blue\",main=\"Sex\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nMore than twice as many passengers died as survived\nMore Male than Female passengers on board - more than 3x as many!\nVery few Child passengers\nMore people in Crew than any other class; fewest in second class\n\nThe interesting questions arise when we try to think whether Survived is related to the other variables. But we’ll have to come back to this later on.\n\n\n\nProportions are of particular interest in opinion polls, or studies where the composition of a larger population is of interest. In elections, the values of the counts are far less important than the share of the total - in particular the size of the largest proportion.\nVisualisations of proportions are based around the simple idea of dividing the larger whole into pieces which reflect the corresponding fractions.\n\npiecharts - slices of a circle\ncomposite or stacked barcharts - fractions of a bar\ntreemaps - tiles within a square or rectangle\n\nA stacked barplot is a variation of a standard barplot where the individual bars are broken up into portions reflecting the different. When we just have one variable, the effect is to stack the individual bars on top of each other.\n\nbarplot(as.matrix(xtabs(~vote,data=beps)), ## note we have to conver to a matrix here\n  beside=FALSE,\n  horiz=TRUE,\n  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nHere we can see the individual sizes of the bars, as well as a clear indication of their contribution to the overall total. So, it’s easy to read that the red category (Labour) has the highest share of the vote in this poll.\nThe traditional pie chart can be generated using pie:\n\npie(xtabs(~vote,data=beps),  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nWe’re probably all familiar with this, but the basic idea is to use slices of the ‘pie’ to represent the proportion. Unfortunately, pie charts are often not the best visualisation as it can be difficult to detect differences between similarly-sized slices of a circle when compared to similiarly sized rectangles.\nHistorical aside: the invention of the pie chart is often attributed to Florence Nightingale. In one of the earliest conventional uses of data visualisation, she used an early version of the pie chart highlight the poor conditions of soldiers in field hospitals during the Crimean War (1854-6). Florence was an early pioneer of statistics and the first female member of the Royal Statistical Society – this aspect of her life and achivements is often overlooked given her frequent association with nursing.\n\n\n\n\n\n\n\nA stem and leaf plot presents the numerical values of the data in a similar form to a histogram.\n\nstem(geyser$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   4 | 3\n   4 | 577888889999999\n   5 | 00000000000011111222223333333444444444\n   5 | 5556677777777788888999\n   6 | 0000001112222234\n   6 | 5555555668889999\n   7 | 01111122222233333344444444\n   7 | 5555555556666666677777777778888888888888888899999999999\n   8 | 00000000000001111111111112222222233333344444444444\n   8 | 5555555666667777777777777788888889999999\n   9 | 0011222333333334\n   9 | 668\n  10 | \n  10 | 8\n\n\nThis plot resembles a sideways histogram, only it shows extra information on the values within each bar.\n\n\n\nA stripplot is similar again, but displays the data as points rather than the numerical value\n\nstripchart(geyser$waiting, method='stack')\n\n\n\n\n\n\n\n\nBy default, it draws a hollow box for each data point which can make it difficult to read. We can modify the shape of the points by the pch (plot character) argument can help improve readability:\n\nstripchart(geyser$waiting, method='stack', pch=16)\n\n\n\n\n\n\n\n\n\n\n\nA beeswarm plot is similar to stripplot, but uses various techniques to separate nearby points such that each point remains visible. It also draws the plot symmetric about a central axis, rather than stacking up points from a baseline.\n\nlibrary(beeswarm)\nbeeswarm(geyser$waiting,horizontal = TRUE)"
  },
  {
    "objectID": "Lecture1a_IntroViz.html",
    "href": "Lecture1a_IntroViz.html",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "In this lecture, we look into the concept of exploratory data analysis and learn what makes good (and bad) data visualisation.\n\n\n\nExploratory data analysis (EDA) should be one of the first steps in analysing any data set and was pioneered as a discipline of its own by John Tukey in the 1960s and 1970s.\n\n\n\n\nExploratory Data Analysis chart\n\n\n\nIn the words of Tukey:\n\n“Exploratory data analysis is detective work — in the purest sense — finding and revealing the clues.”\n“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone — as the first step.”\n\nSo, the definition of EDA is fairly self-explanatory. We seek to explore the data, by asking questions and looking for clues to help us better understand the data. The purpose is generally to gain sufficient information to make reasoned and justifiable hypotheses to explore with more formal methods, or to identify sensible modelling approaches (and exclude others). The features and insights we gather while exploring will suggest what appropriate strategies for our subsequent analysis.\nTherefore, it is simply good practice to try to understand and gather as many insights from the data as possible before even attempting before any modelling or inference. Without a solid understanding of the data, we do not know if the techniques we apply are appropriate, and so we risk our inferences and conclusions being invalid.\nThat said, EDA is not a one-off process. It is often iterative, going back and forth between exploration and modelling/analysis as each part of the process can suggest new questions or hypotheses to investigate.\nExploratory Data Analysis is not a formal process and it does not have strict rules to follow or methods to apply: the overarching goal is simply to develop an understanding of the data set. However, typically we do try and do this without fitting any complex models or making assumptions about our data. We’re looking to see what the data tell us, not what our choice of technique or model says. We have insufficient knowledge of the properties of our data to exploit sophisticated techniques.\nThe “detective work” of EDA is essentially looking for clues in the data that will reveal insights about what is actually going on with the problem from which they come — but this requires looking both in the right places and with the right magnifying glass.\nData sets rarely arrive with a manual, or a shopping list of specific features to investigate. Absent any formal structure, the best approach is to pursue any lead that occurs to you, and ask questions - lots of them — some will lead to insights, others will be dead ends.\nSome obvious ‘clues’ and features to investigate in an unseen data set are: 1 Features and properties of individual variables and collections 2 Identifying important and unimportant variables 3 Identifying structure, patterns and relationships between variables 4 Anomalies, errors, and outliers 5 Variation and distributions 6 Missing values\nAs we’re only exploring the data with minimal assumptions, the tools of EDA must be mathematically quite simple and robust as we should not be relying on assumptions of distributions or structure that may not be justified. We rely primarily on:\n\nStatistical summaries\nGraphics and visualisations\n\nOur focus will be extensively on making a graphical exploration of the data.\n\n\n\n\n\nData visualisation is the creation and study of the visual representation of data.\nLike EDA, there is no complex theory about graphics — in fact, there is not much theory at all! The topics are not usually covered in depth in books or lectures as they build on relatively simple statistical concepts. Once the basic graphical forms have been described, textbooks usually move on to more mathematical ideas such as proving the central limit theorem.\nExploratory Data Analysis through investigation of data graphics is sometimes called Graphical Data Analysis (GDA).\nThere are some standard plots and graphics that are applicable in some fairly generaly situations, but\nA good visualisation reveals the data, and communicates complex ideas with clarity, precision and efficiency. Some features of a good data visualisation would be\n\nShow the data!\nInduce the viewer to think about the substance rather than the methodology, design, etc.\nAvoid distorting what the data have to say\nPresent many numbers in a small space\nMake large data sets coherent\nEncourage comparisons between different pieces of data\n\n\nGood data visualisations can communicate the key features of complex data sets more convincingly and more effectively than the raw data often can achieve.\nTypically, data visualisation is used for one of two purposes:\n1- Analysis - used to find patterns, trends, aid in data description, interpretation * Goal: the “Eureka!” moment * Many images for the analyst only, linked to analysis/modelling * Typically many rough and simple plots used to detect interesting features of the data and suggest directions for future investigation, analysis or modelling\n2- Presentation - used to attract attention, make a point, illustrate a conclusion * Goal: The “Wow!” moment. * A single image suitable for a large audience which tells a clear story * Once the key features and behaviours of the data are known, the best graphic can be produced to show those features in a clear way. Often targetting a less technical audience.\nFor example, the visualisation below shows a presentation of the number of cases of measles per 100,000 for the 50 US states over time. The impact of vaccination on the levels of measles is striking and clear.\n\nPresentation quality graphics can venture into the realm of data art, but this is rather beyond what we could hope to achieve in our short course. These visualisations, often called infographics, try to present data in a non-technical way that can easily be understood by non-experts. For example, the following graph illustrates the scale of the amount of waste plastic from plastic bottle sales over 10 years, relative to New York.\n\n\n\n\nFirst, a little bit of historical context. Data visualisation (and statistics) are relatively new disciplines - relative to the rest of mathematics. For data visualisation, William Playfair (1759-1823) is often credited for pioneering many of the graphical forms we still use today. Slightly later, Florence Nightingale (1820-1910) became one of the first people to persuade the public and influence public policy with data visualisation. Despite being better known for her achievements in nursing, Florence was the first female member of the Royal Statistical Society. Her rose diagrams were an innovative combination of pie chart and time series, and were used to illustrate the terrible conditions suffered by soldiers in the Crimean war.\nThese early visualisations were difficult to produce and required a combination of art and intuition. The 20th century brought computers and the ability to process and visualise increasing amounts of data with ease. Ultimately, a number of standard graphics were developed that exploit our visual perception to interpret complex data - most of which we have seen in the course so far.\nSo, despite having a long history, what is it about data visualisation that is so effective that we continue to do it? The answer is that depicting data graphically can be extremely effective as it takes advantage of the human brain’s natural strengths at quickly and efficiently processing visual information. Understanding this will help you make better visualisations!\nThe human brain has developed many subconscious natural abilities to process visual information and make sense of the world around us. We are constantly processing and interpreting the visual signals from our eyes and much of this happens sub-consciously without any actual effort. The reason for this is that this analysis relies on the visual perception part of our brain, rather than cognitive “thinking” part.\nVisual perception is the ability to see, interpret, and organise our environment. It’s fast and efficient, and happens automatically. Cognition, which is handled by the cerebral cortex, is much slower and requires more effort to process information. So, presenting data visually exploits our quick perception capabilities and helps to reduce cognitive load.\nTo illustrate these difference consider the following table and plots. From which of the three presentations of the data is it easiest to identify which 3 regions have the highest available renewable water resources?\n\n\n\n\n\nregion\nkm3\n\n\n\n\nCentral America and Caribbean\n735\n\n\nCentral Asia\n242\n\n\nEast Asia\n3410\n\n\nEastern Europe\n4448\n\n\nMiddle East\n484\n\n\nNorth America\n6077\n\n\nNorthern Africa\n47\n\n\nOceania\n902\n\n\nSouth America\n12724\n\n\nSouth Asia\n1935\n\n\nSub-Saharan Africa\n3884\n\n\nWestern & Central Europe\n2129\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table takes longer to process, as we must read each row, process that information into numbers, that we then compare. The first plot abstracts this for us by using bigger bars for bigger numbers - this makes it much easier to assess the sizes, but we must compare many sets of bars to decide which are the largest. The final plot simplifies this for us, by sorting the bars by size.\nThe difference in speeds at which our human senses can process information was compared by Danish physicist Tor Nørretranders to standard computer throughputs.\n\nNotice how sight comes out on top as it has the same bandwidth as a computer network. This is followed by touch, and hearing, with taste having the same processing power as a pocket calculator. The small white square in the bottom-right corner is the portion of this processing of which we are cognitively aware.\nNot only do our visual senses dominate our sensory processing, but the amount of data and the speed with which we process are far higher than we are aware of. This is known as pre-attentive processing. Pre-attentive processing is subconscious and fast - it take 50-500ms for the eye and brain to process and react to simple visual stimuli. This is clearly much faster thanour brain could process the data table in the small example above. So, turning our data into visual representations means we can process far more information much more quickly.\n\n\n\nThe key idea of data visualisation is that quantitative and categorical information is encoded by visual variables that can be easily perceived. Visual variables are “the differences in elements of a visual as perceived by the human eye”. Essentially these are the fundamental ways in which graphic symbols can be distinguished. When we view a graph, the goal is to decode the graphical attributes and extract information about the data that was encoded\nA number of authors have proposed sets of visual variables that are easy to detect visually:\n\nPosition\nLength\nDirection\nAngle\nArea\nShape\nColour, Texture\nVolume\n\n\n\nWhen using a common coordinate system, position is the easiest feature to recognise and evaluate with regard to elements in space.\nExample: Scatter plots, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s easy to compare separate scales repeated with the same axis even if they are not aligned.\nExample: Lattice/Grid/Facet plots\n\n\n\n\n\n\n\n\n\n\n\n\nLength can effectively represent quantitative information. The human brain easily recognises proportions and evaluates length, even if the objects are not aligned.\nExample: Bar charts, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nAngles help to make comparisons by providing a sense of proportion. Angles are harder to evaluate than length or position, but pie charts are as efficient with small numbers of categories.\nExample: Pie charts\n\n\n\n\n\n\n\n\n\n\n\n\nThe relative magnitude of areas is harder to compare versus the length of lines. The second direction requires more effort to process and interpret.\nExample: Bubble plots, Treemaps, Mosaic plots, Corrplots\n\n\n\n\n\n\n\n\n\n\n\n\nHue is what we usually mean by ‘colour’. Hue can be used to highlight, identify, and group elements in a visual display.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nColour has many aspects. Saturation is the intensity of a single hue. Increasing intensities of colour can be perceived intuitively as numbers of increasing value.\nExample: Heatmaps\n\n\n\n\n\n\n\n\n\n\n\n\nGroups can be distinguished by different shapes, though comparison requires cognition which makes it less effective than with colour.\nExample: Glyph plots, Scatterplots\n\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen colour is not available, different shadings or fills can be applied where previously we would use hue. Generally, these textures are seldom used in modern visualisations as they are less effective than colour.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nVolume refers to using 3D shapes to represent information. But 3D objects are hard to evaluate in a 2D space, making them particularly difficult to read effectively.\nExample: 3D charts\n To make plots appear 3D in a 2D plot, we must introduce a forced perspective, which distorts the quantitative information that we’re trying to present. For example, consider the following 3D barplot:\n\n\nSome bars get hidden behind others\nThe perspective effect makes bars at the front appear taller than those at the back\nIts difficult to read the numerical values\nThe quantitative data is only 1D -the vertical axis. The 3D chart has added 2 un-needed dimensions to the plot, which has compromised its ability to present the data without distortion.\n\n\n3D pie charts are even worse\n\n\n\nThe different visual variables have different levels of efficiency when visually interpreting values of different size. Different tasks will have different rankings. In general, we should use encodings at the top of the scale where possible (and sensible). For instance, when assessing the magnitude of a quantitative variable, we would rank the encodings something like this:\n\nPosition: Common Scale\nPosition: Non-aligned Scale\nLength\nDirection\nAngle\nArea\nColour: Hue\nColour: Saturation\nShape\nTexture\nVolume\n\n\n\n\nWhen displaying multiple quantities, we can combine encodings:\n\nSome encodings can be combined and visually decoded separately. These are separable encodings.\nOther combinations cannot be easily decoded separately, and are integral encodings.\n\n\nSuppose the red point is of interest - finding it among a low number of points is relatively easy (top left). With only two encodings and low density, it is easy to spot the unusual point.\nIncreasing the number of points makes it a little harder to find, but the contrast between colours helps (top centre). Position and hue are clearly separable encodings.\nIf we repeat the same experiment but changing point shape instead, the task becomes harder (top right). Shape requires more effor to process as an encoding.\nAmong 100 points, the triangle is almost lost. Shape is a more challenging feature to distinguish. (bottom left)\nMixing colour and shape compounds the problem further! (bottom centre)\n\n\n\n\n\n\n\n\n\n\nHere, we are juggling many data encodings at once. Horizontal and vertical position of the points indicate numerical values of two variables Colour and point shape indicating values of two categorical variables. Clearly, some aspects are more separable than others (e.g. x and y postition). Using many encodings with multiple different options to show your data become rapidly uninterpretable (below left), unless your data has a great deal of structure to help make sense of things (below right).\n\nN &lt;- 150\nlibrary(mvtnorm)\nxs &lt;- rmvnorm(150, c(0,0),matrix(c(1,0.5,0.5,1),nr=2))\ncs &lt;- cols[sample(1:4,N,replace=TRUE)]\nps &lt;- c(3,15,16,17)[sample(1:4,N,replace=TRUE)]\npar(mfrow=c(1,2))\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=ps,col=cs);graphics::box()\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=c(3,15,16,17)[cut(xs[,2],4)],col=cols[cut(xs[,1],4)]);graphics::box()\n\n\n\n\n\n\n\n\nThe plot below shows the prevalence of Diabetes and Obesity by county in the USA. Here, multiple inseparable encodings have been used, namely two colour hues with blue indicating obesity, and red indicating Diabetes, with intensity of the colours and their combinations showing the level of prevalance in each county. It is almost impossible to disentangle the obesity information from the diabetes information - these are integral encodings. The only obvious features are dark vs light shades of colour - the saturation.\n\n\n\n\n\nThe human brain is wired to see structure, logic, and patterns. It attempts to simplify what it sees by subconsciously arranging parts of an image into an organised whole, rather than just a series of disparate elements. The Gestalt principles were developed to explain how the brain does this, and can be used to aid (and break) data visualisation.\n\nProximity\nSimilarity\nEnclosure\nConnectedness\nClosure\nContinuity\nFigure and Ground\nFocal Point\n\n\n\nThe Proximity principle says that we perceive visual elements which are close together as belonging to the same group. This is easily seen in scatterplots, where we associatethe proximity in the plot with similarity of the object.\n\n\n\n\n\n\n\n\n\nThe same idea applies more generally and can be applied to other plots, where we can arrange the plot to group items we want to perceive as belonging together. Which of the two plots below best compares the sales per country?\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nSpatial proximity takes precedence over all other principles of grouping.\nUse proximity to focus on the visualisation goal, by keeping the main data points closer together.\n\n\n\n\nWe perceive elements with shared visual properties as being “similar”. Objects of similar colors, similar shapes, similar sizes, and similar orientations are instinctively perceived as a group. For example, in the scatterplots below the use of colour reinforces a sense of commonality with points of the same colour that is stronger than the three loose clusters we observe with proximity alone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nUse colour, shape, or size to group visual objects together.\nThe Similarity Principle can help you more readily identify which groups the displayed data belong to.\nColour can be used effectively when associated with intuitive quantities, e.g. red=financial loss, blue=negative temperature. Beware that association of colour with particular concepts varies around the world. An important example is that in Europe and the Americas coloring an upward trend in finanical markets usually uses green or blue is used to denote an upward trend and red is used to denote a downward trend, but in mainland China, Japan, South Korea, and Taiwan, the reverse is true.\nThe idea can be useful when similar colours, shapes, sizes are used consistently across multiple graphics.\n\n\n\n\nThe enclosure principle addresses the fact that enclosing a group of objects brings our attention to them, they are processed together, and our mind perceives them as connected.\n\nRecommendations to aid effective data visualisation:\n\nEnclose objects that you want to be perceived as grouped in a container.\nEnclosures can be used to highlight regions of a visualisation that can be “zoomed in” to give extra detail.\n\n\n\n\nConnectedness says that objects that are connected in some way, such as by a line, are seen as part of the same group. This supersedes other principles like proximity and similarity in terms of visual grouping perception because putting a direct connection between objects is a strong factor in determining the grouping of objects.\nRecommendations to aid effective data visualisation:\n\nConnecting grouped elements by lines is one of the strongest ways to visualise a grouping in the data\nThis is particularly natural with time series, but generally should be avoided unless the x-axis has a similar meaning\nParallel coordinate plots exploit this principle to connect individual data observations\n\n\n\n\nClosure states that when the brain sees complex arrangements of elements, it organises them into recognisable patterns. While this is usually helpful, it can occasionally cause problems. When the human brain is confronted with an incomplete image, it will fill in the blanks to complete the image and make it make sense.\nConsider the three plots of a time series below. The first plot is incomplete, showing a gap around 2013. Absent any other information, our brains would intuitively connect the lines on either side to give the impression of a behaviour like the plot in the middle. The true data actually followed the right plot. If we ignored the gap entirely and plotted all the data, we would draw a time series like the middle curve which would be highly misleading.\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nBe careful when showing graphs with breaks because the human mind tends to form complete shapes even if the shape is incomplete.\nSimilarly, beware joining all points up with lines when there are large gaps in the data - this is just falling into the same trap!\nIf your data have a lot of gaps, use points not lines.\n\n\n\n\nContinuity states that human brains tend to perceive any line or trend as continuing its established direction. The eye follows lines, curves, or a sequence of shapes to determine a relationship between elements.\nFor example, compare the two barplots below. The plot on the right is more easily readable than the one on the left.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nArrange visual objects in a line to simplify grouping and comparison. This happens naturally on scatterplots with obvious trends. Another example would be using bar chars ordered by y-value.\nUsing lines in time series graphs exploits continuity by joining points into a series\nThis can be paired with using colour saturation to emphasise the continuity along a secondary encoding.\n\n\n\n\nThe Figure and Ground principle says the brain will unconsciously place objects either in the foreground or the background. Background subtraction is a “brain technique” which allows an images foreground shapes to be extracted for further processing. To ensure that we can easily recognise patterns and features in a visualisation, we must ensure that the background and foreground elements are sufficiently different that we can easily identify the data from the background of the plot.\nThe plots below are two examples of doing this badly. The low contrast between the background and the data points makes it difficult to read. In particular, beware using yellow or other pale shades on a white background as they can be rendered nearly invisible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nEnsure there is enough contrast between your foreground and background to make charts and graphs more legible and not misleading.\nChoose colours and contrast levels that make your foreground image stand out.\nTransparency can help push less important features to the background.\nAvoid colour overload with many different and contrasting colours. Stick to a small number of distinct hues, or a scale of different intensities.\n\n\n\n\nA relatively recent addition, the focal point principle says that elements that visually stand out are the first thing we see and process in an image. This is related to the Figure and Ground principle, where we make particular elements stand out prominently from the background.\n\n\nLoading required package: grid\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nDistinctive characteristics (e.g., a different color or a different shape) can be used to highlight and create focal points.\nMosaicplots with \\(\\chi^2\\) shading automatically highlight “interesting” features, which immediately draws the eye\nUsing a substantially different colour or intensity can make the feature ‘pop out’ into the foreground\n\n\n\n\n\nThe idea of ‘graphical excellence’ was developed by Edward Tufte. Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. In particular, he said that good graphical displays of data should:\n\nshow the data,\ninduce the viewer to think about the substance,\navoid distorting what the data says,\npresent many numbers in small space,\nmake large data sets coherent,\nencourage comparison between data,\nreveal the data at several levels of detail,\nhave a clear purpose: description, exploration, tabulation or decoration.\n\nUnfortunately, it’s all too easy (and sometimes tempting) to ignore some of these principles to try and prove a particular point.\n\n\nRecall the Anscombe quartet data were identical when examined using simple summary statistics, but vary considerably when graphed\n\nlibrary(datasets)\ndata(anscombe)\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn ill-specified hypothesis or model cannot be rescued by a graphic. No matter how clever or fancy they are!\n\n\n\n\n\n\n\n\n\nThe correlation between these two series is \\(0.952\\), however there is obviously no genuine relationship here. Beware spurious correlations that lead to spurious conclusions, and remember that correlation does not imply causation!\n\n\n\nGraphs rely on our understanding that a number is represented visually by the magnitude of some graphical element.\n“The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the quantities represented.” — E Tufte\nTufte proposed measuring the violation of this principle by: \\[ \\text{Lie factor} = \\frac{\\text{size of effect in graphic}}{\\text{size of effect in data}}\\]\nA good graph should be between 0.95 and 1.05. Anything outside of this is distorting the numerical effect in the data.\nThe image below is hopelessly distoring the proportions, which don’t even add up to one. The graphical element of six equal sized segments bear no resemblance to the data whatsoever!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelective choice of axis ranges is one of the most common abuses of data graphics by disproportionately exaggerating visual effects. Where possible common axes should be used, and when emphasising relative values the inclusion of the origin (0) is recommended.\nThis plot from the Daily Mail below substantially distorts the data by starting the horizontal axis at 0.55 rather than zero. Notice how this substantially inflates the size of the blue bar relative to the red one.\n\n\n\n\n\n\n\n\n\nA more truthful plot would look like this.\n\n\n\n\n\n\n\n\n\nThough it is questionable as to whether a plot is needed to compare \\(0.6\\) with \\(0.7\\)! We probably don’t need the machinery of data visualisation to assess this difference.\n\n\n\nOmitting axis labels is perhaps even worse than being selective about what ranges you draw. Omitting numerical labels on the axes makes any meaningful comparison or interpretation impossible by removing the connection of the graphic with the numerical quantity it represents.\nPolitics is a common source of badly presented data distorted to prove a point. For instance, this tweet showing a selective part of a data set with no numbers to give any sens of scale:\n\n\n\n\n\n\n\n\n\nHow big is the difference between these time series? 0.1? 1? 100? Accurate interpretation is impossible.\nWhereas this shows a more complete picture, with a longer history:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe rely on our visual perception to interpret the graphical elements in terms of numerical values. Plotting simple data using 3D plots introduce a forced perspective, unnecessarily distorting our perception of the data. In 3D plots, the plot region is no longer a rectangle but is distorted - compressing distances at one side and expanding them at the other.\n\n\n\n\n\n\n\n\n\nThis is only plotting the integers 1 to 4, but it is not easy to identify the sizes of the bars. All of the bars appear to be smaller than their defined values, and it is difficult to assess relative sizes - does bar D really look \\(4\\times\\) larger than bar A? Do we really need a barplot with a fake 3rd dimension to compare four integers?\n\n\n\nChartjunk is defined as content-free decoration of a data graphic that hinders the interpretation. This sort of nuisance decoration becomes problematic as it becomes hard to extract the data in the foreground of the plot when it is cluttered and surrounded by other decorations (see the Figure and Ground principle earlier). In general, avoid unnecessary distraction and focus on the data!\n\n\n\n\n\n\n\n\n\nThere are a lot of unnecessary and confusing elements to this simple plot:\n\nA very heavy 3D projection which seriously distorts the plot region\nA drop shadow that has no value\nRedundant use of bar labels and a plot legend.\nColouring the bars is probably not even necessary, as the bars are already labelled and each bar has a unique colour\nPoor choice of colours - Europe and Oceania are two shades of red, implying a degree of similarity\n\n\n\n\nA plot isn’t always the best way to show simple data - ask yourself if a drawing a plot is necessary. In particular, if the data are simple then keen the plot simple - contriving elaborate plots out of very little information is just confusing.\nThe plot below shows the proportion of students enrolling at a US college for two age groups - below 25, and 25+.\n\n\n\n\n\n\n\n\n\nAccording to Edward Tufte in his book `The Visual Display of Quantitative Information’ (2002):\n“This may well be the worst graphic ever to find its way into print.” — E Tufte\nSince all students will fall into one group or the other, it is clear that we can get one time series by subtracting the other from 100%. So, this is trying to show a single time series of 4 points. However, they do just about everything wrong:\n\nThe two time series being plotted are complementary (i.e. they sum to one), so plotting both series is redundant\nAn exaggerated 3D effect is used for no reason\nEach series is shaded using two different colours\nThe \\(y\\) axes ranges includes neither 0 nor 100, so and skips over two sizeable ranges of values (notice the squiggles)\nThe data are interpolated with smooth lines in a rather strange way\n\nIf we were to just plot the data, we would simply see the following\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have extensively used colour in our graphics to highlight features of interest, distinguish different groups, or even indicate values of quantitative variables. Encoding features with colour can be effective, but colour needs to be used appropriately to be successful.\n\n\nThere are three common patterns of use of colour encodings:\n\nCategorical - distinguish different groups\nSequential - indicate different levels of a quantitative variable\nDiverging - indicate different levels and direction of a quantitative variable\n\nCategorical encodings are used to distinguish multiple different groups that have no intrinsic ordering, e.g. levels of a categorical variable. It is important to use a collection of sufficiently distinct and easily recognisable hues (colours) to easily distinguish multiple groups in your visualisation. The groups have no relationship, the hues should be as different as possible and preferably of a similar intensity. The main challenge with this encoding is that only a small number of categories can be encoded this way before we run out of sufficiently different colours.\n\n\n\n\n\n\n\n\n\nSequential encodings are used to represent values of a quantitative variable. By varying the intensity of the colour we can indicate a quantitivate variable by associating large values with more intense (saturated) colour, and lower values with less intense colours. A common example of this is on heatmaps, or more general maps such as of rainfall levels in weather forecasts. Typically, we restrict the colour to many shades of a single hue, but additional shades can be used if meaningful (e.g. to indicate extreme rainfall). While effective at highlighting major differences by major contrasts in the colour intensity, it is more challenging to detect smaller differences as subtle changes in colour and to decode numerical information from the plot.\n\n\n\n\n\n\n\n\n\nDiverging encodings are used to represent the values and direction of a quantitative variable. Combining the two previous ideas, we use two sequential schemes based on substantially different hues (e.g. red and blue) that meet in the middle. Now the colour intensity indicates the magnitude of the value, and the hue of the colour indicates its direction. For example, we have seen this used already in plots of correlation matrices, where strong colour indicated strong correlations and red/blue indicated negative/positive correlation. Another common example is a maps of temperatures in weather forecasts, where warm temperatures use one hue and cold temperatures use another, and they meet in the middle.\n\n\n\n\n\n\n\n\n\n\n\n\nWhile we may have a particular set of colours in mind to use with our visualisations, it can be difficult to set this up in R. There are many different ways to specify colours, and not all of them are intuitive to use:\n\nIntegers codes: R interprets integer values as particular colours from its default palette. For instance, 1=“black”, 2=“red”, 3=“green”, etc.\nColour names: R will a long list of named colours, e.g. “black”, “red”, “green3”, “skyblue“, “cyan”. The full list of names can be found http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\nRGB and similar: Any colour can be represented as a combination of proportions of red, green and blue. R’s rgb function will convert those red, green and blue amounts to a usable colour: black=rgb(0,0,0); green3=rgb(0, 205, 0, max=255), cyan=rgb(0, 255, 255,max=255). The hsv and hcl provide similar functions using alternative colour specifications.\nHexadecimal: The RGB values can also be expressed as a hexademical code: black=“#000000”; cyan=“#00FFFF”.\n\n\n\nR has a default palette of colours used for the default colours in graphs. Integer colour codes are the corresponding colour in this default list. The default palette contains the following eight elements.\n\npalette()\n\n[1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n[8] \"gray\"   \n\npie(rep(1, 8), labels = sprintf(\"%d (%s)\", 1:8, palette()), col = 1:8)\n\n\n\n\n\n\n\n\nWe can replace the standard palette with a vector of our own colours.\n\npalette(c(\"black\",\"#3366cc\",\"#61D04F\",\"#C45560\", \"#F5C710\", \"#CD0BBC\"))\npie(rep(1, 6), labels = sprintf(\"%d (%s)\", 1:6, palette()), col = 1:6)\n\n\n\n\n\n\n\n\nR also has a number of functions that will generate a list of n colours according to a particular scheme. These can be used to replace the default palette, or as input for a particular plot.\n\n\n\n\n\n\n\n\n\nWhich of these are better for: * categorical? * sequential? * diverging?\nIt is also worth noting that we’ve used these to colour area. Their effectiveness may vary when colouring points or lines.\n\n\n\n\n\n\nThe human brain can read and process visual information far faster than any other form - this is why data visualisation is so effective.\nData visualisation uses visual variables to encode the data in a graphical way.\nSome encodings are more effective than others. Some encodings work well together, and others less so.\nThe Gestalt principles describe how the brain sees patterns - we can use this to create better/worse visualisations.\nColour is an effective encoding, but we should carefully choose the colour palette to get the most out of it"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#exploratory-data-analysis",
    "href": "Lecture1a_IntroViz.html#exploratory-data-analysis",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "Exploratory data analysis (EDA) should be one of the first steps in analysing any data set and was pioneered as a discipline of its own by John Tukey in the 1960s and 1970s.\n\n\n\n\nExploratory Data Analysis chart\n\n\n\nIn the words of Tukey:\n\n“Exploratory data analysis is detective work — in the purest sense — finding and revealing the clues.”\n“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone — as the first step.”\n\nSo, the definition of EDA is fairly self-explanatory. We seek to explore the data, by asking questions and looking for clues to help us better understand the data. The purpose is generally to gain sufficient information to make reasoned and justifiable hypotheses to explore with more formal methods, or to identify sensible modelling approaches (and exclude others). The features and insights we gather while exploring will suggest what appropriate strategies for our subsequent analysis.\nTherefore, it is simply good practice to try to understand and gather as many insights from the data as possible before even attempting before any modelling or inference. Without a solid understanding of the data, we do not know if the techniques we apply are appropriate, and so we risk our inferences and conclusions being invalid.\nThat said, EDA is not a one-off process. It is often iterative, going back and forth between exploration and modelling/analysis as each part of the process can suggest new questions or hypotheses to investigate.\nExploratory Data Analysis is not a formal process and it does not have strict rules to follow or methods to apply: the overarching goal is simply to develop an understanding of the data set. However, typically we do try and do this without fitting any complex models or making assumptions about our data. We’re looking to see what the data tell us, not what our choice of technique or model says. We have insufficient knowledge of the properties of our data to exploit sophisticated techniques.\nThe “detective work” of EDA is essentially looking for clues in the data that will reveal insights about what is actually going on with the problem from which they come — but this requires looking both in the right places and with the right magnifying glass.\nData sets rarely arrive with a manual, or a shopping list of specific features to investigate. Absent any formal structure, the best approach is to pursue any lead that occurs to you, and ask questions - lots of them — some will lead to insights, others will be dead ends.\nSome obvious ‘clues’ and features to investigate in an unseen data set are: 1 Features and properties of individual variables and collections 2 Identifying important and unimportant variables 3 Identifying structure, patterns and relationships between variables 4 Anomalies, errors, and outliers 5 Variation and distributions 6 Missing values\nAs we’re only exploring the data with minimal assumptions, the tools of EDA must be mathematically quite simple and robust as we should not be relying on assumptions of distributions or structure that may not be justified. We rely primarily on:\n\nStatistical summaries\nGraphics and visualisations\n\nOur focus will be extensively on making a graphical exploration of the data."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#data-visualisation",
    "href": "Lecture1a_IntroViz.html#data-visualisation",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "Data visualisation is the creation and study of the visual representation of data.\nLike EDA, there is no complex theory about graphics — in fact, there is not much theory at all! The topics are not usually covered in depth in books or lectures as they build on relatively simple statistical concepts. Once the basic graphical forms have been described, textbooks usually move on to more mathematical ideas such as proving the central limit theorem.\nExploratory Data Analysis through investigation of data graphics is sometimes called Graphical Data Analysis (GDA).\nThere are some standard plots and graphics that are applicable in some fairly generaly situations, but\nA good visualisation reveals the data, and communicates complex ideas with clarity, precision and efficiency. Some features of a good data visualisation would be\n\nShow the data!\nInduce the viewer to think about the substance rather than the methodology, design, etc.\nAvoid distorting what the data have to say\nPresent many numbers in a small space\nMake large data sets coherent\nEncourage comparisons between different pieces of data\n\n\nGood data visualisations can communicate the key features of complex data sets more convincingly and more effectively than the raw data often can achieve.\nTypically, data visualisation is used for one of two purposes:\n1- Analysis - used to find patterns, trends, aid in data description, interpretation * Goal: the “Eureka!” moment * Many images for the analyst only, linked to analysis/modelling * Typically many rough and simple plots used to detect interesting features of the data and suggest directions for future investigation, analysis or modelling\n2- Presentation - used to attract attention, make a point, illustrate a conclusion * Goal: The “Wow!” moment. * A single image suitable for a large audience which tells a clear story * Once the key features and behaviours of the data are known, the best graphic can be produced to show those features in a clear way. Often targetting a less technical audience.\nFor example, the visualisation below shows a presentation of the number of cases of measles per 100,000 for the 50 US states over time. The impact of vaccination on the levels of measles is striking and clear.\n\nPresentation quality graphics can venture into the realm of data art, but this is rather beyond what we could hope to achieve in our short course. These visualisations, often called infographics, try to present data in a non-technical way that can easily be understood by non-experts. For example, the following graph illustrates the scale of the amount of waste plastic from plastic bottle sales over 10 years, relative to New York."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#making-effective-data-visualisation",
    "href": "Lecture1a_IntroViz.html#making-effective-data-visualisation",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "First, a little bit of historical context. Data visualisation (and statistics) are relatively new disciplines - relative to the rest of mathematics. For data visualisation, William Playfair (1759-1823) is often credited for pioneering many of the graphical forms we still use today. Slightly later, Florence Nightingale (1820-1910) became one of the first people to persuade the public and influence public policy with data visualisation. Despite being better known for her achievements in nursing, Florence was the first female member of the Royal Statistical Society. Her rose diagrams were an innovative combination of pie chart and time series, and were used to illustrate the terrible conditions suffered by soldiers in the Crimean war.\nThese early visualisations were difficult to produce and required a combination of art and intuition. The 20th century brought computers and the ability to process and visualise increasing amounts of data with ease. Ultimately, a number of standard graphics were developed that exploit our visual perception to interpret complex data - most of which we have seen in the course so far.\nSo, despite having a long history, what is it about data visualisation that is so effective that we continue to do it? The answer is that depicting data graphically can be extremely effective as it takes advantage of the human brain’s natural strengths at quickly and efficiently processing visual information. Understanding this will help you make better visualisations!\nThe human brain has developed many subconscious natural abilities to process visual information and make sense of the world around us. We are constantly processing and interpreting the visual signals from our eyes and much of this happens sub-consciously without any actual effort. The reason for this is that this analysis relies on the visual perception part of our brain, rather than cognitive “thinking” part.\nVisual perception is the ability to see, interpret, and organise our environment. It’s fast and efficient, and happens automatically. Cognition, which is handled by the cerebral cortex, is much slower and requires more effort to process information. So, presenting data visually exploits our quick perception capabilities and helps to reduce cognitive load.\nTo illustrate these difference consider the following table and plots. From which of the three presentations of the data is it easiest to identify which 3 regions have the highest available renewable water resources?\n\n\n\n\n\nregion\nkm3\n\n\n\n\nCentral America and Caribbean\n735\n\n\nCentral Asia\n242\n\n\nEast Asia\n3410\n\n\nEastern Europe\n4448\n\n\nMiddle East\n484\n\n\nNorth America\n6077\n\n\nNorthern Africa\n47\n\n\nOceania\n902\n\n\nSouth America\n12724\n\n\nSouth Asia\n1935\n\n\nSub-Saharan Africa\n3884\n\n\nWestern & Central Europe\n2129\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table takes longer to process, as we must read each row, process that information into numbers, that we then compare. The first plot abstracts this for us by using bigger bars for bigger numbers - this makes it much easier to assess the sizes, but we must compare many sets of bars to decide which are the largest. The final plot simplifies this for us, by sorting the bars by size.\nThe difference in speeds at which our human senses can process information was compared by Danish physicist Tor Nørretranders to standard computer throughputs.\n\nNotice how sight comes out on top as it has the same bandwidth as a computer network. This is followed by touch, and hearing, with taste having the same processing power as a pocket calculator. The small white square in the bottom-right corner is the portion of this processing of which we are cognitively aware.\nNot only do our visual senses dominate our sensory processing, but the amount of data and the speed with which we process are far higher than we are aware of. This is known as pre-attentive processing. Pre-attentive processing is subconscious and fast - it take 50-500ms for the eye and brain to process and react to simple visual stimuli. This is clearly much faster thanour brain could process the data table in the small example above. So, turning our data into visual representations means we can process far more information much more quickly."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#encoding-data",
    "href": "Lecture1a_IntroViz.html#encoding-data",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The key idea of data visualisation is that quantitative and categorical information is encoded by visual variables that can be easily perceived. Visual variables are “the differences in elements of a visual as perceived by the human eye”. Essentially these are the fundamental ways in which graphic symbols can be distinguished. When we view a graph, the goal is to decode the graphical attributes and extract information about the data that was encoded\nA number of authors have proposed sets of visual variables that are easy to detect visually:\n\nPosition\nLength\nDirection\nAngle\nArea\nShape\nColour, Texture\nVolume\n\n\n\nWhen using a common coordinate system, position is the easiest feature to recognise and evaluate with regard to elements in space.\nExample: Scatter plots, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s easy to compare separate scales repeated with the same axis even if they are not aligned.\nExample: Lattice/Grid/Facet plots\n\n\n\n\n\n\n\n\n\n\n\n\nLength can effectively represent quantitative information. The human brain easily recognises proportions and evaluates length, even if the objects are not aligned.\nExample: Bar charts, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nAngles help to make comparisons by providing a sense of proportion. Angles are harder to evaluate than length or position, but pie charts are as efficient with small numbers of categories.\nExample: Pie charts\n\n\n\n\n\n\n\n\n\n\n\n\nThe relative magnitude of areas is harder to compare versus the length of lines. The second direction requires more effort to process and interpret.\nExample: Bubble plots, Treemaps, Mosaic plots, Corrplots\n\n\n\n\n\n\n\n\n\n\n\n\nHue is what we usually mean by ‘colour’. Hue can be used to highlight, identify, and group elements in a visual display.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nColour has many aspects. Saturation is the intensity of a single hue. Increasing intensities of colour can be perceived intuitively as numbers of increasing value.\nExample: Heatmaps\n\n\n\n\n\n\n\n\n\n\n\n\nGroups can be distinguished by different shapes, though comparison requires cognition which makes it less effective than with colour.\nExample: Glyph plots, Scatterplots\n\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen colour is not available, different shadings or fills can be applied where previously we would use hue. Generally, these textures are seldom used in modern visualisations as they are less effective than colour.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nVolume refers to using 3D shapes to represent information. But 3D objects are hard to evaluate in a 2D space, making them particularly difficult to read effectively.\nExample: 3D charts\n To make plots appear 3D in a 2D plot, we must introduce a forced perspective, which distorts the quantitative information that we’re trying to present. For example, consider the following 3D barplot:\n\n\nSome bars get hidden behind others\nThe perspective effect makes bars at the front appear taller than those at the back\nIts difficult to read the numerical values\nThe quantitative data is only 1D -the vertical axis. The 3D chart has added 2 un-needed dimensions to the plot, which has compromised its ability to present the data without distortion.\n\n\n3D pie charts are even worse\n\n\n\nThe different visual variables have different levels of efficiency when visually interpreting values of different size. Different tasks will have different rankings. In general, we should use encodings at the top of the scale where possible (and sensible). For instance, when assessing the magnitude of a quantitative variable, we would rank the encodings something like this:\n\nPosition: Common Scale\nPosition: Non-aligned Scale\nLength\nDirection\nAngle\nArea\nColour: Hue\nColour: Saturation\nShape\nTexture\nVolume\n\n\n\n\nWhen displaying multiple quantities, we can combine encodings:\n\nSome encodings can be combined and visually decoded separately. These are separable encodings.\nOther combinations cannot be easily decoded separately, and are integral encodings.\n\n\nSuppose the red point is of interest - finding it among a low number of points is relatively easy (top left). With only two encodings and low density, it is easy to spot the unusual point.\nIncreasing the number of points makes it a little harder to find, but the contrast between colours helps (top centre). Position and hue are clearly separable encodings.\nIf we repeat the same experiment but changing point shape instead, the task becomes harder (top right). Shape requires more effor to process as an encoding.\nAmong 100 points, the triangle is almost lost. Shape is a more challenging feature to distinguish. (bottom left)\nMixing colour and shape compounds the problem further! (bottom centre)\n\n\n\n\n\n\n\n\n\n\nHere, we are juggling many data encodings at once. Horizontal and vertical position of the points indicate numerical values of two variables Colour and point shape indicating values of two categorical variables. Clearly, some aspects are more separable than others (e.g. x and y postition). Using many encodings with multiple different options to show your data become rapidly uninterpretable (below left), unless your data has a great deal of structure to help make sense of things (below right).\n\nN &lt;- 150\nlibrary(mvtnorm)\nxs &lt;- rmvnorm(150, c(0,0),matrix(c(1,0.5,0.5,1),nr=2))\ncs &lt;- cols[sample(1:4,N,replace=TRUE)]\nps &lt;- c(3,15,16,17)[sample(1:4,N,replace=TRUE)]\npar(mfrow=c(1,2))\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=ps,col=cs);graphics::box()\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=c(3,15,16,17)[cut(xs[,2],4)],col=cols[cut(xs[,1],4)]);graphics::box()\n\n\n\n\n\n\n\n\nThe plot below shows the prevalence of Diabetes and Obesity by county in the USA. Here, multiple inseparable encodings have been used, namely two colour hues with blue indicating obesity, and red indicating Diabetes, with intensity of the colours and their combinations showing the level of prevalance in each county. It is almost impossible to disentangle the obesity information from the diabetes information - these are integral encodings. The only obvious features are dark vs light shades of colour - the saturation."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#gestalt-principles",
    "href": "Lecture1a_IntroViz.html#gestalt-principles",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The human brain is wired to see structure, logic, and patterns. It attempts to simplify what it sees by subconsciously arranging parts of an image into an organised whole, rather than just a series of disparate elements. The Gestalt principles were developed to explain how the brain does this, and can be used to aid (and break) data visualisation.\n\nProximity\nSimilarity\nEnclosure\nConnectedness\nClosure\nContinuity\nFigure and Ground\nFocal Point\n\n\n\nThe Proximity principle says that we perceive visual elements which are close together as belonging to the same group. This is easily seen in scatterplots, where we associatethe proximity in the plot with similarity of the object.\n\n\n\n\n\n\n\n\n\nThe same idea applies more generally and can be applied to other plots, where we can arrange the plot to group items we want to perceive as belonging together. Which of the two plots below best compares the sales per country?\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nSpatial proximity takes precedence over all other principles of grouping.\nUse proximity to focus on the visualisation goal, by keeping the main data points closer together.\n\n\n\n\nWe perceive elements with shared visual properties as being “similar”. Objects of similar colors, similar shapes, similar sizes, and similar orientations are instinctively perceived as a group. For example, in the scatterplots below the use of colour reinforces a sense of commonality with points of the same colour that is stronger than the three loose clusters we observe with proximity alone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nUse colour, shape, or size to group visual objects together.\nThe Similarity Principle can help you more readily identify which groups the displayed data belong to.\nColour can be used effectively when associated with intuitive quantities, e.g. red=financial loss, blue=negative temperature. Beware that association of colour with particular concepts varies around the world. An important example is that in Europe and the Americas coloring an upward trend in finanical markets usually uses green or blue is used to denote an upward trend and red is used to denote a downward trend, but in mainland China, Japan, South Korea, and Taiwan, the reverse is true.\nThe idea can be useful when similar colours, shapes, sizes are used consistently across multiple graphics.\n\n\n\n\nThe enclosure principle addresses the fact that enclosing a group of objects brings our attention to them, they are processed together, and our mind perceives them as connected.\n\nRecommendations to aid effective data visualisation:\n\nEnclose objects that you want to be perceived as grouped in a container.\nEnclosures can be used to highlight regions of a visualisation that can be “zoomed in” to give extra detail.\n\n\n\n\nConnectedness says that objects that are connected in some way, such as by a line, are seen as part of the same group. This supersedes other principles like proximity and similarity in terms of visual grouping perception because putting a direct connection between objects is a strong factor in determining the grouping of objects.\nRecommendations to aid effective data visualisation:\n\nConnecting grouped elements by lines is one of the strongest ways to visualise a grouping in the data\nThis is particularly natural with time series, but generally should be avoided unless the x-axis has a similar meaning\nParallel coordinate plots exploit this principle to connect individual data observations\n\n\n\n\nClosure states that when the brain sees complex arrangements of elements, it organises them into recognisable patterns. While this is usually helpful, it can occasionally cause problems. When the human brain is confronted with an incomplete image, it will fill in the blanks to complete the image and make it make sense.\nConsider the three plots of a time series below. The first plot is incomplete, showing a gap around 2013. Absent any other information, our brains would intuitively connect the lines on either side to give the impression of a behaviour like the plot in the middle. The true data actually followed the right plot. If we ignored the gap entirely and plotted all the data, we would draw a time series like the middle curve which would be highly misleading.\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nBe careful when showing graphs with breaks because the human mind tends to form complete shapes even if the shape is incomplete.\nSimilarly, beware joining all points up with lines when there are large gaps in the data - this is just falling into the same trap!\nIf your data have a lot of gaps, use points not lines.\n\n\n\n\nContinuity states that human brains tend to perceive any line or trend as continuing its established direction. The eye follows lines, curves, or a sequence of shapes to determine a relationship between elements.\nFor example, compare the two barplots below. The plot on the right is more easily readable than the one on the left.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nArrange visual objects in a line to simplify grouping and comparison. This happens naturally on scatterplots with obvious trends. Another example would be using bar chars ordered by y-value.\nUsing lines in time series graphs exploits continuity by joining points into a series\nThis can be paired with using colour saturation to emphasise the continuity along a secondary encoding.\n\n\n\n\nThe Figure and Ground principle says the brain will unconsciously place objects either in the foreground or the background. Background subtraction is a “brain technique” which allows an images foreground shapes to be extracted for further processing. To ensure that we can easily recognise patterns and features in a visualisation, we must ensure that the background and foreground elements are sufficiently different that we can easily identify the data from the background of the plot.\nThe plots below are two examples of doing this badly. The low contrast between the background and the data points makes it difficult to read. In particular, beware using yellow or other pale shades on a white background as they can be rendered nearly invisible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nEnsure there is enough contrast between your foreground and background to make charts and graphs more legible and not misleading.\nChoose colours and contrast levels that make your foreground image stand out.\nTransparency can help push less important features to the background.\nAvoid colour overload with many different and contrasting colours. Stick to a small number of distinct hues, or a scale of different intensities.\n\n\n\n\nA relatively recent addition, the focal point principle says that elements that visually stand out are the first thing we see and process in an image. This is related to the Figure and Ground principle, where we make particular elements stand out prominently from the background.\n\n\nLoading required package: grid\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nDistinctive characteristics (e.g., a different color or a different shape) can be used to highlight and create focal points.\nMosaicplots with \\(\\chi^2\\) shading automatically highlight “interesting” features, which immediately draws the eye\nUsing a substantially different colour or intensity can make the feature ‘pop out’ into the foreground"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#what-to-avoid",
    "href": "Lecture1a_IntroViz.html#what-to-avoid",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The idea of ‘graphical excellence’ was developed by Edward Tufte. Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. In particular, he said that good graphical displays of data should:\n\nshow the data,\ninduce the viewer to think about the substance,\navoid distorting what the data says,\npresent many numbers in small space,\nmake large data sets coherent,\nencourage comparison between data,\nreveal the data at several levels of detail,\nhave a clear purpose: description, exploration, tabulation or decoration.\n\nUnfortunately, it’s all too easy (and sometimes tempting) to ignore some of these principles to try and prove a particular point.\n\n\nRecall the Anscombe quartet data were identical when examined using simple summary statistics, but vary considerably when graphed\n\nlibrary(datasets)\ndata(anscombe)\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn ill-specified hypothesis or model cannot be rescued by a graphic. No matter how clever or fancy they are!\n\n\n\n\n\n\n\n\n\nThe correlation between these two series is \\(0.952\\), however there is obviously no genuine relationship here. Beware spurious correlations that lead to spurious conclusions, and remember that correlation does not imply causation!\n\n\n\nGraphs rely on our understanding that a number is represented visually by the magnitude of some graphical element.\n“The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the quantities represented.” — E Tufte\nTufte proposed measuring the violation of this principle by: \\[ \\text{Lie factor} = \\frac{\\text{size of effect in graphic}}{\\text{size of effect in data}}\\]\nA good graph should be between 0.95 and 1.05. Anything outside of this is distorting the numerical effect in the data.\nThe image below is hopelessly distoring the proportions, which don’t even add up to one. The graphical element of six equal sized segments bear no resemblance to the data whatsoever!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelective choice of axis ranges is one of the most common abuses of data graphics by disproportionately exaggerating visual effects. Where possible common axes should be used, and when emphasising relative values the inclusion of the origin (0) is recommended.\nThis plot from the Daily Mail below substantially distorts the data by starting the horizontal axis at 0.55 rather than zero. Notice how this substantially inflates the size of the blue bar relative to the red one.\n\n\n\n\n\n\n\n\n\nA more truthful plot would look like this.\n\n\n\n\n\n\n\n\n\nThough it is questionable as to whether a plot is needed to compare \\(0.6\\) with \\(0.7\\)! We probably don’t need the machinery of data visualisation to assess this difference.\n\n\n\nOmitting axis labels is perhaps even worse than being selective about what ranges you draw. Omitting numerical labels on the axes makes any meaningful comparison or interpretation impossible by removing the connection of the graphic with the numerical quantity it represents.\nPolitics is a common source of badly presented data distorted to prove a point. For instance, this tweet showing a selective part of a data set with no numbers to give any sens of scale:\n\n\n\n\n\n\n\n\n\nHow big is the difference between these time series? 0.1? 1? 100? Accurate interpretation is impossible.\nWhereas this shows a more complete picture, with a longer history:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe rely on our visual perception to interpret the graphical elements in terms of numerical values. Plotting simple data using 3D plots introduce a forced perspective, unnecessarily distorting our perception of the data. In 3D plots, the plot region is no longer a rectangle but is distorted - compressing distances at one side and expanding them at the other.\n\n\n\n\n\n\n\n\n\nThis is only plotting the integers 1 to 4, but it is not easy to identify the sizes of the bars. All of the bars appear to be smaller than their defined values, and it is difficult to assess relative sizes - does bar D really look \\(4\\times\\) larger than bar A? Do we really need a barplot with a fake 3rd dimension to compare four integers?\n\n\n\nChartjunk is defined as content-free decoration of a data graphic that hinders the interpretation. This sort of nuisance decoration becomes problematic as it becomes hard to extract the data in the foreground of the plot when it is cluttered and surrounded by other decorations (see the Figure and Ground principle earlier). In general, avoid unnecessary distraction and focus on the data!\n\n\n\n\n\n\n\n\n\nThere are a lot of unnecessary and confusing elements to this simple plot:\n\nA very heavy 3D projection which seriously distorts the plot region\nA drop shadow that has no value\nRedundant use of bar labels and a plot legend.\nColouring the bars is probably not even necessary, as the bars are already labelled and each bar has a unique colour\nPoor choice of colours - Europe and Oceania are two shades of red, implying a degree of similarity\n\n\n\n\nA plot isn’t always the best way to show simple data - ask yourself if a drawing a plot is necessary. In particular, if the data are simple then keen the plot simple - contriving elaborate plots out of very little information is just confusing.\nThe plot below shows the proportion of students enrolling at a US college for two age groups - below 25, and 25+.\n\n\n\n\n\n\n\n\n\nAccording to Edward Tufte in his book `The Visual Display of Quantitative Information’ (2002):\n“This may well be the worst graphic ever to find its way into print.” — E Tufte\nSince all students will fall into one group or the other, it is clear that we can get one time series by subtracting the other from 100%. So, this is trying to show a single time series of 4 points. However, they do just about everything wrong:\n\nThe two time series being plotted are complementary (i.e. they sum to one), so plotting both series is redundant\nAn exaggerated 3D effect is used for no reason\nEach series is shaded using two different colours\nThe \\(y\\) axes ranges includes neither 0 nor 100, so and skips over two sizeable ranges of values (notice the squiggles)\nThe data are interpolated with smooth lines in a rather strange way\n\nIf we were to just plot the data, we would simply see the following"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#bonus-content-using-colour-effectively",
    "href": "Lecture1a_IntroViz.html#bonus-content-using-colour-effectively",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "We have extensively used colour in our graphics to highlight features of interest, distinguish different groups, or even indicate values of quantitative variables. Encoding features with colour can be effective, but colour needs to be used appropriately to be successful.\n\n\nThere are three common patterns of use of colour encodings:\n\nCategorical - distinguish different groups\nSequential - indicate different levels of a quantitative variable\nDiverging - indicate different levels and direction of a quantitative variable\n\nCategorical encodings are used to distinguish multiple different groups that have no intrinsic ordering, e.g. levels of a categorical variable. It is important to use a collection of sufficiently distinct and easily recognisable hues (colours) to easily distinguish multiple groups in your visualisation. The groups have no relationship, the hues should be as different as possible and preferably of a similar intensity. The main challenge with this encoding is that only a small number of categories can be encoded this way before we run out of sufficiently different colours.\n\n\n\n\n\n\n\n\n\nSequential encodings are used to represent values of a quantitative variable. By varying the intensity of the colour we can indicate a quantitivate variable by associating large values with more intense (saturated) colour, and lower values with less intense colours. A common example of this is on heatmaps, or more general maps such as of rainfall levels in weather forecasts. Typically, we restrict the colour to many shades of a single hue, but additional shades can be used if meaningful (e.g. to indicate extreme rainfall). While effective at highlighting major differences by major contrasts in the colour intensity, it is more challenging to detect smaller differences as subtle changes in colour and to decode numerical information from the plot.\n\n\n\n\n\n\n\n\n\nDiverging encodings are used to represent the values and direction of a quantitative variable. Combining the two previous ideas, we use two sequential schemes based on substantially different hues (e.g. red and blue) that meet in the middle. Now the colour intensity indicates the magnitude of the value, and the hue of the colour indicates its direction. For example, we have seen this used already in plots of correlation matrices, where strong colour indicated strong correlations and red/blue indicated negative/positive correlation. Another common example is a maps of temperatures in weather forecasts, where warm temperatures use one hue and cold temperatures use another, and they meet in the middle.\n\n\n\n\n\n\n\n\n\n\n\n\nWhile we may have a particular set of colours in mind to use with our visualisations, it can be difficult to set this up in R. There are many different ways to specify colours, and not all of them are intuitive to use:\n\nIntegers codes: R interprets integer values as particular colours from its default palette. For instance, 1=“black”, 2=“red”, 3=“green”, etc.\nColour names: R will a long list of named colours, e.g. “black”, “red”, “green3”, “skyblue“, “cyan”. The full list of names can be found http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\nRGB and similar: Any colour can be represented as a combination of proportions of red, green and blue. R’s rgb function will convert those red, green and blue amounts to a usable colour: black=rgb(0,0,0); green3=rgb(0, 205, 0, max=255), cyan=rgb(0, 255, 255,max=255). The hsv and hcl provide similar functions using alternative colour specifications.\nHexadecimal: The RGB values can also be expressed as a hexademical code: black=“#000000”; cyan=“#00FFFF”.\n\n\n\nR has a default palette of colours used for the default colours in graphs. Integer colour codes are the corresponding colour in this default list. The default palette contains the following eight elements.\n\npalette()\n\n[1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n[8] \"gray\"   \n\npie(rep(1, 8), labels = sprintf(\"%d (%s)\", 1:8, palette()), col = 1:8)\n\n\n\n\n\n\n\n\nWe can replace the standard palette with a vector of our own colours.\n\npalette(c(\"black\",\"#3366cc\",\"#61D04F\",\"#C45560\", \"#F5C710\", \"#CD0BBC\"))\npie(rep(1, 6), labels = sprintf(\"%d (%s)\", 1:6, palette()), col = 1:6)\n\n\n\n\n\n\n\n\nR also has a number of functions that will generate a list of n colours according to a particular scheme. These can be used to replace the default palette, or as input for a particular plot.\n\n\n\n\n\n\n\n\n\nWhich of these are better for: * categorical? * sequential? * diverging?\nIt is also worth noting that we’ve used these to colour area. Their effectiveness may vary when colouring points or lines."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#summary",
    "href": "Lecture1a_IntroViz.html#summary",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The human brain can read and process visual information far faster than any other form - this is why data visualisation is so effective.\nData visualisation uses visual variables to encode the data in a graphical way.\nSome encodings are more effective than others. Some encodings work well together, and others less so.\nThe Gestalt principles describe how the brain sees patterns - we can use this to create better/worse visualisations.\nColour is an effective encoding, but we should carefully choose the colour palette to get the most out of it"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site has been made with Quarto. It is the repository of notes for the Data Visualisation, Exploration and Unsupervised Learning course led by Daniele Turchetti at Durham University."
  },
  {
    "objectID": "Workshop2_DepRelAss.html",
    "href": "Workshop2_DepRelAss.html",
    "title": "Workshop 2 - Dependency, Relationships and Associations",
    "section": "",
    "text": "In this workshop, we will learn how to:\n\n\nDraw scatterplots of pairs of continuous variables\n\n\nUse transparency in our visualisations\n\n\nAssess dependency, relationships and associations\n\n\nVisualise correlation through heatmaps and scatterplot matrices\n\n\n\nYou will need to install the following packages for today’s workshop:\n\nscales for the alpha() function for transparency in plots\npsych for making improved version of scatterplot matrices\n\nCustom packages are exceptionally useful in R, as they provide more specialised functionality beyond what is included in the base version of R. We will typically make use of custom packages to access specialised visualisation functions or particular data sets that are included in the package.\nThere are many ways to install a new package. Here are a few of the easiest:\n\nIn RStudio, open the Tools menu and select Install Packages.... Type the name of the package, and click install.\nIn the console, type install.packages(\"pkgname\") where you replace pkgname with the name of the package you want to install.\nNote that you can’t install your own packages on the NCC server, a set of advanced packages (including the above) are already installed, so you don’t have to worry about this step in that case.\n\n\ninstall.packages(\"scales\")\n\n\n\nScatterplots are a rather simple plot, but incredibly effective at showing structure in the data. By simply plotting points at the coordinates of two numerical variables, we can easily detect any patterns that may appear.\nThe plot function produces a scatterplot of its two arguments. For illustration, let us use the mtcars data set again, containing information on the characteristics of 23 cars. We can plot miles per gallon against weight with the command\n\ndata(mtcars)\nplot(x=mtcars$wt, y=mtcars$mpg)\n\n\n\n\n\n\n\n\nUnsurprisingly, heavier cars do fewer miles per gallon and are less efficient. The relationship here, while clear and negative, is far from exact with a lot of noise and variation.\nIf the argument labels x and y are not supplied to plot, R will assume the first argument is x and the second is y. If only one vector of data is supplied, this will be taken as the \\(y\\) value and will be plotted against the integers 1:length(y), i.e. in the sequence in which they appear in the data.\n\n\nAnother useful optional argument is type, which can substantially change how plot draws the data. The type argument can take a number of different values to produce different types of plot:\n\ntype=\"p\" - draws a standard scatterplot with a point for every \\((x,y)\\) pair\ntype=\"l\" - connects adjacent \\((x,y)\\) pairs with straight lines, does not draw points. Note this is a lowercase L, not a number 1.\ntype=\"b\" - draws both points and connecting line segments\ntype=\"s\" - connects points with ‘steps’ rather than straight lines\n\n\npar(mfrow=c(2,2))\no &lt;- order(mtcars$wt)\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"p\"', ty='p')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"l\"', ty='l')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"b\"', ty='b')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"s\"', ty='s')\n\n\n\n\n\n\n\n\n\n\n\nThe symbols used for points in scatter plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is pch=1 which is a hollow circle. The possible values of pch are shown in the plot below:\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"2\"', pch=2)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"3\"', pch=3)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"7\"', pch=7)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"10\"', pch=10)\n\n\n\n\n\n\n\n\n\n\n\n\nTo deal with issues of overplotting - where dense areas of points are drawn ontop of each other - we can use transparency to make the plot symbols. For example, here are 500 points randomly generated from a 2-D normal distribution. Notice how the middle of the plot is a solid lump of black\n\nplot(x=rnorm(5000),y=rnorm(5000),pch=16)\n\n\n\n\n\n\n\n\nTo ‘fix’ this, we can specify a transparent colour in the col argument by using the alpha function from the scales package:\n\nlibrary(scales)\nplot(x=rnorm(5000),y=rnorm(5000),pch=16,col=alpha('black',0.2))\n\n\n\n\n\n\n\n\nThe alpha function takes two arguments - a colour first, and then the alpha level itself. This should be a number in \\([0,1]\\) with smaller values being more transparent. Finding a good value for alpha is usually a case of trial-and-error, but in general it will be smaller than you might first expect!\nNow with the transparency we can see a bit more structure in the data, and the darker areas now highlight regions of high data density.\n\n\n Download data: engine\nThis rather simple data set contains three numerical variables, each representing different amounts of pollutants emitted by 46 light-duty engines. The pollutants recorded are Carbon monoxide (CO), Hydrocarbons (HC), and Nitrogen oxide (NOX), all recorded as grammes emitted per mile.\n\n\n\nDownload the engine data set and load it into your workspace\nConstruct three scatterplots to investigate the relationships between every pair of pollutants:\n\nCO versus HC\nCO versus NOX\nHC versus NOX\n\nWhat are the relationships between the amounts of different pollutants emitted by the various engines? In particular, which pollutants are positively associated and which are negatively associated?\n\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,3))\nplot(x=engine$CO, y=engine$HC, pch=16) \nplot(x=engine$CO, y=engine$NOX, pch=16)  \nplot(x=engine$HC, y=engine$NOX, pch=16) \n\n\n\n\n\n\n\n\nThe first plot shows a strong positive linear relationship; The second plot a fairly strong, decreasing non-linear relationship; The third plot a weaker linear relationship.\n\nThe correlation between these variables is checked using the cor function.\n\n\n\nCorrelation Matrix\n\n\n\nCO\nHC\nNOX\n\n\n\n\nCO\n1.00\n0.90\n-0.69\n\n\nHC\n0.90\n1.00\n-0.56\n\n\nNOX\n-0.69\n-0.56\n1.00\n\n\n\n\n\nThe following is code to produce a nice correlation matrix\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncor_matrix &lt;- cor(engine)\n\ncorrplot(cor_matrix, method = \"color\", addCoef.col = \"white\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nand a scatterplot matrix can be plotted using the pairs() command.\n\npairs(engine, panel = panel.smooth, main = \"Scatterplot Matrix\")\n\n\n\n\n\n\n\n\nIf you want include correlation coefficients in a scatterplot matrix, you might want to look into the pairs.panels command. You need to have the psych package installed for this.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:scales':\n\n    alpha, rescale\n\npairs.panels(engine, scale=TRUE, ellipses=FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplots are useful at identifying outliers and other distributional features.\nData points can be outliers in two dimensions without being outliers in the separate dimensions! For example taller people are generally heavier, but a particularly heavy person of average height stands out more.\nScatterplots help you determine which points are outliers and if they are outliers in more than one variable.\nThe scatterplots suggest maybe one large outlier on NOX, and two on CO.\n\nboxplot(engine)\n\n\n\n\n\n\n\n\nThe boxplots suggests a few outliers at the top end of each scale.\n\nengine[engine$NOX&gt;2.5,]\n\n     CO   HC  NOX\n39 4.29 0.52 2.94\n\nengine[engine$CO&gt;20,]\n\n      CO   HC  NOX\n34 23.53 1.02 0.86\n35 22.92 1.10 0.57\n\nengine[engine$HC&gt;1,]\n\n      CO   HC  NOX\n34 23.53 1.02 0.86\n35 22.92 1.10 0.57\n\n\nOutliers are identified as cases 39 (for NOX) and 34, 35 (for CO).\nWe are now ready to create a vector of colours to assign\n\ncolours &lt;- scales::alpha(rep('black', length=46 ),0.5) ## transparent black for all points\ncolours[39] &lt;- 'red' ## NOX outlier will be solid red\ncolours[c(34,35)] &lt;- 'blue' ## CO outliers will be solid blue\n\n\npar(mfrow=c(1,3)) ## redraw plots with new colours\nplot(x=engine$CO, y=engine$HC, col=colours, pch=16)\nplot(x=engine$CO, y=engine$NOX, col=colours, pch=16)\nplot(x=engine$HC, y=engine$NOX, col=colours, pch=16)\n\n\n\n\n\n\n\n## now we can see that the outliers on CO are also the two largest values on HC\n\n\n\n\nType data(airquality) to load the built-in airquality data set\nProduce a boxplot of the variables Ozone and Wind. How many outliers are there?\nCheck which observations correspond to these outliers and answer the question again: how many outliers? (caution! The variable Ozone has some missing data and the code above might need to be modified)\nProduce a scatterplot of the two variables where the outliers are highlighted (by colour, as above or in other ways)\n\n\n\n\nClick for solution\n\n\ndata(airquality)\n\npar(mfrow=c(1,2))\nboxplot(airquality$Ozone, col = \"lightblue\")\n\nboxplot(airquality$Wind, col = \"lightgreen\")\n\n\n\n\n\n\n\n\n\nairquality[!is.na(airquality$Ozone) & airquality$Ozone &gt; 120, ]\n\n    Ozone Solar.R Wind Temp Month Day\n62    135     269  4.1   84     7   1\n99    122     255  4.0   89     8   7\n117   168     238  3.4   81     8  25\n\nairquality[airquality$Wind&gt;20,]\n\n   Ozone Solar.R Wind Temp Month Day\n9      8      19 20.1   61     5   9\n48    37     284 20.7   72     6  17\n\n\n\ncolours &lt;- scales::alpha(rep('black',length=153),0.5) ## transparent black for all points\ncolours[c(62, 99, 117)] &lt;- 'red' ## NOX outlier will be solid red\ncolours[c(9,48)] &lt;- 'blue' ## CO outliers will be solid blue\n\n\nplot(x=airquality$Ozone, y=airquality$Wind, col=colours, pch=16)"
  },
  {
    "objectID": "Workshop2_DepRelAss.html#using-the-plot-function",
    "href": "Workshop2_DepRelAss.html#using-the-plot-function",
    "title": "Workshop 2 - Dependency, Relationships and Associations",
    "section": "",
    "text": "Scatterplots are a rather simple plot, but incredibly effective at showing structure in the data. By simply plotting points at the coordinates of two numerical variables, we can easily detect any patterns that may appear.\nThe plot function produces a scatterplot of its two arguments. For illustration, let us use the mtcars data set again, containing information on the characteristics of 23 cars. We can plot miles per gallon against weight with the command\n\ndata(mtcars)\nplot(x=mtcars$wt, y=mtcars$mpg)\n\n\n\n\n\n\n\n\nUnsurprisingly, heavier cars do fewer miles per gallon and are less efficient. The relationship here, while clear and negative, is far from exact with a lot of noise and variation.\nIf the argument labels x and y are not supplied to plot, R will assume the first argument is x and the second is y. If only one vector of data is supplied, this will be taken as the \\(y\\) value and will be plotted against the integers 1:length(y), i.e. in the sequence in which they appear in the data.\n\n\nAnother useful optional argument is type, which can substantially change how plot draws the data. The type argument can take a number of different values to produce different types of plot:\n\ntype=\"p\" - draws a standard scatterplot with a point for every \\((x,y)\\) pair\ntype=\"l\" - connects adjacent \\((x,y)\\) pairs with straight lines, does not draw points. Note this is a lowercase L, not a number 1.\ntype=\"b\" - draws both points and connecting line segments\ntype=\"s\" - connects points with ‘steps’ rather than straight lines\n\n\npar(mfrow=c(2,2))\no &lt;- order(mtcars$wt)\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"p\"', ty='p')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"l\"', ty='l')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"b\"', ty='b')\nplot(x=mtcars$wt[o], mtcars$mpg[o], xlab=\"Weight\", ylab=\"MPG\", main='type=\"s\"', ty='s')\n\n\n\n\n\n\n\n\n\n\n\nThe symbols used for points in scatter plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is pch=1 which is a hollow circle. The possible values of pch are shown in the plot below:\n\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"2\"', pch=2)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"3\"', pch=3)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"7\"', pch=7)\nplot(x=mtcars$wt, mtcars$mpg, xlab=\"Weight\", ylab=\"MPG\", main='pch=\"10\"', pch=10)"
  },
  {
    "objectID": "Workshop2_DepRelAss.html#overplotting-and-transparency",
    "href": "Workshop2_DepRelAss.html#overplotting-and-transparency",
    "title": "Workshop 2 - Dependency, Relationships and Associations",
    "section": "",
    "text": "To deal with issues of overplotting - where dense areas of points are drawn ontop of each other - we can use transparency to make the plot symbols. For example, here are 500 points randomly generated from a 2-D normal distribution. Notice how the middle of the plot is a solid lump of black\n\nplot(x=rnorm(5000),y=rnorm(5000),pch=16)\n\n\n\n\n\n\n\n\nTo ‘fix’ this, we can specify a transparent colour in the col argument by using the alpha function from the scales package:\n\nlibrary(scales)\nplot(x=rnorm(5000),y=rnorm(5000),pch=16,col=alpha('black',0.2))\n\n\n\n\n\n\n\n\nThe alpha function takes two arguments - a colour first, and then the alpha level itself. This should be a number in \\([0,1]\\) with smaller values being more transparent. Finding a good value for alpha is usually a case of trial-and-error, but in general it will be smaller than you might first expect!\nNow with the transparency we can see a bit more structure in the data, and the darker areas now highlight regions of high data density.\n\n\n Download data: engine\nThis rather simple data set contains three numerical variables, each representing different amounts of pollutants emitted by 46 light-duty engines. The pollutants recorded are Carbon monoxide (CO), Hydrocarbons (HC), and Nitrogen oxide (NOX), all recorded as grammes emitted per mile.\n\n\n\nDownload the engine data set and load it into your workspace\nConstruct three scatterplots to investigate the relationships between every pair of pollutants:\n\nCO versus HC\nCO versus NOX\nHC versus NOX\n\nWhat are the relationships between the amounts of different pollutants emitted by the various engines? In particular, which pollutants are positively associated and which are negatively associated?\n\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,3))\nplot(x=engine$CO, y=engine$HC, pch=16) \nplot(x=engine$CO, y=engine$NOX, pch=16)  \nplot(x=engine$HC, y=engine$NOX, pch=16) \n\n\n\n\n\n\n\n\nThe first plot shows a strong positive linear relationship; The second plot a fairly strong, decreasing non-linear relationship; The third plot a weaker linear relationship.\n\nThe correlation between these variables is checked using the cor function.\n\n\n\nCorrelation Matrix\n\n\n\nCO\nHC\nNOX\n\n\n\n\nCO\n1.00\n0.90\n-0.69\n\n\nHC\n0.90\n1.00\n-0.56\n\n\nNOX\n-0.69\n-0.56\n1.00\n\n\n\n\n\nThe following is code to produce a nice correlation matrix\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncor_matrix &lt;- cor(engine)\n\ncorrplot(cor_matrix, method = \"color\", addCoef.col = \"white\", tl.col = \"black\", tl.srt = 45)\n\n\n\n\n\n\n\n\nand a scatterplot matrix can be plotted using the pairs() command.\n\npairs(engine, panel = panel.smooth, main = \"Scatterplot Matrix\")\n\n\n\n\n\n\n\n\nIf you want include correlation coefficients in a scatterplot matrix, you might want to look into the pairs.panels command. You need to have the psych package installed for this.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:scales':\n\n    alpha, rescale\n\npairs.panels(engine, scale=TRUE, ellipses=FALSE)"
  },
  {
    "objectID": "Workshop2_DepRelAss.html#visualizing-outliers",
    "href": "Workshop2_DepRelAss.html#visualizing-outliers",
    "title": "Workshop 2 - Dependency, Relationships and Associations",
    "section": "",
    "text": "Scatterplots are useful at identifying outliers and other distributional features.\nData points can be outliers in two dimensions without being outliers in the separate dimensions! For example taller people are generally heavier, but a particularly heavy person of average height stands out more.\nScatterplots help you determine which points are outliers and if they are outliers in more than one variable.\nThe scatterplots suggest maybe one large outlier on NOX, and two on CO.\n\nboxplot(engine)\n\n\n\n\n\n\n\n\nThe boxplots suggests a few outliers at the top end of each scale.\n\nengine[engine$NOX&gt;2.5,]\n\n     CO   HC  NOX\n39 4.29 0.52 2.94\n\nengine[engine$CO&gt;20,]\n\n      CO   HC  NOX\n34 23.53 1.02 0.86\n35 22.92 1.10 0.57\n\nengine[engine$HC&gt;1,]\n\n      CO   HC  NOX\n34 23.53 1.02 0.86\n35 22.92 1.10 0.57\n\n\nOutliers are identified as cases 39 (for NOX) and 34, 35 (for CO).\nWe are now ready to create a vector of colours to assign\n\ncolours &lt;- scales::alpha(rep('black', length=46 ),0.5) ## transparent black for all points\ncolours[39] &lt;- 'red' ## NOX outlier will be solid red\ncolours[c(34,35)] &lt;- 'blue' ## CO outliers will be solid blue\n\n\npar(mfrow=c(1,3)) ## redraw plots with new colours\nplot(x=engine$CO, y=engine$HC, col=colours, pch=16)\nplot(x=engine$CO, y=engine$NOX, col=colours, pch=16)\nplot(x=engine$HC, y=engine$NOX, col=colours, pch=16)\n\n\n\n\n\n\n\n## now we can see that the outliers on CO are also the two largest values on HC\n\n\n\n\nType data(airquality) to load the built-in airquality data set\nProduce a boxplot of the variables Ozone and Wind. How many outliers are there?\nCheck which observations correspond to these outliers and answer the question again: how many outliers? (caution! The variable Ozone has some missing data and the code above might need to be modified)\nProduce a scatterplot of the two variables where the outliers are highlighted (by colour, as above or in other ways)\n\n\n\n\nClick for solution\n\n\ndata(airquality)\n\npar(mfrow=c(1,2))\nboxplot(airquality$Ozone, col = \"lightblue\")\n\nboxplot(airquality$Wind, col = \"lightgreen\")\n\n\n\n\n\n\n\n\n\nairquality[!is.na(airquality$Ozone) & airquality$Ozone &gt; 120, ]\n\n    Ozone Solar.R Wind Temp Month Day\n62    135     269  4.1   84     7   1\n99    122     255  4.0   89     8   7\n117   168     238  3.4   81     8  25\n\nairquality[airquality$Wind&gt;20,]\n\n   Ozone Solar.R Wind Temp Month Day\n9      8      19 20.1   61     5   9\n48    37     284 20.7   72     6  17\n\n\n\ncolours &lt;- scales::alpha(rep('black',length=153),0.5) ## transparent black for all points\ncolours[c(62, 99, 117)] &lt;- 'red' ## NOX outlier will be solid red\ncolours[c(9,48)] &lt;- 'blue' ## CO outliers will be solid blue\n\n\nplot(x=airquality$Ozone, y=airquality$Wind, col=colours, pch=16)"
  },
  {
    "objectID": "Lecture2b_ManyContVar.html",
    "href": "Lecture2b_ManyContVar.html",
    "title": "Lecture 2b - Exploring Many Categorical Variables",
    "section": "",
    "text": "In this lecture, we explore data sets with more than one continuous variable. The main aspects we are concerned with are:\n\nWhen studying the relationship between two numerical variables, the simplest and most powerful visualisation is the scatterplot\nWhen a third categorical variable is studied on top of the two numerical ones, one can either use colours (for fewer groups) or a grid or trellis of individual scatterplots (for larger groups)\nCorrelation is the appropriate summary statistic for assessing association, but it is limited to quantifying linear associations.\n\nAs a motivation, we start by looking at the example of Fisher’s Iris data. We choose one continuous variable, say Petal.Length, and start our exploration with a simple histogram:\n\ndata(iris)\nhist(iris$Petal.Length, breaks = 30)\n\n\n\n\n\n\n\n\nThere is a clear separation in two distinct groups, but without further exploration we can’t conclude much. Luckily for us, we have other variables in the same data set: let us plot Petal.Length against Petal.Width:\n\nlibrary(ggplot2)\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"Petal Width vs Petal Length\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFrom this simple plot, we reinforce our idea that at least two distinct group of observations are in our data set. Moreover, we can see a strong association, as flowers with longer petals show also a greater width of petals.\nLet’s now exploit a third variable, species. This is a categorical variable, containing only three groups. A good solution is then to use colour:\n\nggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  scale_color_manual(\n    values = c(\n      setosa = \"black\",\n      versicolor = \"yellow3\",\n      virginica = \"lightblue3\"\n    )\n  ) +\n  labs(\n    x = \"Petal Length\",\n    y = \"Petal Width\",\n    title = \"Petal Width vs Petal Length by Species\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe discover that the two groups of observations correspond to\n\nthe irises of species setosa\nthe irises of the two other species.\n\nMoreover, we see that inside each group the correlation between petal length and petal width is much lower: the variable species determines a big proportion of the petal size, and once one knows that two given irises belong to a certain species, Petal.Length is less of a good predictor of Petal.Width.\nThe takeout message of this example is that assessing more variables allows for the emergence of complex features of a data set.\nWe are now going to see other examples of features of a data set with many continuous variables.\n\n\nLet’s start our exploration with scatterplots:\n\nDrawing scatterplots is one of the first things statisticians do when looking at data.\nA scatterplot displays two quantitative variables against each other data by plotting each data points values as \\((x,y)\\) coordinates.\nScatterplots can reveal structure not readily apparent from summaries, and are both easy to present and interpret.\nThe major role of scatterplots is in exposing associations between variables - not just linear associations, but any kind of association.\nScatterplots are also useful at identifying outliers and other distributional features.\nHowever, marginal distributions cannot always be easily seen from a scatterplot.\n\n\n\nFor example, let’s consider the Weight and Height of the 10,384 athletes competing in the London 2012 Olympics:\n\nlibrary(VGAMdata)\ndata(oly12)\n\nWe can draw a scatterplot of these variables using the plot function:\n\nplot(x=oly12$Height, y=oly12$Weight)\n\n\n\n\n\n\n\n\nNote the choice of which variable is drawn as the horizontal coordinate and which is the vertical.\nThe plot function accepts the usual arguments to customise the graphic.\n\nxlab, ylab, main - axis labels and main title\nxlim, ylim - axis ranges, a vector of length two\npch - changes the plot character, integer\ncol - changes the point colour, either specifying one colour for all points or one colour for each point\ncex - relative point size, defaults to 1\n\nIn particular, the plot character (pch) can be changed to something more solid to give a clearer picture.\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=20)\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships - associations between variables will manifest as trends (either linear or nonlinear) in a scatterplot. Here, we’re interested in the nature, direction, and strength of any discovered relationship\nCausal relationships - Great care should be taken to distinguish association from possible causation.\nOutliers or groups of outliers - Cases can be outliers in two dimensions without being outliers in the separate dimensions. Taller people are generally heavier, but a particularly heavy person of average height stands out more.\nClusters - Groups of cases occurring separately from the rest of the data, such as the iris species in Fisher’s iris data.\nGranularity - Values may line up in regular columns or rows indicating some form of rounding or grouping of the data has occurred before analysis.\nBarriers - Some combinations of values may be impossible. Age cannot be negative, and years of employment cannot be more than Age.\nGaps and holes - Some combinations of values may be theoretically possible, but do not occur in the data, e.g. very tall but very light people.\nConditional relationships - sometimes the relationship can change fundamentally given another variable, e.g. income vs age will look quite different for working age and retired people, and iris flowers look different for different species.\n\nSo, what do features do we see in this plot:\n\nThere is a fairly strong positive relationship - taller athletes are heavier. This makes rather obvious sense. Sometimes part of statistical exploration is to confirm common-sense inutuitions and to reassure ourselves that the data are correctly recorded (and that any pre-conceptions we have are correct!)\nSome outliers break this pattern - the single athlete with a weight over 200 is a rather heavy Judo player\nNote how some points arrange themselves into parallel vertical and horizontal lines - this is because the data values have been rounded, which forces the points onto a grid of regular values and leaving gaps in between.\nWe have 10384 athletes in the data, but there area far fewer visible points here - this is a problem of over plotting\n\n\n\n\nOverplotting is a problem in a scatterplot where the drawn data points overlap one another. This typically occurs when there are a large number of data points and/or a small number of unique values in the dataset.\nAs multiple stacked points look the same as a single point, this makes it difficult to identify areas of high density. In the scatterplot above, we cannot tell if there is one person with a weight over 200 or one hundred.\nPossible solutions include:\n\nUsing transparency - higher density is then evident by darker regions, where the degree of darkness is caused by the multiple overlapping points\nJittering - add random noise to the points to turn the stacks into point clouds\nUsing smaller points - only feasible for modestly sized problems.\n\nWe can apply transparency to the scatterplot of the Olympic athletes as follows:\n\nlibrary(scales)\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=20,\n    col=alpha('black',0.2)) ## this tells R to use 20% transparent black for each point\n\n\n\n\n\n\n\n\nNow the areas of high density in the main cloud of data stand out as darker, and the more unusual values fade out.\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe Old Faithful geyser in Yellowstone National Park, Wyoming, USA which a very regular pattern of eruption.\nConsider the duration of the eruptions and the waiting time until the next eruption.\n\nplot(y=geyser$waiting,x=geyser$duration, pch=16, xlab='duration',ylab='waiting')\n\n\n\n\n\n\n\n\nWhat can we see?\n\nPossible evidence of 2 or 3 clusters of points in the data\nOne cluster of shorter duration eruptions associated with a longer waiting time.\nA further cluster (or two) of longer eruptions with a short or long waiting time.\nClear signs of rounding producing a line of data points with a duration of exactly 4. Definitely suspicious!\n\n\n\n\n Download data: movies\nMost of the data sets we’ve seen so far are not very big. Consider instead the movies data set, which contained 24 variables and 58 788 cases corresponding to various attributes of different movies taken from IMDB. Let’s focus on two of the variables: the average IMDB user rating, and the number of users who rated that movie on IMDB.\n\nplot(x=movies$votes, y=movies$rating, ylab=\"Rating\", xlab=\"Votes\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nThere are no films with many votes and very low average rating.\nFor films with more than a small number of votes, the average rating increases with number of votes.\nNo film with lots of votes has an average rating close to the maximum possible. There is almost an invisible barrier preventing very high scores.\nA few films with a high number of votes (over 50 000) appear far from the rest of the data with unusually low ratings compared to others.\nFilms with a low number of votes may have any average ratin from the worst to the best.\nThe only films with high average ratings are films with relatively few votes.\n\nWe can extract a lot of information from a scatterplot!\n\n\n\nPatterns and trends observed within a scatterplot can often be explained by the action of other variables.\nIn particular, other categorical variables may induce different behaviours for each group (e.g. male and female).\n\nThere are a variety of ways to explore and compare possible groupings:\nIndicate the groups within the plot via colour or plot symbol\nSplit the data and draw separate plots for each potential group\n\n\n\nConsider the average values of fertility (number of children per mother) versus percentage of contraceptors among women of childbearing age in developing nations around 1990.\n\nlibrary(carData)\ndata(Robey)\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=20,\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\n\n\n\n\n\n\n\n\n\nWe find a rough straight line relationship for most of the points - suggests a negative linear association between fertility and contraceptor percentage\nStill a lot of spread about the rough line, i.e. the relationship is clear but far from perfect.\nPragmatically, this seems to agree with the common-sense view that fertility declines on average as the percentage of contraceptors in a country rise.\nSometimes part of the job of exploring the dat is confirming the data agrees with what might be considered obvious - finding the opposite effect in the data would be far more surprising!\n\nThe fertility data set also contains a third variable representing the region of the world, which we can indicate in colour.\n\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=20, col=Robey$region,\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\nlegend(x='topright',pch=20,lwd=NA, col=1:nlevels(Robey$region), legend=levels(Robey$region))\n\n\n\n\n\n\n\n\n\nThis doesn’t seem to affect the overall conclusion, in that the pattern seems to be the same, and linear, in each region.\nSometimes, not finding anything is a useful thing to learn!\n\nWe could have used a different symbol for each region, but this is generally less effective than colour:\n\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=as.numeric(Robey$region),\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\nlegend(x='topright',lwd=NA, pch=1:nlevels(Robey$region), legend=levels(Robey$region))\n\n\n\n\n\n\n\n\n\nColour creates more of a contrast, making it much easier to distinguish the groups\nIn general, stick to a single symbol - unless you must use black & white, or you have multiple different grouping variables!\n\n\n\n\n\nThe Olympic athletes data set also could be investigated by using colour to represent Gender. This shows an expected differentiation between the two:\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=16,\n     col=alpha(c('#000000','#ff0000'),0.2)[oly12$Sex])\nlegend(x='topleft', legend=levels(oly12$Sex), pch=16, col=c(1,2))\n\n\n\n\n\n\n\n\nColouring the 42 sport categories is less helpful.\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=16,\n     col=oly12$Sport)\n\n\n\n\n\n\n\n\nUnfortunately, with many categories the differences between 42 colours become too subtle to detect easily. Colour is only useful when indicating a small number of groups.\nIn this case, a better solution is to break the data up and draw separate mini scatterplots for each of the 42 sports. This is called a lattice, grid or trellis of plots:\n\n\n\n\n\n\n\n\n\n\nThough the plots are small, we can still see the key features.\nThe increasing trend is common across all sports.\nFor some sports, the relationship looks nonlinear.\nThe picture is less clear for Athletics, which groups a lot of very different events together.\nSome sports seem to have very few or no athletes - we’re missing data on 1346 of the athletes.\n\n\n\n\n\nThe standard measure of the strength of linear relationship between two variables is the correlation coefficient . Assuming that we have \\(n\\) pairs of observations \\((x_1,y_1)\\), …,\\((x_n,y_n)\\), the correlation coefficient is defined to be: [ r=_{i=1}^n()() ] In R, we can use the cor function to compute this.\nThe correlation can take any value between \\(-1\\) and \\(+1\\), inclusive, with positive values representing positive correlation (as one variable increases, so does the other), and negative values representing negative correlation (as one variable increases, the other decreases). A correlation of \\(0\\) means uncorrelated: no association (as one variable increases, the other doesn’t consistently increase or decrease).\nThus the value of \\(r\\) can range from \\(r=-1\\) (perfect negative correlation) through weaker negative correlation until \\(r=0\\) (no correlation) through weak positive correlation to \\(r=1\\) (perfect +ve correlation).\nThe correlation tells us two things:\n\nthe strength of association (strong, weak or zero);\nthe direction of association (negative or positive).\n\nFor the fertility data:\n\ncor(y=Robey$tfr, x=Robey$contraceptors)\n\n[1] -0.9203109\n\n\nThe value here is negative reflecting the negative aassociation - the increase in contraception corresponds to a reduction in numbers of children. The value itself is quite large (close to -1) which indicates a strong association - this is reflected by the fact the data are roughly organised along a straight line.\nDifficulties with correlation:\n\nCan be difficult to interpret numerically: a value of \\(r = 0.7\\) can mean different strengths of association for different numbers of pairs of points. Generally, the more points there are, the lower the correlation has to be to indicate association.\nStrong correlations (near \\(\\pm 1\\)) are quite rare, especially if the size of the distributions is large. Weak correlations (near 0) are common. Quite small correlations can indicate a linear association if the size of distributions is large, though it is difficult to set a hard and fast rule.\n\\(r\\) doesn’t tell us by how much a change in one variable affects change in the other - it just tells us the rough direction and degree of change. For that, you need a regression.\nThe correlation coefficient measures linear association, and so can be misleading when there is nonlinear association, or when their are outliers present.\nFor two separate scatter plots, the correlation coefficient might be numerically the same, but the \\(x,y\\) relationship between the variables may be very different.\n\nRecall, for example, Anscombe’s quartet of data points:\n\nlibrary(datasets)\ndata(anscombe)\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\n\nTheir correlations are almost the same (up to 2 decimal places), but the actual patterns of the points are quite different:\n\npar(mfrow=c(2,2),mai = c(0.3, 0.3, 0.3, 0.3))\nplot(x=anscombe$x1, y=anscombe$y1,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x2, y=anscombe$y2,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x3, y=anscombe$y3,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x4, y=anscombe$y4,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\n\n\n\n\n\n\n\n\n\n\n\nVisualising large numbers of continuous variables has the potential to uncover enven more features, but also presents more challenges.\n\n\n\ndata(iris)\nboxplot(iris[,4:1])\n\n\n\n\n\n\n\n\nBoxplots are effective for getting an overview of major differences between variables - particularly in terms of their location (position vertically) and scale (length of the boxplot). However, boxplots are too simple to show whether a variable splits into different modes or groups. Boxplots are also fundamentally a 1-dimensional graphic - they cannot detect or display whether the variables are related.\n\npairs(iris[,4:1], pch=16)\n\n\n\n\n\n\n\n\nConversely, the scatterplot matrix exposes a great deal of the structure of the data, and makes relationships clear. Patterns, trends, and groups emerge quite easily and can be easily spotted by eye. But it is not as effective for comparing scales and locations - we still need the boxplots, even if they are limited!\n\n\n\nWhat if we need to include a third numerical variable into a scatter plot? One way to achieve this is a bubble plot, where we use the value of a third variable to control the size of the points we draw on a scatterplot:\n\nlibrary(plotly)\nplot_ly(iris, x=~Petal.Width, y=~Petal.Length, size=~Sepal.Width, color=~Species,\n    sizes = c(1,50), type='scatter', mode='markers', marker = list(opacity = 0.7, sizemode = \"diameter\"))\n\n\nHere we have positioned the points of the iris data according to the Petal measurements and used the size of the point to indicate the Sepal.Width. We can note that the orange points (versicolor) appear to have more small bubbles than the others, and the green points (setosa) look to have larger values.\nThis technique can be quite effective when the variable corresponding to point size corresponds to some measure of importance, scale, or size (such as population per country, number of samples in a survey, number of patients in a medical trial). While it can be effectively used in three-variable problems, for more variables than this we require other methods.\nDealing with more continuous variables will require scaling up a lot of the familiar techniques from earlier. Individual numerical summaries can still be computed, though it is even more difficult to easily compare. Some of the standard visualisations can be easily applied to many variables, such as histograms and box plots.\nOne particularly useful technique is to take scatterplots, and draw many of them in a matrix to effectively compare multiple variables at once. Scatterplot matrices are a matrix of scatterplots with each variable plotted against all of the others.\nLike the grid or trellis scatterplot we produce an array of plots, but now we plot all variables simultaneously!\nThese give excellent initial overviews of the relationship between continuous variables in data sets with relatively small numbers of variables.\n\n\n\nLet’s use a data set on Swiss banknotes to illustrate variations we can make to a scatterplot matrix.\nThe data are six measurements on each of 100 genuine and 100 forged Swiss banknotes. The idea is to see if we can distinguish the real notes from the fakes on the basis of the notes dimensions only:\n\nlibrary(car)\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:VGAM':\n\n    logit\n\ndata(bank,package=\"gclus\")\nscatterplotMatrix(bank[,-1], smooth=FALSE, regLine=FALSE, diagonal=TRUE, groups=bank$Status,\n                  col=c(5,7), pch=c(16,16))\n\n\n\n\n\n\n\n\n\nBlue=Genuine, Yellow=Fake.\nWe can use the diagonal elements to show histograms (or smoothed histograms)\nCan we see any variables for which the two groups are well separated? “Diagonal” looks like a good candidate. Looking at the (smoothed) histograms in the Diagonal panel, we can see the two distributions barely overlap indicating a useful variable for separating the groups.\nWe can see some assocations and relationships here “Left”/“Right” are positively correlated, but “Top”/“Diagonal” are negatively associated.\nThere also seem to be a few possible outliers, not all are forgeries!\n\n\n\n\nParallel coordinate plots (PCP) have become a popular tool for highly-multivariate data.\nRather than using perpendicular axes for pairs of variables and points for data, we draw all variables on parallel vertical axes and connect data with lines.\nThe variables are standardised, so that a common vertical axis makes sense.\n\npar(mfrow=c(1,2))\npairs(iris[,4:1],pch=16,col=c('#f8766d','#00ba38','#619cff')[iris$Species])\n\n\n\n\n\n\n\nlibrary(lattice)\nparallelplot(iris[,1:4], col=c('#f8766d','#00ba38','#619cff')[iris$Species], horizontal=FALSE)\n\n\n\n\n\n\n\n\nIn general:\n\nEach line represents one data point.\nThe height of each line above the variable label indicates the (relative) size of the value for that data point. The variables are transformed to a common scale to make comparison possible.\nAll the values for one observation are connected across the plot, so we can see how things change for individuals as we move from one variable to another. Note that this is sensitive to the ordering we choose for the variables in the plot – some orderings of the variables may give clearer pictures than others.\n\nFor these data:\n\nWe can see how the setosa species (red) is separated from the others on the Petal measurements in the PCP by the separation between the group of red lines and the rest.\nSetosa (red) is also generally smaller than the others, except on Sepal Width where it is larger than the other species.\nWe can also pick out outliers, for instance one setosa iris has a particularly small value of Sepal Width compared to all the others.\n\nUsing parallel coordinate plots:\n\nReading and interpreting a parallel coordinate plot (PCP) is a bit more difficult than a scatterplot, but can be just as effective for bigger problems.\nWhen we identify interesting features, we can then investigate them using more familiar graphics.\nA PCP gives a quick overview of the univariate distribution for each variable, and so we can identify skewness, outliers, gaps and concentrations.\nHowever, for pairwise properties and associations then it’s best to draw a scatterplot.\n\n\n\nThe Guardian newspaper in the UK publishes a ranking of British universities each year and it reported these data in May, 2012 as a guide for 2013. 120 Universities are ranked using a combination of 8 criteria, combined into an ‘Average Teaching Score’ used to form the ranking.\n\nlibrary(GDAdata)\ndata(uniranks)\n## a little bit of data tidying\nnames(uniranks)[c(5,6,8,10,11,13)] &lt;- c('AvTeach','NSSTeach','SpendPerSt','Careers',\n                                        'VAddScore','NSSFeedb')\nuniranks1 &lt;- within(uniranks, StaffStu &lt;- 1/(StudentStaffRatio))\n## draw the scatterplot matrix\npairs(uniranks1[,c(5:8,10:14)])\n\n\n\n\n\n\n\n\nWe can see some obvious patterns and dependencies here. What about the parallel coordinate plot?\n\nparallelplot(uniranks1[,c(5:8,10:14)], col='black',  horizontal=FALSE)\n\n\n\n\n\n\n\n\nCan we learn anything from this crazy mess of spaghetti?\nPCPs are most effective if we colour the lines to represent subgroups of the data. We can colour the Russell Group universities, which unsurprisingly are usually at the top (except on NSSFeedback!)\n\n## create a new variable to represent the group we want\nuniranks2 &lt;- within(uniranks1,\n                    Rus &lt;- factor(ifelse(UniGroup==\"Russell\", \"Russell\", \"not\")))\n\nparallelplot(uniranks2[,c(5:8,10:14)], col=c(\"#2297E6\",\"#DF536B\")[uniranks2$Rus],  horizontal=FALSE)\n\n\n\n\n\n\n\n\nUsing colour helps us to diffentiate the lines from the different data points more clearly. Now we can start to explore for features and patterns:\n\nAvTeach is the overall ranking, so notice how high/low values here are connected to high/low values elsewhere.\nThere are some exceptions - the 3rd ranked university on AvTeach has a surprisingly low NSSTeaching score (note the rapidly descending red line)\nNotice how the data ‘pinch’ at certain values on NSS Teach and NSSOverall leaving gaps - this suggests a granularity effect due to limited options of values on the NSS survey scores, e.g. integer scores out of 10. This creates heaping and gaps in the data values as the data is really ordinal.\nAlso, notice how most of the lines are moving upwards when connecting these NSSTeach and NSSOverall - this would suggest a positive correlation.\nThe extremes of these two NSS scores are rarely observed, and they also correspond strongly to extremes on the overall score and ranking.\nThe heavy concentration on low values on EntryTariff (entry requirements) and StaffStu (staff:student ratio) suggests strong skewness in the distributions\n\nA scatterplot matrix corroborates most of these features, though we’re probably near the limit of what we can read and extract from such a plot!\n\npairs(uniranks2[,c(5:8,10:14)],col=c(\"#2297E6\",\"#DF536B\")[uniranks2$Rus], pch=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot matrix is easily overloaded with many variables and cases.\nCorrelations provide a useful numerical summary, and the corrplot is a good way to visualise that information.\nNote that the standard correlation coefficient is not a good measure of non-linear relationships or suitable for categorical variables! Alternative correlations exist for such variables, but these are not computed by default.\n\n\nround(cor(Boston),2) ## round to 2dp\n\n         crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio\ncrim     1.00 -0.20  0.41 -0.06  0.42 -0.22  0.35 -0.38  0.63  0.58    0.29\nzn      -0.20  1.00 -0.53 -0.04 -0.52  0.31 -0.57  0.66 -0.31 -0.31   -0.39\nindus    0.41 -0.53  1.00  0.06  0.76 -0.39  0.64 -0.71  0.60  0.72    0.38\nchas    -0.06 -0.04  0.06  1.00  0.09  0.09  0.09 -0.10 -0.01 -0.04   -0.12\nnox      0.42 -0.52  0.76  0.09  1.00 -0.30  0.73 -0.77  0.61  0.67    0.19\nrm      -0.22  0.31 -0.39  0.09 -0.30  1.00 -0.24  0.21 -0.21 -0.29   -0.36\nage      0.35 -0.57  0.64  0.09  0.73 -0.24  1.00 -0.75  0.46  0.51    0.26\ndis     -0.38  0.66 -0.71 -0.10 -0.77  0.21 -0.75  1.00 -0.49 -0.53   -0.23\nrad      0.63 -0.31  0.60 -0.01  0.61 -0.21  0.46 -0.49  1.00  0.91    0.46\ntax      0.58 -0.31  0.72 -0.04  0.67 -0.29  0.51 -0.53  0.91  1.00    0.46\nptratio  0.29 -0.39  0.38 -0.12  0.19 -0.36  0.26 -0.23  0.46  0.46    1.00\nblack   -0.39  0.18 -0.36  0.05 -0.38  0.13 -0.27  0.29 -0.44 -0.44   -0.18\nlstat    0.46 -0.41  0.60 -0.05  0.59 -0.61  0.60 -0.50  0.49  0.54    0.37\nmedv    -0.39  0.36 -0.48  0.18 -0.43  0.70 -0.38  0.25 -0.38 -0.47   -0.51\n        black lstat  medv\ncrim    -0.39  0.46 -0.39\nzn       0.18 -0.41  0.36\nindus   -0.36  0.60 -0.48\nchas     0.05 -0.05  0.18\nnox     -0.38  0.59 -0.43\nrm       0.13 -0.61  0.70\nage     -0.27  0.60 -0.38\ndis      0.29 -0.50  0.25\nrad     -0.44  0.49 -0.38\ntax     -0.44  0.54 -0.47\nptratio -0.18  0.37 -0.51\nblack    1.00 -0.37  0.33\nlstat   -0.37  1.00 -0.74\nmedv     0.33 -0.74  1.00\n\n\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncorrplot(cor(Boston))\n\n\n\n\n\n\n\n\nThere are clearly strong associations between age and dis, indus and dis, and lstat and medv that may be worth studying closer. chas appears almost uncorrelated to all the other variables – but remember chas was a binary variable, and so the correlation coefficient is meaningless here and we should not draw conclusions from this feature!"
  },
  {
    "objectID": "Lecture2b_ManyContVar.html#scatterplots",
    "href": "Lecture2b_ManyContVar.html#scatterplots",
    "title": "Lecture 2b - Exploring Many Categorical Variables",
    "section": "",
    "text": "Let’s start our exploration with scatterplots:\n\nDrawing scatterplots is one of the first things statisticians do when looking at data.\nA scatterplot displays two quantitative variables against each other data by plotting each data points values as \\((x,y)\\) coordinates.\nScatterplots can reveal structure not readily apparent from summaries, and are both easy to present and interpret.\nThe major role of scatterplots is in exposing associations between variables - not just linear associations, but any kind of association.\nScatterplots are also useful at identifying outliers and other distributional features.\nHowever, marginal distributions cannot always be easily seen from a scatterplot.\n\n\n\nFor example, let’s consider the Weight and Height of the 10,384 athletes competing in the London 2012 Olympics:\n\nlibrary(VGAMdata)\ndata(oly12)\n\nWe can draw a scatterplot of these variables using the plot function:\n\nplot(x=oly12$Height, y=oly12$Weight)\n\n\n\n\n\n\n\n\nNote the choice of which variable is drawn as the horizontal coordinate and which is the vertical.\nThe plot function accepts the usual arguments to customise the graphic.\n\nxlab, ylab, main - axis labels and main title\nxlim, ylim - axis ranges, a vector of length two\npch - changes the plot character, integer\ncol - changes the point colour, either specifying one colour for all points or one colour for each point\ncex - relative point size, defaults to 1\n\nIn particular, the plot character (pch) can be changed to something more solid to give a clearer picture.\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=20)\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships - associations between variables will manifest as trends (either linear or nonlinear) in a scatterplot. Here, we’re interested in the nature, direction, and strength of any discovered relationship\nCausal relationships - Great care should be taken to distinguish association from possible causation.\nOutliers or groups of outliers - Cases can be outliers in two dimensions without being outliers in the separate dimensions. Taller people are generally heavier, but a particularly heavy person of average height stands out more.\nClusters - Groups of cases occurring separately from the rest of the data, such as the iris species in Fisher’s iris data.\nGranularity - Values may line up in regular columns or rows indicating some form of rounding or grouping of the data has occurred before analysis.\nBarriers - Some combinations of values may be impossible. Age cannot be negative, and years of employment cannot be more than Age.\nGaps and holes - Some combinations of values may be theoretically possible, but do not occur in the data, e.g. very tall but very light people.\nConditional relationships - sometimes the relationship can change fundamentally given another variable, e.g. income vs age will look quite different for working age and retired people, and iris flowers look different for different species.\n\nSo, what do features do we see in this plot:\n\nThere is a fairly strong positive relationship - taller athletes are heavier. This makes rather obvious sense. Sometimes part of statistical exploration is to confirm common-sense inutuitions and to reassure ourselves that the data are correctly recorded (and that any pre-conceptions we have are correct!)\nSome outliers break this pattern - the single athlete with a weight over 200 is a rather heavy Judo player\nNote how some points arrange themselves into parallel vertical and horizontal lines - this is because the data values have been rounded, which forces the points onto a grid of regular values and leaving gaps in between.\nWe have 10384 athletes in the data, but there area far fewer visible points here - this is a problem of over plotting\n\n\n\n\nOverplotting is a problem in a scatterplot where the drawn data points overlap one another. This typically occurs when there are a large number of data points and/or a small number of unique values in the dataset.\nAs multiple stacked points look the same as a single point, this makes it difficult to identify areas of high density. In the scatterplot above, we cannot tell if there is one person with a weight over 200 or one hundred.\nPossible solutions include:\n\nUsing transparency - higher density is then evident by darker regions, where the degree of darkness is caused by the multiple overlapping points\nJittering - add random noise to the points to turn the stacks into point clouds\nUsing smaller points - only feasible for modestly sized problems.\n\nWe can apply transparency to the scatterplot of the Olympic athletes as follows:\n\nlibrary(scales)\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=20,\n    col=alpha('black',0.2)) ## this tells R to use 20% transparent black for each point\n\n\n\n\n\n\n\n\nNow the areas of high density in the main cloud of data stand out as darker, and the more unusual values fade out.\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe Old Faithful geyser in Yellowstone National Park, Wyoming, USA which a very regular pattern of eruption.\nConsider the duration of the eruptions and the waiting time until the next eruption.\n\nplot(y=geyser$waiting,x=geyser$duration, pch=16, xlab='duration',ylab='waiting')\n\n\n\n\n\n\n\n\nWhat can we see?\n\nPossible evidence of 2 or 3 clusters of points in the data\nOne cluster of shorter duration eruptions associated with a longer waiting time.\nA further cluster (or two) of longer eruptions with a short or long waiting time.\nClear signs of rounding producing a line of data points with a duration of exactly 4. Definitely suspicious!\n\n\n\n\n Download data: movies\nMost of the data sets we’ve seen so far are not very big. Consider instead the movies data set, which contained 24 variables and 58 788 cases corresponding to various attributes of different movies taken from IMDB. Let’s focus on two of the variables: the average IMDB user rating, and the number of users who rated that movie on IMDB.\n\nplot(x=movies$votes, y=movies$rating, ylab=\"Rating\", xlab=\"Votes\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nThere are no films with many votes and very low average rating.\nFor films with more than a small number of votes, the average rating increases with number of votes.\nNo film with lots of votes has an average rating close to the maximum possible. There is almost an invisible barrier preventing very high scores.\nA few films with a high number of votes (over 50 000) appear far from the rest of the data with unusually low ratings compared to others.\nFilms with a low number of votes may have any average ratin from the worst to the best.\nThe only films with high average ratings are films with relatively few votes.\n\nWe can extract a lot of information from a scatterplot!\n\n\n\nPatterns and trends observed within a scatterplot can often be explained by the action of other variables.\nIn particular, other categorical variables may induce different behaviours for each group (e.g. male and female).\n\nThere are a variety of ways to explore and compare possible groupings:\nIndicate the groups within the plot via colour or plot symbol\nSplit the data and draw separate plots for each potential group\n\n\n\nConsider the average values of fertility (number of children per mother) versus percentage of contraceptors among women of childbearing age in developing nations around 1990.\n\nlibrary(carData)\ndata(Robey)\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=20,\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\n\n\n\n\n\n\n\n\n\nWe find a rough straight line relationship for most of the points - suggests a negative linear association between fertility and contraceptor percentage\nStill a lot of spread about the rough line, i.e. the relationship is clear but far from perfect.\nPragmatically, this seems to agree with the common-sense view that fertility declines on average as the percentage of contraceptors in a country rise.\nSometimes part of the job of exploring the dat is confirming the data agrees with what might be considered obvious - finding the opposite effect in the data would be far more surprising!\n\nThe fertility data set also contains a third variable representing the region of the world, which we can indicate in colour.\n\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=20, col=Robey$region,\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\nlegend(x='topright',pch=20,lwd=NA, col=1:nlevels(Robey$region), legend=levels(Robey$region))\n\n\n\n\n\n\n\n\n\nThis doesn’t seem to affect the overall conclusion, in that the pattern seems to be the same, and linear, in each region.\nSometimes, not finding anything is a useful thing to learn!\n\nWe could have used a different symbol for each region, but this is generally less effective than colour:\n\nplot(y=Robey$tfr, x=Robey$contraceptors, pch=as.numeric(Robey$region),\n     ylab='Total fertility rate (children per woman)',\n     xlab='Percent of contraceptors among married women of childbearing age')\nlegend(x='topright',lwd=NA, pch=1:nlevels(Robey$region), legend=levels(Robey$region))\n\n\n\n\n\n\n\n\n\nColour creates more of a contrast, making it much easier to distinguish the groups\nIn general, stick to a single symbol - unless you must use black & white, or you have multiple different grouping variables!\n\n\n\n\n\nThe Olympic athletes data set also could be investigated by using colour to represent Gender. This shows an expected differentiation between the two:\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=16,\n     col=alpha(c('#000000','#ff0000'),0.2)[oly12$Sex])\nlegend(x='topleft', legend=levels(oly12$Sex), pch=16, col=c(1,2))\n\n\n\n\n\n\n\n\nColouring the 42 sport categories is less helpful.\n\nplot(x=oly12$Height,y=oly12$Weight,xlab='Height', ylab='Weight',main='',pch=16,\n     col=oly12$Sport)\n\n\n\n\n\n\n\n\nUnfortunately, with many categories the differences between 42 colours become too subtle to detect easily. Colour is only useful when indicating a small number of groups.\nIn this case, a better solution is to break the data up and draw separate mini scatterplots for each of the 42 sports. This is called a lattice, grid or trellis of plots:\n\n\n\n\n\n\n\n\n\n\nThough the plots are small, we can still see the key features.\nThe increasing trend is common across all sports.\nFor some sports, the relationship looks nonlinear.\nThe picture is less clear for Athletics, which groups a lot of very different events together.\nSome sports seem to have very few or no athletes - we’re missing data on 1346 of the athletes."
  },
  {
    "objectID": "Lecture2b_ManyContVar.html#correlation",
    "href": "Lecture2b_ManyContVar.html#correlation",
    "title": "Lecture 2b - Exploring Many Categorical Variables",
    "section": "",
    "text": "The standard measure of the strength of linear relationship between two variables is the correlation coefficient . Assuming that we have \\(n\\) pairs of observations \\((x_1,y_1)\\), …,\\((x_n,y_n)\\), the correlation coefficient is defined to be: [ r=_{i=1}^n()() ] In R, we can use the cor function to compute this.\nThe correlation can take any value between \\(-1\\) and \\(+1\\), inclusive, with positive values representing positive correlation (as one variable increases, so does the other), and negative values representing negative correlation (as one variable increases, the other decreases). A correlation of \\(0\\) means uncorrelated: no association (as one variable increases, the other doesn’t consistently increase or decrease).\nThus the value of \\(r\\) can range from \\(r=-1\\) (perfect negative correlation) through weaker negative correlation until \\(r=0\\) (no correlation) through weak positive correlation to \\(r=1\\) (perfect +ve correlation).\nThe correlation tells us two things:\n\nthe strength of association (strong, weak or zero);\nthe direction of association (negative or positive).\n\nFor the fertility data:\n\ncor(y=Robey$tfr, x=Robey$contraceptors)\n\n[1] -0.9203109\n\n\nThe value here is negative reflecting the negative aassociation - the increase in contraception corresponds to a reduction in numbers of children. The value itself is quite large (close to -1) which indicates a strong association - this is reflected by the fact the data are roughly organised along a straight line.\nDifficulties with correlation:\n\nCan be difficult to interpret numerically: a value of \\(r = 0.7\\) can mean different strengths of association for different numbers of pairs of points. Generally, the more points there are, the lower the correlation has to be to indicate association.\nStrong correlations (near \\(\\pm 1\\)) are quite rare, especially if the size of the distributions is large. Weak correlations (near 0) are common. Quite small correlations can indicate a linear association if the size of distributions is large, though it is difficult to set a hard and fast rule.\n\\(r\\) doesn’t tell us by how much a change in one variable affects change in the other - it just tells us the rough direction and degree of change. For that, you need a regression.\nThe correlation coefficient measures linear association, and so can be misleading when there is nonlinear association, or when their are outliers present.\nFor two separate scatter plots, the correlation coefficient might be numerically the same, but the \\(x,y\\) relationship between the variables may be very different.\n\nRecall, for example, Anscombe’s quartet of data points:\n\nlibrary(datasets)\ndata(anscombe)\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\n\nTheir correlations are almost the same (up to 2 decimal places), but the actual patterns of the points are quite different:\n\npar(mfrow=c(2,2),mai = c(0.3, 0.3, 0.3, 0.3))\nplot(x=anscombe$x1, y=anscombe$y1,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x2, y=anscombe$y2,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x3, y=anscombe$y3,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))\nplot(x=anscombe$x4, y=anscombe$y4,xlab='',ylab='',pch=20,xlim=c(0,20),ylim=c(0,14))"
  },
  {
    "objectID": "Lecture2b_ManyContVar.html#many-continuous-variables",
    "href": "Lecture2b_ManyContVar.html#many-continuous-variables",
    "title": "Lecture 2b - Exploring Many Categorical Variables",
    "section": "",
    "text": "Visualising large numbers of continuous variables has the potential to uncover enven more features, but also presents more challenges.\n\n\n\ndata(iris)\nboxplot(iris[,4:1])\n\n\n\n\n\n\n\n\nBoxplots are effective for getting an overview of major differences between variables - particularly in terms of their location (position vertically) and scale (length of the boxplot). However, boxplots are too simple to show whether a variable splits into different modes or groups. Boxplots are also fundamentally a 1-dimensional graphic - they cannot detect or display whether the variables are related.\n\npairs(iris[,4:1], pch=16)\n\n\n\n\n\n\n\n\nConversely, the scatterplot matrix exposes a great deal of the structure of the data, and makes relationships clear. Patterns, trends, and groups emerge quite easily and can be easily spotted by eye. But it is not as effective for comparing scales and locations - we still need the boxplots, even if they are limited!\n\n\n\nWhat if we need to include a third numerical variable into a scatter plot? One way to achieve this is a bubble plot, where we use the value of a third variable to control the size of the points we draw on a scatterplot:\n\nlibrary(plotly)\nplot_ly(iris, x=~Petal.Width, y=~Petal.Length, size=~Sepal.Width, color=~Species,\n    sizes = c(1,50), type='scatter', mode='markers', marker = list(opacity = 0.7, sizemode = \"diameter\"))\n\n\nHere we have positioned the points of the iris data according to the Petal measurements and used the size of the point to indicate the Sepal.Width. We can note that the orange points (versicolor) appear to have more small bubbles than the others, and the green points (setosa) look to have larger values.\nThis technique can be quite effective when the variable corresponding to point size corresponds to some measure of importance, scale, or size (such as population per country, number of samples in a survey, number of patients in a medical trial). While it can be effectively used in three-variable problems, for more variables than this we require other methods.\nDealing with more continuous variables will require scaling up a lot of the familiar techniques from earlier. Individual numerical summaries can still be computed, though it is even more difficult to easily compare. Some of the standard visualisations can be easily applied to many variables, such as histograms and box plots.\nOne particularly useful technique is to take scatterplots, and draw many of them in a matrix to effectively compare multiple variables at once. Scatterplot matrices are a matrix of scatterplots with each variable plotted against all of the others.\nLike the grid or trellis scatterplot we produce an array of plots, but now we plot all variables simultaneously!\nThese give excellent initial overviews of the relationship between continuous variables in data sets with relatively small numbers of variables.\n\n\n\nLet’s use a data set on Swiss banknotes to illustrate variations we can make to a scatterplot matrix.\nThe data are six measurements on each of 100 genuine and 100 forged Swiss banknotes. The idea is to see if we can distinguish the real notes from the fakes on the basis of the notes dimensions only:\n\nlibrary(car)\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:VGAM':\n\n    logit\n\ndata(bank,package=\"gclus\")\nscatterplotMatrix(bank[,-1], smooth=FALSE, regLine=FALSE, diagonal=TRUE, groups=bank$Status,\n                  col=c(5,7), pch=c(16,16))\n\n\n\n\n\n\n\n\n\nBlue=Genuine, Yellow=Fake.\nWe can use the diagonal elements to show histograms (or smoothed histograms)\nCan we see any variables for which the two groups are well separated? “Diagonal” looks like a good candidate. Looking at the (smoothed) histograms in the Diagonal panel, we can see the two distributions barely overlap indicating a useful variable for separating the groups.\nWe can see some assocations and relationships here “Left”/“Right” are positively correlated, but “Top”/“Diagonal” are negatively associated.\nThere also seem to be a few possible outliers, not all are forgeries!\n\n\n\n\nParallel coordinate plots (PCP) have become a popular tool for highly-multivariate data.\nRather than using perpendicular axes for pairs of variables and points for data, we draw all variables on parallel vertical axes and connect data with lines.\nThe variables are standardised, so that a common vertical axis makes sense.\n\npar(mfrow=c(1,2))\npairs(iris[,4:1],pch=16,col=c('#f8766d','#00ba38','#619cff')[iris$Species])\n\n\n\n\n\n\n\nlibrary(lattice)\nparallelplot(iris[,1:4], col=c('#f8766d','#00ba38','#619cff')[iris$Species], horizontal=FALSE)\n\n\n\n\n\n\n\n\nIn general:\n\nEach line represents one data point.\nThe height of each line above the variable label indicates the (relative) size of the value for that data point. The variables are transformed to a common scale to make comparison possible.\nAll the values for one observation are connected across the plot, so we can see how things change for individuals as we move from one variable to another. Note that this is sensitive to the ordering we choose for the variables in the plot – some orderings of the variables may give clearer pictures than others.\n\nFor these data:\n\nWe can see how the setosa species (red) is separated from the others on the Petal measurements in the PCP by the separation between the group of red lines and the rest.\nSetosa (red) is also generally smaller than the others, except on Sepal Width where it is larger than the other species.\nWe can also pick out outliers, for instance one setosa iris has a particularly small value of Sepal Width compared to all the others.\n\nUsing parallel coordinate plots:\n\nReading and interpreting a parallel coordinate plot (PCP) is a bit more difficult than a scatterplot, but can be just as effective for bigger problems.\nWhen we identify interesting features, we can then investigate them using more familiar graphics.\nA PCP gives a quick overview of the univariate distribution for each variable, and so we can identify skewness, outliers, gaps and concentrations.\nHowever, for pairwise properties and associations then it’s best to draw a scatterplot.\n\n\n\nThe Guardian newspaper in the UK publishes a ranking of British universities each year and it reported these data in May, 2012 as a guide for 2013. 120 Universities are ranked using a combination of 8 criteria, combined into an ‘Average Teaching Score’ used to form the ranking.\n\nlibrary(GDAdata)\ndata(uniranks)\n## a little bit of data tidying\nnames(uniranks)[c(5,6,8,10,11,13)] &lt;- c('AvTeach','NSSTeach','SpendPerSt','Careers',\n                                        'VAddScore','NSSFeedb')\nuniranks1 &lt;- within(uniranks, StaffStu &lt;- 1/(StudentStaffRatio))\n## draw the scatterplot matrix\npairs(uniranks1[,c(5:8,10:14)])\n\n\n\n\n\n\n\n\nWe can see some obvious patterns and dependencies here. What about the parallel coordinate plot?\n\nparallelplot(uniranks1[,c(5:8,10:14)], col='black',  horizontal=FALSE)\n\n\n\n\n\n\n\n\nCan we learn anything from this crazy mess of spaghetti?\nPCPs are most effective if we colour the lines to represent subgroups of the data. We can colour the Russell Group universities, which unsurprisingly are usually at the top (except on NSSFeedback!)\n\n## create a new variable to represent the group we want\nuniranks2 &lt;- within(uniranks1,\n                    Rus &lt;- factor(ifelse(UniGroup==\"Russell\", \"Russell\", \"not\")))\n\nparallelplot(uniranks2[,c(5:8,10:14)], col=c(\"#2297E6\",\"#DF536B\")[uniranks2$Rus],  horizontal=FALSE)\n\n\n\n\n\n\n\n\nUsing colour helps us to diffentiate the lines from the different data points more clearly. Now we can start to explore for features and patterns:\n\nAvTeach is the overall ranking, so notice how high/low values here are connected to high/low values elsewhere.\nThere are some exceptions - the 3rd ranked university on AvTeach has a surprisingly low NSSTeaching score (note the rapidly descending red line)\nNotice how the data ‘pinch’ at certain values on NSS Teach and NSSOverall leaving gaps - this suggests a granularity effect due to limited options of values on the NSS survey scores, e.g. integer scores out of 10. This creates heaping and gaps in the data values as the data is really ordinal.\nAlso, notice how most of the lines are moving upwards when connecting these NSSTeach and NSSOverall - this would suggest a positive correlation.\nThe extremes of these two NSS scores are rarely observed, and they also correspond strongly to extremes on the overall score and ranking.\nThe heavy concentration on low values on EntryTariff (entry requirements) and StaffStu (staff:student ratio) suggests strong skewness in the distributions\n\nA scatterplot matrix corroborates most of these features, though we’re probably near the limit of what we can read and extract from such a plot!\n\npairs(uniranks2[,c(5:8,10:14)],col=c(\"#2297E6\",\"#DF536B\")[uniranks2$Rus], pch=16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe scatterplot matrix is easily overloaded with many variables and cases.\nCorrelations provide a useful numerical summary, and the corrplot is a good way to visualise that information.\nNote that the standard correlation coefficient is not a good measure of non-linear relationships or suitable for categorical variables! Alternative correlations exist for such variables, but these are not computed by default.\n\n\nround(cor(Boston),2) ## round to 2dp\n\n         crim    zn indus  chas   nox    rm   age   dis   rad   tax ptratio\ncrim     1.00 -0.20  0.41 -0.06  0.42 -0.22  0.35 -0.38  0.63  0.58    0.29\nzn      -0.20  1.00 -0.53 -0.04 -0.52  0.31 -0.57  0.66 -0.31 -0.31   -0.39\nindus    0.41 -0.53  1.00  0.06  0.76 -0.39  0.64 -0.71  0.60  0.72    0.38\nchas    -0.06 -0.04  0.06  1.00  0.09  0.09  0.09 -0.10 -0.01 -0.04   -0.12\nnox      0.42 -0.52  0.76  0.09  1.00 -0.30  0.73 -0.77  0.61  0.67    0.19\nrm      -0.22  0.31 -0.39  0.09 -0.30  1.00 -0.24  0.21 -0.21 -0.29   -0.36\nage      0.35 -0.57  0.64  0.09  0.73 -0.24  1.00 -0.75  0.46  0.51    0.26\ndis     -0.38  0.66 -0.71 -0.10 -0.77  0.21 -0.75  1.00 -0.49 -0.53   -0.23\nrad      0.63 -0.31  0.60 -0.01  0.61 -0.21  0.46 -0.49  1.00  0.91    0.46\ntax      0.58 -0.31  0.72 -0.04  0.67 -0.29  0.51 -0.53  0.91  1.00    0.46\nptratio  0.29 -0.39  0.38 -0.12  0.19 -0.36  0.26 -0.23  0.46  0.46    1.00\nblack   -0.39  0.18 -0.36  0.05 -0.38  0.13 -0.27  0.29 -0.44 -0.44   -0.18\nlstat    0.46 -0.41  0.60 -0.05  0.59 -0.61  0.60 -0.50  0.49  0.54    0.37\nmedv    -0.39  0.36 -0.48  0.18 -0.43  0.70 -0.38  0.25 -0.38 -0.47   -0.51\n        black lstat  medv\ncrim    -0.39  0.46 -0.39\nzn       0.18 -0.41  0.36\nindus   -0.36  0.60 -0.48\nchas     0.05 -0.05  0.18\nnox     -0.38  0.59 -0.43\nrm       0.13 -0.61  0.70\nage     -0.27  0.60 -0.38\ndis      0.29 -0.50  0.25\nrad     -0.44  0.49 -0.38\ntax     -0.44  0.54 -0.47\nptratio -0.18  0.37 -0.51\nblack    1.00 -0.37  0.33\nlstat   -0.37  1.00 -0.74\nmedv     0.33 -0.74  1.00\n\n\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\ncorrplot(cor(Boston))\n\n\n\n\n\n\n\n\nThere are clearly strong associations between age and dis, indus and dis, and lstat and medv that may be worth studying closer. chas appears almost uncorrelated to all the other variables – but remember chas was a binary variable, and so the correlation coefficient is meaningless here and we should not draw conclusions from this feature!"
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html",
    "href": "Lecture2a_ManyCatVar.html",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "Categorical data poses a number of problems when we have multiple variables. Suppose we have \\(J\\) categorical variables \\(X_1,\\dots,X_J\\), and each categorical variable has \\(c_j\\) possible categories.\nFor example, \\(X_1\\) could be Gender with levels Male/Female and \\(c_1=2\\); then \\(X_2\\) could be Eye Colour with levels Blue/Green/Brown and \\(c_2=3\\); \\(X_3\\) could be Hair colour with levels Brown/Blond/Black/Red/Grey/White and \\(c_3=6\\), etc.\nEach observed data point is then one combination of the possible categories from each variable - Male + Green Eyes + Red hair, or Female + Brown Eyes + Black hair. In total, there are \\(C^*=c_1\\times c_2\\times \\dots\\times c_J= \\prod_j c_j\\) possible combinations of categories! In our example, this would be \\(2\\times3\\times6=36\\). As the number of variables \\(J\\) and number of categories for the variables \\(c_j\\) get bigger, then \\(C^*\\) can grow very large very quickly and can rapidly become challenging to deal with. It can quickly become possible for there to be more possible combinations of categories than you have data observations - a problem known as sparsity.\n\n\nMultivariate categorical data can be summarised by the counts of the number of observations in each possible combination of levels of the categorical variables. This collection of counts forms a contingency table. In general, the contingency table can be represented as\n\na \\(J\\)-dimensional array,\ncontaining a total of \\(C^*\\) cells\neach containing the observed count of each possible combination of categories.\n\nSparsity manifests as many of the table entries being zero.\nFor example, the Alligator data set in the vcdExtra package contains data on 219 observations from a study of the primary food choices of alligators in four Florida lakes. The data set has \\(J=4\\) variables, all categorical:\n\nSex - male, or female\nSize - large (&gt;2.3m) small (&lt;=2.3m)\nLake - one of four possible lakes\nFood - primary food choice: bird, fish, invertebrate, reptile, other\n\nThis seems like a relatively modest data set - how many different combinations of categorical variables are there? The answer is \\(2\\times2\\times4\\times 5=80\\).\n\n\n\nTo explore multivariate categorical data, we start with some low-dimensional plots, for 2 or 3 variables. The most effective visualisations are:\n\nMosaicplots use area to indicate the counts within categories and combinations of categories.\nGrids of barplots can also be effective to visualise how a distribution can change across categories.\nGrouped barplots or stacked barplots\n\nThese plots have the advantage of highlighting some important association features. One shall look at the following questions:\nWhich categories appear most/least often?\nDo the counts and their distributions differ across subgroups?\nIs there evidence of dependence or independence between categories?\n\n\n\n\ntitanic &lt;- data.frame(Titanic)\n\nData on the 2201 people on board the Titanic at the time of its sinking:\n\nClass - 1st, 2nd, 3rd, or crew\nSex - male, or female\nAge - child, or adult\nSurvived - survived or died\n\nThere are 32 combinations of factors here. Interest in these data centres on whether factors like Class or Sex affected survival. We’ll come to this soon, but first let’s just focus on the individual variables.\nThe raw data for categorical variables are not terribly easy to interpret:\n\nprint(titanic[1:10,])\n\n   Class    Sex   Age Survived Freq\n1    1st   Male Child       No    0\n2    2nd   Male Child       No    0\n3    3rd   Male Child       No   35\n4   Crew   Male Child       No    0\n5    1st Female Child       No    0\n6    2nd Female Child       No    0\n7    3rd Female Child       No   17\n8   Crew Female Child       No    0\n9    1st   Male Adult       No  118\n10   2nd   Male Adult       No  154\n\n\nSimple barplots of the individual factors show some general features of the marginal distributions and are a good place to start:\n\n\n\n\n\n\n\n\n\nHere we see that:\n\nTwice as many people died than survived\nMost people were crew rather than passengers\nConsiderably more people onboard were male than female (crew were mostly male)\nFar more adults onboard than children\n\n\n\nIt can be helpful to focus on a single response variable of primary interest to investigate in relation to the others. In the case of the Titanic data, the Survived variable is most of interest. We want to explore how the distribution of the number of survivors is affected by the other variables.\nA simple variation of the barplot is to break apart each bar into the pieces according to combinations with other variables. This gives a stacked barplot. For example, we can decompose the number of survivors by Class:\n\nbarplot(xtabs(Freq~Survived+Class, titanic),  beside=FALSE, col=c('#D9344A','green3'))\n\n\n\n\n\n\n\n\nNow we can see the variations in the nunmber of survivors within each of the bars. This can be helpful to compare the proportions of the sub-groups of Survivors within each bar. For instance, we see that a greater share of passengers in 1st class survived, compared to the rest. However, as the heights of each bar differ it can be difficult to directly compare the numbers in the subgroups for the different bars.\nAlternatively, we can group the bars side-by-side rather than stacking them. This can help if we want to compare the total amounts across classes, rather than proportions.\n\nbarplot(xtabs(Freq~Survived+Class, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\n\n\n\n\n\n\n\n\nIt’s now far clearer that more 1st class passengers survived than did not, and this situation was dramatically reversed for the other passengers. Only a small proportion of the Crew and those in 3rd class surivived.\nWe can repeat this process to look at the effects of the other variables:\n\npar(mfrow=c(2,2))\nbarplot(xtabs(Freq~Survived+Sex, titanic),  beside=FALSE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Sex, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Age, titanic),  beside=FALSE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Age, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\n\n\n\n\n\n\n\n\nThese plots show that a greater proportion of Female passengers survived than Male, but there were far fewer Female passengers overall. For Age, the number of Children appears very (surprisingly?) small and seem to be roughly equally likely to survive or not. The majority of Adults did not survive.\nFeatures of composite barplots:\n\nStacked barplots can be useful to indicate something about the relative composition of each bar in terms of any subgroups within each bar\nGrouped barplots are more effective to compare sizes of subgroups across different categories and bars\nWhen the totals within the bars differ substantially, they become difficult to read\nTo adequately compare proportions, we would have to rescale each bar to have the same overall height.\n\n\n\n\nA mosaic plot is a modification of a stacked barplot which rescales the bars to be the same height so we can focus on the proportions of the subgroups. It also allows the width of the bar to vary. For example, the mosaic plot of Survived by Sex looks like this:\n\nmosaicplot(xtabs(Freq~Sex+Survived, titanic), col=c('#D9344A','#1DB100'), main='')\n\n\n\n\n\n\n\n\nBefore we try and interpret the plot, it is helpful to understand a little about how it was constructed. The general algorithm is as follows:\n\nBegin with an empty rectangle to represent the data set\nTake the first variable, and divide the horizontal axis into sections proportional to the sizes of its categories.\nTake each column, and divide it into rectangles vertically according to the size of the second variable categories.\nContinue as needed, splitting each tile alternately horizontally and vertically\n\nSo, the plot we have generated has columns with widths proportional to the numbers of the two Sexes of passengers. As there were more Male passengers than Female, the Male column is wider. The columns are then split into tiles according to the proportion of each Sex that Survived or did not, with the Green area of each representing the proportion which Survived. If the two Sexes had the same rate of survival, then the two columns would split into similarly sized rows. What we see here is a substantial imbalance in the heights of the rows which is indicative of an association. Female survival rates were much better than the Male survival rate, so there’s an association between Sex and Survived.\nIn general, this functions just like a stacked barplot where we stretch each bar to have the same height. The main advantage of these plots comes when we have more than two variables to explore.\nDid Class affect Survival?\n\nmosaicplot(xtabs(Freq~Class+Survived, titanic), col=c('#D9344A','#1DB100'), main='')\n\n\n\n\n\n\n\n\nAs we said above, if there was no effect of Class then we would expect the four bars would divide equally for each Class as the same proportion of passengers would have survived or not irrespective of Class. Clearly, the bars do not divide equally and again we have signs of an association. We can see that 1st class passengers had far better survival rates, followed by 2nd class, and 3rd class passengers didn’t fare much better than the Crew.\n\n\n\n\n\nlibrary(vcd)\n\nLoading required package: grid\n\ndata(Arthritis)\n\nThe arthritis data contains the results from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. The data set contains observations on 84 patients with variables:\n\nID - patient ID.\nTreatment - factor indicating treatment (Placebo, Treated).\nSex - factor indicating sex (Female, Male).\nAge - age of patient.\nImproved - ordered factor indicating treatment outcome (None, Some, Marked).\n\n\nhead(Arthritis)\n\n  ID Treatment  Sex Age Improved\n1 57   Treated Male  27     Some\n2 46   Treated Male  29     None\n3 77   Treated Male  30     None\n4 17   Treated Male  32   Marked\n5 36   Treated Male  46   Marked\n6 23   Treated Male  58   Marked\n\n\nTreatment, Sex, and Improved are all nominal categorical variables. Improved is ordinal, since the category levels can be placed in a meaningful order. The question is whether the patient Improvement depends on Treatment and/or Sex.\nA mosaic plot of a single variable is basically a simple stacked barplot with only one bar. Looking at the patient improvement only gives:\n\nmosaicplot(xtabs(~Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nSo, no improvement is most common, but of the two groups which do have an improvement the improvement is more likely to be Marked than Some. Of course, we could just get this from a simple summary table:\n\nxtabs(~Improved, Arthritis)\n\nImproved\n  None   Some Marked \n    42     14     28 \n\n\nWe can now start splitting things up by Treatment type. The data used to construct the plot is the 2-way contingency table, obtained by summarising the data:\n\nxtabs(~Treatment+Improved, Arthritis)\n\n         Improved\nTreatment None Some Marked\n  Placebo   29    7      7\n  Treated   13    7     21\n\n\n\nmosaicplot(xtabs(~Treatment+Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nThere are a number of things to note here:\n\nThe number of patients in the Placebo and Treated group appears equal, as the two columns are equal in size\nThe main outcome from the Placebo group is No Improvement - perhaps no surprise! But some patients improve anyway.\nIn the Treatment group, the most likely outcome is a Marked Improvement. An Improvement of Some is the least common outcome.\nClearly there is an association between Treatment and Improvement.\n\nFor comparison, a mosaic plot with no association between Treatment and Improvement would look like this:\n\n\n\n\n\n\n\n\n\nAs we can see, in the event of no association the bars decompose into regular tiles. A loose test of this is whether we can draw a straight line from one side of the plot to the other without going through any of the tiles.\nThe order in which we introduce the variables into the mosaic plot also affects the plot we draw. Here, we have split on Treatment first and then on Improved. We could do this the other way around:\n\nmosaicplot(xtabs(~Improved+Treatment, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nNow we split first by Improved, which creates three columns that are then each split into Treatment type. This plot is most useful for showing how Improvement types decompose into Treatment groups, which is less helpful! Usually, we would split on the dependent or response variable last.\n\n\nWith more than two variables, we can still apply the same techniques to compare proportions, but it leads to two slightly different visualisations\n\nthe mosaicplot plot - for more than two variables draws the plots in a (nested) grid\nfor small numbers of variables, we can look at how barplots for different levels of other variables\nthe doubledecker plot - draws a row of simple mosaic plots to compare the different levels of a third (or fourth, …) variable\n\nIf we were to introduce Sex as a third variable to the mosaic plot of the Arthritis data, we will sub-divide each of the four tiles in the plots above into Male and Female halves.\n\nmosaicplot(xtabs(~Treatment+Sex+Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nNow the data are first split into columns by Improved, then each column split into rows by Treatment, and now further we split these tiles into smaller columns by Sex. This is clearly getting more complicated, but the same ideas apply. Substantial differences in sizes of tiles would suggest something is potentially going on. Possible observations we could make:\n\nThe Female tiles (red) are usually wider than the Male tiles (green) - we have overall more Female patients\nFemale patients generally seem to improve more than Males, regardless of treatment\nTreatment appears to increase the proportion of Improved patients\nFemale patients appear to improve more than Male.\n\n\n\n\nA doubledecker plot is a particular type of mosaic plot that splits all of the tiles vertically, except for the last one. The doubledecker plot is a lot like a sequence of stacked barplots for combinations of the categorical variables.\n\nlibrary(vcd)\ndoubledecker(Improved~Treatment+Sex,data=Arthritis)\n\n\n\n\n\n\n\n\nWe interpret this plot in much the same way. Here the columns are divided into the Treatment groups first, then each Treatment group divided into the two Sexes. This creates four columns, that are split into proportions according to Improved. So, we can observe similar features to before:\n\nThe Female columns are wider than the Male, suggesting more Female patients\nThe Marked improvement tiles in the Female columns are larger than those in the Male, suggesting Female patients response better\nThere was nobody with the combination Male + Placebo + Some Improvement. This is denoted by the hollow circle where this tile should be.\n\nIf we have a particular response variable in mind, the double-decker plot is often more useful than the general mosaic as we can split the independent variables (Sex, Treatment) into columns, and split the columns by the dependent variable (Improved). Overall, we can identify the same features from both types of plots, but the information is presented differently.\n\n\n\nLet’s return to the Titanic data and explore whether Survived depends on combinations of Sex or Class. In fact, since the scale of this problem is relatively modest. we can actually visualise the data as a matrix of barplots.\n\n\n\n\n\n\n\n\n\nWhile they show the shape of the distribution, making detailed comparisons is not terribly easy.\nHow did the combination of Class and Sex affect Survival? We can incorporate more variables into the doubledecker plot which will highlight differences in proportions rather than counts.\nThe columns are now grouped by combinations of Class & Sex.\n\n\n\n\n\n\n\n\n\n\nFemale survival was still higher overall, though declined rapidly with Class.\nAlmost all Female 1st class passengers survived, though those in 3rd class were not so fortunate\nMale survival rates don’t decline so much with Class, and Males in 2nd class have the lowest survival.\n\nAn alternative presentation of the same information is a mosaic plot:\n\n\n\n\n\n\n\n\n\n\nA mosaic plot show the data in a grid, rather than row.\nThe doubledecker plot uses a fixed height for its bars, but the mosaic plot varies the height and width of the panels according to the proportions of the factor variable.\nThis can make it a bit easier to distinguish the relative sizes of the combinations.\n\nThe mosaic plot algorithm is sensitive to the ordering of variables, so changing this will radically affect the plot drawn:\n\n\n\n\n\n\n\n\n\nHowever all of these methods start to struggle with more than a few variables. Unfortunately, too many combinations make the plots difficult to read and introduce a lot of ‘0’ counts into the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe visualisation of Titanic data provided above suggests that there is some association here to explore. The rigorous way to assess association is through \\(\\chi^2\\) tests. Here’s how they work.\n\n\nFirst, we need to set up a general 2-way contingency table problem. Suppose that:\n\nWe have observed \\(n\\) individuals\nWe have recorded that value of two categorical variables for each:\n\nVariable 1, which has \\(C\\) distinct levels\nVariable 2, which has \\(R\\) distinct levels\n\nThe number of individuals who are observed in level \\(i\\) for variable 1 and level \\(j\\) for variable 2 is called \\(o_{ij}\\).\n\nWe can then summarise this in the 2-way table below, where the levels of variable 1 are the rows of the table, and the levels of variable 2 are the columns:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVar 1\\ Var 2\n1\n2\n…\n\\(j\\)\n…\n\\(C\\)\nTotal\n\n\n\n\n1\n\\(o_{11}\\)\n\\(o_{12}\\)\n…\n\\(o_{1j}\\)\n…\n\\(o_{1C}\\)\n\\(r_1\\)\n\n\n2\n\\(o_{21}\\)\n\\(o_{22}\\)\n…\n\\(o_{2j}\\)\n…\n\\(o_{2C}\\)\n\\(r_2\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\ni\n\\(o_{i1}\\)\n\\(o_{i2}\\)\n…\n\\(o_{ij}\\)\n…\n\\(o_{iC}\\)\n\\(r_i\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nR\n\\(o_{R1}\\)\n\\(o_{R2}\\)\n…\n\\(o_{Rj}\\)\n…\n\\(o_{RC}\\)\n\\(r_R\\)\n\n\nTotal\n\\(c_1\\)\n\\(c_2\\)\n…\n\\(c_j\\)\n…\n\\(c_C\\)\n\\(n\\)\n\n\n\nHere we have also introduce notation for the row and columns sums:\n\n\\(r_i\\) is the sum of counts in the \\(i\\)th row. This corresponds to the number of observations of level \\(i\\) of Variable 1. The collection of row counts \\(r_1,\\dots,r_R\\) corresponds to the counts for all the different levels of Variable 1.\n\\(c_j\\) is the sum of counts in the \\(j\\)th row. This corresponds to the number of observations of level \\(j\\) of Variable 2. The collection of row counts \\(c_1,\\dots,c_C\\) corresponds to the counts for all the different levels of Variable 2.\n\n\n\nWe can use the xtabs function from Lecture 1 to construct these contingency tables from data. There are two different usages, depending on whether the data has been pre-summarised as counts (like the Titanic data), or if the data is not summarised and each row corresponds to a single observation.\nIf the data is summarised already, and contains the totals in column called counts, then the summary of counts for the levels of variable1 is obtained by:\n\nxtabs(counts~variable1, data=dataset)\n\nFor example, to summarise survival:\n\nxtabs(Freq~Survived,data=titanic)\n\nSurvived\n  No  Yes \n1490  711 \n\n\nFor 2-way tables, we simply add another variable to the right of the ~ symbol:\n\nxtabs(counts~variable1+variable2, data=dataset)\n\nFor example, to summarise combinations of survival with class of passenger:\n\nxtabs(Freq~Survived+Class,data=titanic)\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  122 167 528  673\n     Yes 203 118 178  212\n\n\n\n\n\nWhen we have \\(n\\) categorical variables, we could create an \\(n\\)-way contingency table. The table would summarise the counts of every possible combination of every possible level of the \\(n\\) variables. This obviously gets more complicated with larger \\(n\\), but we’ll come back to this later.\nFor instance, we could summarise the combinations of Titanic passenger survival, passenger class and passenger sex:\n\nxtabs(Freq~Survived+Class+Sex, data=titanic)\n\n, , Sex = Male\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  118 154 422  670\n     Yes  62  25  88  192\n\n, , Sex = Female\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No    4  13 106    3\n     Yes 141  93  90   20\n\n\nNow we should have a 3-dimensional table with \\(2\\times 4\\times 2\\) cells! But R can only print at most 2-dimensional tables. Instead, it has given us two separate 2-way table for Survived+Class, one for each of Sex=Male (top) and Sex=Female (bottom). We can note here that Female passengers and crew had better survivale than their male counterparts (with a notable exception being those in 3rd class).\n\n\n\n\nThe key idea behind using independence as the property to verify a \\(\\chi^2\\)-test is that if two variables \\(X\\) and \\(Y\\) are independent, then we can write \\[P[X=x,Y=y]=P[X=x]P[Y=y],\\] or equivalently in terms of conditional probabilitiy: \\[P[X=x | Y=y]=P[X=x].\\] The second version tells us that even if I were to tell you the value of Y, that would not cause you to change your probability distribution for \\(X\\). In other words, knowing \\(Y\\) has no influence on how we thinkg \\(X\\) will behave - in other words, the two variables are completely unrelated.\nThe \\(\\chi^2\\)-test is a relatively simple test for assessing independence. As a running example, let’s continue with the Titanic data and investigate possible relationships between Survived and Class. The idea behind our test is as follows:\n\nUsing the data, we can estimate the probabilities of the different levels of Survived (\\(p_i\\)) and Class (\\(p_j\\)) separately using the proportions of the data observed in each category.\nUnder a hypothesis of independence, I can use the first equation above to tell me that the probability of any combination of Survived and Class is just the product of individual probabilities from the previous step, \\(p_{ij}=p_i p_j\\). Applying this to all combinations of categories gives me the probability for every possible combination of the two variables, \\(p_{ij}\\).\nThe expected number of observations in each combination of categories is obtained by multiplying the probabilities from step 2 by the sample size \\(n\\). This gives the expected counts for every combination, \\(E_{ij}=np_{ij}\\).\nWe can then compare our expected counts to the observed counts in the data, and construct a test statistic.\n\nLet’s see how this goes with our data:\nObserved counts: We observe the following counts of the different categories of Survived and Class\n\n\n\n\n\nNo\nYes\n\n\n\n\n1490\n711\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\n325\n285\n706\n885\n\n\n\n\n\nObserved proportions: Dividing the observed counts by the sample size (\\(n=2201\\)) gives the proportions of the data observed in each category:\n\n\n\n\n\nNo\nYes\n\n\n\n\n0.676965\n0.323035\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\n0.1476602\n0.1294866\n0.3207633\n0.40209\n\n\n\n\n\nUnsuprisingly, the more common categories are now associated with larger proportions.\nProbabilities of combinations under independence We assume the null hypothesis to be \\(H_0\\): survival is independent from class. Then we can say, for example:\n\\[P[Survived=Yes,Class=1st] = P[Survived=Yes]\\times P[Class=1st],\\]\nand the same for all the other combinations. By multiplying our probabilities from the previous step, we can make the following probability table for the joint distribution of Survived and Class under our null hypothesis of independence:\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n0.0999608\n0.0876579\n0.2171455\n0.2722008\n\n\nYes\n0.0476994\n0.0418287\n0.1036178\n0.1298891\n\n\n\n\n\nExpected counts under independence Multiplying these probabilities by the number of passengers will give us the expected number of passengers (\\(E\\)) in each group under independence:\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n220.0136\n192.93503\n477.9373\n599.114\n\n\nYes\n104.9864\n92.06497\n228.0627\n285.886\n\n\n\n\n\n\n\n\nWe can now compare these expected counts with the ones we actually observed. If we find big differences, then we would conclude our hypothesis of independence must be wrong and that there is evidence of some form of relationship.\nBefore we do that, however, we’ll just set this up mathematically so we can obtain a formula for our test statistic.\nFor the Titanic data problem, we can construct the test statistic using the method above and get a test statistic of \\(X^2=190.46\\) (check!). Comparing to the \\(\\chi^2\\) distribution - this has a \\(p\\)-value of approximately 0! So, we strongly reject the hypothesis of independence and would do so at any of the usual levels of significance. Survival of the Titanic disaser and passenger class appear to be related!\nHaving discovered an interesting relationship, a good question to ask now is: why? Why did we reject this hypothesis? What about the data seemed to be at odds with the variables being independent?\nTo answer this, we look at the Pearson residuals - the components inside the double sum in our test statistic. For each combination of categories, we get a value of\\(\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\). We can inspect those values and see what we find. In particular, large values of the residuals indicate substantial departures from the null hypothesis. The sign of the residuals indicates whether there are \\(\\color{red}{\\text{fewer}}\\) (&lt;0), or \\(\\color{red}{\\text{more}}\\) (&gt;0) values than expected under independence.\n\n\n\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n\\(\\color{blue}{\\text{-6.608}}\\)\n\\(\\color{blue}{\\text{-1.867}}\\)\n\\(\\color{red}{\\text{2.290}}\\)\n\\(\\color{red}{\\text{3.019}}\\)\n\n\nYes\n\\(\\color{red}{\\text{9.566}}\\)\n\\(\\color{red}{\\text{2.703}}\\)\n\\(\\color{blue}{\\text{-3.315}}\\)\n\\(\\color{blue}{\\text{-4.370}}\\)\n\n\n\nHere we notice the very large values in the 1st class column - we see far \\(\\color{red}{\\text{more}}\\) survivors than expected and far \\(\\color{red}{\\text{fewer}}\\) fatalities. The pattern appears to be reversed for those in 3rd class and the ship’s crew. Clearly, first class passengers had far better outcomes than would be expected if the chances were equal for everyone on board.\nFinally, there’s no real need to do this entire calculation by hand as R easily perform an independence test for a contingency table. All we need to supply is the table of observed values obtained from xtabs:\n\nchisq.test( xtabs(Freq~Survived+Class,data=titanic) )\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(Freq ~ Survived + Class, data = titanic)\nX-squared = 190.4, df = 3, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#contingency-tables",
    "href": "Lecture2a_ManyCatVar.html#contingency-tables",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "Multivariate categorical data can be summarised by the counts of the number of observations in each possible combination of levels of the categorical variables. This collection of counts forms a contingency table. In general, the contingency table can be represented as\n\na \\(J\\)-dimensional array,\ncontaining a total of \\(C^*\\) cells\neach containing the observed count of each possible combination of categories.\n\nSparsity manifests as many of the table entries being zero.\nFor example, the Alligator data set in the vcdExtra package contains data on 219 observations from a study of the primary food choices of alligators in four Florida lakes. The data set has \\(J=4\\) variables, all categorical:\n\nSex - male, or female\nSize - large (&gt;2.3m) small (&lt;=2.3m)\nLake - one of four possible lakes\nFood - primary food choice: bird, fish, invertebrate, reptile, other\n\nThis seems like a relatively modest data set - how many different combinations of categorical variables are there? The answer is \\(2\\times2\\times4\\times 5=80\\)."
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#visualising-many-categorical-variables",
    "href": "Lecture2a_ManyCatVar.html#visualising-many-categorical-variables",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "To explore multivariate categorical data, we start with some low-dimensional plots, for 2 or 3 variables. The most effective visualisations are:\n\nMosaicplots use area to indicate the counts within categories and combinations of categories.\nGrids of barplots can also be effective to visualise how a distribution can change across categories.\nGrouped barplots or stacked barplots\n\nThese plots have the advantage of highlighting some important association features. One shall look at the following questions:\nWhich categories appear most/least often?\nDo the counts and their distributions differ across subgroups?\nIs there evidence of dependence or independence between categories?"
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#example-sinking-of-the-titanic",
    "href": "Lecture2a_ManyCatVar.html#example-sinking-of-the-titanic",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "titanic &lt;- data.frame(Titanic)\n\nData on the 2201 people on board the Titanic at the time of its sinking:\n\nClass - 1st, 2nd, 3rd, or crew\nSex - male, or female\nAge - child, or adult\nSurvived - survived or died\n\nThere are 32 combinations of factors here. Interest in these data centres on whether factors like Class or Sex affected survival. We’ll come to this soon, but first let’s just focus on the individual variables.\nThe raw data for categorical variables are not terribly easy to interpret:\n\nprint(titanic[1:10,])\n\n   Class    Sex   Age Survived Freq\n1    1st   Male Child       No    0\n2    2nd   Male Child       No    0\n3    3rd   Male Child       No   35\n4   Crew   Male Child       No    0\n5    1st Female Child       No    0\n6    2nd Female Child       No    0\n7    3rd Female Child       No   17\n8   Crew Female Child       No    0\n9    1st   Male Adult       No  118\n10   2nd   Male Adult       No  154\n\n\nSimple barplots of the individual factors show some general features of the marginal distributions and are a good place to start:\n\n\n\n\n\n\n\n\n\nHere we see that:\n\nTwice as many people died than survived\nMost people were crew rather than passengers\nConsiderably more people onboard were male than female (crew were mostly male)\nFar more adults onboard than children\n\n\n\nIt can be helpful to focus on a single response variable of primary interest to investigate in relation to the others. In the case of the Titanic data, the Survived variable is most of interest. We want to explore how the distribution of the number of survivors is affected by the other variables.\nA simple variation of the barplot is to break apart each bar into the pieces according to combinations with other variables. This gives a stacked barplot. For example, we can decompose the number of survivors by Class:\n\nbarplot(xtabs(Freq~Survived+Class, titanic),  beside=FALSE, col=c('#D9344A','green3'))\n\n\n\n\n\n\n\n\nNow we can see the variations in the nunmber of survivors within each of the bars. This can be helpful to compare the proportions of the sub-groups of Survivors within each bar. For instance, we see that a greater share of passengers in 1st class survived, compared to the rest. However, as the heights of each bar differ it can be difficult to directly compare the numbers in the subgroups for the different bars.\nAlternatively, we can group the bars side-by-side rather than stacking them. This can help if we want to compare the total amounts across classes, rather than proportions.\n\nbarplot(xtabs(Freq~Survived+Class, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\n\n\n\n\n\n\n\n\nIt’s now far clearer that more 1st class passengers survived than did not, and this situation was dramatically reversed for the other passengers. Only a small proportion of the Crew and those in 3rd class surivived.\nWe can repeat this process to look at the effects of the other variables:\n\npar(mfrow=c(2,2))\nbarplot(xtabs(Freq~Survived+Sex, titanic),  beside=FALSE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Sex, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Age, titanic),  beside=FALSE, col=c('#D9344A','#1DB100'))\nbarplot(xtabs(Freq~Survived+Age, titanic), beside=TRUE, col=c('#D9344A','#1DB100'))\n\n\n\n\n\n\n\n\nThese plots show that a greater proportion of Female passengers survived than Male, but there were far fewer Female passengers overall. For Age, the number of Children appears very (surprisingly?) small and seem to be roughly equally likely to survive or not. The majority of Adults did not survive.\nFeatures of composite barplots:\n\nStacked barplots can be useful to indicate something about the relative composition of each bar in terms of any subgroups within each bar\nGrouped barplots are more effective to compare sizes of subgroups across different categories and bars\nWhen the totals within the bars differ substantially, they become difficult to read\nTo adequately compare proportions, we would have to rescale each bar to have the same overall height.\n\n\n\n\nA mosaic plot is a modification of a stacked barplot which rescales the bars to be the same height so we can focus on the proportions of the subgroups. It also allows the width of the bar to vary. For example, the mosaic plot of Survived by Sex looks like this:\n\nmosaicplot(xtabs(Freq~Sex+Survived, titanic), col=c('#D9344A','#1DB100'), main='')\n\n\n\n\n\n\n\n\nBefore we try and interpret the plot, it is helpful to understand a little about how it was constructed. The general algorithm is as follows:\n\nBegin with an empty rectangle to represent the data set\nTake the first variable, and divide the horizontal axis into sections proportional to the sizes of its categories.\nTake each column, and divide it into rectangles vertically according to the size of the second variable categories.\nContinue as needed, splitting each tile alternately horizontally and vertically\n\nSo, the plot we have generated has columns with widths proportional to the numbers of the two Sexes of passengers. As there were more Male passengers than Female, the Male column is wider. The columns are then split into tiles according to the proportion of each Sex that Survived or did not, with the Green area of each representing the proportion which Survived. If the two Sexes had the same rate of survival, then the two columns would split into similarly sized rows. What we see here is a substantial imbalance in the heights of the rows which is indicative of an association. Female survival rates were much better than the Male survival rate, so there’s an association between Sex and Survived.\nIn general, this functions just like a stacked barplot where we stretch each bar to have the same height. The main advantage of these plots comes when we have more than two variables to explore.\nDid Class affect Survival?\n\nmosaicplot(xtabs(Freq~Class+Survived, titanic), col=c('#D9344A','#1DB100'), main='')\n\n\n\n\n\n\n\n\nAs we said above, if there was no effect of Class then we would expect the four bars would divide equally for each Class as the same proportion of passengers would have survived or not irrespective of Class. Clearly, the bars do not divide equally and again we have signs of an association. We can see that 1st class passengers had far better survival rates, followed by 2nd class, and 3rd class passengers didn’t fare much better than the Crew."
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#example-arthritis-treatment",
    "href": "Lecture2a_ManyCatVar.html#example-arthritis-treatment",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "library(vcd)\n\nLoading required package: grid\n\ndata(Arthritis)\n\nThe arthritis data contains the results from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. The data set contains observations on 84 patients with variables:\n\nID - patient ID.\nTreatment - factor indicating treatment (Placebo, Treated).\nSex - factor indicating sex (Female, Male).\nAge - age of patient.\nImproved - ordered factor indicating treatment outcome (None, Some, Marked).\n\n\nhead(Arthritis)\n\n  ID Treatment  Sex Age Improved\n1 57   Treated Male  27     Some\n2 46   Treated Male  29     None\n3 77   Treated Male  30     None\n4 17   Treated Male  32   Marked\n5 36   Treated Male  46   Marked\n6 23   Treated Male  58   Marked\n\n\nTreatment, Sex, and Improved are all nominal categorical variables. Improved is ordinal, since the category levels can be placed in a meaningful order. The question is whether the patient Improvement depends on Treatment and/or Sex.\nA mosaic plot of a single variable is basically a simple stacked barplot with only one bar. Looking at the patient improvement only gives:\n\nmosaicplot(xtabs(~Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nSo, no improvement is most common, but of the two groups which do have an improvement the improvement is more likely to be Marked than Some. Of course, we could just get this from a simple summary table:\n\nxtabs(~Improved, Arthritis)\n\nImproved\n  None   Some Marked \n    42     14     28 \n\n\nWe can now start splitting things up by Treatment type. The data used to construct the plot is the 2-way contingency table, obtained by summarising the data:\n\nxtabs(~Treatment+Improved, Arthritis)\n\n         Improved\nTreatment None Some Marked\n  Placebo   29    7      7\n  Treated   13    7     21\n\n\n\nmosaicplot(xtabs(~Treatment+Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nThere are a number of things to note here:\n\nThe number of patients in the Placebo and Treated group appears equal, as the two columns are equal in size\nThe main outcome from the Placebo group is No Improvement - perhaps no surprise! But some patients improve anyway.\nIn the Treatment group, the most likely outcome is a Marked Improvement. An Improvement of Some is the least common outcome.\nClearly there is an association between Treatment and Improvement.\n\nFor comparison, a mosaic plot with no association between Treatment and Improvement would look like this:\n\n\n\n\n\n\n\n\n\nAs we can see, in the event of no association the bars decompose into regular tiles. A loose test of this is whether we can draw a straight line from one side of the plot to the other without going through any of the tiles.\nThe order in which we introduce the variables into the mosaic plot also affects the plot we draw. Here, we have split on Treatment first and then on Improved. We could do this the other way around:\n\nmosaicplot(xtabs(~Improved+Treatment, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nNow we split first by Improved, which creates three columns that are then each split into Treatment type. This plot is most useful for showing how Improvement types decompose into Treatment groups, which is less helpful! Usually, we would split on the dependent or response variable last.\n\n\nWith more than two variables, we can still apply the same techniques to compare proportions, but it leads to two slightly different visualisations\n\nthe mosaicplot plot - for more than two variables draws the plots in a (nested) grid\nfor small numbers of variables, we can look at how barplots for different levels of other variables\nthe doubledecker plot - draws a row of simple mosaic plots to compare the different levels of a third (or fourth, …) variable\n\nIf we were to introduce Sex as a third variable to the mosaic plot of the Arthritis data, we will sub-divide each of the four tiles in the plots above into Male and Female halves.\n\nmosaicplot(xtabs(~Treatment+Sex+Improved, Arthritis), col=c('#D9344A','#1DB100','#2297E6'), main='')\n\n\n\n\n\n\n\n\nNow the data are first split into columns by Improved, then each column split into rows by Treatment, and now further we split these tiles into smaller columns by Sex. This is clearly getting more complicated, but the same ideas apply. Substantial differences in sizes of tiles would suggest something is potentially going on. Possible observations we could make:\n\nThe Female tiles (red) are usually wider than the Male tiles (green) - we have overall more Female patients\nFemale patients generally seem to improve more than Males, regardless of treatment\nTreatment appears to increase the proportion of Improved patients\nFemale patients appear to improve more than Male.\n\n\n\n\nA doubledecker plot is a particular type of mosaic plot that splits all of the tiles vertically, except for the last one. The doubledecker plot is a lot like a sequence of stacked barplots for combinations of the categorical variables.\n\nlibrary(vcd)\ndoubledecker(Improved~Treatment+Sex,data=Arthritis)\n\n\n\n\n\n\n\n\nWe interpret this plot in much the same way. Here the columns are divided into the Treatment groups first, then each Treatment group divided into the two Sexes. This creates four columns, that are split into proportions according to Improved. So, we can observe similar features to before:\n\nThe Female columns are wider than the Male, suggesting more Female patients\nThe Marked improvement tiles in the Female columns are larger than those in the Male, suggesting Female patients response better\nThere was nobody with the combination Male + Placebo + Some Improvement. This is denoted by the hollow circle where this tile should be.\n\nIf we have a particular response variable in mind, the double-decker plot is often more useful than the general mosaic as we can split the independent variables (Sex, Treatment) into columns, and split the columns by the dependent variable (Improved). Overall, we can identify the same features from both types of plots, but the information is presented differently.\n\n\n\nLet’s return to the Titanic data and explore whether Survived depends on combinations of Sex or Class. In fact, since the scale of this problem is relatively modest. we can actually visualise the data as a matrix of barplots.\n\n\n\n\n\n\n\n\n\nWhile they show the shape of the distribution, making detailed comparisons is not terribly easy.\nHow did the combination of Class and Sex affect Survival? We can incorporate more variables into the doubledecker plot which will highlight differences in proportions rather than counts.\nThe columns are now grouped by combinations of Class & Sex.\n\n\n\n\n\n\n\n\n\n\nFemale survival was still higher overall, though declined rapidly with Class.\nAlmost all Female 1st class passengers survived, though those in 3rd class were not so fortunate\nMale survival rates don’t decline so much with Class, and Males in 2nd class have the lowest survival.\n\nAn alternative presentation of the same information is a mosaic plot:\n\n\n\n\n\n\n\n\n\n\nA mosaic plot show the data in a grid, rather than row.\nThe doubledecker plot uses a fixed height for its bars, but the mosaic plot varies the height and width of the panels according to the proportions of the factor variable.\nThis can make it a bit easier to distinguish the relative sizes of the combinations.\n\nThe mosaic plot algorithm is sensitive to the ordering of variables, so changing this will radically affect the plot drawn:\n\n\n\n\n\n\n\n\n\nHowever all of these methods start to struggle with more than a few variables. Unfortunately, too many combinations make the plots difficult to read and introduce a lot of ‘0’ counts into the data."
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#connection-with-chi2-tests",
    "href": "Lecture2a_ManyCatVar.html#connection-with-chi2-tests",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "",
    "text": "The visualisation of Titanic data provided above suggests that there is some association here to explore. The rigorous way to assess association is through \\(\\chi^2\\) tests. Here’s how they work.\n\n\nFirst, we need to set up a general 2-way contingency table problem. Suppose that:\n\nWe have observed \\(n\\) individuals\nWe have recorded that value of two categorical variables for each:\n\nVariable 1, which has \\(C\\) distinct levels\nVariable 2, which has \\(R\\) distinct levels\n\nThe number of individuals who are observed in level \\(i\\) for variable 1 and level \\(j\\) for variable 2 is called \\(o_{ij}\\).\n\nWe can then summarise this in the 2-way table below, where the levels of variable 1 are the rows of the table, and the levels of variable 2 are the columns:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVar 1\\ Var 2\n1\n2\n…\n\\(j\\)\n…\n\\(C\\)\nTotal\n\n\n\n\n1\n\\(o_{11}\\)\n\\(o_{12}\\)\n…\n\\(o_{1j}\\)\n…\n\\(o_{1C}\\)\n\\(r_1\\)\n\n\n2\n\\(o_{21}\\)\n\\(o_{22}\\)\n…\n\\(o_{2j}\\)\n…\n\\(o_{2C}\\)\n\\(r_2\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\ni\n\\(o_{i1}\\)\n\\(o_{i2}\\)\n…\n\\(o_{ij}\\)\n…\n\\(o_{iC}\\)\n\\(r_i\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nR\n\\(o_{R1}\\)\n\\(o_{R2}\\)\n…\n\\(o_{Rj}\\)\n…\n\\(o_{RC}\\)\n\\(r_R\\)\n\n\nTotal\n\\(c_1\\)\n\\(c_2\\)\n…\n\\(c_j\\)\n…\n\\(c_C\\)\n\\(n\\)\n\n\n\nHere we have also introduce notation for the row and columns sums:\n\n\\(r_i\\) is the sum of counts in the \\(i\\)th row. This corresponds to the number of observations of level \\(i\\) of Variable 1. The collection of row counts \\(r_1,\\dots,r_R\\) corresponds to the counts for all the different levels of Variable 1.\n\\(c_j\\) is the sum of counts in the \\(j\\)th row. This corresponds to the number of observations of level \\(j\\) of Variable 2. The collection of row counts \\(c_1,\\dots,c_C\\) corresponds to the counts for all the different levels of Variable 2.\n\n\n\nWe can use the xtabs function from Lecture 1 to construct these contingency tables from data. There are two different usages, depending on whether the data has been pre-summarised as counts (like the Titanic data), or if the data is not summarised and each row corresponds to a single observation.\nIf the data is summarised already, and contains the totals in column called counts, then the summary of counts for the levels of variable1 is obtained by:\n\nxtabs(counts~variable1, data=dataset)\n\nFor example, to summarise survival:\n\nxtabs(Freq~Survived,data=titanic)\n\nSurvived\n  No  Yes \n1490  711 \n\n\nFor 2-way tables, we simply add another variable to the right of the ~ symbol:\n\nxtabs(counts~variable1+variable2, data=dataset)\n\nFor example, to summarise combinations of survival with class of passenger:\n\nxtabs(Freq~Survived+Class,data=titanic)\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  122 167 528  673\n     Yes 203 118 178  212\n\n\n\n\n\nWhen we have \\(n\\) categorical variables, we could create an \\(n\\)-way contingency table. The table would summarise the counts of every possible combination of every possible level of the \\(n\\) variables. This obviously gets more complicated with larger \\(n\\), but we’ll come back to this later.\nFor instance, we could summarise the combinations of Titanic passenger survival, passenger class and passenger sex:\n\nxtabs(Freq~Survived+Class+Sex, data=titanic)\n\n, , Sex = Male\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No  118 154 422  670\n     Yes  62  25  88  192\n\n, , Sex = Female\n\n        Class\nSurvived 1st 2nd 3rd Crew\n     No    4  13 106    3\n     Yes 141  93  90   20\n\n\nNow we should have a 3-dimensional table with \\(2\\times 4\\times 2\\) cells! But R can only print at most 2-dimensional tables. Instead, it has given us two separate 2-way table for Survived+Class, one for each of Sex=Male (top) and Sex=Female (bottom). We can note here that Female passengers and crew had better survivale than their male counterparts (with a notable exception being those in 3rd class).\n\n\n\n\nThe key idea behind using independence as the property to verify a \\(\\chi^2\\)-test is that if two variables \\(X\\) and \\(Y\\) are independent, then we can write \\[P[X=x,Y=y]=P[X=x]P[Y=y],\\] or equivalently in terms of conditional probabilitiy: \\[P[X=x | Y=y]=P[X=x].\\] The second version tells us that even if I were to tell you the value of Y, that would not cause you to change your probability distribution for \\(X\\). In other words, knowing \\(Y\\) has no influence on how we thinkg \\(X\\) will behave - in other words, the two variables are completely unrelated.\nThe \\(\\chi^2\\)-test is a relatively simple test for assessing independence. As a running example, let’s continue with the Titanic data and investigate possible relationships between Survived and Class. The idea behind our test is as follows:\n\nUsing the data, we can estimate the probabilities of the different levels of Survived (\\(p_i\\)) and Class (\\(p_j\\)) separately using the proportions of the data observed in each category.\nUnder a hypothesis of independence, I can use the first equation above to tell me that the probability of any combination of Survived and Class is just the product of individual probabilities from the previous step, \\(p_{ij}=p_i p_j\\). Applying this to all combinations of categories gives me the probability for every possible combination of the two variables, \\(p_{ij}\\).\nThe expected number of observations in each combination of categories is obtained by multiplying the probabilities from step 2 by the sample size \\(n\\). This gives the expected counts for every combination, \\(E_{ij}=np_{ij}\\).\nWe can then compare our expected counts to the observed counts in the data, and construct a test statistic.\n\nLet’s see how this goes with our data:\nObserved counts: We observe the following counts of the different categories of Survived and Class\n\n\n\n\n\nNo\nYes\n\n\n\n\n1490\n711\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\n325\n285\n706\n885\n\n\n\n\n\nObserved proportions: Dividing the observed counts by the sample size (\\(n=2201\\)) gives the proportions of the data observed in each category:\n\n\n\n\n\nNo\nYes\n\n\n\n\n0.676965\n0.323035\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\n0.1476602\n0.1294866\n0.3207633\n0.40209\n\n\n\n\n\nUnsuprisingly, the more common categories are now associated with larger proportions.\nProbabilities of combinations under independence We assume the null hypothesis to be \\(H_0\\): survival is independent from class. Then we can say, for example:\n\\[P[Survived=Yes,Class=1st] = P[Survived=Yes]\\times P[Class=1st],\\]\nand the same for all the other combinations. By multiplying our probabilities from the previous step, we can make the following probability table for the joint distribution of Survived and Class under our null hypothesis of independence:\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n0.0999608\n0.0876579\n0.2171455\n0.2722008\n\n\nYes\n0.0476994\n0.0418287\n0.1036178\n0.1298891\n\n\n\n\n\nExpected counts under independence Multiplying these probabilities by the number of passengers will give us the expected number of passengers (\\(E\\)) in each group under independence:\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n220.0136\n192.93503\n477.9373\n599.114\n\n\nYes\n104.9864\n92.06497\n228.0627\n285.886\n\n\n\n\n\n\n\n\nWe can now compare these expected counts with the ones we actually observed. If we find big differences, then we would conclude our hypothesis of independence must be wrong and that there is evidence of some form of relationship.\nBefore we do that, however, we’ll just set this up mathematically so we can obtain a formula for our test statistic.\nFor the Titanic data problem, we can construct the test statistic using the method above and get a test statistic of \\(X^2=190.46\\) (check!). Comparing to the \\(\\chi^2\\) distribution - this has a \\(p\\)-value of approximately 0! So, we strongly reject the hypothesis of independence and would do so at any of the usual levels of significance. Survival of the Titanic disaser and passenger class appear to be related!\nHaving discovered an interesting relationship, a good question to ask now is: why? Why did we reject this hypothesis? What about the data seemed to be at odds with the variables being independent?\nTo answer this, we look at the Pearson residuals - the components inside the double sum in our test statistic. For each combination of categories, we get a value of\\(\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\). We can inspect those values and see what we find. In particular, large values of the residuals indicate substantial departures from the null hypothesis. The sign of the residuals indicates whether there are \\(\\color{red}{\\text{fewer}}\\) (&lt;0), or \\(\\color{red}{\\text{more}}\\) (&gt;0) values than expected under independence.\n\n\n\n\n\n\n\n\n\n\n\n1st\n2nd\n3rd\nCrew\n\n\n\n\nNo\n\\(\\color{blue}{\\text{-6.608}}\\)\n\\(\\color{blue}{\\text{-1.867}}\\)\n\\(\\color{red}{\\text{2.290}}\\)\n\\(\\color{red}{\\text{3.019}}\\)\n\n\nYes\n\\(\\color{red}{\\text{9.566}}\\)\n\\(\\color{red}{\\text{2.703}}\\)\n\\(\\color{blue}{\\text{-3.315}}\\)\n\\(\\color{blue}{\\text{-4.370}}\\)\n\n\n\nHere we notice the very large values in the 1st class column - we see far \\(\\color{red}{\\text{more}}\\) survivors than expected and far \\(\\color{red}{\\text{fewer}}\\) fatalities. The pattern appears to be reversed for those in 3rd class and the ship’s crew. Clearly, first class passengers had far better outcomes than would be expected if the chances were equal for everyone on board.\nFinally, there’s no real need to do this entire calculation by hand as R easily perform an independence test for a contingency table. All we need to supply is the table of observed values obtained from xtabs:\n\nchisq.test( xtabs(Freq~Survived+Class,data=titanic) )\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(Freq ~ Survived + Class, data = titanic)\nX-squared = 190.4, df = 3, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "Lecture2a_ManyCatVar.html#modelling-testing",
    "href": "Lecture2a_ManyCatVar.html#modelling-testing",
    "title": "Lecture 2a - Exploring Many Categorical Variables",
    "section": "Modelling & Testing",
    "text": "Modelling & Testing\n\nContingency tables - the standard for checking the association of two categorical variables is the \\(\\chi^2\\) test based on the summary table of counts.\nAssociations between categorical variables - if there are a small number of variables each with only a few categories, then log linear models could be considered.\nBinary dependent variables - for a binary (two-category) dependent variable, logistic regression is a good approach."
  },
  {
    "objectID": "Lab2_ManyCatVariables.html",
    "href": "Lab2_ManyCatVariables.html",
    "title": "Practical 2 - Exploring Many Categorical Variables with R",
    "section": "",
    "text": "This practical will lead you into producing some high quality visualisations of multivariate categorical data.\n\n\nBy the end of the lab, you will have acquired the following skills:\n\n\nPlotting stacked barplots for simple data sets\n\n\nPlotting doubledecker and mosaicplot plots for more complex data sets\n\n\n\nYou will need to install the following packages:\n\nvcd for the doubledecker functions. You also need to load the corresponding library in your environment.\n\n\ninstall.packages(\"vcd\")\n\n\nlibrary(vcd)\n\nLoading required package: grid\n\n\n\n\nDisplaying combinations of categorical variables can be quite difficult, as we can’t represent our data as simple points in a space. Instead, we summarise a categorical variable (e.g. eye colour) with multiple levels (blue, green, brown, …) by the frequencies of each of the levels in the data. When we have multiple categorical variables, we work instead with the counts of the combinations of the levels (e.g. red hair + green eyes).\nAll of our plots are some sort of visualisation of these counts and so are (in one way or another) variations and manipulations of stacked barplots. Multivariate categorical data is a little more complex to work with, so generally it is recommended to start with single or pairs of variables and progressively add more, rather than visualising everything all at once like a scatterplot matrix.\n Download data: arthritis\nThe arthritis data contains the results from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. The variables are:\n\nID - patient ID.\nTreatment - factor indicating treatment (Placebo, Treated).\nSex - factor indicating sex (Female, Male).\nAge - age of patient.\nImproved - ordered factor indicating treatment outcome (None, Some, Marked).\n\nTreatment, Sex, and Improved are all categorical variables. Improved is also ordinal, since the category levels can be ordered. The question is whether the patient improvement depends on the Treatment and Sex.\nMosaic plots can display the relationship between categorical variables using rectangular tiles, whose areas represent the proportion of cases for any given combination of levels. A mosaic plot of a single variable is basically a simple stacked barplot with only one bar. Looking at the patient improvement only, we see the following\n\nmosaicplot(~Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nSince the None box is largest, we see that this appears to be the most common patient outcome. But taking the Some and Marked improvements together, it’s actually an even split. Let’s see how this depends on what Treatment the patient received - we would hope to see more improvement from those in the Treated group:\n\nmosaicplot(~Treatment+Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nNote that the plot now has two splits: the first split is horizontal into bars for the “Treated” and “Placebo” groups, while the second set of splits divides each bar up into the different Improved categories. If Treatment had no effect on the Improved state, then we would see a regular grid where the two sets of bars would have splits of approximately the same size and we could draw lines from the left of the plot to the right without cutting through any of the tiles.\nHowever, we can see clearly that a greater proportion of patients improved on treatment than in the placebo group and so the distribution of Improved is different for different values of Treatment, which may point towards an association between these variables and maybe hints at treatment being effective!\nIf we reverse the positions of Improved and Treatment in the function, it change the order in which the bars are split:\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the mosaicplot command, try to reproduce the mosaicplot above\nWhat are the new features that become apparent when you reverse the positions of the variables?\n\n\n\nClick for solution\n\n\nmosaicplot(~Improved+Treatment, data=arthritis, col=2:4,main='')\n\n\n\n\n\n\n\n\nNow we have three vertical bars for Improved, each sub-divided into the Treatment groups. This plot now shows how the patients with different Improved levels break down into the Treatment groups. So, we would read this as saying for those patients with a Marked improvement, the majority were Treated rather than given Placebo. Usually, it is best to split on the response variable at the end, as we had done in the previous plot.\n\n\n\n\nWe can continue to add variables to the plot and break down the results into more groups. For instance, we can introduce Sex as a variable\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nHere we can see:\n\nThe Treatment groups look to be roughly equal in size\nThere are fewer men than women in the study (the Male rows are narrower than the Female)\nThere are no Male patients in the Placebo group who only had Some improvement - this is indicated by the dashed line where this bar should be.\nTreatment appears to have a positive effect\nFemale patients seemed to improve the most in general, and particularly under Treatment\n\nIt is often worth reordering the variables in the mosaic plot formula to see if a different sequence of splits is a more effective visualisation for your problem. Generally, our dependent variable of interest is split last following the ~ in the function call.\n\n\n\n\nProduce mosaicplots of combinations of one, two and three variables for the built-in R data set HairEyeColor\nExperiment with the ordering of the variables in the call to the mosaicplot function to see how different orderings produce different presentations of the data.\nWhich visualisation best highlights the interesting patterns in the data?\n\n\n\nClick for solution\n\n\ndata(\"HairEyeColor\")\n\nmosaicplot(~Hair, data=HairEyeColor,col=2:5,main='')\n\n\n\n\n\n\n\nmosaicplot(~Hair+Sex, data=HairEyeColor,col=2:5,main='')\n\n\n\n\n\n\n\n## With three variables there are 6 different orderings - here are a couple (with a different coloring option)\nmosaicplot(~Hair+Sex+Eye, data=HairEyeColor,col=3:6,main='')\n\n\n\n\n\n\n\nmosaicplot(~Eye+Hair+Sex, data=HairEyeColor,col=3:6,main='')\n\n\n\n\n\n\n\n\nIn the case of three variables, the mosaicplot with Hair highlighted by colours is more readable. With the second one, comparisons are hard to make.\n\n\n\n\nA doubledecker plot is a version of a mosiac plot where all of the splits in the data are vertical, except for the last one. This effectively produces a type of stacked barplot, which we can achieve by setting the direction argument as follows:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, dir = c(\"v\", \"v\", \"h\"),col=2:4)\n\n\n\n\n\n\n\n\nOr, using the doubledecker function directly gives an almost identical plot:\n\ndoubledecker(Improved~Treatment+Sex, data=arthritis)\n\n\n\n\n\n\n\n\n\n\n\nThe default plots are somewhat dull and monochrome. Note that the different shadings used in the plot correspond to the different levels of the last cut variable, i.e. the dependent variable which is Improved here. So, if we supply one colour for each level of that variable we get the following plot:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, main='', col=c(\"wheat\", \"cornflowerblue\", \"tomato1\"))\n\n\n\n\n\n\n\n\nNote: In general, setting colours on mosaic plots can be quite fiddly. We won’t make much use of it, apart from an automatic coloring technique that we describe below.\n\n\n\n\nA different but helpful use of colour is the shade=TRUE option. The shade=TRUE option works by colouring any tiles in the mosaic that are in disagreement with the independence hypothesis of a chi-square test. This can help us identify any combinations that are unusually common or rate:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis,shade=TRUE,main='')\n\n\n\n\n\n\n\n\nTiles are shaded blue when more cases are observed than expected given independence, and shaded red when there are fewer cases than expected under independence. The strength of colour indicates how “surprising” those values are. The plot here is showing that most of the variation is not significant (coloured white), but in the Female and Treated group there is a surprisingly high number of patients who display a Marked improvement (blue-ish) - and consequently, fewer than expected (red-ish) whose improvement was None.\n\n\n\nUse the mosaicplot function with shade and the dir arguments to create a “doubledecker” version of the mosaic plot above.\n\n\n\nClick for solution\n\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, shade=TRUE, main='',dir = c(\"v\", \"v\", \"h\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n Download data: alligator\nThe alligator data, from Agresti (2002), comes from a study of the primary food choices of alligators in four Florida lakes. The goal is to try and learn something about the food choice of the different alligators. The variables are:\n\nlake - one of four lakes: George, Hancock, Oklawaha, and Trafford\nsex - male or female\nsize - small or large\nfood - the food preferences of the alligators in five categories: fish, invertebrates, reptile, bird and other.\n\nAs usual, we begin with a quick look at the data to see what we’re dealing with:\n\nhead(alligator)\n\n\n\n\n\n\nlake\nsex\nsize\nfood\ncount\n\n\n\n\nHancock\nmale\nsmall\nfish\n7\n\n\nHancock\nmale\nsmall\ninvert\n1\n\n\nHancock\nmale\nsmall\nreptile\n0\n\n\nHancock\nmale\nsmall\nbird\n0\n\n\nHancock\nmale\nsmall\nother\n5\n\n\nHancock\nmale\nlarge\nfish\n4\n\n\n\n\n\nHere, unlike the arthritis data, each row does not represent an individual alligator but all of the alligators found with the given combinations of categorical variables. So, for example, we have seen 7 alligators with attributes (Hancock, male, small, fish). This is a slightly different format than we saw above, so we’ll need to deal with it slightly differently.\nTo produce the counts needed for our plots, we need to use the cross-tabulation function xtabs that we used with our barplots. So, to generate the counts of alligators in each lake, we first compute\n\nxtabs(count~lake, data=alligator)\n\nlake\n  George  Hancock Oklawaha Trafford \n      63       55       48       53 \n\n\nand then pass this to our mosaic function for plotting:\n\nmosaicplot(xtabs(count~lake, data=alligator),col=2:5)\n\n\n\n\n\n\n\n\nAlternatively, with a single variable we could just draw a barplot, which is probably a little easier to read!\n\nbarplot(xtabs(count~lake, data=alligator),col=2:5)\n\n\n\n\n\n\n\n\nNote that the mosaicplot is showing the proportions in the different lakes by the width of the bars, whereas the barplot uses the height. We see there are slight differences between the numbers of alligators observed in the different lakes, but they don’t appear to be substantial.\nNote that the only difference with working with these data (which include the counts as a variable) and the previous data set (which did not include the counts) is that we must do the aggregating of the data in the xtabs function first, instead of directly in mosaicplot. The syntax and formula for splitting the data is the same.\n\n\n\nInvestigate the distributions of the other categorical variables individually: sex, size, and food. You can use whatever plot you prefer. Try and answer the following questions:\n\nAre the sexes of alligators evenly distributed?\nWhat about the different sizes?\nWhich food type is most popular?\n\n\n\n\nClick for solution\n\n\nmosaicplot(xtabs(count~sex, data=alligator))\n\n\n\n\n\n\n\n\nMore males than female\n\nmosaicplot(xtabs(count~size, data=alligator))\n\n\n\n\n\n\n\n\nslightly more small than large\n\nmosaicplot(xtabs(count~food, data=alligator))\n\n\n\n\n\n\n\n\nmostly eat fish, then invertebrates.\n\n\n\n\nThe strength of mosaic plots is when considering the combination of multiple categorical variables at once. To keep things manageable, let’s looks at some potentially interesting pairs of variables first:\n\n\n\n\nUse mosaicplot to visualise the size and sex variables together. Remember, if there is no association here then we would expect a regular grid. What associations do you find?\nWhat about size and food?\nDraw the doubledecker plots of the same variables - how do they compare to the mosaic plot?\n\n\n\nClick for solution\n\n\nmosaicplot(xtabs(count~size+sex, data=alligator))\n\n\n\n\n\n\n\ndoubledecker(xtabs(count~size+sex, data=alligator))\n\n\n\n\n\n\n\n\nmore males are larger, females are smaller\n\nmosaicplot(xtabs(count~size+food, data=alligator))\n\n\n\n\n\n\n\ndoubledecker(xtabs(count~size+food, data=alligator))\n\n\n\n\n\n\n\n\nbig alligators eat more fish+reptile, and less invertebrates.\n\nWe can even make a matrix of all the two-way mosaic plots in the style of a scatterplot matrix by the following command:\n\npairs(xtabs(count~.,data=alligator))\n\nUsing a . on the right side of the formula is a shorthand for “include everything”.\n\n\n\n\nCan you locate the plots of size and sex, and size and food within the matrix?\nDo you see any other potential associations (or lack of associations) here? Remember, “no association” will mean the mosaic is divided into an approximately regular grid.\nMake a doubledecker plot of food, size and sex - order the variables so that each bar is split into sections according to the food.\nNow try the mosaic plot and use the shade=TRUE option. Try and achieve the same ordering so that food is the final split. What combinations have been highlighted, and how would you interpret them?\nDo you see any other potentially interesting features here?\n\n\n\nClick for solution\n\nSize:sex plot is in position (1,3), size:food is in (3,4). Seem to be a fair few associations here, except for food:sex Let’s take a closer look:\n\nmosaicplot(xtabs(count~sex+food, data=alligator))\n\n\n\n\n\n\n\n\nThis looks pretty regular, no obvious surprisingly large/small blocks. Let’s try shading the big plot for more emphasis\n\npairs(xtabs(count~.,data=alligator), shade=TRUE)\n\n\n\n\n\n\n\n\nCombining more that two variables in a mosaic can get a bit crazy. Let’s look at size, sex and food all together.\n\ndoubledecker(xtabs(count~size+sex+food, data=alligator))\n\n\n\n\n\n\n\nmosaicplot(xtabs(count~size+sex+food, data=alligator),shade=TRUE)\n\n\n\n\n\n\n\n\nLarge males eat more reptiles, small males eat surprisingly few (are they eating each other…?) Small females eat more invertebrates, large females eat few\nThe four-way table goes a bit mad\n\nmosaicplot(xtabs(count~size+sex+food+lake, data=alligator),shade=TRUE)\n\n\n\n\n\n\n\n\nbut does reveal some features that may be worth digging into, though it has become very hard to digest.\n\n\n\n\n\nBarplots can be used effectively to display combinations of categorical variables. However, they require a little more setup to provide the data in the correct format.\nFirst, a grouped barplot displays a numeric value (e.g. counts) split in groups and subgroups. A few explanation about the code below: * the input dataset must be a numeric matrix. Each group is a column. Each subgroup is a row. So we can only deal with two variables at once. * the barplot function will recognize this format, and automatically perform the grouping for you. * the beside option allows to toggle between the grouped and the stacked barchart\n\n## make a table of counts by Treatment and Improved from the arthritis data\ntab &lt;- xtabs(~Improved+Treatment,data=arthritis)\ntab\n\n        Treatment\nImproved Placebo Treated\n  None        29      13\n  Some         7       7\n  Marked       7      21\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\nAnd to stack the bars, set beside=FALSE\n\nbarplot(tab, beside=FALSE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\nStacked bars are often used to display the proportions of the respective columns attributable to each sub-group. Thankfully, we can easily convert tables of counts to proportions with the prop.table function. If we want the proportions computed within a column, set the margin=2 argument:\n\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\n\n\nWe are now going to compare all the techniques seen so far on a new dataset.\n Download data: airlineArrival\nThe airlineArrival data contains 11000 observations of 3 categorical variables: * Airport - a factor with levels LosAngeles, Phoenix, SanDiego, SanFrancisco, Seattle * Result - a factor with levels Delayed,OnTime * Airline - a factor with levels Alaska, AmericaWest\n\n\n\n\nCreate a grouped barplot of the counts of delayed flights versus ontime flights for both levels of the Airline variable.\nCreate a stacked barplot of the same data above\nIs there much difference in the amount of delayed flights between the two airlines? Which plot is better to assess this?\n\n\n\nClick for solution\n\n\ntab &lt;- xtabs(~Result+Airline,data=airlineArrival)\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n# Stacked barplot\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n\nIn the second plot we can see much better that the proportion is roughly the same. In the first plot, it is easier to see the actual counts of delayed and on time flights.\n\n\n\n\n\nProduce grouped and stacked barcharts of counts of Airport against Result. What is the response variable here?\n\nWhich airports look best for flights being on time? Which look worst?\n\n\n\n\nClick for solution\n\n\ntab &lt;- xtabs(~Result+Airport,data=airlineArrival)\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n# Stacked barplot\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n\nOnce again, stacked plots are more useful: Result is clearly the response variable here and hence we need to produce five different bars.\nSan Francisco is the worse airport and Phoenix the best in terms of flights being on time.\n\n\n\n\n\nNow look at whether both Airport and Airline are associated with delays by producing a mosaicplot and a doubledecker plot of all three variables.\nWhat do you find? Try turning on shade=TRUE.\nIn which plot are the associations most pronounced?\n\n\n\nClick for solution\n\nHere is the mosaicplot:\n\nmosaicplot(~Airport+Result+Airline, data=airlineArrival,col=c(2,7),main='')\n\n\n\n\n\n\n\n\nWithout reformatting, it is hard to infer any possible associations from this. In fact, while some differences between airports exist, the contribution of the airline variable doesn’t show a clear pattern. THis is due to the relatively wide size of the empty spaces in a mosaicplot.\nIn this sense, the doubledecker plot gives a more precise picture\n\ndoubledecker(xtabs(~Airport+Airline+Result, data=airlineArrival))\n\n\n\n\n\n\n\n\nThere one can see that airlines show differences: Alaska airlines flights are slightly more on time compared to AmericaWest flights, from every airport in the data set.\nThe shaded mosaicplot confirms this finding, and gives much more information about unusual associations:\n\nmosaicplot(~Airport+Result+Airline, data=airlineArrival, col=c(2,7), shade=TRUE, main='')"
  },
  {
    "objectID": "Lab2_ManyCatVariables.html#exercise-1-mosaicplots",
    "href": "Lab2_ManyCatVariables.html#exercise-1-mosaicplots",
    "title": "Practical 2 - Exploring Many Categorical Variables with R",
    "section": "",
    "text": "Displaying combinations of categorical variables can be quite difficult, as we can’t represent our data as simple points in a space. Instead, we summarise a categorical variable (e.g. eye colour) with multiple levels (blue, green, brown, …) by the frequencies of each of the levels in the data. When we have multiple categorical variables, we work instead with the counts of the combinations of the levels (e.g. red hair + green eyes).\nAll of our plots are some sort of visualisation of these counts and so are (in one way or another) variations and manipulations of stacked barplots. Multivariate categorical data is a little more complex to work with, so generally it is recommended to start with single or pairs of variables and progressively add more, rather than visualising everything all at once like a scatterplot matrix.\n Download data: arthritis\nThe arthritis data contains the results from a double-blind clinical trial investigating a new treatment for rheumatoid arthritis. The variables are:\n\nID - patient ID.\nTreatment - factor indicating treatment (Placebo, Treated).\nSex - factor indicating sex (Female, Male).\nAge - age of patient.\nImproved - ordered factor indicating treatment outcome (None, Some, Marked).\n\nTreatment, Sex, and Improved are all categorical variables. Improved is also ordinal, since the category levels can be ordered. The question is whether the patient improvement depends on the Treatment and Sex.\nMosaic plots can display the relationship between categorical variables using rectangular tiles, whose areas represent the proportion of cases for any given combination of levels. A mosaic plot of a single variable is basically a simple stacked barplot with only one bar. Looking at the patient improvement only, we see the following\n\nmosaicplot(~Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nSince the None box is largest, we see that this appears to be the most common patient outcome. But taking the Some and Marked improvements together, it’s actually an even split. Let’s see how this depends on what Treatment the patient received - we would hope to see more improvement from those in the Treated group:\n\nmosaicplot(~Treatment+Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nNote that the plot now has two splits: the first split is horizontal into bars for the “Treated” and “Placebo” groups, while the second set of splits divides each bar up into the different Improved categories. If Treatment had no effect on the Improved state, then we would see a regular grid where the two sets of bars would have splits of approximately the same size and we could draw lines from the left of the plot to the right without cutting through any of the tiles.\nHowever, we can see clearly that a greater proportion of patients improved on treatment than in the placebo group and so the distribution of Improved is different for different values of Treatment, which may point towards an association between these variables and maybe hints at treatment being effective!\nIf we reverse the positions of Improved and Treatment in the function, it change the order in which the bars are split:\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the mosaicplot command, try to reproduce the mosaicplot above\nWhat are the new features that become apparent when you reverse the positions of the variables?\n\n\n\nClick for solution\n\n\nmosaicplot(~Improved+Treatment, data=arthritis, col=2:4,main='')\n\n\n\n\n\n\n\n\nNow we have three vertical bars for Improved, each sub-divided into the Treatment groups. This plot now shows how the patients with different Improved levels break down into the Treatment groups. So, we would read this as saying for those patients with a Marked improvement, the majority were Treated rather than given Placebo. Usually, it is best to split on the response variable at the end, as we had done in the previous plot.\n\n\n\n\nWe can continue to add variables to the plot and break down the results into more groups. For instance, we can introduce Sex as a variable\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis,col=2:4,main='')\n\n\n\n\n\n\n\n\nHere we can see:\n\nThe Treatment groups look to be roughly equal in size\nThere are fewer men than women in the study (the Male rows are narrower than the Female)\nThere are no Male patients in the Placebo group who only had Some improvement - this is indicated by the dashed line where this bar should be.\nTreatment appears to have a positive effect\nFemale patients seemed to improve the most in general, and particularly under Treatment\n\nIt is often worth reordering the variables in the mosaic plot formula to see if a different sequence of splits is a more effective visualisation for your problem. Generally, our dependent variable of interest is split last following the ~ in the function call.\n\n\n\n\nProduce mosaicplots of combinations of one, two and three variables for the built-in R data set HairEyeColor\nExperiment with the ordering of the variables in the call to the mosaicplot function to see how different orderings produce different presentations of the data.\nWhich visualisation best highlights the interesting patterns in the data?\n\n\n\nClick for solution\n\n\ndata(\"HairEyeColor\")\n\nmosaicplot(~Hair, data=HairEyeColor,col=2:5,main='')\n\n\n\n\n\n\n\nmosaicplot(~Hair+Sex, data=HairEyeColor,col=2:5,main='')\n\n\n\n\n\n\n\n## With three variables there are 6 different orderings - here are a couple (with a different coloring option)\nmosaicplot(~Hair+Sex+Eye, data=HairEyeColor,col=3:6,main='')\n\n\n\n\n\n\n\nmosaicplot(~Eye+Hair+Sex, data=HairEyeColor,col=3:6,main='')\n\n\n\n\n\n\n\n\nIn the case of three variables, the mosaicplot with Hair highlighted by colours is more readable. With the second one, comparisons are hard to make.\n\n\n\n\nA doubledecker plot is a version of a mosiac plot where all of the splits in the data are vertical, except for the last one. This effectively produces a type of stacked barplot, which we can achieve by setting the direction argument as follows:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, dir = c(\"v\", \"v\", \"h\"),col=2:4)\n\n\n\n\n\n\n\n\nOr, using the doubledecker function directly gives an almost identical plot:\n\ndoubledecker(Improved~Treatment+Sex, data=arthritis)\n\n\n\n\n\n\n\n\n\n\n\nThe default plots are somewhat dull and monochrome. Note that the different shadings used in the plot correspond to the different levels of the last cut variable, i.e. the dependent variable which is Improved here. So, if we supply one colour for each level of that variable we get the following plot:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, main='', col=c(\"wheat\", \"cornflowerblue\", \"tomato1\"))\n\n\n\n\n\n\n\n\nNote: In general, setting colours on mosaic plots can be quite fiddly. We won’t make much use of it, apart from an automatic coloring technique that we describe below."
  },
  {
    "objectID": "Lab2_ManyCatVariables.html#using-colour-to-highlight-unexpected-patterns",
    "href": "Lab2_ManyCatVariables.html#using-colour-to-highlight-unexpected-patterns",
    "title": "Practical 2 - Exploring Many Categorical Variables with R",
    "section": "",
    "text": "A different but helpful use of colour is the shade=TRUE option. The shade=TRUE option works by colouring any tiles in the mosaic that are in disagreement with the independence hypothesis of a chi-square test. This can help us identify any combinations that are unusually common or rate:\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis,shade=TRUE,main='')\n\n\n\n\n\n\n\n\nTiles are shaded blue when more cases are observed than expected given independence, and shaded red when there are fewer cases than expected under independence. The strength of colour indicates how “surprising” those values are. The plot here is showing that most of the variation is not significant (coloured white), but in the Female and Treated group there is a surprisingly high number of patients who display a Marked improvement (blue-ish) - and consequently, fewer than expected (red-ish) whose improvement was None.\n\n\n\nUse the mosaicplot function with shade and the dir arguments to create a “doubledecker” version of the mosaic plot above.\n\n\n\nClick for solution\n\n\nmosaicplot(~Treatment+Sex+Improved, data=arthritis, shade=TRUE, main='',dir = c(\"v\", \"v\", \"h\"))"
  },
  {
    "objectID": "Lab2_ManyCatVariables.html#exercise-2-mosaicplots-on-a-data-set-containing-counts",
    "href": "Lab2_ManyCatVariables.html#exercise-2-mosaicplots-on-a-data-set-containing-counts",
    "title": "Practical 2 - Exploring Many Categorical Variables with R",
    "section": "",
    "text": "Download data: alligator\nThe alligator data, from Agresti (2002), comes from a study of the primary food choices of alligators in four Florida lakes. The goal is to try and learn something about the food choice of the different alligators. The variables are:\n\nlake - one of four lakes: George, Hancock, Oklawaha, and Trafford\nsex - male or female\nsize - small or large\nfood - the food preferences of the alligators in five categories: fish, invertebrates, reptile, bird and other.\n\nAs usual, we begin with a quick look at the data to see what we’re dealing with:\n\nhead(alligator)\n\n\n\n\n\n\nlake\nsex\nsize\nfood\ncount\n\n\n\n\nHancock\nmale\nsmall\nfish\n7\n\n\nHancock\nmale\nsmall\ninvert\n1\n\n\nHancock\nmale\nsmall\nreptile\n0\n\n\nHancock\nmale\nsmall\nbird\n0\n\n\nHancock\nmale\nsmall\nother\n5\n\n\nHancock\nmale\nlarge\nfish\n4\n\n\n\n\n\nHere, unlike the arthritis data, each row does not represent an individual alligator but all of the alligators found with the given combinations of categorical variables. So, for example, we have seen 7 alligators with attributes (Hancock, male, small, fish). This is a slightly different format than we saw above, so we’ll need to deal with it slightly differently.\nTo produce the counts needed for our plots, we need to use the cross-tabulation function xtabs that we used with our barplots. So, to generate the counts of alligators in each lake, we first compute\n\nxtabs(count~lake, data=alligator)\n\nlake\n  George  Hancock Oklawaha Trafford \n      63       55       48       53 \n\n\nand then pass this to our mosaic function for plotting:\n\nmosaicplot(xtabs(count~lake, data=alligator),col=2:5)\n\n\n\n\n\n\n\n\nAlternatively, with a single variable we could just draw a barplot, which is probably a little easier to read!\n\nbarplot(xtabs(count~lake, data=alligator),col=2:5)\n\n\n\n\n\n\n\n\nNote that the mosaicplot is showing the proportions in the different lakes by the width of the bars, whereas the barplot uses the height. We see there are slight differences between the numbers of alligators observed in the different lakes, but they don’t appear to be substantial.\nNote that the only difference with working with these data (which include the counts as a variable) and the previous data set (which did not include the counts) is that we must do the aggregating of the data in the xtabs function first, instead of directly in mosaicplot. The syntax and formula for splitting the data is the same.\n\n\n\nInvestigate the distributions of the other categorical variables individually: sex, size, and food. You can use whatever plot you prefer. Try and answer the following questions:\n\nAre the sexes of alligators evenly distributed?\nWhat about the different sizes?\nWhich food type is most popular?\n\n\n\n\nClick for solution\n\n\nmosaicplot(xtabs(count~sex, data=alligator))\n\n\n\n\n\n\n\n\nMore males than female\n\nmosaicplot(xtabs(count~size, data=alligator))\n\n\n\n\n\n\n\n\nslightly more small than large\n\nmosaicplot(xtabs(count~food, data=alligator))\n\n\n\n\n\n\n\n\nmostly eat fish, then invertebrates.\n\n\n\n\nThe strength of mosaic plots is when considering the combination of multiple categorical variables at once. To keep things manageable, let’s looks at some potentially interesting pairs of variables first:\n\n\n\n\nUse mosaicplot to visualise the size and sex variables together. Remember, if there is no association here then we would expect a regular grid. What associations do you find?\nWhat about size and food?\nDraw the doubledecker plots of the same variables - how do they compare to the mosaic plot?\n\n\n\nClick for solution\n\n\nmosaicplot(xtabs(count~size+sex, data=alligator))\n\n\n\n\n\n\n\ndoubledecker(xtabs(count~size+sex, data=alligator))\n\n\n\n\n\n\n\n\nmore males are larger, females are smaller\n\nmosaicplot(xtabs(count~size+food, data=alligator))\n\n\n\n\n\n\n\ndoubledecker(xtabs(count~size+food, data=alligator))\n\n\n\n\n\n\n\n\nbig alligators eat more fish+reptile, and less invertebrates.\n\nWe can even make a matrix of all the two-way mosaic plots in the style of a scatterplot matrix by the following command:\n\npairs(xtabs(count~.,data=alligator))\n\nUsing a . on the right side of the formula is a shorthand for “include everything”.\n\n\n\n\nCan you locate the plots of size and sex, and size and food within the matrix?\nDo you see any other potential associations (or lack of associations) here? Remember, “no association” will mean the mosaic is divided into an approximately regular grid.\nMake a doubledecker plot of food, size and sex - order the variables so that each bar is split into sections according to the food.\nNow try the mosaic plot and use the shade=TRUE option. Try and achieve the same ordering so that food is the final split. What combinations have been highlighted, and how would you interpret them?\nDo you see any other potentially interesting features here?\n\n\n\nClick for solution\n\nSize:sex plot is in position (1,3), size:food is in (3,4). Seem to be a fair few associations here, except for food:sex Let’s take a closer look:\n\nmosaicplot(xtabs(count~sex+food, data=alligator))\n\n\n\n\n\n\n\n\nThis looks pretty regular, no obvious surprisingly large/small blocks. Let’s try shading the big plot for more emphasis\n\npairs(xtabs(count~.,data=alligator), shade=TRUE)\n\n\n\n\n\n\n\n\nCombining more that two variables in a mosaic can get a bit crazy. Let’s look at size, sex and food all together.\n\ndoubledecker(xtabs(count~size+sex+food, data=alligator))\n\n\n\n\n\n\n\nmosaicplot(xtabs(count~size+sex+food, data=alligator),shade=TRUE)\n\n\n\n\n\n\n\n\nLarge males eat more reptiles, small males eat surprisingly few (are they eating each other…?) Small females eat more invertebrates, large females eat few\nThe four-way table goes a bit mad\n\nmosaicplot(xtabs(count~size+sex+food+lake, data=alligator),shade=TRUE)\n\n\n\n\n\n\n\n\nbut does reveal some features that may be worth digging into, though it has become very hard to digest."
  },
  {
    "objectID": "Lab2_ManyCatVariables.html#exercise-3-for-extra-practice-grouped-and-stacked-barplots",
    "href": "Lab2_ManyCatVariables.html#exercise-3-for-extra-practice-grouped-and-stacked-barplots",
    "title": "Practical 2 - Exploring Many Categorical Variables with R",
    "section": "",
    "text": "Barplots can be used effectively to display combinations of categorical variables. However, they require a little more setup to provide the data in the correct format.\nFirst, a grouped barplot displays a numeric value (e.g. counts) split in groups and subgroups. A few explanation about the code below: * the input dataset must be a numeric matrix. Each group is a column. Each subgroup is a row. So we can only deal with two variables at once. * the barplot function will recognize this format, and automatically perform the grouping for you. * the beside option allows to toggle between the grouped and the stacked barchart\n\n## make a table of counts by Treatment and Improved from the arthritis data\ntab &lt;- xtabs(~Improved+Treatment,data=arthritis)\ntab\n\n        Treatment\nImproved Placebo Treated\n  None        29      13\n  Some         7       7\n  Marked       7      21\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\nAnd to stack the bars, set beside=FALSE\n\nbarplot(tab, beside=FALSE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\nStacked bars are often used to display the proportions of the respective columns attributable to each sub-group. Thankfully, we can easily convert tables of counts to proportions with the prop.table function. If we want the proportions computed within a column, set the margin=2 argument:\n\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=2:4)\n\n\n\n\n\n\n\n\n\n\nWe are now going to compare all the techniques seen so far on a new dataset.\n Download data: airlineArrival\nThe airlineArrival data contains 11000 observations of 3 categorical variables: * Airport - a factor with levels LosAngeles, Phoenix, SanDiego, SanFrancisco, Seattle * Result - a factor with levels Delayed,OnTime * Airline - a factor with levels Alaska, AmericaWest\n\n\n\n\nCreate a grouped barplot of the counts of delayed flights versus ontime flights for both levels of the Airline variable.\nCreate a stacked barplot of the same data above\nIs there much difference in the amount of delayed flights between the two airlines? Which plot is better to assess this?\n\n\n\nClick for solution\n\n\ntab &lt;- xtabs(~Result+Airline,data=airlineArrival)\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n# Stacked barplot\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n\nIn the second plot we can see much better that the proportion is roughly the same. In the first plot, it is easier to see the actual counts of delayed and on time flights.\n\n\n\n\n\nProduce grouped and stacked barcharts of counts of Airport against Result. What is the response variable here?\n\nWhich airports look best for flights being on time? Which look worst?\n\n\n\n\nClick for solution\n\n\ntab &lt;- xtabs(~Result+Airport,data=airlineArrival)\n\n# Grouped barplot\nbarplot(tab, beside=TRUE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n# Stacked barplot\nbarplot(prop.table(tab, margin=2), beside=FALSE, legend=rownames(tab), col=c(2,7))\n\n\n\n\n\n\n\n\nOnce again, stacked plots are more useful: Result is clearly the response variable here and hence we need to produce five different bars.\nSan Francisco is the worse airport and Phoenix the best in terms of flights being on time.\n\n\n\n\n\nNow look at whether both Airport and Airline are associated with delays by producing a mosaicplot and a doubledecker plot of all three variables.\nWhat do you find? Try turning on shade=TRUE.\nIn which plot are the associations most pronounced?\n\n\n\nClick for solution\n\nHere is the mosaicplot:\n\nmosaicplot(~Airport+Result+Airline, data=airlineArrival,col=c(2,7),main='')\n\n\n\n\n\n\n\n\nWithout reformatting, it is hard to infer any possible associations from this. In fact, while some differences between airports exist, the contribution of the airline variable doesn’t show a clear pattern. THis is due to the relatively wide size of the empty spaces in a mosaicplot.\nIn this sense, the doubledecker plot gives a more precise picture\n\ndoubledecker(xtabs(~Airport+Airline+Result, data=airlineArrival))\n\n\n\n\n\n\n\n\nThere one can see that airlines show differences: Alaska airlines flights are slightly more on time compared to AmericaWest flights, from every airport in the data set.\nThe shaded mosaicplot confirms this finding, and gives much more information about unusual associations:\n\nmosaicplot(~Airport+Result+Airline, data=airlineArrival, col=c(2,7), shade=TRUE, main='')"
  },
  {
    "objectID": "Lecture3a_SmoothingKDE.html",
    "href": "Lecture3a_SmoothingKDE.html",
    "title": "Lecture 3a - Smoothing and Kernel Denisty Estimation",
    "section": "",
    "text": "In this lecture we’ll introduce some new techniques to produce smooth estimates of trends and density functions, and then consider how to deal with data that change over time.\n\nSmoothing - introduce smoothing as a method for producing a smoothed estimated trend.\nKernel Density Estimation - apply the smoothing ideas to produce a smooth estimated density for our data.\nExploring Time Series - methods for data which change over time.\n\n\n\n\nWhen exploring the data, we usually try and avoid fitting complex models.\nHowever, identifying a trend line or curve from the raw data can be helpful for emphasising obvious patterns and informing our choice of model.\nBefore exploring the data, we don’t know what kind of relationships or models to expect so we need to do things in a model-free way.\nWe use smoothing which averages out (in various ways) the noise in our data to reveal a general trend or pattern.\n\n\n\nThe data below are from the 2008 US Presidential election and show the polling lead of Barack Obama over John McCain.\n\nlibrary(dslabs)\ndata(polls_2008)\npolls_2008 &lt;- as.data.frame(polls_2008)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\n\n\n\n\n\n\n\n\nThere is definitely a pattern here, but it is obscured in a lot of noise. The pattern is clearly more complicated than a straight line, so how can we represent this?\n\n\n\nA flexible approach would be to take a moving average of the data. If we assume that for any \\(x\\), the trend — ie \\(\\mathbb{E}[Y]\\) — is a constant in the vicinity of \\(x\\). We can estimate this value by the smoothed trend at any point \\(x_0\\) as the sample mean of the data points close to \\(x_0\\). As the values of \\(x_0\\) changes, the nearby data points change, and so the smoothed trend will adapt to the shape of the data. This will obviously be more flexible than the linear regression, but a lot will depend on our definition of “close to \\(x_0\\)”.\n\n\nThe simple (central) moving average of our data \\((x_1,Y_1), \\dots,(x_n,Y_n)\\) at a new location \\(x_0\\) is defined as \\[\\hat{f}(x_0)=\\frac{1}{N(x_0)}\\sum_{x_i\\in\\mathcal{N}(x_0)} Y_i\\] where \\(\\mathcal{N}(x_0)\\) is the set of all data points in the neighbourhood of \\(x_0\\), and \\(N(x_0)=\\#\\mathcal{N}(x_0)\\).\nThe neighbourhood of \\(x_0\\) is defined as \\[\\mathcal{N}(x_0)=\\left\\{x_i: |x_i-x_0|&lt;h\\right\\}\\] and \\(h\\) is the bandwidth parameter.\nTo perform a moving average smooth in R, we can use the ksmooth function, specifying the bandwidth parameter as \\(h\\) above. The result is a list with x and y components that give the smoothed values, which can then be used to draw the smoothed curve with the lines function.\n\n\n\nUsing \\(h=3.5\\) (so we average over 1 week of data at a time) we obtain:\n\nfit &lt;- ksmooth(polls_2008$day, polls_2008$margin, kernel = \"box\", bandwidth = 7)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fit$x, y=fit$y,col=2,lwd=4)\n\n\n\n\n\n\n\n\nReducing the bandwidth (\\(h=3.5\\)) reduces the smoothness:\n\n\n\n\n\n\n\n\n\nIncreasing the bandwidth (\\(h=14\\)) increases the smoothness:\n\nfit &lt;- ksmooth(polls_2008$day, polls_2008$margin, kernel = \"box\", bandwidth = 14)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fit$x, y=fit$y,col=2,lwd=4)\n\n\n\n\n\n\n\n\nBut we must be careful not to go too far (\\(h=70\\)):\n\nfit &lt;- ksmooth(polls_2008$day, polls_2008$margin, kernel = \"box\", bandwidth = 70)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fit$x, y=fit$y,col=2,lwd=4)\n\n\n\n\n\n\n\n\n\nThe choice of bandwidth is very influential when smoothing data.\nIf we under-smooth, there will still be a lot of noise in the trend.\nIf we over-smooth, we can average out genuine features of the data.\nThis is an immediate example of the bias-variance tradeoff! Under-smoothing leads to high-variance but low bias, and over-smoothing gives low variance but high bias.\n\n\n\n\n\nThe downside of the flexibility of the moving average models is that they can be quite volatile. As we move along the \\(x\\) axis, the data set will change - with points appearing and disappearing according to our neighbourhood rule.\nWe can think of the moving average as computing the following weighted average: \\[\\hat{f}(x_0) = \\sum_{i=1}^n w_i(x_0) Y_i\\] with each point receiving a weight \\(w_i(x_0)\\) of either \\(0\\) or \\(1/N_0\\) depending on whether we’re in the neighbourhood or not.\nA smoother way to do this is to use a continuous weight function, to give nearer points more influence and distant points less influence.\n\n\nThe kernel smoother of our data \\((x_1,Y_1), \\dots,(x_n,Y_n)\\) at a new location \\(x_0\\) is defined as: \\[\\hat{f}(x_0)= \\sum_{i=1}^n w_i(x_0) Y_i\\] where \\(w_i(x_0)\\) is the weight of each data point \\(x_i\\) on the point of interest \\(x_0\\) and \\(\\sum\\nolimits_{i=1}^n w_i=1\\).\nThe weights are defined in terms of a kernel function \\(K\\) which controls its shape and behaviour such that \\[w_i(x;h)=\\frac{K\\left(\\frac{x-x_i}{h}\\right)}{\\sum\\nolimits_{i=1}^nK\\left(\\frac{x-x_i}{h}\\right)}\\].\n\n\n\nFor our purposes, the kernel function is non-negative real-valued integrable function \\(K\\). Typically, we also require\n\nNormalisation: \\(\\int K(u)~\\text{d}u=1\\), making \\(K(\\cdot)\\) a pdf.\nSymmetry: \\(K(u)=K(-u)\\) for all \\(u\\)\n\nSome examples of kernel functions:\n\nGaussian: \\(K(u)=\\phi(u)\\), probably the most popular\nUniform: \\(K(u)=\\frac{1}{2}I_{\\{|u|&lt;1\\}}\\), gives us the simple moving average\nEpanechnikov: \\(K(u)=\\frac{3}{4}(1-u^2)I_{\\{|u|&lt;1\\}}\\), is optimally efficient.\n\n\n\n\n\n\n\n\n\n\nWe can again perform kernel smoothing using the ksmooth function, but by specifying the kernel argument we can choose the kernel function. Only the Uniform (kernel=\"box\") and Gaussian (kernel=\"normal\") options are supported.\n\nfitN &lt;- ksmooth(polls_2008$day, polls_2008$margin, kernel = \"normal\", bandwidth = 7)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=fitN$y,col=4,lwd=4)\n\n\n\n\n\n\n\n\n\n\n\nOur kernel smoother estimate will inherit the smoothness properties of the kernel.\nFor well-behaved data, the choice of the kernel has little practical impact, at least compared with the choice of the bandwidth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA limitation of this weighted average approach is the assumption that the trend is approximately constant in the neighbourhood of any given point.\nFor this to make sense, we must consider small bandwidths, containing only a few data points to average, and giving noisy estimates of the trend \\(\\hat{f}(x)\\).\nInstead, if we assume the function is locally linear then its reasonable to consider larger window sizes and fit a local linear regression.\nFor a smooth function, we can consider a Taylor expansion to see why this is a valid approach.\n\n\n\nTo obtain the local linear fit, we use a weighted approach. Instead of using least squares, we minimise a weighted version instead: \\[\\sum_{i=1}^N w_0(x_i) \\left[Y_i-\\{\\beta_0+\\beta_1x_i\\}\\right]^2.\\]\nThe standard kernel used to produce the weights is the Tukey tri-weight \\[K(u)=\\begin{cases}\n\\left(1-|u|^3\\right)^3 & |u|\\leq 1\\\\\n0&|u|&gt;1\\end{cases}\\] This combination of methods is known as loess or lowess.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of using a bandwidth to define a neighbourhood, loess methods fix the number of points used in each local fit at a specified proportion \\(\\alpha\\) of the nearest points in the data set, known as the span.\nTechniques such as cross-validation are often used to find the ‘best’ \\(\\alpha\\), but when just exploring the data this is probably over-kill.\nAs the linear regression changes depending on the value of \\(x\\), we cannot extract a single model for the loess fit on a given data set.\n\nTo apply local regression by LOESS, we can use the loess function, which works much like the lm function does for standard regression. First we fit the model using loess:\n\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.25)\n\nWe can then use the predict functions to predict values of the smoothed trend:\n\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\n\nThe predictions can then be used to draw the smoothed trend on the plot\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=lpred$fit,col=cols[1],lwd=4)\n\n\n\n\n\n\n\n\nAdjusting the span argument has a similar effect to adjusting the bandwidth of the kernel smoother on the smoothness of the resulting trend:\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=lpred$fit,col=cols[1],lwd=4)\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.5)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\nlines(x=fitN$x, y=lpred$fit,col=cols[2],lwd=4)\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.75)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\nlines(x=fitN$x, y=lpred$fit,col=cols[3],lwd=4)\nlegend(x='bottomright',col=cols[1:3],pch=NA,lwd=4,legend=c(0.25,0.5,0.75))\n\n\n\n\n\n\n\n\nAs LOESS just uses linear regressions to generate its smoothed trend, we can apply linear regression theory to obtain the standard errors of the trend and form confidence intervals around it.\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.25)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\npolygon(x=c(fitN$x,rev(fitN$x)), y=c(lpred$fit- qt(0.975,lpred$df)*lpred$se,\n                                        rev(lpred$fit+ qt(0.975,lpred$df)*lpred$se)),\n           col=alpha(cols[1],0.5),border=NA)\nlines(x=fitN$x, y=lpred$fit,col=2,lwd=4)\n\n\n\n\n\n\n\n\n\nSplines offer a powerful alternative method for smoothing data, but we don’t have time to go into those…\n\n\n\n\n\n\n\nSmoothing and kernel methods provide a flexible way of estimating general functions of our data without needing many assumptions or complex techniques.\nWhat about using them to estimate the most useful function of all - the probability density function?\nThis leads to an area of statistics known as kernel density estimation.\nWe’ve already seen the simplest method for estimating a density function - the histogram!\n\n\n\nFirst, define the fixed intervals (or bins) of the histogram by the intervals \\(\\mathcal{B}_k=[a_k,a_{k+1})\\), where \\(a_{k+1}=a_k+h\\) for a bin-width (or bandwidth) \\(h&gt;0\\).\nThe density of \\(X\\) in bin \\(k\\) can be expressed as \\[f(x_0)=\\lim_{h\\rightarrow 0+} \\frac{\\mathbb{P}[a_k&lt;X&lt;a_{k}+h]}{h}.\\]\nAn unbiased estimate of a probability is a sample proportion, so if we define the number of points in bin \\(\\mathcal{B}_k\\) as \\(n_k=\\#\\{X_i:X\\in[a_k,a_{k+1})\\}\\) then our histogram estimate of the density above can be written as \\[\\hat{f}_\\text{hist}(x_0)=\\frac{1}{h}\\hat{\\mathbb{P}}[a_k&lt;X&lt;a_{k}+h]=\\frac{1}{h}\\frac{n_k}{n}\\]\nWith a bit more theory (and glossing over some details), we can show that for an \\(x\\in\\mathcal{B}_k\\):\n\nIf \\(h\\rightarrow 0\\), then \\(\\mathbb{E}\\left[\\hat{f}_\\text{hist}(x)\\right]\\rightarrow f(x)\\) - so we reduce bias by reducing \\(h\\)\n\\(\\mathbb{V}\\text{ar}\\left[\\hat{f}_\\text{hist}(x)\\right]=\\frac{f(x)[1-hf(x)]}{nh}\\), so if \\(h\\rightarrow 0\\) then the variance increases!\nWe can also reduce the variance by letting \\(n\\rightarrow\\infty\\).\n\nWe have another bias-variance tradeoff, and another problem very sensitive to the choice of bandwidth!\n\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndata(galaxies)\ngalaxies &lt;- galaxies/1000\n\nPlotted below are four histograms of the velocities in km/sec of 82 galaxies from 6 well-separated conic sections of an unfilled survey of the Corona Borealis region. The sensitivity of the histogram estimate of the density to the bin-width is clear, with the top-left plot showing a lot of structure but also a lot of noise. The bottom-right plot is clearly over-smoothed, with many of the data features obscured resulting in a high bias.\n\nhist(galaxies,breaks=seq(9,35,by=0.5),xlab='Velocity',freq=FALSE,main='h=0.5',col=2)\nrug(galaxies)\n\n\n\n\n\n\n\nhist(galaxies,breaks=seq(9,35,by=2),xlab='Velocity',freq=FALSE,main='h=2',col=2)\nrug(galaxies)\n\n\n\n\n\n\n\nhist(galaxies,breaks=seq(9,39,by=5),xlab='Velocity',freq=FALSE,main='h=5',col=2)\nrug(galaxies)\n\n\n\n\n\n\n\nhist(galaxies,breaks=seq(9,39,by=10),,xlab='Velocity',freq=FALSE,main='h=10',col=2)\nrug(galaxies)\n\n\n\n\n\n\n\n\n\n\n\nLike the simple moving average, the histogram suffers a lot of similar issues as an estimate for a density function:\n\nThe shape of the estimate is very sensitive to the choice of bin-width (or bandwidth)\nThe fixed and rigid intervals include variable numbers of points, giving variable levels of accuracy\nMost importantly, we’re using something non-smooth (i.e. bars which are constant in their interval) to estimate a smooth function!\n\n\n\n\n\nThe kernel density estimate of a probability density function \\(f(x)\\) from data \\(X_1,\\dots,X_n\\) is defined as \\[\\hat{f}_K(x;h)=\\frac{1}{nh}\\sum_{i=1}^n K\\left(\\frac{x-X_i}{h}\\right),\\] where \\(h\\) is the bandwidth, and \\(K(\\cdot)\\) is a kernel function.\nThe histogram estimator for bin \\(k\\) can be expressed in this form: \\(\\hat{f}_\\text{hist}(x;h)=\\frac{1}{nh}\\sum_{i=1}^n  \\mathbf{I}\\{X_i\\in\\mathcal{B}_k\\}\\)\nThe plot below illustrates the kernel density estimate. We begin with data points (left), each data point then makes a contribution to the density function estimate according to the kernel function (middle), the contributions are then averaged and we obtain the smooth density estimate (right).\n\n\n\n\n\n\n\n\n\nAs the bandwidth affects the kernel width and hence the distance over which each data point influences the density estimate, the effect of bandwidth on the shape of the final KDE is quite strong:\n\n\n\n\n\n\n\n\n\nFor density estimates, the shape of the kernel has a much stronger impact on the smoothness of the estimated density:\n\n\n\n\n\n\n\n\n\n\n\nWe can (but we won’t) derive the theoretical properties of the KDE and find very similar results to those seen earlier:\n\n\\(\\text{bias}\\left[\\hat{f}_K(x),f(x)\\right]\\rightarrow 0\\) as \\(h\\rightarrow 0\\) — bias is reduced for smaller bandwidths\n\\(\\mathbb{V}\\text{ar}\\left[\\hat{f}_K(x)\\right]\\rightarrow\\infty\\) as \\(h\\rightarrow 0\\) — variance increases for smaller bandwidths\nBut \\(\\mathbb{V}\\text{ar}\\left[\\hat{f}_K(x)\\right]\\rightarrow0\\) if \\(nh\\rightarrow\\infty\\) — small bandwidths are okay so long as \\(n\\) is big enough!\n\nThe problem of identifying the ‘best’ \\(h\\) is called bandwidth selection and is a substantial area in itself.\n\n\n\nThe density function can perform kernel density estimation and is used in a similar way to ksmooth:\n\nd &lt;- density(galaxies, bw=0.25)\n\nAgain, the result is a list with x and y components that give the smoothed densities, which can then be used to draw the smoothed curve with the lines function. If the bandwidth (bw) is not specified, R will try and estimate it.\n\nplot(x=range(galaxies),y=c(0,0.25),ty='n',ylab='Density',xlab='Velocity')\nrug(galaxies)\nd &lt;- density(galaxies,bw=0.25)\nlines(x=d$x,y=d$y, lwd=3,col=2)\nd &lt;- density(galaxies,bw=0.5)\nlines(x=d$x,y=d$y, lwd=3,col=3)\nd &lt;- density(galaxies,bw=1)\nlines(x=d$x,y=d$y, lwd=3,col=4)\nd &lt;- density(galaxies,bw=3)\nlines(x=d$x,y=d$y, lwd=3,col=5)\nlegend(x='topright',legend=paste0('h=',c(0.25,0.5,1,3)),\n       lwd=3,col=2:5,pch=NA)\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ve seen how we can use density estimation to produce a “smooth” histogram for a variable. We can also combine this idea with a boxplot, to produce a violin plot:\n\nlibrary(ggplot2)\nggplot(data.frame(galaxies), aes(x=1,y = galaxies)) + geom_violin(fill = \"lightBlue\", color = \"#473e2c\") \n\n\n\n\n\n\n\n\nThe syntax is rather complicated here, as we’re relying on the ggplot package. GGPlot has a somewhat different syntax than standard R (which is why we didn’t focus on it), but it can be very effective and versatile. Once you get the hang of it!\nThe violin plot functions something like a combination of box plot and histogram. We get the same sort of information on the shape of the distribution that we do with the smoothed histogram, but the simplicity and handling of outliers from box plots is lost.\nNevertheless, the violin plot is most effective when comparing distributions across multiple sub-groups of the data.\n\nlibrary(ggplot2)\ndata(chickwts)\nggplot(chickwts, aes(x = factor(feed), y = weight)) + \n    geom_violin(fill = \"lightBlue\", adjust=0.5) +\n    labs(x = \"Feed Supplement\", y = \"Chick Weight (g)\") \n\n\n\n\n\n\n\n\nIn comparison to the boxplots, we get a lot more information on the shape of the distributions.\n\nlibrary(ggplot2)\ndata(chickwts)\nggplot(chickwts, \n       aes(x = factor(feed), y = weight)) + \n    geom_violin(fill = \"lightBlue\", adjust=0.5) +\n    geom_boxplot(width=0.2) +\n    labs(x = \"Feed Supplement\", y = \"Chick Weight (g)\") \n\n\n\n\n\n\n\n\nThe ridgeline plot is used in similar circumstances, but shows the densities horizontally in a series of ridges.\n\nlibrary(ggridges)\nggplot(chickwts, aes(weight, y=factor(feed), fill=feed)) + \n  geom_density_ridges(scale = 1.5)\n\nPicking joint bandwidth of 24.4\n\n\n\n\n\n\n\n\n\n\n\n\nThe ideas of density estimation can be extended beyond one dimension. If we consider two variables at once, then we can estimate their 2-D joint distribution as a smoothed version of a scatterplot. This also provides another alternative method of dealing with overplotting of dense data sets.\nWe can fit the 2-D density using the kde2d function from the MASS package. Let’s apply this to the Old Faithful data set:\n\nlibrary(MASS)\nk1 &lt;- kde2d(geyser$duration, geyser$waiting, n=100)\n\nNote that we need both an x and a y variable now, as we’re working in 2-dimension. We can set a bandwidth by supplying an h parameter, but this is now a vector specifying a bandwidth for the x and y directions separately.\nThe output of kde2d is a list with x, y, and z components. The x and y are the values of the specified variables used to create a grid of points. The density value at every combination of x and y is then estimated and returned in the z component. The value of n here indicates the number of grid points along each axis.\nHaving estimated the 2-D density, we’ll need some new functions to visualise it. A contour plot is one option:\n\ncontour(k1, col='blue')\n\n\n\n\n\n\n\n\nThough without the data, this is a bit hard to understand. So let’s overlay the contours on a scatterplot:\n\nplot(x=geyser$duration, y=geyser$waiting, pch=16, xlab='duration', ylab='waiting')\ncontour(k1, col='blue', add=TRUE)\n\n\n\n\n\n\n\n\nOur 2-D density function for these data appears to detect three separate modes - one for the short duration eruptions, and two groups for the longer durations.\nWe could also generate a heat map of the density values\n\nimage(k1)\n\n\n\n\n\n\n\n\nNotice the pixelated quality of the plot. This is down to the choice of n when calling kde2d. If n is too small, we don’t produce enough predictions to generate a cleaner plot, so if we increase n this should improve. Note that this is not the same as the smoothing performed by KDE which is controlled by the bandwidth parameter h - this is purely a visual thing.\nLet’s boost the value of n and overlay the points on the plot.\n\nimage(kde2d(geyser$duration, geyser$waiting, n = 500), xlab='duration', ylab='waiting')\npoints(x=geyser$duration, y=geyser$waiting, pch=16)\n\n\n\n\n\n\n\n\nWhen we have extremely large numbers of points (e.g. millions), drawing a scatterplot can be a slow process as each point has to be drawn individually and inevitably a large amount of over-plotting occurs. In such cases, it is often better to skip the scatterplot and look at heatmaps or contour plots of the data instead"
  },
  {
    "objectID": "Lecture3a_SmoothingKDE.html#smoothing",
    "href": "Lecture3a_SmoothingKDE.html#smoothing",
    "title": "Lecture 3a - Smoothing and Kernel Denisty Estimation",
    "section": "",
    "text": "When exploring the data, we usually try and avoid fitting complex models.\nHowever, identifying a trend line or curve from the raw data can be helpful for emphasising obvious patterns and informing our choice of model.\nBefore exploring the data, we don’t know what kind of relationships or models to expect so we need to do things in a model-free way.\nWe use smoothing which averages out (in various ways) the noise in our data to reveal a general trend or pattern.\n\n\n\nThe data below are from the 2008 US Presidential election and show the polling lead of Barack Obama over John McCain.\n\nlibrary(dslabs)\ndata(polls_2008)\npolls_2008 &lt;- as.data.frame(polls_2008)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\n\n\n\n\n\n\n\n\nThere is definitely a pattern here, but it is obscured in a lot of noise. The pattern is clearly more complicated than a straight line, so how can we represent this?\n\n\n\nA flexible approach would be to take a moving average of the data. If we assume that for any \\(x\\), the trend — ie \\(\\mathbb{E}[Y]\\) — is a constant in the vicinity of \\(x\\). We can estimate this value by the smoothed trend at any point \\(x_0\\) as the sample mean of the data points close to \\(x_0\\). As the values of \\(x_0\\) changes, the nearby data points change, and so the smoothed trend will adapt to the shape of the data. This will obviously be more flexible than the linear regression, but a lot will depend on our definition of “close to \\(x_0\\)”.\n\n\nThe simple (central) moving average of our data \\((x_1,Y_1), \\dots,(x_n,Y_n)\\) at a new location \\(x_0\\) is defined as \\[\\hat{f}(x_0)=\\frac{1}{N(x_0)}\\sum_{x_i\\in\\mathcal{N}(x_0)} Y_i\\] where \\(\\mathcal{N}(x_0)\\) is the set of all data points in the neighbourhood of \\(x_0\\), and \\(N(x_0)=\\#\\mathcal{N}(x_0)\\).\nThe neighbourhood of \\(x_0\\) is defined as \\[\\mathcal{N}(x_0)=\\left\\{x_i: |x_i-x_0|&lt;h\\right\\}\\] and \\(h\\) is the bandwidth parameter.\nTo perform a moving average smooth in R, we can use the ksmooth function, specifying the bandwidth parameter as \\(h\\) above. The result is a list with x and y components that give the smoothed values, which can then be used to draw the smoothed curve with the lines function.\n\n\n\nUsing \\(h=3.5\\) (so we average over 1 week of data at a time) we obtain:\n\nfit &lt;- ksmooth(polls_2008$day, polls_2008$margin, kernel = \"box\", bandwidth = 7)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fit$x, y=fit$y,col=2,lwd=4)\n\n\n\n\n\n\n\n\nReducing the bandwidth (\\(h=3.5\\)) reduces the smoothness:\n\n\n\n\n\n\n\n\n\nIncreasing the bandwidth (\\(h=14\\)) increases the smoothness:\n\nfit &lt;- ksmooth(polls_2008$day, polls_2008$margin, kernel = \"box\", bandwidth = 14)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fit$x, y=fit$y,col=2,lwd=4)\n\n\n\n\n\n\n\n\nBut we must be careful not to go too far (\\(h=70\\)):\n\nfit &lt;- ksmooth(polls_2008$day, polls_2008$margin, kernel = \"box\", bandwidth = 70)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fit$x, y=fit$y,col=2,lwd=4)\n\n\n\n\n\n\n\n\n\nThe choice of bandwidth is very influential when smoothing data.\nIf we under-smooth, there will still be a lot of noise in the trend.\nIf we over-smooth, we can average out genuine features of the data.\nThis is an immediate example of the bias-variance tradeoff! Under-smoothing leads to high-variance but low bias, and over-smoothing gives low variance but high bias.\n\n\n\n\n\nThe downside of the flexibility of the moving average models is that they can be quite volatile. As we move along the \\(x\\) axis, the data set will change - with points appearing and disappearing according to our neighbourhood rule.\nWe can think of the moving average as computing the following weighted average: \\[\\hat{f}(x_0) = \\sum_{i=1}^n w_i(x_0) Y_i\\] with each point receiving a weight \\(w_i(x_0)\\) of either \\(0\\) or \\(1/N_0\\) depending on whether we’re in the neighbourhood or not.\nA smoother way to do this is to use a continuous weight function, to give nearer points more influence and distant points less influence.\n\n\nThe kernel smoother of our data \\((x_1,Y_1), \\dots,(x_n,Y_n)\\) at a new location \\(x_0\\) is defined as: \\[\\hat{f}(x_0)= \\sum_{i=1}^n w_i(x_0) Y_i\\] where \\(w_i(x_0)\\) is the weight of each data point \\(x_i\\) on the point of interest \\(x_0\\) and \\(\\sum\\nolimits_{i=1}^n w_i=1\\).\nThe weights are defined in terms of a kernel function \\(K\\) which controls its shape and behaviour such that \\[w_i(x;h)=\\frac{K\\left(\\frac{x-x_i}{h}\\right)}{\\sum\\nolimits_{i=1}^nK\\left(\\frac{x-x_i}{h}\\right)}\\].\n\n\n\nFor our purposes, the kernel function is non-negative real-valued integrable function \\(K\\). Typically, we also require\n\nNormalisation: \\(\\int K(u)~\\text{d}u=1\\), making \\(K(\\cdot)\\) a pdf.\nSymmetry: \\(K(u)=K(-u)\\) for all \\(u\\)\n\nSome examples of kernel functions:\n\nGaussian: \\(K(u)=\\phi(u)\\), probably the most popular\nUniform: \\(K(u)=\\frac{1}{2}I_{\\{|u|&lt;1\\}}\\), gives us the simple moving average\nEpanechnikov: \\(K(u)=\\frac{3}{4}(1-u^2)I_{\\{|u|&lt;1\\}}\\), is optimally efficient.\n\n\n\n\n\n\n\n\n\n\nWe can again perform kernel smoothing using the ksmooth function, but by specifying the kernel argument we can choose the kernel function. Only the Uniform (kernel=\"box\") and Gaussian (kernel=\"normal\") options are supported.\n\nfitN &lt;- ksmooth(polls_2008$day, polls_2008$margin, kernel = \"normal\", bandwidth = 7)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=fitN$y,col=4,lwd=4)\n\n\n\n\n\n\n\n\n\n\n\nOur kernel smoother estimate will inherit the smoothness properties of the kernel.\nFor well-behaved data, the choice of the kernel has little practical impact, at least compared with the choice of the bandwidth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA limitation of this weighted average approach is the assumption that the trend is approximately constant in the neighbourhood of any given point.\nFor this to make sense, we must consider small bandwidths, containing only a few data points to average, and giving noisy estimates of the trend \\(\\hat{f}(x)\\).\nInstead, if we assume the function is locally linear then its reasonable to consider larger window sizes and fit a local linear regression.\nFor a smooth function, we can consider a Taylor expansion to see why this is a valid approach.\n\n\n\nTo obtain the local linear fit, we use a weighted approach. Instead of using least squares, we minimise a weighted version instead: \\[\\sum_{i=1}^N w_0(x_i) \\left[Y_i-\\{\\beta_0+\\beta_1x_i\\}\\right]^2.\\]\nThe standard kernel used to produce the weights is the Tukey tri-weight \\[K(u)=\\begin{cases}\n\\left(1-|u|^3\\right)^3 & |u|\\leq 1\\\\\n0&|u|&gt;1\\end{cases}\\] This combination of methods is known as loess or lowess.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of using a bandwidth to define a neighbourhood, loess methods fix the number of points used in each local fit at a specified proportion \\(\\alpha\\) of the nearest points in the data set, known as the span.\nTechniques such as cross-validation are often used to find the ‘best’ \\(\\alpha\\), but when just exploring the data this is probably over-kill.\nAs the linear regression changes depending on the value of \\(x\\), we cannot extract a single model for the loess fit on a given data set.\n\nTo apply local regression by LOESS, we can use the loess function, which works much like the lm function does for standard regression. First we fit the model using loess:\n\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.25)\n\nWe can then use the predict functions to predict values of the smoothed trend:\n\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\n\nThe predictions can then be used to draw the smoothed trend on the plot\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=lpred$fit,col=cols[1],lwd=4)\n\n\n\n\n\n\n\n\nAdjusting the span argument has a similar effect to adjusting the bandwidth of the kernel smoother on the smoothness of the resulting trend:\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=lpred$fit,col=cols[1],lwd=4)\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.5)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\nlines(x=fitN$x, y=lpred$fit,col=cols[2],lwd=4)\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.75)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\nlines(x=fitN$x, y=lpred$fit,col=cols[3],lwd=4)\nlegend(x='bottomright',col=cols[1:3],pch=NA,lwd=4,legend=c(0.25,0.5,0.75))\n\n\n\n\n\n\n\n\nAs LOESS just uses linear regressions to generate its smoothed trend, we can apply linear regression theory to obtain the standard errors of the trend and form confidence intervals around it.\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.25)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\npolygon(x=c(fitN$x,rev(fitN$x)), y=c(lpred$fit- qt(0.975,lpred$df)*lpred$se,\n                                        rev(lpred$fit+ qt(0.975,lpred$df)*lpred$se)),\n           col=alpha(cols[1],0.5),border=NA)\nlines(x=fitN$x, y=lpred$fit,col=2,lwd=4)\n\n\n\n\n\n\n\n\n\nSplines offer a powerful alternative method for smoothing data, but we don’t have time to go into those…"
  },
  {
    "objectID": "Lecture3a_SmoothingKDE.html#density-estimation",
    "href": "Lecture3a_SmoothingKDE.html#density-estimation",
    "title": "Lecture 3a - Smoothing and Kernel Denisty Estimation",
    "section": "",
    "text": "Smoothing and kernel methods provide a flexible way of estimating general functions of our data without needing many assumptions or complex techniques.\nWhat about using them to estimate the most useful function of all - the probability density function?\nThis leads to an area of statistics known as kernel density estimation.\nWe’ve already seen the simplest method for estimating a density function - the histogram!\n\n\n\nFirst, define the fixed intervals (or bins) of the histogram by the intervals \\(\\mathcal{B}_k=[a_k,a_{k+1})\\), where \\(a_{k+1}=a_k+h\\) for a bin-width (or bandwidth) \\(h&gt;0\\).\nThe density of \\(X\\) in bin \\(k\\) can be expressed as \\[f(x_0)=\\lim_{h\\rightarrow 0+} \\frac{\\mathbb{P}[a_k&lt;X&lt;a_{k}+h]}{h}.\\]\nAn unbiased estimate of a probability is a sample proportion, so if we define the number of points in bin \\(\\mathcal{B}_k\\) as \\(n_k=\\#\\{X_i:X\\in[a_k,a_{k+1})\\}\\) then our histogram estimate of the density above can be written as \\[\\hat{f}_\\text{hist}(x_0)=\\frac{1}{h}\\hat{\\mathbb{P}}[a_k&lt;X&lt;a_{k}+h]=\\frac{1}{h}\\frac{n_k}{n}\\]\nWith a bit more theory (and glossing over some details), we can show that for an \\(x\\in\\mathcal{B}_k\\):\n\nIf \\(h\\rightarrow 0\\), then \\(\\mathbb{E}\\left[\\hat{f}_\\text{hist}(x)\\right]\\rightarrow f(x)\\) - so we reduce bias by reducing \\(h\\)\n\\(\\mathbb{V}\\text{ar}\\left[\\hat{f}_\\text{hist}(x)\\right]=\\frac{f(x)[1-hf(x)]}{nh}\\), so if \\(h\\rightarrow 0\\) then the variance increases!\nWe can also reduce the variance by letting \\(n\\rightarrow\\infty\\).\n\nWe have another bias-variance tradeoff, and another problem very sensitive to the choice of bandwidth!\n\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndata(galaxies)\ngalaxies &lt;- galaxies/1000\n\nPlotted below are four histograms of the velocities in km/sec of 82 galaxies from 6 well-separated conic sections of an unfilled survey of the Corona Borealis region. The sensitivity of the histogram estimate of the density to the bin-width is clear, with the top-left plot showing a lot of structure but also a lot of noise. The bottom-right plot is clearly over-smoothed, with many of the data features obscured resulting in a high bias.\n\nhist(galaxies,breaks=seq(9,35,by=0.5),xlab='Velocity',freq=FALSE,main='h=0.5',col=2)\nrug(galaxies)\n\n\n\n\n\n\n\nhist(galaxies,breaks=seq(9,35,by=2),xlab='Velocity',freq=FALSE,main='h=2',col=2)\nrug(galaxies)\n\n\n\n\n\n\n\nhist(galaxies,breaks=seq(9,39,by=5),xlab='Velocity',freq=FALSE,main='h=5',col=2)\nrug(galaxies)\n\n\n\n\n\n\n\nhist(galaxies,breaks=seq(9,39,by=10),,xlab='Velocity',freq=FALSE,main='h=10',col=2)\nrug(galaxies)\n\n\n\n\n\n\n\n\n\n\n\nLike the simple moving average, the histogram suffers a lot of similar issues as an estimate for a density function:\n\nThe shape of the estimate is very sensitive to the choice of bin-width (or bandwidth)\nThe fixed and rigid intervals include variable numbers of points, giving variable levels of accuracy\nMost importantly, we’re using something non-smooth (i.e. bars which are constant in their interval) to estimate a smooth function!\n\n\n\n\n\nThe kernel density estimate of a probability density function \\(f(x)\\) from data \\(X_1,\\dots,X_n\\) is defined as \\[\\hat{f}_K(x;h)=\\frac{1}{nh}\\sum_{i=1}^n K\\left(\\frac{x-X_i}{h}\\right),\\] where \\(h\\) is the bandwidth, and \\(K(\\cdot)\\) is a kernel function.\nThe histogram estimator for bin \\(k\\) can be expressed in this form: \\(\\hat{f}_\\text{hist}(x;h)=\\frac{1}{nh}\\sum_{i=1}^n  \\mathbf{I}\\{X_i\\in\\mathcal{B}_k\\}\\)\nThe plot below illustrates the kernel density estimate. We begin with data points (left), each data point then makes a contribution to the density function estimate according to the kernel function (middle), the contributions are then averaged and we obtain the smooth density estimate (right).\n\n\n\n\n\n\n\n\n\nAs the bandwidth affects the kernel width and hence the distance over which each data point influences the density estimate, the effect of bandwidth on the shape of the final KDE is quite strong:\n\n\n\n\n\n\n\n\n\nFor density estimates, the shape of the kernel has a much stronger impact on the smoothness of the estimated density:\n\n\n\n\n\n\n\n\n\n\n\nWe can (but we won’t) derive the theoretical properties of the KDE and find very similar results to those seen earlier:\n\n\\(\\text{bias}\\left[\\hat{f}_K(x),f(x)\\right]\\rightarrow 0\\) as \\(h\\rightarrow 0\\) — bias is reduced for smaller bandwidths\n\\(\\mathbb{V}\\text{ar}\\left[\\hat{f}_K(x)\\right]\\rightarrow\\infty\\) as \\(h\\rightarrow 0\\) — variance increases for smaller bandwidths\nBut \\(\\mathbb{V}\\text{ar}\\left[\\hat{f}_K(x)\\right]\\rightarrow0\\) if \\(nh\\rightarrow\\infty\\) — small bandwidths are okay so long as \\(n\\) is big enough!\n\nThe problem of identifying the ‘best’ \\(h\\) is called bandwidth selection and is a substantial area in itself.\n\n\n\nThe density function can perform kernel density estimation and is used in a similar way to ksmooth:\n\nd &lt;- density(galaxies, bw=0.25)\n\nAgain, the result is a list with x and y components that give the smoothed densities, which can then be used to draw the smoothed curve with the lines function. If the bandwidth (bw) is not specified, R will try and estimate it.\n\nplot(x=range(galaxies),y=c(0,0.25),ty='n',ylab='Density',xlab='Velocity')\nrug(galaxies)\nd &lt;- density(galaxies,bw=0.25)\nlines(x=d$x,y=d$y, lwd=3,col=2)\nd &lt;- density(galaxies,bw=0.5)\nlines(x=d$x,y=d$y, lwd=3,col=3)\nd &lt;- density(galaxies,bw=1)\nlines(x=d$x,y=d$y, lwd=3,col=4)\nd &lt;- density(galaxies,bw=3)\nlines(x=d$x,y=d$y, lwd=3,col=5)\nlegend(x='topright',legend=paste0('h=',c(0.25,0.5,1,3)),\n       lwd=3,col=2:5,pch=NA)\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ve seen how we can use density estimation to produce a “smooth” histogram for a variable. We can also combine this idea with a boxplot, to produce a violin plot:\n\nlibrary(ggplot2)\nggplot(data.frame(galaxies), aes(x=1,y = galaxies)) + geom_violin(fill = \"lightBlue\", color = \"#473e2c\") \n\n\n\n\n\n\n\n\nThe syntax is rather complicated here, as we’re relying on the ggplot package. GGPlot has a somewhat different syntax than standard R (which is why we didn’t focus on it), but it can be very effective and versatile. Once you get the hang of it!\nThe violin plot functions something like a combination of box plot and histogram. We get the same sort of information on the shape of the distribution that we do with the smoothed histogram, but the simplicity and handling of outliers from box plots is lost.\nNevertheless, the violin plot is most effective when comparing distributions across multiple sub-groups of the data.\n\nlibrary(ggplot2)\ndata(chickwts)\nggplot(chickwts, aes(x = factor(feed), y = weight)) + \n    geom_violin(fill = \"lightBlue\", adjust=0.5) +\n    labs(x = \"Feed Supplement\", y = \"Chick Weight (g)\") \n\n\n\n\n\n\n\n\nIn comparison to the boxplots, we get a lot more information on the shape of the distributions.\n\nlibrary(ggplot2)\ndata(chickwts)\nggplot(chickwts, \n       aes(x = factor(feed), y = weight)) + \n    geom_violin(fill = \"lightBlue\", adjust=0.5) +\n    geom_boxplot(width=0.2) +\n    labs(x = \"Feed Supplement\", y = \"Chick Weight (g)\") \n\n\n\n\n\n\n\n\nThe ridgeline plot is used in similar circumstances, but shows the densities horizontally in a series of ridges.\n\nlibrary(ggridges)\nggplot(chickwts, aes(weight, y=factor(feed), fill=feed)) + \n  geom_density_ridges(scale = 1.5)\n\nPicking joint bandwidth of 24.4\n\n\n\n\n\n\n\n\n\n\n\n\nThe ideas of density estimation can be extended beyond one dimension. If we consider two variables at once, then we can estimate their 2-D joint distribution as a smoothed version of a scatterplot. This also provides another alternative method of dealing with overplotting of dense data sets.\nWe can fit the 2-D density using the kde2d function from the MASS package. Let’s apply this to the Old Faithful data set:\n\nlibrary(MASS)\nk1 &lt;- kde2d(geyser$duration, geyser$waiting, n=100)\n\nNote that we need both an x and a y variable now, as we’re working in 2-dimension. We can set a bandwidth by supplying an h parameter, but this is now a vector specifying a bandwidth for the x and y directions separately.\nThe output of kde2d is a list with x, y, and z components. The x and y are the values of the specified variables used to create a grid of points. The density value at every combination of x and y is then estimated and returned in the z component. The value of n here indicates the number of grid points along each axis.\nHaving estimated the 2-D density, we’ll need some new functions to visualise it. A contour plot is one option:\n\ncontour(k1, col='blue')\n\n\n\n\n\n\n\n\nThough without the data, this is a bit hard to understand. So let’s overlay the contours on a scatterplot:\n\nplot(x=geyser$duration, y=geyser$waiting, pch=16, xlab='duration', ylab='waiting')\ncontour(k1, col='blue', add=TRUE)\n\n\n\n\n\n\n\n\nOur 2-D density function for these data appears to detect three separate modes - one for the short duration eruptions, and two groups for the longer durations.\nWe could also generate a heat map of the density values\n\nimage(k1)\n\n\n\n\n\n\n\n\nNotice the pixelated quality of the plot. This is down to the choice of n when calling kde2d. If n is too small, we don’t produce enough predictions to generate a cleaner plot, so if we increase n this should improve. Note that this is not the same as the smoothing performed by KDE which is controlled by the bandwidth parameter h - this is purely a visual thing.\nLet’s boost the value of n and overlay the points on the plot.\n\nimage(kde2d(geyser$duration, geyser$waiting, n = 500), xlab='duration', ylab='waiting')\npoints(x=geyser$duration, y=geyser$waiting, pch=16)\n\n\n\n\n\n\n\n\nWhen we have extremely large numbers of points (e.g. millions), drawing a scatterplot can be a slow process as each point has to be drawn individually and inevitably a large amount of over-plotting occurs. In such cases, it is often better to skip the scatterplot and look at heatmaps or contour plots of the data instead"
  },
  {
    "objectID": "Lecture3b_TimeSeries.html",
    "href": "Lecture3b_TimeSeries.html",
    "title": "Lecture 3b - Exploring Time Series",
    "section": "",
    "text": "A time series is a sequence of repeated observations of a particular variable over time.\nThe US opinion poll data is an example of a time series.\nUnlike a regular variable, observations of a time series are not independent.\nTime series analysis is a whole subject within statistics, but we won’t be going into that.\nVisualising time series is relatively simple - if we observe \\(y_i\\) at time \\(t_i\\), then we simply plot the pairs \\((y_i,t_i)\\) on a scatterplot and often then join the points by lines.\nIt is common to add a trend estimate using some form of smoothing.\nMultiple time series of the same variable can be stacked or standardised.\n\nFeatures to look for:\n\nAn overall trend or pattern of behaviour over time (or lack thereof)\nPresence of regular patterns of behaviour around the trend\nAny changes in or deviations from the trend\nSimilarities and differences between:\n\nDifferent but related series for the same population\nThe same series for different sub-groups\n\n\nThe key thing to remember is that the observations of a time series are not independent! So, looking at one-variable summaries can be misleading – the most important feature is how things behave over time. For instance, consider this data on the population of lynxes over time. If we ignore the time aspect, then the data look like this\n\n\n\n\n\n\n\n\n\nBut by explicitly including time in the visualisation, we radically change what we can learn.\n\n\n\n\n\n\n\n\n\nWe can see there are cyclical changes in the populations, with regular peaks and troughs in the population numbers. All of this is lost if we forget about the dependence on time.\nTo plot data using connected lines, we can use the same plot function as for scatterplots, but add the ty argument with value l for line. Alternatively, if the time series is relatively short, we can use b instead to show both lines and points. Another option is s for steps.\nNote also that when drawing time series we prefer to join our points by lines. As these quantities evolve over time, it is natural to connect the points to indicate the transition from time point to time point. Usually, we default to connecting points with straight lines, but smoothers or other curves can also be used.\n\n\nThis is a time series of polling data. We want to analyse trends in it:\n\nlibrary(dslabs)\ndata(polls_2008)\npolls_2008 &lt;- as.data.frame(polls_2008)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\n\n\n\n\n\n\n\n\nThe smoothing technique used for time series is called LOESS. Here is how a LOESS curve on the polling data looks like:\n\n\n\n\n\n\n\n\n\nbut it is just one of many possible smoothings! In general, a LOESS depends on a parameter, known as the span.\n\n\n\nLOESS methods fix the number of points used in each local fit at a specified proportion \\(\\alpha\\) of the nearest points in the data set, known as the span.\nTechniques such as cross-validation are often used to find the ‘best’ \\(\\alpha\\), but when just exploring the data this is probably over-kill.\nAs the linear regression changes depending on the value of \\(x\\), we cannot extract a single model for the loess fit on a given data set.\n\nTo apply local regression by LOESS, we can use the loess function, which works much like the lm function does for standard regression. First we fit the model using loess:\n\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.25)\n\nWe can then use the predict functions to predict values of the smoothed trend:\n\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\n\nThe predictions can then be used to draw the smoothed trend on the plot\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=lpred$fit,col=cols[1],lwd=4)\n\n\n\n\n\n\n\n\nAdjusting the span argument has a similar effect to adjusting the bandwidth of the kernel smoother on the smoothness of the resulting trend:\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=lpred$fit,col=cols[1],lwd=4)\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.5)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\nlines(x=fitN$x, y=lpred$fit,col=cols[2],lwd=4)\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.75)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\nlines(x=fitN$x, y=lpred$fit,col=cols[3],lwd=4)\nlegend(x='bottomright',col=cols[1:3],pch=NA,lwd=4,legend=c(0.25,0.5,0.75))\n\n\n\n\n\n\n\n\nAs LOESS just uses linear regressions to generate its smoothed trend, we can apply linear regression theory to obtain the standard errors of the trend and form confidence intervals around it.\n\n\n\nTime series of financial variables are very often noisy and highly variable. The plot below shows the US Personal Savings Rate over time.\n\nload('economics.Rda')\nplot(x=economics$date, y=economics$psavert, ty='l', xlab='Date', ylab='Personal Savings Rate')\n\n\n\n\n\n\n\n\nThere are signs of a global trend to these data, which slowly evolves over time. However, separating the ‘trend’ from ‘noise’ is difficult. Additionally, there is no clear regular patterns to observe like with the lynx data. One approach to unpacking the behaviour of time series data is to apply smoothing techniques with different bandwidths.\nA long bandwidth smoother could identify a trend such as the red line below. A smoother with a narrower bandwidth could detect some of the smaller variations around that trend, such as the green line. However there is plenty of residual variation that is still unexplained!\n\n## create some times to predict the curve at\nxs &lt;- seq(min(as.numeric(economics$date)), max(as.numeric(economics$date)), length=200)\n## fit the model, note we have to convert our 'date' to numbers here\nlfit1 &lt;- loess(psavert~as.numeric(date), data=economics) \n## predict\nlpred1 &lt;- predict(lfit1, data.frame(date=xs), se = TRUE) \n## same again, with smaller span\nlfit2 &lt;- loess(psavert~as.numeric(date), data=economics, span=0.1) \nlpred2 &lt;- predict(lfit2, data.frame(date=xs), se = TRUE) \n## draw the plot\nplot(x=economics$date, y=economics$psavert, xlab='Date',ylab='Personal Savings Rate', ty='l')\nlines(x=xs, y=lpred1$fit, col='red',lwd=4) \nlines(x=xs, y=lpred2$fit, col='green',lwd=4) \n\n\n\n\n\n\n\n\n\n\n\n\nIt is often useful to think of a time series as being made up of multiple components:\n\nA trend - which describes the ‘big picture’ behaviour of the data over time\nSeasonal effects - which describes predictable behaviour around the trend. This is often periodic, though not all time series exhibit seasonal effects.\nResiduals - which are the random noise left over.\n\nThe time series is then thought of has a sum of these terms: \\[Y_t=T_t+S_t+\\epsilon_t.\\]\nSometimes, time series also incorporate an ‘irregular’ component to represent non-seasonal departures from the trend.\nOur example above could have a trend described by the red line. The green line doesn’t have a regular periodic behaviour - i.e. a regular pattern that repeats - so this would probably be the irregular component. The remaining noise of the data as indicated by the deviations of the black lines from the green would then constitute the residuals.\n\n\nThe figure below shows the unadjusted quartery (West) German unemployment rates from 1961 to just after the unification of the two Germanies.\n\nlibrary(AER)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\ndata(GermanUnemployment)\nts &lt;- seq(1962,1991.75,by=0.25)\ngeu &lt;- data.frame(GermanUnemployment)$unadjusted\nplot(geu ~ts,ty='l',xlab='Time',ylab='Unadjusted Unemployment')\n\n\n\n\n\n\n\n\nUnemployment was very low in the 60s and 70s, apart from a short spell in 1967. There were distinct jumps in unemployment in the mid 70s and early 80s due to the oil crises of 1973 and 1979. Unemployment was declining at the end of the series from the high levels in the 1980s. The series shows strong but regular variation around the trend corresponding to higher levels in winter than in summer.\nThe smoothing methods we’ve seen are particularly effective in extracting a smooth trend from the data:\n\n\n\n\n\n\n\n\n\nWe can then take our trend and subtract it from the time series. What’s left over will be the seasonal component plus the residuals!\n\nres &lt;- geu - lpred$fit\nplot(x=ts, y=res,col=3,lwd=2,ty='l',xlab='Time',ylab='Residual')\nabline(h=0)\n\n\n\n\n\n\n\n\nThe seasonal pattern here is regular, with peaks and troughs occuring at regular intervals.\nWe can extract the time series components by hand by doing loess smooths of the data with different bandwidths. However, R can do most of this for us:\n\ngeu &lt;- ts(GermanUnemployment[,1], ## Need to turn the data to a time series object\n          frequency=4) # Four observations per year defines the frequency of the regular pattern\ndecomp &lt;- stl(geu, s.window = \"periodic\") ## pass `periodic` to seek a periodic seasonal component\nplot(decomp)\n\n\n\n\n\n\n\n\nThe results of the decomposition are shown in the various panels of the plot. The top panel gives the original data, the second panel shows the periodic seasonal component, the third panel shows the general trend, and the bottom panel shows the residuals leftover at the end.\n\n\n\n\nDepending on what time serieswe’re showing, there are a number of things to consider:\n\nRelated series for the same population - if possible, show the different time series within the same plot to ease comparison. Be careful of units!\nSeries for different subgroups - showing subgroups together helps draw comparisons. Are we interested in the values or the proportions of the subgroups?\nSeries with different scales - may need standardising to a common scale to show together, otherwise separate plots will be needed.\n\n\n\nFlorence Nightingale famously used data and visualisations to highlight the poor conditions of soldiers in field hospitals during the Crimean War (1854-6). While certainly more well-known as a nurse, she was also a statistician and the first female member of the Royal Statistical Society. She used data visualisations to highlight the causes of mortality of soliders in field hospitals during the Crimean War (1854-6).\n\nWhile this is not a plot we would recognise today, it is a time series represented in polar form like a pie chart. The colours represent the different sources of mortality, and the segments represent sequential observations. Unwrapping this as a more conventional plot gives the more readable form:\n\n\n\n\n\n\n\n\n\nPlotting the annualised monthly death rates from disease, wounds, and other causes makes her case clearly. The death rate from diseases (black) due to poor conditions far outstrips the deaths due to injury sustained in combat (red). The most dangerous conflicts occured at the end of 1854, but even then the number of lives lost was dwarfed by deaths from disease.\n\n\n\n\nThe simplest way to compare time series is to show them simultaneously on the same plot.\nAs an example, William Playfair graphed England’s trade to the East Indies in the 18th century. Let’s revisit this plot:\n\nA more modern version would look something like this:\n\nlibrary(GDAdata)\ndata(EastIndiesTrade)\nplot(x=EastIndiesTrade$Year,y=EastIndiesTrade$Imports, col=2, xlab='Year',\n     ylab='Exports (blue) and Imports (red)', lwd=4,ty='l',ylim=c(0,2000))\nlines(x=EastIndiesTrade$Year,y=EastIndiesTrade$Exports,col=4,lwd=4)\nlibrary(scales)\npolygon(x=c(EastIndiesTrade$Year,rev(EastIndiesTrade$Year)),\n        y=c(EastIndiesTrade$Imports,rev(EastIndiesTrade$Exports)),\n        col=scales::alpha('green',0.25),border=NA)\n\n\n\n\n\n\n\n\nAs the time series were recorded for the same time points, we can directly calculate differences, and show the trade deficit. The line \\(y=0\\) is significant as it indicates whether we’re in deficit or surplus, so we can add that for reference.\n\nplot(x=EastIndiesTrade$Year,y=(EastIndiesTrade$Exports-EastIndiesTrade$Imports), col='green2', xlab='Year',\n     ylab='Exports - Imports', lwd=4,ty='l')\nabline(h=0,col=2)\n\n\n\n\n\n\n\n\nMany of the features can be associated with major events of the time: the War of Spanish Succession, 1701-14; the South Sea Bubble, 1720; the Seven Year’s War, 1756-63; and the American Revolutionary War, 1775-83. However, it would be rather harder to identify these from the original.\n\n\nThe number of cases of coronavirus were recorded from March 2020 for the four UK nations during the Covid pandemic. Let’s consider a portion of that data running to late January 2021 covering the first “peaks” of the pandemic. This gives us four time series of the same variable, over the same time period. A sensible first plot would be to draw all time series in the same graph.\n\n\n\n\n\n\n\n\n\nHowever, a major issue with comparing the case numbers between the four nations all together is it is difficult to distinguish the key features due to the vastly different scales. The variation in cases in England are quite clear, as are the three peaks of the pandemic, but since the numbers in the other nations are far smaller it is difficult to detect any pattern.\nThe official presentation of these data on the Government website is as a stacked total, where each nations total is successively added to the previous one. While this can be useful for showing the composition between groups of similar sizes, its really not very useful as the case numbers are still dominated by the England data and it’s larger population. This means a lot of detail is obscured and difficult to see.\n\n\n\n\n\n\n\n\n\nWhat would make more sense is to plotting England separately. Doing so allows us to see the patterns across the UK more clearly. Placing the time series above each other also allows us to easily compare similar times on the same plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is obvious now is that the patterns of cases is very similar across the UK, with each of the nations exhibiting peaks in cases around the same time. One feature that does slightly deviate from this trend is that it appears that in Wales the third peak arrived earlier than in other nations.\nAn improvement to this plot would be to standardise the case numbers by the population sizes of the nations. As England is much more populous than the other nations, it naturally will have more cases. If we compute the rates of coronavirus (per 100000 people) across the UK then we should have four time series that are directly comparable on the same plot:\n\n\n\n\n\n\n\n\n\nUsing the rates have corrected for the major differences in scale, and now we can see that the case rates are very close all over the UK. One feature that was not visible on the previous plots is the spike in cases in Northern Ireland in October.\nWhen we have correlated time series such as these, it can also make sense to draw the data as a scatterplot. For instance, we can plot the England data against that for Wales:\n\n\n\n\n\n\n\n\n\nWhen we have correlated time series, such as these, an alternative visualisation is to plot one against the other like a scatterplot. Here we have plotted the cases in England versus those in Wales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we can see the that most of the time cases were low and concentrated in the bottom left corner, and the peaks in cases are visible as the sizable deviations towards the top right corner. The early peak in Wales is visible when the plot moves vertically upwards. A more effective presentation of this information would use animation to show how the data points change over time, rather than showing everything at once.\n\n\n\n\n\nLength of time series - not all time series will have data for the same period. Are we interested in the long-term or short-term features?\nIrregular vs regular series - time series data may not be recorded on a regular schedule, making detection or exploitation of periodic behaviour difficult. Similarly, where we have multiple time series they may not be all recorded at the same times, making direct comparisons difficult\nData definitions may change - for long-term time series, the definition of the variable may change during the series, e.g. definitions of unemployment, GDP, net migration etc can change which makes comparison difficult.\nTime series of discrete variables - barcharts may be better for ordinal variables, but categorical variables may be best illustrated by the observed proportions of the different categories. Alternatively, Sankey plots can be used to display how proportions of a categorical variable change.\nOutliers - as usual, be careful with outliers but be mindful that they may be part of the pattern over time, e.g. peaks of the coronavirus epidemic.\nLength of time series - not all time series will have data for the same period, making comparisons difficult. We should also think about whether we are interested in the long-term behaviour (trend) or short-term features?\nIrregular vs regular series - multiple time series may not be all recorded at the same time points, again making direct comparisons difficult\nData definitions may change - for long-term time series, the definition of the variable may change during the series. The UK changed its definition of unemployment over 20 times duringfrom 1979 to 1993!\nTime series of discrete variables - barcharts may be better for ordinal variables, but categorical variables may be best illustrated by the observed proportions of the different categories.\nOutliers - be careful with outliers, as they may be part of the pattern over time\n\n\n\n\n\n\nSmoothing is an effective technique for estimating a trend of a data set without requiring complex modelling.\nKernel density estimation does a similar job for estimating the density function of a variable.\nBoth techniques are sensitive to their kernel function, and bandwidth parameter.\nTime series are a special kind of data representing a variable changing over time, and smoothing can be useful to expose the trend.\nSimple line plots are effective for time series, but plotting multiple time series simultaneously needs to be done with care."
  },
  {
    "objectID": "Lecture3b_TimeSeries.html#quick-recall-of-loess-smoothing",
    "href": "Lecture3b_TimeSeries.html#quick-recall-of-loess-smoothing",
    "title": "Lecture 3b - Exploring Time Series",
    "section": "",
    "text": "This is a time series of polling data. We want to analyse trends in it:\n\nlibrary(dslabs)\ndata(polls_2008)\npolls_2008 &lt;- as.data.frame(polls_2008)\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\n\n\n\n\n\n\n\n\nThe smoothing technique used for time series is called LOESS. Here is how a LOESS curve on the polling data looks like:\n\n\n\n\n\n\n\n\n\nbut it is just one of many possible smoothings! In general, a LOESS depends on a parameter, known as the span.\n\n\n\nLOESS methods fix the number of points used in each local fit at a specified proportion \\(\\alpha\\) of the nearest points in the data set, known as the span.\nTechniques such as cross-validation are often used to find the ‘best’ \\(\\alpha\\), but when just exploring the data this is probably over-kill.\nAs the linear regression changes depending on the value of \\(x\\), we cannot extract a single model for the loess fit on a given data set.\n\nTo apply local regression by LOESS, we can use the loess function, which works much like the lm function does for standard regression. First we fit the model using loess:\n\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.25)\n\nWe can then use the predict functions to predict values of the smoothed trend:\n\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\n\nThe predictions can then be used to draw the smoothed trend on the plot\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=lpred$fit,col=cols[1],lwd=4)\n\n\n\n\n\n\n\n\nAdjusting the span argument has a similar effect to adjusting the bandwidth of the kernel smoother on the smoothness of the resulting trend:\n\nplot(x=polls_2008$day, y=polls_2008$margin,pch=16,xlab='Day',ylab='Margin')\nlines(x=fitN$x, y=lpred$fit,col=cols[1],lwd=4)\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.5)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\nlines(x=fitN$x, y=lpred$fit,col=cols[2],lwd=4)\nlfit &lt;- loess(margin~day, data=polls_2008,span=0.75)\nlpred &lt;- predict(lfit, data.frame(day = fitN$x), se = TRUE)\nlines(x=fitN$x, y=lpred$fit,col=cols[3],lwd=4)\nlegend(x='bottomright',col=cols[1:3],pch=NA,lwd=4,legend=c(0.25,0.5,0.75))\n\n\n\n\n\n\n\n\nAs LOESS just uses linear regressions to generate its smoothed trend, we can apply linear regression theory to obtain the standard errors of the trend and form confidence intervals around it.\n\n\n\nTime series of financial variables are very often noisy and highly variable. The plot below shows the US Personal Savings Rate over time.\n\nload('economics.Rda')\nplot(x=economics$date, y=economics$psavert, ty='l', xlab='Date', ylab='Personal Savings Rate')\n\n\n\n\n\n\n\n\nThere are signs of a global trend to these data, which slowly evolves over time. However, separating the ‘trend’ from ‘noise’ is difficult. Additionally, there is no clear regular patterns to observe like with the lynx data. One approach to unpacking the behaviour of time series data is to apply smoothing techniques with different bandwidths.\nA long bandwidth smoother could identify a trend such as the red line below. A smoother with a narrower bandwidth could detect some of the smaller variations around that trend, such as the green line. However there is plenty of residual variation that is still unexplained!\n\n## create some times to predict the curve at\nxs &lt;- seq(min(as.numeric(economics$date)), max(as.numeric(economics$date)), length=200)\n## fit the model, note we have to convert our 'date' to numbers here\nlfit1 &lt;- loess(psavert~as.numeric(date), data=economics) \n## predict\nlpred1 &lt;- predict(lfit1, data.frame(date=xs), se = TRUE) \n## same again, with smaller span\nlfit2 &lt;- loess(psavert~as.numeric(date), data=economics, span=0.1) \nlpred2 &lt;- predict(lfit2, data.frame(date=xs), se = TRUE) \n## draw the plot\nplot(x=economics$date, y=economics$psavert, xlab='Date',ylab='Personal Savings Rate', ty='l')\nlines(x=xs, y=lpred1$fit, col='red',lwd=4) \nlines(x=xs, y=lpred2$fit, col='green',lwd=4)"
  },
  {
    "objectID": "Lecture3b_TimeSeries.html#time-series-components",
    "href": "Lecture3b_TimeSeries.html#time-series-components",
    "title": "Lecture 3b - Exploring Time Series",
    "section": "",
    "text": "It is often useful to think of a time series as being made up of multiple components:\n\nA trend - which describes the ‘big picture’ behaviour of the data over time\nSeasonal effects - which describes predictable behaviour around the trend. This is often periodic, though not all time series exhibit seasonal effects.\nResiduals - which are the random noise left over.\n\nThe time series is then thought of has a sum of these terms: \\[Y_t=T_t+S_t+\\epsilon_t.\\]\nSometimes, time series also incorporate an ‘irregular’ component to represent non-seasonal departures from the trend.\nOur example above could have a trend described by the red line. The green line doesn’t have a regular periodic behaviour - i.e. a regular pattern that repeats - so this would probably be the irregular component. The remaining noise of the data as indicated by the deviations of the black lines from the green would then constitute the residuals.\n\n\nThe figure below shows the unadjusted quartery (West) German unemployment rates from 1961 to just after the unification of the two Germanies.\n\nlibrary(AER)\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\ndata(GermanUnemployment)\nts &lt;- seq(1962,1991.75,by=0.25)\ngeu &lt;- data.frame(GermanUnemployment)$unadjusted\nplot(geu ~ts,ty='l',xlab='Time',ylab='Unadjusted Unemployment')\n\n\n\n\n\n\n\n\nUnemployment was very low in the 60s and 70s, apart from a short spell in 1967. There were distinct jumps in unemployment in the mid 70s and early 80s due to the oil crises of 1973 and 1979. Unemployment was declining at the end of the series from the high levels in the 1980s. The series shows strong but regular variation around the trend corresponding to higher levels in winter than in summer.\nThe smoothing methods we’ve seen are particularly effective in extracting a smooth trend from the data:\n\n\n\n\n\n\n\n\n\nWe can then take our trend and subtract it from the time series. What’s left over will be the seasonal component plus the residuals!\n\nres &lt;- geu - lpred$fit\nplot(x=ts, y=res,col=3,lwd=2,ty='l',xlab='Time',ylab='Residual')\nabline(h=0)\n\n\n\n\n\n\n\n\nThe seasonal pattern here is regular, with peaks and troughs occuring at regular intervals.\nWe can extract the time series components by hand by doing loess smooths of the data with different bandwidths. However, R can do most of this for us:\n\ngeu &lt;- ts(GermanUnemployment[,1], ## Need to turn the data to a time series object\n          frequency=4) # Four observations per year defines the frequency of the regular pattern\ndecomp &lt;- stl(geu, s.window = \"periodic\") ## pass `periodic` to seek a periodic seasonal component\nplot(decomp)\n\n\n\n\n\n\n\n\nThe results of the decomposition are shown in the various panels of the plot. The top panel gives the original data, the second panel shows the periodic seasonal component, the third panel shows the general trend, and the bottom panel shows the residuals leftover at the end."
  },
  {
    "objectID": "Lecture3b_TimeSeries.html#graphics-for-multiple-time-series",
    "href": "Lecture3b_TimeSeries.html#graphics-for-multiple-time-series",
    "title": "Lecture 3b - Exploring Time Series",
    "section": "",
    "text": "Depending on what time serieswe’re showing, there are a number of things to consider:\n\nRelated series for the same population - if possible, show the different time series within the same plot to ease comparison. Be careful of units!\nSeries for different subgroups - showing subgroups together helps draw comparisons. Are we interested in the values or the proportions of the subgroups?\nSeries with different scales - may need standardising to a common scale to show together, otherwise separate plots will be needed.\n\n\n\nFlorence Nightingale famously used data and visualisations to highlight the poor conditions of soldiers in field hospitals during the Crimean War (1854-6). While certainly more well-known as a nurse, she was also a statistician and the first female member of the Royal Statistical Society. She used data visualisations to highlight the causes of mortality of soliders in field hospitals during the Crimean War (1854-6).\n\nWhile this is not a plot we would recognise today, it is a time series represented in polar form like a pie chart. The colours represent the different sources of mortality, and the segments represent sequential observations. Unwrapping this as a more conventional plot gives the more readable form:\n\n\n\n\n\n\n\n\n\nPlotting the annualised monthly death rates from disease, wounds, and other causes makes her case clearly. The death rate from diseases (black) due to poor conditions far outstrips the deaths due to injury sustained in combat (red). The most dangerous conflicts occured at the end of 1854, but even then the number of lives lost was dwarfed by deaths from disease."
  },
  {
    "objectID": "Lecture3b_TimeSeries.html#comparing-time-series",
    "href": "Lecture3b_TimeSeries.html#comparing-time-series",
    "title": "Lecture 3b - Exploring Time Series",
    "section": "",
    "text": "The simplest way to compare time series is to show them simultaneously on the same plot.\nAs an example, William Playfair graphed England’s trade to the East Indies in the 18th century. Let’s revisit this plot:\n\nA more modern version would look something like this:\n\nlibrary(GDAdata)\ndata(EastIndiesTrade)\nplot(x=EastIndiesTrade$Year,y=EastIndiesTrade$Imports, col=2, xlab='Year',\n     ylab='Exports (blue) and Imports (red)', lwd=4,ty='l',ylim=c(0,2000))\nlines(x=EastIndiesTrade$Year,y=EastIndiesTrade$Exports,col=4,lwd=4)\nlibrary(scales)\npolygon(x=c(EastIndiesTrade$Year,rev(EastIndiesTrade$Year)),\n        y=c(EastIndiesTrade$Imports,rev(EastIndiesTrade$Exports)),\n        col=scales::alpha('green',0.25),border=NA)\n\n\n\n\n\n\n\n\nAs the time series were recorded for the same time points, we can directly calculate differences, and show the trade deficit. The line \\(y=0\\) is significant as it indicates whether we’re in deficit or surplus, so we can add that for reference.\n\nplot(x=EastIndiesTrade$Year,y=(EastIndiesTrade$Exports-EastIndiesTrade$Imports), col='green2', xlab='Year',\n     ylab='Exports - Imports', lwd=4,ty='l')\nabline(h=0,col=2)\n\n\n\n\n\n\n\n\nMany of the features can be associated with major events of the time: the War of Spanish Succession, 1701-14; the South Sea Bubble, 1720; the Seven Year’s War, 1756-63; and the American Revolutionary War, 1775-83. However, it would be rather harder to identify these from the original.\n\n\nThe number of cases of coronavirus were recorded from March 2020 for the four UK nations during the Covid pandemic. Let’s consider a portion of that data running to late January 2021 covering the first “peaks” of the pandemic. This gives us four time series of the same variable, over the same time period. A sensible first plot would be to draw all time series in the same graph.\n\n\n\n\n\n\n\n\n\nHowever, a major issue with comparing the case numbers between the four nations all together is it is difficult to distinguish the key features due to the vastly different scales. The variation in cases in England are quite clear, as are the three peaks of the pandemic, but since the numbers in the other nations are far smaller it is difficult to detect any pattern.\nThe official presentation of these data on the Government website is as a stacked total, where each nations total is successively added to the previous one. While this can be useful for showing the composition between groups of similar sizes, its really not very useful as the case numbers are still dominated by the England data and it’s larger population. This means a lot of detail is obscured and difficult to see.\n\n\n\n\n\n\n\n\n\nWhat would make more sense is to plotting England separately. Doing so allows us to see the patterns across the UK more clearly. Placing the time series above each other also allows us to easily compare similar times on the same plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is obvious now is that the patterns of cases is very similar across the UK, with each of the nations exhibiting peaks in cases around the same time. One feature that does slightly deviate from this trend is that it appears that in Wales the third peak arrived earlier than in other nations.\nAn improvement to this plot would be to standardise the case numbers by the population sizes of the nations. As England is much more populous than the other nations, it naturally will have more cases. If we compute the rates of coronavirus (per 100000 people) across the UK then we should have four time series that are directly comparable on the same plot:\n\n\n\n\n\n\n\n\n\nUsing the rates have corrected for the major differences in scale, and now we can see that the case rates are very close all over the UK. One feature that was not visible on the previous plots is the spike in cases in Northern Ireland in October.\nWhen we have correlated time series such as these, it can also make sense to draw the data as a scatterplot. For instance, we can plot the England data against that for Wales:\n\n\n\n\n\n\n\n\n\nWhen we have correlated time series, such as these, an alternative visualisation is to plot one against the other like a scatterplot. Here we have plotted the cases in England versus those in Wales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we can see the that most of the time cases were low and concentrated in the bottom left corner, and the peaks in cases are visible as the sizable deviations towards the top right corner. The early peak in Wales is visible when the plot moves vertically upwards. A more effective presentation of this information would use animation to show how the data points change over time, rather than showing everything at once.\n\n\n\n\n\nLength of time series - not all time series will have data for the same period. Are we interested in the long-term or short-term features?\nIrregular vs regular series - time series data may not be recorded on a regular schedule, making detection or exploitation of periodic behaviour difficult. Similarly, where we have multiple time series they may not be all recorded at the same times, making direct comparisons difficult\nData definitions may change - for long-term time series, the definition of the variable may change during the series, e.g. definitions of unemployment, GDP, net migration etc can change which makes comparison difficult.\nTime series of discrete variables - barcharts may be better for ordinal variables, but categorical variables may be best illustrated by the observed proportions of the different categories. Alternatively, Sankey plots can be used to display how proportions of a categorical variable change.\nOutliers - as usual, be careful with outliers but be mindful that they may be part of the pattern over time, e.g. peaks of the coronavirus epidemic.\nLength of time series - not all time series will have data for the same period, making comparisons difficult. We should also think about whether we are interested in the long-term behaviour (trend) or short-term features?\nIrregular vs regular series - multiple time series may not be all recorded at the same time points, again making direct comparisons difficult\nData definitions may change - for long-term time series, the definition of the variable may change during the series. The UK changed its definition of unemployment over 20 times duringfrom 1979 to 1993!\nTime series of discrete variables - barcharts may be better for ordinal variables, but categorical variables may be best illustrated by the observed proportions of the different categories.\nOutliers - be careful with outliers, as they may be part of the pattern over time"
  },
  {
    "objectID": "Lecture3b_TimeSeries.html#summary",
    "href": "Lecture3b_TimeSeries.html#summary",
    "title": "Lecture 3b - Exploring Time Series",
    "section": "",
    "text": "Smoothing is an effective technique for estimating a trend of a data set without requiring complex modelling.\nKernel density estimation does a similar job for estimating the density function of a variable.\nBoth techniques are sensitive to their kernel function, and bandwidth parameter.\nTime series are a special kind of data representing a variable changing over time, and smoothing can be useful to expose the trend.\nSimple line plots are effective for time series, but plotting multiple time series simultaneously needs to be done with care."
  },
  {
    "objectID": "Lab3_Smoothing.html",
    "href": "Lab3_Smoothing.html",
    "title": "Practical 3 - Smoothing and Density Estimation",
    "section": "",
    "text": "This practical is designed to enhance your understanding of smoothing and density estimation techniques.\n\n\nBy the end of the lab, you will have acquired the following skills:\n\n\nSmoothing a data set to extract a smooth trend using ksmooth and loess\n\n\nAdd curves to a plot using the lines function, and shaded regions using polygon\n\n\nAdd a smooth trend and confidence interval to a scatterplot\n\n\nEstimate a smooth density function of a single variable using the density function.\n\n\nAdd smooth densities to histograms, and draw violin plots and ridgeline plots\n\n\n\nYou will need to install the following packages:\n\nggplot2 for the violin plot\nggridges for the ridgeline plot\n\n\ninstall.packages(c(\"ggplot2\",\"ggridges\"))\n\n\n\nSmoothing is a very powerful technique used all across data analysis. Other names given to this technique are curve fitting and low pass filtering. It is designed to detect trends in the presence of noisy data where the shape of the trend is unknown. The smoothing name comes from the fact that to accomplish this feat, we assume that the trend is smooth, as in a smooth surface. In contrast, the noise, or deviation from the trend, is unpredictably wobbly. The justification for this is that conditional expectations or probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty.\n Download data: UN\nThe data we’ll use to explore smoothing is the UN data set comprising various measures of national statistics on health, welfare, and education for 213 countries and other areas from 2009-2011, as reported by the UN. Let’s consider the relationship between the wealth of the country (as measured by the GDP per person, in 1000s US dollars) and infant mortality (defined as deaths by age 1 year per 1000 live births).\n\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16)\n\n\n\n\n\n\n\n\nWe quite clearly see a strong relationship here, but it is far from linear. Poorer countries have higher levels of infant mortality, and wealthier countries have lower mortality rates. We also seee that beyond a point, further increases in GDP do not result in further reduction in mortality, and similarly the very poor countries have higher, and much more variable, infant mortality rates. We also notice one possible outlier with a very high infant mortality compared to other countries of similar GDP (this turns out to be Equatorial Guinea).\n\n\n\nThe general idea of a moving average is to group data points into a local neighbourhood in which the value of the trend is assumed to be constant, which we estimate by the average of these nearby points. Then as we move along the x-axis, our neighbourhood and hence the estimate of the trend will change.\nTo fit a moving average, we use the ksmooth function using a \"box\" kernel and a bandwidth to govern the size of the neighbourhood as follows:\n\nfit &lt;- ksmooth(x=UN$ppgdp, y=UN$infantMortality, kernel = \"box\", bandwidth = 5)\n\nThe object we’ve created is a list with components x and y corresponding to our estimates of the smooth trend (y) at the corresponding values of GDP (x). We now just need to add this to our plot.\nWe can use R’s line drawing functions to add curves to a plot via the lines function, which takes a vector of x and y values and draws straight-line segments connecting then on top of the current plot.\nLet’s add our moving average trend to the scatterplot\n\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16, col=\"gray\")\nlines(x=fit$x, y=fit$y, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\nWhile it mostly follows the shape of the data, there are some obvious issues:\n\nThe trend is still quite noisy, and not quite the smooth curve we might expect\nThere are gaps in the trend - this is where our data points are so far apart that we have no data points in the neighbourhood to produce an average\nThe moving average is very sensitive to extremes and outliers. Notice how the trend is ‘pulled upwards’ by a few large mortality values at the left of the plot.\n\n\n\n\n\nRepeat the commands above but increase the bandwidth\n\n\nAdd your new smooth trend to the plot using a different colour of line.\n\n\nHow does the outlier affect the shape of the line?\n\n\n\n\n\nClick for solution\n\nHere’s a comparison with a smooth of bandwidth 15:\n\nfitlarge &lt;- ksmooth(x=UN$ppgdp, y=UN$infantMortality, kernel = \"box\", bandwidth = 15)\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16, col=\"gray\")\nlines(x=fit$x, y=fit$y, col=\"red\", lwd=3)\nlines(x=fitlarge$x, y=fitlarge$y, col=\"blue\", lwd=3)\n\n\n\n\n\n\n\n\nWe see a much smaller effect of the outlier on the blue smooth compared to the red one.\n\n\n\nKernel smoothers replace a moving average of nearby points with a weighted average of all the data, with nearer points having more contribution than more distant points.\nWe can fit the kernel smoother using the same ksmooth function, but now we must change the kernel argument. This smoother only supports the normal (Gaussian) or box (Uniform) kernel functions.\n\nfit &lt;- ksmooth(x=UN$ppgdp, y=UN$infantMortality, kernel = \"normal\", bandwidth = 5)\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16, col=\"gray\")\nlines(x=fit$x, y=fit$y, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\nWe can see a number of notable changes here:\n\nThe trend is much smoother\nWe still have a gap where our data are sparse - the bandwidth is still too small\nThere is a bump in the trend near the outlier we identified earlier, and a bump around GDP=75 where our data are sparse and influenced by one larger value\n\nOverall, this looks to be doing a better job, but still rather susceptible to extreme values.\nIf we want to implement a different kernel function, we can either use some extra package or do it manually. In theory, the Epanechnikov kernel is the optimal one: here’s code to define this kernel function:\n\nK_epa &lt;- function(u){0.75 * (1 - u^2) * (abs(u) &lt;= 1)}\n\nWe can plot this function, to make sure that it has the expected shape\n\nu &lt;- seq(-1.5, 1.5, length = 400)\nplot(u, K_epa(u), type = \"l\", lwd = 2,\n     xlab = \"u = (x - x_i) / h\",\n     ylab = \"K(u)\",\n     main = \"Epanechnikov Kernel Function\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nUse K_epa to create a manual Epanechnikov smoothing on the plot of ppgdp versus infantMortality. You can choose a bandwidth yourself, or create a function that allows for different choices of bandwidth\n\n\nTry your smoothing with different bandwidths and compare your results\n\n\nWhich of the following has a bigger effect on the smoothing: the choice of kernel or the choice of bandwidth?\n\n\n\n\n\nClick for solution\n\nRecall from lecture that the kernel smoother of \\((x_1,y_1), \\dots,(x_n,y_n)\\) at \\(x_0\\) is defined as: \\[\\hat{f}(x_0)= \\sum_{i=1}^n w_i(x_0) y_i\\]\nand that the weigths are defined in terms of the Kernel function as \\[w_i(x;h)=\\frac{K\\left(\\frac{x-x_i}{h}\\right)}{\\sum\\nolimits_{i=1}^nK\\left(\\frac{x-x_i}{h}\\right)}.\\] We can use this to create our kernel smoother\n\nepa_smooth &lt;- function(x, y, xi, h) {\n  u &lt;- (x - xi) / h\n  sum(K_epa(u) * y) / sum(K_epa(u))\n}\n\nand we can then fit it to our data\n\nh &lt;- 5 #Here you choose your bandwidth\nx &lt;- UN$ppgdp\ny &lt;- UN$infantMortality\n\nord &lt;- order(x) #Remember to order your x values, or the line is going to look messy!\nx &lt;- x[ord]\ny &lt;- y[ord]\n\nx_grid &lt;- x\ny_epa &lt;- sapply(x_grid, function(x0) epa_smooth(x, y, x0, h))\n\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16, col=\"gray\")\n\nlines(x_grid, y_epa, lwd = 2, col = \"purple\")\n\n\n\n\n\n\n\n\nPlaying around with both bandwidth and kernels, we realise that the choice of bandwidth is much more consequential than the choice of kernel (that’s perhaps why the ksmooth function doesn’t support many different kernels!)\n\n\n\n\nLocal regression methods work in a similar way to our kernel smoother above, only instead of estimating the trend by a local mean weighting by nearby data we fit a linear regression using a similar weighting. We typically need to use a much larger bandwidth to have enough points to adequately estimate the regression line, but the results are generally closer to a more stable and smooth trend function.\nThe technique we have looked at is achieved using the loess function, which takes a span argument to represent the proportion of the entire data set used to estimate the trend. This span plays the same role as the bandwidth parameter does.\nIt’s a little more work to add the loess smoother to our plots, as we must separately fit the model and then predict the trend at some new locations (the ksmooth did both jobs for us). However,\n\n## fit the model, using 2/3 of the data\nlfit &lt;- loess(infantMortality~ppgdp, data=UN, span=0.66) \n## create a sequence of points to cover the x-axis\nxs &lt;- seq(min(UN$ppgdp), max(UN$ppgdp), length=200)\n## get the predicted trend at the new locations, and the error too\nlpred &lt;- predict(lfit, data.frame(ppgdp=xs), se = TRUE) \n\n## redraw the plot\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16) \n## add the loess prediction\nlines(x=xs, y=lpred$fit, col='red',lwd=4) \n\n\n\n\n\n\n\n\nWe see the loess curve is much smoother and better behaved, however it is still influenced by the outliers in the data.\nSince the loess curve is built upon ideas of regression, we can use regression theory to extact confidence intervals for our loess trend. The predict function here returns a list with many elements. We have used fit to get the predicted trend lines, but we can use the se.fit component to build an approximate confidence interval.\nTo do this, we’ll create a vector for the upper and lower limits of the confidence interval and use lines to add these limits to our plot:\n\nupr &lt;- lpred$fit + 1.96*lpred$se.fit ## upper limit of 95% CI\nlwr &lt;- lpred$fit - 1.96*lpred$se.fit ## lower limit of 95% CI\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16)\nlines(x=xs, y=lpred$fit,col='red',lwd=4)\nlines(x=xs, y=upr,col='red',lwd=1, lty=2)\nlines(x=xs, y=lwr,col='red',lwd=1, lty=2)\n\n\n\n\n\n\n\n\nA couple of things to note here:\n\nThese confidence intervals are intervals for the location of the trend, not for the data. We do not expect most of our data to be between these limits.\nNotice how our uncertainty grows to the right of the plot where we have fewest data.\n\nA nicer way to represent the confidence interval is by using a shaded region.\n\n\n\n\nFamiliarize yourself with the function polygon, in particular look at the type of input that this function takes, and at how to set options col and border\n\n\nUsing the upper and lower limits for the confidence intervals computed above, construct a polygon corresponding to the confidence region\n\n\nAdd a shaded colour to the polygon and remove the border, in such a way to get a nice visualisation of the confidence interval as a shaded region\n\n\n\n\n\nClick for solution\n\nThe polygon function can be used to draw shaded regions on an existing plot. The default syntax is:\npolygon(x, y, col=NA, border=NULL)\n\nThe x and y arguments take the \\((x,y)\\) coordinates of the shape to draw.\nThe default for colour draws no fill, so it is advisable to set a colour here.\nThe default border outlines the region with a black solid line. To disable the border, set this to NA.\n\nWe can use our confidence limits to draw the polygon, however we need to combine the points before calling polygon.\n\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16)\nlines(x=xs, y=lpred$fit,col='red',lwd=4)\n## we need 'scales' to use the alpha function for transparency\nlibrary(scales) \npolygon(x=c(xs,rev(xs)), \n        y=c(upr,rev(lwr)), col=alpha('red',0.2), border=NA)\n\n\n\n\n\n\n\n\nHere we defined y as c(upr,rev(lwr)), i.e. the upper CI followed by the reversed lower CI. This is because polygon joins successive points together, so we must reverse the order of one of the vectors so we can draw around the shape correctly.\n\n\n\n\n\n Download data: Salaries\nThe Salaries data set contains the 2008-09 nine-month academic salary for Assistant Professors, Associate Professors and Professors in a University in the USA. The data were collected as part of the on-going effort of the college’s administration to monitor salary differences between male and female faculty members. Let’s take a look at two variables in particular - salary, and yrs.since.phd - the number of years since they completed their PhD, as a measure of “experience”. Let’s take a look at the data.\n\n\n\n\nFamiliarize yourself with the dataset, then draw a scatterplot of the yrs.since.phd and salary, with salary on the vertical axis.\n\n\nWhat do you conclude about the relationship? Does it agree with what you expected?\n\n\nChoose two types of smoother among those seen above and add them to your plot as a different coloured lines. Experiment with different bandwidth values and spans. How do your different smoothers compare?\n\n\n\n\n\nClick for solution\n\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\n\n\n\n\n\n\n\n\nSalary increases with experience, but appears to drop off at high values of experience (seems odd). Recall that this is “years since PhD”, so anyone with 50+ years since PhD is probably age 75+, these are possibly semi-retired senior professors. Let us now fit a moving average smoother\n\nfit &lt;- ksmooth(x=Salaries$yrs.since.phd, y=Salaries$salary,\n               kernel = \"box\", bandwidth = 10)\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=fit$x, y=fit$y, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\na kernel smoother\n\nfit &lt;- ksmooth(x=Salaries$yrs.since.phd, y=Salaries$salary,\n               kernel = \"normal\", bandwidth = 10)\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=fit$x, y=fit$y, col=\"blue\", lwd=3)\n\n\n\n\n\n\n\n\nand a LOESS smoother\n\nlfit &lt;- loess(salary~yrs.since.phd, data=Salaries, span=0.66) \n## create a sequence of points to cover the x-axis\nxs &lt;- seq(min(Salaries$yrs.since.phd), max(Salaries$yrs.since.phd), length=200)\n## get the predicted trend at the new locations, and the error too\nlpred &lt;- predict(lfit, data.frame(yrs.since.phd=xs), se = TRUE) \n## add the loess prediction\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=xs, y=lpred$fit, col='green',lwd=4) \n\n\n\n\n\n\n\n\nAs expected, the simple moving average is the most rough, the kernel is a much more smooth function, and the loess is smoother still. All three make the general trend of the data clear: it appears nonlinear\n\n\n\n\n\nRedraw the scatterplot, and add your loess curve as a solid line.\n\n\nFollow the steps above to add the confidence interval to your plot - either as curves, or as a shaded area, or both!\n\n\nUse lm to fit a simple linear regression of the form salary~yrs.since.phd, and add this to the plot. Is this consistent with your smoothed trend and its confidence interval?\n\n\n\n\n\nClick for solution\n\nLet us redraw both the plot and the LOESS smoother, adding confidence interval as a shaded region\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=xs, y=lpred$fit, col='green',lwd=4) \n## confidence limits\nupr &lt;- lpred$fit + 1.96*lpred$se.fit\nlwr &lt;- lpred$fit - 1.96*lpred$se.fit\n## draw the shape\npolygon(x=c(xs,rev(xs)), \n        y=c(upr,rev(lwr)),\n        col=alpha('green',0.2), border=NA)\n\n\n\n\n\n\n\n\nThe relationship is not linear, but let us look at the straight line fit anyway\n\nlmfit &lt;- lm(salary~yrs.since.phd, data=Salaries)\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=xs, y=lpred$fit, col='green',lwd=4) \nabline(lmfit,col='orange')\n\n\n\n\n\n\n\n\nThis really doesn’t look right - if it were consistent we’d expect it to be close to the smoothed trend and its confidence interval.\n\nThere are a couple of categorical variables in this data set - the sex of the academic, and their rank - one of Prof, AssocProf or AsstProf representing Professor, Associate Professor, and Assistant Professor (in descending order of seniority).\n\n\n\n\nUse colour to highlight the different sexes. What features can you see?\n\n\nUse colour to highlight the different ranks - what do you see?\n\n\nWhat do you think about your trend now?\n\n\n\n\n\nClick for solution\n\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16,col=Salaries$sex)\n## red=male, black=female\nlegend(x='topright', legend=levels(Salaries$sex), pch=16, col=1:2)\n\n\n\n\n\n\n\n\nThe legend function can add the colour and labels and save us having to decipher that female academics are younger (fewer years of experience), and seemingly less well paid.\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16,col=Salaries$rank)\nlegend(x='topright', legend=levels(Salaries$rank), pch=16, col=1:nlevels(Salaries$rank))\n\n\n\n\n\n\n\n\nThis plot changes everything! It looks strongly like salary just changes with the job rank. For the lower ranks, the relationship appears flat. Though there is some curvature in the Professor group.\nAs a little extra, we can use a for loop to fit and draw a LOESS curve for each subgroup\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16,col=Salaries$rank)\nlegend(x='topright', legend=levels(Salaries$rank), pch=16, col=1:nlevels(Salaries$rank))\n## loop over each salary level\nfor(i in 1:nlevels(Salaries$rank)){\n  r &lt;- levels(Salaries$rank)[i] ## extract the factor label\n  ## fit a loess on the subset of the data\n  tmp &lt;- loess(salary~yrs.since.phd, data=Salaries[Salaries$rank==r,], span=0.66) \n  ## and extract and plot\n  prd &lt;- predict(tmp, data.frame(yrs.since.phd=xs), se = TRUE) \n  lines(x=xs, y=prd$fit, col=i,lwd=4) \n}\n\n\n\n\n\n\n\n\nWe find that red and black are indeed mostly flat, though green shows curvature (due to those old profs seen earlier on).\n\n\n\n\nKernel density estimation (KDE) is a nonparametric method which uses kernels (as we’ve seen above) to estimate the probability density function of a continuous random variable. In simpler words, we are trying to produce a smoothed version of a histogram.\nA density estimate is relatively easy to construct using the density function in R. It takes a few useful arguments:\n\nx - the data\nbw - the bandwidth parameter. If not specified, R will try and determine a value for itself.\nkernel - the kernel function to use. Options include gaussian, rectangular, triangular, epanechnikov. Default is gaussian.\n\nIt then spits out an object with x and y components that we can plot or add to a plot with lines.\n\n## draw a histogram of GDP on density scale\nhist(UN$ppgdp, freq=FALSE,main='',xlab='GDP') \n ## add a rugplot to see where the points lie\nrug(UN$ppgdp)\n## a density estimate with Gaussian kernel and bandwidth of 5\nd1 &lt;- density(UN$ppgdp, bw=5)\nlines(x=d1$x, y=d1$y, col='red', lwd=2)\n## bandwidth of 10\nd2 &lt;- density(UN$ppgdp, bw=10)\nlines(x=d2$x, y=d2$y, col='green', lwd=2)\n## bandwidth of 2\nd3 &lt;- density(UN$ppgdp, bw=2)\nlines(x=d3$x, y=d3$y, col='blue', lwd=2)\n## let R choose a bandwidth\nd4 &lt;- density(UN$ppgdp)\nlines(x=d4$x, y=d4$y, col='orange', lwd=2)\n\n\n\n\n\n\n\n\nR does a reasonable job at guessing a not-too-stupid bandwidth parameter. To estimate the bandwidth, R calls the bw.nrd0 function on the data - read the help file if you want to know more.\nAlternatively, we could skip the histogram altogether and just draw the density estimate:\n\n## draw the line\nplot(x=d4$x, y=d4$y,xlab='GDP',ylab='Density',ty='l')\n## and add some fill to make it prettier\npolygon(x=d4$x, y=d4$y, border=NA, col=alpha('red',0.4)) \n\n\n\n\n\n\n\n\nThis can be a helpful tool for comparing the densities from different groups in a single plot.\n\n\nThe violin plot is a combination of ideas of a boxplot and the kernel density estimate above. Rather than drawing the crude box-and-whiskers of a boxplot, we instead draw a back-to-back density estimate.\n Download data: chickwts\nTo illustrate, this consider the chickwts data set, which records the weight of 71 chicks fed on a six different feed supplements. Newly-hatched chicks were randomly allocated into six groups, and each group was given a different feed supplement. They were then weighed after six weeks. A separate boxplot can be drawn for each level of a categorical variable using the following code (not again the use of the linear model formula notation)\n\nboxplot(weight~feed,data=chickwts)\n\n\n\n\n\n\n\n\nA violin plot takes the same layout of the boxplot, but draws a (heavily) smoothed density estimate instead of the box and whiskers.\n\nlibrary(ggplot2)\nggplot(chickwts, \n       aes(x = factor(feed), y = weight)) + \n  geom_violin(fill = \"lightBlue\", color = \"#473e2c\", adjust=0.5) + \n  labs(x = \"Feed Supplement\", y = \"Chick Weight (g)\")\n\n\n\n\n\n\n\n\nViolin plots can be quite effective at conveying the information of both a histogram and a boxplot in one graphic. As with most density estimates, experimenting with the bandwidth is usually a good idea - in this case the adjust parameter.\nFinally, the plotting code for making a violin plot is a lot more involved than usual! ggplot is a powerful, but alternative, method of plotting in R. We’ve generally avoided it as it uses a fundamentally different plotting system to our regular graphics and takes rather more work to understand. However, it is exceptionally flexible and with it you can do many things that would be difficult otherwise, for instance:\n\nlibrary(ggplot2)\nggplot(chickwts, \n       aes(x = factor(feed), y = weight)) + \n  geom_violin(fill = \"lightBlue\", color = \"#473e2c\", adjust=0.5) + \n  labs(x = \"Feed Supplement\", y = \"Chick Weight (g)\") + \n   geom_boxplot(width=0.1)\n\n\n\n\n\n\n\n\n\n\n\nRidgeline plots, also called ridge plots, are another way to show density estimates for a number of groups. Much like the violin plot, we draw a separate density estimate for each group, but in the ridge line plot, the densities are stacked vertically to look like a line of hills or ridges (hence the name).\nAgain, this uses the gg family of plotting functions, but can easily be adapted. The main control parameter here is the scale which affects the height of the individual densities:\n\nlibrary(ggridges)\nggplot(chickwts, aes(weight, y=factor(feed), fill=feed)) + \n  geom_density_ridges(scale = 1.5)\n\n\n\n\n\n\n\n\n\n\n\n Download data: diamonds\nThe diamonds data set contains information on the prices and other attributes of 53,940 diamonds. In particular, it gives the weight (carat) and price of each diamond, as well as categorical variables representing the quality of the cut (Fair, Good, Very Good, Premium, Ideal), the diamond color (from D=best, to J=worst), and clarity representing how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)).\n\n\n\n\nExplore the distribution of the carat of the diamonds. Start with a histogram and experiment with different numbers of bars.\n\n\nNow make some density estimates and experiment with different bandwidths to produce a smoothed version of your histogram. Add these to your plot.\n\n\nDo the same as above for the distribution of prices.\n\n\n\n\n\nClick for solution\n\nIt is OK to try a few histograms before finding one that works\n\nhist(diamonds$carat,breaks=100,freq=FALSE)\n\n\n\n\n\n\n\n\nIn this one, we see several distinct peaks (perhaps groups?), and very strong skewness. We can try density estimation with different bandwidths\n\nhist(diamonds$carat,breaks=100,freq=FALSE)\nd1 &lt;- density(diamonds$carat, bw=1)\nlines(x=d1$x, y=d1$y, col='red', lwd=2)\nd2 &lt;- density(diamonds$carat, bw=0.5)\nlines(x=d2$x, y=d2$y, col='green', lwd=2)\nd3 &lt;- density(diamonds$carat, bw=0.1)\nlines(x=d3$x, y=d3$y, col='blue', lwd=2)\n\n\n\n\n\n\n\n\nWe can also let R decide the optimal bandwidth\n\nhist(diamonds$carat,breaks=100,freq=FALSE)\nbw.nrd0(diamonds$carat)\n\n[1] 0.04826685\n\nd4 &lt;- density(diamonds$carat)\nlines(x=d4$x, y=d4$y, col='orange', lwd=2)\n\n\n\n\n\n\n\n\nAs for the variable price, the code is the same as above\n\nhist(diamonds$price,breaks=100,freq=FALSE) \nd4 &lt;- density(diamonds$price)\nlines(x=d4$x, y=d4$y, col='red', lwd=2)\n\n\n\n\n\n\n\n\nwe see here fewer peaks, but still an unusual structure and strong skewness.\n\nWe can now try to use the density estimation techniques to compare differences across subgroups.\n\n\n\n\nHow many diamonds are there of the different cuts?\n\n\nHow does the distribution of prices change according to the cut of the diamond?\n\n\n\nTry drawing boxplots for each level of cut.\n\n\nHow do these compare to violin plots? And ridgeline plots?\n\n\nDo you find any evidence of different behaviour among the groups?\n\n\n\nPerform a similar investigation of the diamond’s carat.\n\n\n\n\n\nClick for solution\n\n\nbarplot(xtabs(~cut,data=diamonds))\n\n\n\n\n\n\n\n\nA histogram shows wide variation in numbers for the different groups of cuts.\n\nboxplot(price~cut, data=diamonds)\n\n\n\n\n\n\n\n\nThe distribution is very skewed in all cases. There is a slight variation in medians, and more obvious variation in the upper inter-quartile range. There are lots of candidates for outliers! Butthis is just a side-effect of a big and skewed data set. Without further information, they seem very unlikely to be due to error. Let’s look at the violin plots\n\nggplot(diamonds, \n       aes(x = factor(cut), y = price)) + \n  geom_violin(fill = \"lightBlue\")\n\n\n\n\n\n\n\n\nHere skewness is apparent again. We can also notice how the shape changes as we go from Fair to Ideal. All groups prices peak around low values. And here are the ridge plots\n\nggplot(diamonds, aes(price, y=factor(cut), fill=cut)) + \n  geom_density_ridges(scale = 1.5)\n\nPicking joint bandwidth of 458\n\n\n\n\n\n\n\n\n\nHere we can clearly see how the shape changes with the cut.\n\n\n\n\n\nDraw plots to show how price changes according to the color of the diamond.\n\n\nPerform a similar investigation of the diamond’s carat with clarity.\n\n\nWhat conclusions do you draw?\n\n\n\n\n\nClick for solution\n\nHere again, the best is to start with boxplots\n\nboxplot(price~color, data=diamonds)\n\n\n\n\n\n\n\n\nWe see a similar shape as before, but the median is clearly increasing as we go from D to J.\n\nggplot(diamonds, \n       aes(x = factor(color), y = price)) + \n  geom_violin(fill = \"lightBlue\")\n\n\n\n\n\n\n\n\nFrom the violin plots, we see that the distribution is becoming flatter as D-&gt;J.\n\nggplot(diamonds, aes(price, y=factor(color), fill=color)) + \n  geom_density_ridges(scale = 1.5)\n\nPicking joint bandwidth of 535\n\n\n\n\n\n\n\n\n\nAnd as before the ridge plots are easier to interpret: the distribution is more spread out with more ‘humps’ for colours H-J. This is what drags up the median.\nIn the case of carat vs clarity, we repeat the procedure\n\nboxplot(carat~clarity, data=diamonds)\n\n\n\n\n\n\n\n\ngetting the “reverse” picture: going from left to right, the carat clearly drops.\n\nggplot(diamonds, \n       aes(x = factor(clarity), y = carat)) + \n  geom_violin(fill = \"lightBlue\")\n\n\n\n\n\n\n\n\nViolin plots show a shape that is definitely changing. The distribution is more spread on the left, and more concentrated on the right.\n\nggplot(diamonds, aes(carat, y=factor(clarity), fill=clarity)) + \n  geom_density_ridges(scale = 1.5)\n\nPicking joint bandwidth of 0.057\n\n\n\n\n\n\n\n\n\nOnce again, the pattern is easier to see from the ridgeplots: the distribution is moving and becoming more concentrated. We might suspect a bit of undersmoothing as the shapes are very wiggly.\n\n\n\n\nThe ideas of density estimation can be extended beyond one dimension. If we consider two variables at once, we can estimate their 2-d joint distribution as a smoothed version of a scatterplot. This also provides another alternative method of dealing with overplotting of dense data sets.\nLet’s return to the Old Faithful data:\n\nlibrary(MASS) ## this is a standard package, you shouldn't need to install it\ndata(geyser)\nplot(x=geyser$duration, y=geyser$waiting,\n     xlab='duration', ylab='waiting', pch=16)\n\n\n\n\n\n\n\n\nA 2-D density estimate would smooth the distribution into a surface, picking out two or three peaks corresponding to the clumps in the data. We can fit the 2-D density using the kde2d function from the MASS package. It has few arguments, just the x and y to be smoothed, and an optional n which controls the amount of detail in the resulting smoothed density.\n\nk1 &lt;- kde2d(geyser$duration, geyser$waiting, n = 100)\n\nThe output of kde2d is a bit more complicated than we get from a 1-D density estimation. It’s now a list with x, y, and z components. The x and y components are the values of the specified variables (here, duration and waiting respectively) which are used to create a grid of points. The density value at every combination of x and y is then estimared and returned in the z component.\nThe value of n here indicates the number of grid points along each axis, effectively acting as an “inverse bandwidth”. Increasing this will give you smoother output due to making more estimates in a bigger z matrix, but will take longer to evaluate.\nWe can then add the smoothed density to our scatterplot as contours:\n\nplot(x=geyser$duration, y=geyser$waiting,\n     xlab='duration', ylab='waiting', pch=16, col=alpha('black',0.6))\ncontour(k1, add=TRUE, col='blue')\n\n\n\n\n\n\n\n\nUsing add=TRUE we can draw the contours on top of an existing plot. See the help on contour (?contour) for more ways to customise.\nAlternatively, we could skip the scatterplot and just draw the contours (useful if we have too many points)\n\ncontour(k1, col='blue')\n\n\n\n\n\n\n\n\nA different presentation of the density estimate is to draw the surface as a heatmap:\n\nimage(k1)\n\n\n\n\n\n\n\n\nEach rectangular region in the plot is now coloured according to the density estimate. The heatmap is a little blocky due to the resolution of our density estimate. Specifying n=100 divides each axis into 100 intervals, giving us 10,000 2-D bins over the range of the data. Like a histogram, we can increase n to see more detail - though it can take a little while to compute.\nIf we want to overlay the data points, we can use the points function:\n\nimage(kde2d(geyser$duration, geyser$waiting, n = 500),\n      xlab='duration', ylab='waiting')\npoints(x=geyser$duration, y=geyser$waiting,pch=16, col=alpha('black',0.6))\n\n\n\n\n\n\n\n\n\n\n\n Download data: HRstars\nThe Hertzsprung-Russell diagram is famous visualisation of the relationship between the absolute magnitude of stars (i.e. their brightness) and their effective temperatures (indicated by their colour). The Hertzsprung-Russell provides an easy way to distinguish different categories of stars according to their position on the plot.\nWe’re going to look at the HRStars data which contain 6220 stars from the Yale Trigonometric Parallax Dataset.\n\nLet’s start with single variables. Have a look at histograms of V and BV.\nDo you see any evidence of groups in the data? You’ll need to adjust the number of bars.\n\nUnivariate summaries can conceal a lot of information when the data are inherently multivariate. So, let’s consider both variables.\n\nDraw a plot of the magnitude V vertically against the observed colour BV.\nThe data contain a lot of points, so reduce your point size by scaling down the symbol size. This can be done by setting the optional argument cex to a value between 0 and 1 - the default size is 1.\nWhat features do you see? Can you identify any groups in the data? Are there any outliers?\nThe Sun has an absolute magnitude of 4.8 and B-V colour of 0.66. Use the points function (it works exactly the same way as lines) to add it to your plot - use a different colour and/or plot character to make it stand out.\nHow does your plot differ from the plots you find on the web? How would you adjust your scatterplot to match?\n\nThe central ‘wiggle’ in the plot corresponds to main sequence stars. Below that wiggle are the white dwarf stars, and above are the giants and supergiants.\n\nFit a 2-D density estimate to the stars data.\nOverlay the contours on your scatterplot (use a different colour and/or transparency).\nThe kde2d function takes an argument h which represents the bandwidth. We can supply a single value, or a vector of two values (one for each variable).\nThe contour plot seems to miss out the less dense region of dwarf stars in the bottom left due to the low density of points. The default number of contours drawn is 10, and it is set by the nlevels argument. Increase the number of contours until a contour around the dwarf group is visible.\nAlternatively, you can pick the levels at which the contour lines are drawn by specifying a vector as input to the levels argument. I found that \\(0.005, 0.01, 0.025, 0.05, 0.1, 0.15\\) work well - try this now.\nCan you identify the main groups of stars?\n\nThis is more tricky and requires more R programming:\n\nCreate a new categorical variable to that represents the values of the Uncert variable, which represents the parallax error in the observations. Use three levels:\n\nUncert under 50\nUncert above 50 but below 100\nUncert above 100\n\nUse your new variable to colour the points in your scatterplot according to this error.\nIs this parallax error associated with the properties (and hence types) of observed stars?\n\n\n\n\n Download data: fatherSon\nThis is another dataset of father and son heights, this time from Karl Pearson (another prominent early statistician). This time we have 1078 paired heights recorded in inches and were published in 1903. Families were invited to provide their measurements to the nearest quarter of an inch with the note that “the Professor trusts to the bona fides of each recorder to send only correct results.”\n\nDraw histograms of the sons’ and fathers’ heights (sheight and fheight). Use a density scale and a bar width of 1 inch.\nCompute a density estimate for both distributions.\nDraw the histograms side-by-side and overlay your density curves.\nDo the data look normally distributed?\n\nWe should be careful about jumping to conclusions here, or as Pearson put it in his 1903 paper: “It is almost in vain that one enters a protest against the mere graphical representation of goodness of fit, now that we have an exact measure of it.” For the time, he was justified in his demanding both graphical and analytical approaches. Since those early days, analytic approaches have often been too dominant, both are needed.\n\nDraw Normal quantile plots of the two variables side-by-side. Do the data look normal?\n\nLet’s look at the pairwise relationship between the two heights.\n\nDraw a scatterplot of sons’ heights (vertically) against fathers’ heights (horizontally).\nIs there a strong association here? Calculate the correlation.\nFit a linear regression (recall in ISDS, you used the lm function). Add the line to your plot.\n\nThe height of a son is influenced by the height of the father, but there is a lot of unexplained variability. This scatterplot also illustrates a phenomenon known as regression towards the mean. Small fathers havs sons who are small, but on average not as small as their fathers. Tall fathers have sons who are tall, but on average not as tall as their fathers.\nTo explore further whether a non-linear model would be warranted, we could plot a smoother andt the best fit regression line together.\n\nFit a smoother to the data.\nAdd it to your plot with a 95% confidence interval.\nAre the regression and smoother in agreement?"
  },
  {
    "objectID": "Lab3_Smoothing.html#smoothing---recap",
    "href": "Lab3_Smoothing.html#smoothing---recap",
    "title": "Practical 3 - Smoothing and Density Estimation",
    "section": "",
    "text": "Smoothing is a very powerful technique used all across data analysis. Other names given to this technique are curve fitting and low pass filtering. It is designed to detect trends in the presence of noisy data where the shape of the trend is unknown. The smoothing name comes from the fact that to accomplish this feat, we assume that the trend is smooth, as in a smooth surface. In contrast, the noise, or deviation from the trend, is unpredictably wobbly. The justification for this is that conditional expectations or probabilities can be thought of as trends of unknown shapes that we need to estimate in the presence of uncertainty.\n Download data: UN\nThe data we’ll use to explore smoothing is the UN data set comprising various measures of national statistics on health, welfare, and education for 213 countries and other areas from 2009-2011, as reported by the UN. Let’s consider the relationship between the wealth of the country (as measured by the GDP per person, in 1000s US dollars) and infant mortality (defined as deaths by age 1 year per 1000 live births).\n\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16)\n\n\n\n\n\n\n\n\nWe quite clearly see a strong relationship here, but it is far from linear. Poorer countries have higher levels of infant mortality, and wealthier countries have lower mortality rates. We also seee that beyond a point, further increases in GDP do not result in further reduction in mortality, and similarly the very poor countries have higher, and much more variable, infant mortality rates. We also notice one possible outlier with a very high infant mortality compared to other countries of similar GDP (this turns out to be Equatorial Guinea)."
  },
  {
    "objectID": "Lab3_Smoothing.html#exercise-1-moving-averages",
    "href": "Lab3_Smoothing.html#exercise-1-moving-averages",
    "title": "Practical 3 - Smoothing and Density Estimation",
    "section": "",
    "text": "The general idea of a moving average is to group data points into a local neighbourhood in which the value of the trend is assumed to be constant, which we estimate by the average of these nearby points. Then as we move along the x-axis, our neighbourhood and hence the estimate of the trend will change.\nTo fit a moving average, we use the ksmooth function using a \"box\" kernel and a bandwidth to govern the size of the neighbourhood as follows:\n\nfit &lt;- ksmooth(x=UN$ppgdp, y=UN$infantMortality, kernel = \"box\", bandwidth = 5)\n\nThe object we’ve created is a list with components x and y corresponding to our estimates of the smooth trend (y) at the corresponding values of GDP (x). We now just need to add this to our plot.\nWe can use R’s line drawing functions to add curves to a plot via the lines function, which takes a vector of x and y values and draws straight-line segments connecting then on top of the current plot.\nLet’s add our moving average trend to the scatterplot\n\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16, col=\"gray\")\nlines(x=fit$x, y=fit$y, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\nWhile it mostly follows the shape of the data, there are some obvious issues:\n\nThe trend is still quite noisy, and not quite the smooth curve we might expect\nThere are gaps in the trend - this is where our data points are so far apart that we have no data points in the neighbourhood to produce an average\nThe moving average is very sensitive to extremes and outliers. Notice how the trend is ‘pulled upwards’ by a few large mortality values at the left of the plot.\n\n\n\n\n\nRepeat the commands above but increase the bandwidth\n\n\nAdd your new smooth trend to the plot using a different colour of line.\n\n\nHow does the outlier affect the shape of the line?\n\n\n\n\n\nClick for solution\n\nHere’s a comparison with a smooth of bandwidth 15:\n\nfitlarge &lt;- ksmooth(x=UN$ppgdp, y=UN$infantMortality, kernel = \"box\", bandwidth = 15)\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16, col=\"gray\")\nlines(x=fit$x, y=fit$y, col=\"red\", lwd=3)\nlines(x=fitlarge$x, y=fitlarge$y, col=\"blue\", lwd=3)\n\n\n\n\n\n\n\n\nWe see a much smaller effect of the outlier on the blue smooth compared to the red one.\n\n\n\nKernel smoothers replace a moving average of nearby points with a weighted average of all the data, with nearer points having more contribution than more distant points.\nWe can fit the kernel smoother using the same ksmooth function, but now we must change the kernel argument. This smoother only supports the normal (Gaussian) or box (Uniform) kernel functions.\n\nfit &lt;- ksmooth(x=UN$ppgdp, y=UN$infantMortality, kernel = \"normal\", bandwidth = 5)\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16, col=\"gray\")\nlines(x=fit$x, y=fit$y, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\nWe can see a number of notable changes here:\n\nThe trend is much smoother\nWe still have a gap where our data are sparse - the bandwidth is still too small\nThere is a bump in the trend near the outlier we identified earlier, and a bump around GDP=75 where our data are sparse and influenced by one larger value\n\nOverall, this looks to be doing a better job, but still rather susceptible to extreme values.\nIf we want to implement a different kernel function, we can either use some extra package or do it manually. In theory, the Epanechnikov kernel is the optimal one: here’s code to define this kernel function:\n\nK_epa &lt;- function(u){0.75 * (1 - u^2) * (abs(u) &lt;= 1)}\n\nWe can plot this function, to make sure that it has the expected shape\n\nu &lt;- seq(-1.5, 1.5, length = 400)\nplot(u, K_epa(u), type = \"l\", lwd = 2,\n     xlab = \"u = (x - x_i) / h\",\n     ylab = \"K(u)\",\n     main = \"Epanechnikov Kernel Function\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nUse K_epa to create a manual Epanechnikov smoothing on the plot of ppgdp versus infantMortality. You can choose a bandwidth yourself, or create a function that allows for different choices of bandwidth\n\n\nTry your smoothing with different bandwidths and compare your results\n\n\nWhich of the following has a bigger effect on the smoothing: the choice of kernel or the choice of bandwidth?\n\n\n\n\n\nClick for solution\n\nRecall from lecture that the kernel smoother of \\((x_1,y_1), \\dots,(x_n,y_n)\\) at \\(x_0\\) is defined as: \\[\\hat{f}(x_0)= \\sum_{i=1}^n w_i(x_0) y_i\\]\nand that the weigths are defined in terms of the Kernel function as \\[w_i(x;h)=\\frac{K\\left(\\frac{x-x_i}{h}\\right)}{\\sum\\nolimits_{i=1}^nK\\left(\\frac{x-x_i}{h}\\right)}.\\] We can use this to create our kernel smoother\n\nepa_smooth &lt;- function(x, y, xi, h) {\n  u &lt;- (x - xi) / h\n  sum(K_epa(u) * y) / sum(K_epa(u))\n}\n\nand we can then fit it to our data\n\nh &lt;- 5 #Here you choose your bandwidth\nx &lt;- UN$ppgdp\ny &lt;- UN$infantMortality\n\nord &lt;- order(x) #Remember to order your x values, or the line is going to look messy!\nx &lt;- x[ord]\ny &lt;- y[ord]\n\nx_grid &lt;- x\ny_epa &lt;- sapply(x_grid, function(x0) epa_smooth(x, y, x0, h))\n\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16, col=\"gray\")\n\nlines(x_grid, y_epa, lwd = 2, col = \"purple\")\n\n\n\n\n\n\n\n\nPlaying around with both bandwidth and kernels, we realise that the choice of bandwidth is much more consequential than the choice of kernel (that’s perhaps why the ksmooth function doesn’t support many different kernels!)\n\n\n\n\nLocal regression methods work in a similar way to our kernel smoother above, only instead of estimating the trend by a local mean weighting by nearby data we fit a linear regression using a similar weighting. We typically need to use a much larger bandwidth to have enough points to adequately estimate the regression line, but the results are generally closer to a more stable and smooth trend function.\nThe technique we have looked at is achieved using the loess function, which takes a span argument to represent the proportion of the entire data set used to estimate the trend. This span plays the same role as the bandwidth parameter does.\nIt’s a little more work to add the loess smoother to our plots, as we must separately fit the model and then predict the trend at some new locations (the ksmooth did both jobs for us). However,\n\n## fit the model, using 2/3 of the data\nlfit &lt;- loess(infantMortality~ppgdp, data=UN, span=0.66) \n## create a sequence of points to cover the x-axis\nxs &lt;- seq(min(UN$ppgdp), max(UN$ppgdp), length=200)\n## get the predicted trend at the new locations, and the error too\nlpred &lt;- predict(lfit, data.frame(ppgdp=xs), se = TRUE) \n\n## redraw the plot\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16) \n## add the loess prediction\nlines(x=xs, y=lpred$fit, col='red',lwd=4) \n\n\n\n\n\n\n\n\nWe see the loess curve is much smoother and better behaved, however it is still influenced by the outliers in the data.\nSince the loess curve is built upon ideas of regression, we can use regression theory to extact confidence intervals for our loess trend. The predict function here returns a list with many elements. We have used fit to get the predicted trend lines, but we can use the se.fit component to build an approximate confidence interval.\nTo do this, we’ll create a vector for the upper and lower limits of the confidence interval and use lines to add these limits to our plot:\n\nupr &lt;- lpred$fit + 1.96*lpred$se.fit ## upper limit of 95% CI\nlwr &lt;- lpred$fit - 1.96*lpred$se.fit ## lower limit of 95% CI\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16)\nlines(x=xs, y=lpred$fit,col='red',lwd=4)\nlines(x=xs, y=upr,col='red',lwd=1, lty=2)\nlines(x=xs, y=lwr,col='red',lwd=1, lty=2)\n\n\n\n\n\n\n\n\nA couple of things to note here:\n\nThese confidence intervals are intervals for the location of the trend, not for the data. We do not expect most of our data to be between these limits.\nNotice how our uncertainty grows to the right of the plot where we have fewest data.\n\nA nicer way to represent the confidence interval is by using a shaded region.\n\n\n\n\nFamiliarize yourself with the function polygon, in particular look at the type of input that this function takes, and at how to set options col and border\n\n\nUsing the upper and lower limits for the confidence intervals computed above, construct a polygon corresponding to the confidence region\n\n\nAdd a shaded colour to the polygon and remove the border, in such a way to get a nice visualisation of the confidence interval as a shaded region\n\n\n\n\n\nClick for solution\n\nThe polygon function can be used to draw shaded regions on an existing plot. The default syntax is:\npolygon(x, y, col=NA, border=NULL)\n\nThe x and y arguments take the \\((x,y)\\) coordinates of the shape to draw.\nThe default for colour draws no fill, so it is advisable to set a colour here.\nThe default border outlines the region with a black solid line. To disable the border, set this to NA.\n\nWe can use our confidence limits to draw the polygon, however we need to combine the points before calling polygon.\n\nplot(x=UN$ppgdp, y=UN$infantMortality, xlab=\"GDP\", ylab=\"Infant Mortality\", pch=16)\nlines(x=xs, y=lpred$fit,col='red',lwd=4)\n## we need 'scales' to use the alpha function for transparency\nlibrary(scales) \npolygon(x=c(xs,rev(xs)), \n        y=c(upr,rev(lwr)), col=alpha('red',0.2), border=NA)\n\n\n\n\n\n\n\n\nHere we defined y as c(upr,rev(lwr)), i.e. the upper CI followed by the reversed lower CI. This is because polygon joins successive points together, so we must reverse the order of one of the vectors so we can draw around the shape correctly."
  },
  {
    "objectID": "Lab3_Smoothing.html#exercise-2-academic-salaries",
    "href": "Lab3_Smoothing.html#exercise-2-academic-salaries",
    "title": "Practical 3 - Smoothing and Density Estimation",
    "section": "",
    "text": "Download data: Salaries\nThe Salaries data set contains the 2008-09 nine-month academic salary for Assistant Professors, Associate Professors and Professors in a University in the USA. The data were collected as part of the on-going effort of the college’s administration to monitor salary differences between male and female faculty members. Let’s take a look at two variables in particular - salary, and yrs.since.phd - the number of years since they completed their PhD, as a measure of “experience”. Let’s take a look at the data.\n\n\n\n\nFamiliarize yourself with the dataset, then draw a scatterplot of the yrs.since.phd and salary, with salary on the vertical axis.\n\n\nWhat do you conclude about the relationship? Does it agree with what you expected?\n\n\nChoose two types of smoother among those seen above and add them to your plot as a different coloured lines. Experiment with different bandwidth values and spans. How do your different smoothers compare?\n\n\n\n\n\nClick for solution\n\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\n\n\n\n\n\n\n\n\nSalary increases with experience, but appears to drop off at high values of experience (seems odd). Recall that this is “years since PhD”, so anyone with 50+ years since PhD is probably age 75+, these are possibly semi-retired senior professors. Let us now fit a moving average smoother\n\nfit &lt;- ksmooth(x=Salaries$yrs.since.phd, y=Salaries$salary,\n               kernel = \"box\", bandwidth = 10)\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=fit$x, y=fit$y, col=\"red\", lwd=3)\n\n\n\n\n\n\n\n\na kernel smoother\n\nfit &lt;- ksmooth(x=Salaries$yrs.since.phd, y=Salaries$salary,\n               kernel = \"normal\", bandwidth = 10)\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=fit$x, y=fit$y, col=\"blue\", lwd=3)\n\n\n\n\n\n\n\n\nand a LOESS smoother\n\nlfit &lt;- loess(salary~yrs.since.phd, data=Salaries, span=0.66) \n## create a sequence of points to cover the x-axis\nxs &lt;- seq(min(Salaries$yrs.since.phd), max(Salaries$yrs.since.phd), length=200)\n## get the predicted trend at the new locations, and the error too\nlpred &lt;- predict(lfit, data.frame(yrs.since.phd=xs), se = TRUE) \n## add the loess prediction\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=xs, y=lpred$fit, col='green',lwd=4) \n\n\n\n\n\n\n\n\nAs expected, the simple moving average is the most rough, the kernel is a much more smooth function, and the loess is smoother still. All three make the general trend of the data clear: it appears nonlinear\n\n\n\n\n\nRedraw the scatterplot, and add your loess curve as a solid line.\n\n\nFollow the steps above to add the confidence interval to your plot - either as curves, or as a shaded area, or both!\n\n\nUse lm to fit a simple linear regression of the form salary~yrs.since.phd, and add this to the plot. Is this consistent with your smoothed trend and its confidence interval?\n\n\n\n\n\nClick for solution\n\nLet us redraw both the plot and the LOESS smoother, adding confidence interval as a shaded region\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=xs, y=lpred$fit, col='green',lwd=4) \n## confidence limits\nupr &lt;- lpred$fit + 1.96*lpred$se.fit\nlwr &lt;- lpred$fit - 1.96*lpred$se.fit\n## draw the shape\npolygon(x=c(xs,rev(xs)), \n        y=c(upr,rev(lwr)),\n        col=alpha('green',0.2), border=NA)\n\n\n\n\n\n\n\n\nThe relationship is not linear, but let us look at the straight line fit anyway\n\nlmfit &lt;- lm(salary~yrs.since.phd, data=Salaries)\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16)\nlines(x=xs, y=lpred$fit, col='green',lwd=4) \nabline(lmfit,col='orange')\n\n\n\n\n\n\n\n\nThis really doesn’t look right - if it were consistent we’d expect it to be close to the smoothed trend and its confidence interval.\n\nThere are a couple of categorical variables in this data set - the sex of the academic, and their rank - one of Prof, AssocProf or AsstProf representing Professor, Associate Professor, and Assistant Professor (in descending order of seniority).\n\n\n\n\nUse colour to highlight the different sexes. What features can you see?\n\n\nUse colour to highlight the different ranks - what do you see?\n\n\nWhat do you think about your trend now?\n\n\n\n\n\nClick for solution\n\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16,col=Salaries$sex)\n## red=male, black=female\nlegend(x='topright', legend=levels(Salaries$sex), pch=16, col=1:2)\n\n\n\n\n\n\n\n\nThe legend function can add the colour and labels and save us having to decipher that female academics are younger (fewer years of experience), and seemingly less well paid.\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16,col=Salaries$rank)\nlegend(x='topright', legend=levels(Salaries$rank), pch=16, col=1:nlevels(Salaries$rank))\n\n\n\n\n\n\n\n\nThis plot changes everything! It looks strongly like salary just changes with the job rank. For the lower ranks, the relationship appears flat. Though there is some curvature in the Professor group.\nAs a little extra, we can use a for loop to fit and draw a LOESS curve for each subgroup\n\nplot(x=Salaries$yrs.since.phd, y=Salaries$salary,pch=16,col=Salaries$rank)\nlegend(x='topright', legend=levels(Salaries$rank), pch=16, col=1:nlevels(Salaries$rank))\n## loop over each salary level\nfor(i in 1:nlevels(Salaries$rank)){\n  r &lt;- levels(Salaries$rank)[i] ## extract the factor label\n  ## fit a loess on the subset of the data\n  tmp &lt;- loess(salary~yrs.since.phd, data=Salaries[Salaries$rank==r,], span=0.66) \n  ## and extract and plot\n  prd &lt;- predict(tmp, data.frame(yrs.since.phd=xs), se = TRUE) \n  lines(x=xs, y=prd$fit, col=i,lwd=4) \n}\n\n\n\n\n\n\n\n\nWe find that red and black are indeed mostly flat, though green shows curvature (due to those old profs seen earlier on)."
  },
  {
    "objectID": "Lab3_Smoothing.html#density-estimation",
    "href": "Lab3_Smoothing.html#density-estimation",
    "title": "Practical 3 - Smoothing and Density Estimation",
    "section": "",
    "text": "Kernel density estimation (KDE) is a nonparametric method which uses kernels (as we’ve seen above) to estimate the probability density function of a continuous random variable. In simpler words, we are trying to produce a smoothed version of a histogram.\nA density estimate is relatively easy to construct using the density function in R. It takes a few useful arguments:\n\nx - the data\nbw - the bandwidth parameter. If not specified, R will try and determine a value for itself.\nkernel - the kernel function to use. Options include gaussian, rectangular, triangular, epanechnikov. Default is gaussian.\n\nIt then spits out an object with x and y components that we can plot or add to a plot with lines.\n\n## draw a histogram of GDP on density scale\nhist(UN$ppgdp, freq=FALSE,main='',xlab='GDP') \n ## add a rugplot to see where the points lie\nrug(UN$ppgdp)\n## a density estimate with Gaussian kernel and bandwidth of 5\nd1 &lt;- density(UN$ppgdp, bw=5)\nlines(x=d1$x, y=d1$y, col='red', lwd=2)\n## bandwidth of 10\nd2 &lt;- density(UN$ppgdp, bw=10)\nlines(x=d2$x, y=d2$y, col='green', lwd=2)\n## bandwidth of 2\nd3 &lt;- density(UN$ppgdp, bw=2)\nlines(x=d3$x, y=d3$y, col='blue', lwd=2)\n## let R choose a bandwidth\nd4 &lt;- density(UN$ppgdp)\nlines(x=d4$x, y=d4$y, col='orange', lwd=2)\n\n\n\n\n\n\n\n\nR does a reasonable job at guessing a not-too-stupid bandwidth parameter. To estimate the bandwidth, R calls the bw.nrd0 function on the data - read the help file if you want to know more.\nAlternatively, we could skip the histogram altogether and just draw the density estimate:\n\n## draw the line\nplot(x=d4$x, y=d4$y,xlab='GDP',ylab='Density',ty='l')\n## and add some fill to make it prettier\npolygon(x=d4$x, y=d4$y, border=NA, col=alpha('red',0.4)) \n\n\n\n\n\n\n\n\nThis can be a helpful tool for comparing the densities from different groups in a single plot.\n\n\nThe violin plot is a combination of ideas of a boxplot and the kernel density estimate above. Rather than drawing the crude box-and-whiskers of a boxplot, we instead draw a back-to-back density estimate.\n Download data: chickwts\nTo illustrate, this consider the chickwts data set, which records the weight of 71 chicks fed on a six different feed supplements. Newly-hatched chicks were randomly allocated into six groups, and each group was given a different feed supplement. They were then weighed after six weeks. A separate boxplot can be drawn for each level of a categorical variable using the following code (not again the use of the linear model formula notation)\n\nboxplot(weight~feed,data=chickwts)\n\n\n\n\n\n\n\n\nA violin plot takes the same layout of the boxplot, but draws a (heavily) smoothed density estimate instead of the box and whiskers.\n\nlibrary(ggplot2)\nggplot(chickwts, \n       aes(x = factor(feed), y = weight)) + \n  geom_violin(fill = \"lightBlue\", color = \"#473e2c\", adjust=0.5) + \n  labs(x = \"Feed Supplement\", y = \"Chick Weight (g)\")\n\n\n\n\n\n\n\n\nViolin plots can be quite effective at conveying the information of both a histogram and a boxplot in one graphic. As with most density estimates, experimenting with the bandwidth is usually a good idea - in this case the adjust parameter.\nFinally, the plotting code for making a violin plot is a lot more involved than usual! ggplot is a powerful, but alternative, method of plotting in R. We’ve generally avoided it as it uses a fundamentally different plotting system to our regular graphics and takes rather more work to understand. However, it is exceptionally flexible and with it you can do many things that would be difficult otherwise, for instance:\n\nlibrary(ggplot2)\nggplot(chickwts, \n       aes(x = factor(feed), y = weight)) + \n  geom_violin(fill = \"lightBlue\", color = \"#473e2c\", adjust=0.5) + \n  labs(x = \"Feed Supplement\", y = \"Chick Weight (g)\") + \n   geom_boxplot(width=0.1)\n\n\n\n\n\n\n\n\n\n\n\nRidgeline plots, also called ridge plots, are another way to show density estimates for a number of groups. Much like the violin plot, we draw a separate density estimate for each group, but in the ridge line plot, the densities are stacked vertically to look like a line of hills or ridges (hence the name).\nAgain, this uses the gg family of plotting functions, but can easily be adapted. The main control parameter here is the scale which affects the height of the individual densities:\n\nlibrary(ggridges)\nggplot(chickwts, aes(weight, y=factor(feed), fill=feed)) + \n  geom_density_ridges(scale = 1.5)\n\n\n\n\n\n\n\n\n\n\n\n Download data: diamonds\nThe diamonds data set contains information on the prices and other attributes of 53,940 diamonds. In particular, it gives the weight (carat) and price of each diamond, as well as categorical variables representing the quality of the cut (Fair, Good, Very Good, Premium, Ideal), the diamond color (from D=best, to J=worst), and clarity representing how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)).\n\n\n\n\nExplore the distribution of the carat of the diamonds. Start with a histogram and experiment with different numbers of bars.\n\n\nNow make some density estimates and experiment with different bandwidths to produce a smoothed version of your histogram. Add these to your plot.\n\n\nDo the same as above for the distribution of prices.\n\n\n\n\n\nClick for solution\n\nIt is OK to try a few histograms before finding one that works\n\nhist(diamonds$carat,breaks=100,freq=FALSE)\n\n\n\n\n\n\n\n\nIn this one, we see several distinct peaks (perhaps groups?), and very strong skewness. We can try density estimation with different bandwidths\n\nhist(diamonds$carat,breaks=100,freq=FALSE)\nd1 &lt;- density(diamonds$carat, bw=1)\nlines(x=d1$x, y=d1$y, col='red', lwd=2)\nd2 &lt;- density(diamonds$carat, bw=0.5)\nlines(x=d2$x, y=d2$y, col='green', lwd=2)\nd3 &lt;- density(diamonds$carat, bw=0.1)\nlines(x=d3$x, y=d3$y, col='blue', lwd=2)\n\n\n\n\n\n\n\n\nWe can also let R decide the optimal bandwidth\n\nhist(diamonds$carat,breaks=100,freq=FALSE)\nbw.nrd0(diamonds$carat)\n\n[1] 0.04826685\n\nd4 &lt;- density(diamonds$carat)\nlines(x=d4$x, y=d4$y, col='orange', lwd=2)\n\n\n\n\n\n\n\n\nAs for the variable price, the code is the same as above\n\nhist(diamonds$price,breaks=100,freq=FALSE) \nd4 &lt;- density(diamonds$price)\nlines(x=d4$x, y=d4$y, col='red', lwd=2)\n\n\n\n\n\n\n\n\nwe see here fewer peaks, but still an unusual structure and strong skewness.\n\nWe can now try to use the density estimation techniques to compare differences across subgroups.\n\n\n\n\nHow many diamonds are there of the different cuts?\n\n\nHow does the distribution of prices change according to the cut of the diamond?\n\n\n\nTry drawing boxplots for each level of cut.\n\n\nHow do these compare to violin plots? And ridgeline plots?\n\n\nDo you find any evidence of different behaviour among the groups?\n\n\n\nPerform a similar investigation of the diamond’s carat.\n\n\n\n\n\nClick for solution\n\n\nbarplot(xtabs(~cut,data=diamonds))\n\n\n\n\n\n\n\n\nA histogram shows wide variation in numbers for the different groups of cuts.\n\nboxplot(price~cut, data=diamonds)\n\n\n\n\n\n\n\n\nThe distribution is very skewed in all cases. There is a slight variation in medians, and more obvious variation in the upper inter-quartile range. There are lots of candidates for outliers! Butthis is just a side-effect of a big and skewed data set. Without further information, they seem very unlikely to be due to error. Let’s look at the violin plots\n\nggplot(diamonds, \n       aes(x = factor(cut), y = price)) + \n  geom_violin(fill = \"lightBlue\")\n\n\n\n\n\n\n\n\nHere skewness is apparent again. We can also notice how the shape changes as we go from Fair to Ideal. All groups prices peak around low values. And here are the ridge plots\n\nggplot(diamonds, aes(price, y=factor(cut), fill=cut)) + \n  geom_density_ridges(scale = 1.5)\n\nPicking joint bandwidth of 458\n\n\n\n\n\n\n\n\n\nHere we can clearly see how the shape changes with the cut.\n\n\n\n\n\nDraw plots to show how price changes according to the color of the diamond.\n\n\nPerform a similar investigation of the diamond’s carat with clarity.\n\n\nWhat conclusions do you draw?\n\n\n\n\n\nClick for solution\n\nHere again, the best is to start with boxplots\n\nboxplot(price~color, data=diamonds)\n\n\n\n\n\n\n\n\nWe see a similar shape as before, but the median is clearly increasing as we go from D to J.\n\nggplot(diamonds, \n       aes(x = factor(color), y = price)) + \n  geom_violin(fill = \"lightBlue\")\n\n\n\n\n\n\n\n\nFrom the violin plots, we see that the distribution is becoming flatter as D-&gt;J.\n\nggplot(diamonds, aes(price, y=factor(color), fill=color)) + \n  geom_density_ridges(scale = 1.5)\n\nPicking joint bandwidth of 535\n\n\n\n\n\n\n\n\n\nAnd as before the ridge plots are easier to interpret: the distribution is more spread out with more ‘humps’ for colours H-J. This is what drags up the median.\nIn the case of carat vs clarity, we repeat the procedure\n\nboxplot(carat~clarity, data=diamonds)\n\n\n\n\n\n\n\n\ngetting the “reverse” picture: going from left to right, the carat clearly drops.\n\nggplot(diamonds, \n       aes(x = factor(clarity), y = carat)) + \n  geom_violin(fill = \"lightBlue\")\n\n\n\n\n\n\n\n\nViolin plots show a shape that is definitely changing. The distribution is more spread on the left, and more concentrated on the right.\n\nggplot(diamonds, aes(carat, y=factor(clarity), fill=clarity)) + \n  geom_density_ridges(scale = 1.5)\n\nPicking joint bandwidth of 0.057\n\n\n\n\n\n\n\n\n\nOnce again, the pattern is easier to see from the ridgeplots: the distribution is moving and becoming more concentrated. We might suspect a bit of undersmoothing as the shapes are very wiggly.\n\n\n\n\nThe ideas of density estimation can be extended beyond one dimension. If we consider two variables at once, we can estimate their 2-d joint distribution as a smoothed version of a scatterplot. This also provides another alternative method of dealing with overplotting of dense data sets.\nLet’s return to the Old Faithful data:\n\nlibrary(MASS) ## this is a standard package, you shouldn't need to install it\ndata(geyser)\nplot(x=geyser$duration, y=geyser$waiting,\n     xlab='duration', ylab='waiting', pch=16)\n\n\n\n\n\n\n\n\nA 2-D density estimate would smooth the distribution into a surface, picking out two or three peaks corresponding to the clumps in the data. We can fit the 2-D density using the kde2d function from the MASS package. It has few arguments, just the x and y to be smoothed, and an optional n which controls the amount of detail in the resulting smoothed density.\n\nk1 &lt;- kde2d(geyser$duration, geyser$waiting, n = 100)\n\nThe output of kde2d is a bit more complicated than we get from a 1-D density estimation. It’s now a list with x, y, and z components. The x and y components are the values of the specified variables (here, duration and waiting respectively) which are used to create a grid of points. The density value at every combination of x and y is then estimared and returned in the z component.\nThe value of n here indicates the number of grid points along each axis, effectively acting as an “inverse bandwidth”. Increasing this will give you smoother output due to making more estimates in a bigger z matrix, but will take longer to evaluate.\nWe can then add the smoothed density to our scatterplot as contours:\n\nplot(x=geyser$duration, y=geyser$waiting,\n     xlab='duration', ylab='waiting', pch=16, col=alpha('black',0.6))\ncontour(k1, add=TRUE, col='blue')\n\n\n\n\n\n\n\n\nUsing add=TRUE we can draw the contours on top of an existing plot. See the help on contour (?contour) for more ways to customise.\nAlternatively, we could skip the scatterplot and just draw the contours (useful if we have too many points)\n\ncontour(k1, col='blue')\n\n\n\n\n\n\n\n\nA different presentation of the density estimate is to draw the surface as a heatmap:\n\nimage(k1)\n\n\n\n\n\n\n\n\nEach rectangular region in the plot is now coloured according to the density estimate. The heatmap is a little blocky due to the resolution of our density estimate. Specifying n=100 divides each axis into 100 intervals, giving us 10,000 2-D bins over the range of the data. Like a histogram, we can increase n to see more detail - though it can take a little while to compute.\nIf we want to overlay the data points, we can use the points function:\n\nimage(kde2d(geyser$duration, geyser$waiting, n = 500),\n      xlab='duration', ylab='waiting')\npoints(x=geyser$duration, y=geyser$waiting,pch=16, col=alpha('black',0.6))\n\n\n\n\n\n\n\n\n\n\n\n Download data: HRstars\nThe Hertzsprung-Russell diagram is famous visualisation of the relationship between the absolute magnitude of stars (i.e. their brightness) and their effective temperatures (indicated by their colour). The Hertzsprung-Russell provides an easy way to distinguish different categories of stars according to their position on the plot.\nWe’re going to look at the HRStars data which contain 6220 stars from the Yale Trigonometric Parallax Dataset.\n\nLet’s start with single variables. Have a look at histograms of V and BV.\nDo you see any evidence of groups in the data? You’ll need to adjust the number of bars.\n\nUnivariate summaries can conceal a lot of information when the data are inherently multivariate. So, let’s consider both variables.\n\nDraw a plot of the magnitude V vertically against the observed colour BV.\nThe data contain a lot of points, so reduce your point size by scaling down the symbol size. This can be done by setting the optional argument cex to a value between 0 and 1 - the default size is 1.\nWhat features do you see? Can you identify any groups in the data? Are there any outliers?\nThe Sun has an absolute magnitude of 4.8 and B-V colour of 0.66. Use the points function (it works exactly the same way as lines) to add it to your plot - use a different colour and/or plot character to make it stand out.\nHow does your plot differ from the plots you find on the web? How would you adjust your scatterplot to match?\n\nThe central ‘wiggle’ in the plot corresponds to main sequence stars. Below that wiggle are the white dwarf stars, and above are the giants and supergiants.\n\nFit a 2-D density estimate to the stars data.\nOverlay the contours on your scatterplot (use a different colour and/or transparency).\nThe kde2d function takes an argument h which represents the bandwidth. We can supply a single value, or a vector of two values (one for each variable).\nThe contour plot seems to miss out the less dense region of dwarf stars in the bottom left due to the low density of points. The default number of contours drawn is 10, and it is set by the nlevels argument. Increase the number of contours until a contour around the dwarf group is visible.\nAlternatively, you can pick the levels at which the contour lines are drawn by specifying a vector as input to the levels argument. I found that \\(0.005, 0.01, 0.025, 0.05, 0.1, 0.15\\) work well - try this now.\nCan you identify the main groups of stars?\n\nThis is more tricky and requires more R programming:\n\nCreate a new categorical variable to that represents the values of the Uncert variable, which represents the parallax error in the observations. Use three levels:\n\nUncert under 50\nUncert above 50 but below 100\nUncert above 100\n\nUse your new variable to colour the points in your scatterplot according to this error.\nIs this parallax error associated with the properties (and hence types) of observed stars?\n\n\n\n\n Download data: fatherSon\nThis is another dataset of father and son heights, this time from Karl Pearson (another prominent early statistician). This time we have 1078 paired heights recorded in inches and were published in 1903. Families were invited to provide their measurements to the nearest quarter of an inch with the note that “the Professor trusts to the bona fides of each recorder to send only correct results.”\n\nDraw histograms of the sons’ and fathers’ heights (sheight and fheight). Use a density scale and a bar width of 1 inch.\nCompute a density estimate for both distributions.\nDraw the histograms side-by-side and overlay your density curves.\nDo the data look normally distributed?\n\nWe should be careful about jumping to conclusions here, or as Pearson put it in his 1903 paper: “It is almost in vain that one enters a protest against the mere graphical representation of goodness of fit, now that we have an exact measure of it.” For the time, he was justified in his demanding both graphical and analytical approaches. Since those early days, analytic approaches have often been too dominant, both are needed.\n\nDraw Normal quantile plots of the two variables side-by-side. Do the data look normal?\n\nLet’s look at the pairwise relationship between the two heights.\n\nDraw a scatterplot of sons’ heights (vertically) against fathers’ heights (horizontally).\nIs there a strong association here? Calculate the correlation.\nFit a linear regression (recall in ISDS, you used the lm function). Add the line to your plot.\n\nThe height of a son is influenced by the height of the father, but there is a lot of unexplained variability. This scatterplot also illustrates a phenomenon known as regression towards the mean. Small fathers havs sons who are small, but on average not as small as their fathers. Tall fathers have sons who are tall, but on average not as tall as their fathers.\nTo explore further whether a non-linear model would be warranted, we could plot a smoother andt the best fit regression line together.\n\nFit a smoother to the data.\nAdd it to your plot with a 95% confidence interval.\nAre the regression and smoother in agreement?"
  },
  {
    "objectID": "Workshop3_TimeSeries.html",
    "href": "Workshop3_TimeSeries.html",
    "title": "Workshop 3 - Exploring Time Series Data",
    "section": "",
    "text": "In this workshop, we will produce some high quality visualisations of time series.\n\n\nBy the end of the workshop, you will have seen how to:\n\n\nVisualise time series with simple line graphs\n\n\nManually smooth time series to extract a trend using loess\n\n\nUse R’s built-in time series decomposition function (stl) to extract time series components where appropriate\n\n\n\n\n\nA graph can be a powerful vehicle for displaying change over time. A time series is a set of quantitative values obtained at successive time points, and time series are most commonly graphed as a simple line graph with time horizontally and the variable being measured shown vertically. Let us have a glance at how to plot time series effectively with an example.\n Download data: economics\nThe economics data set above contains monthly economic data for the USA collected from January 1967 to January 2015. The variables are:\n\ndate - month of data collection\npce - personal consumption expenditures, in billions of dollars\npop - total population, in thousands\npsavert - personal savings rate\nuempmed - median duration of unemployment, in weeks\nunemploy - number of unemployed in thousands\n\nThe date column is stored in R’s Date format so we can plot it directly. However, this isn’t guaranteed of all data sets and sometimes you will need to convert it (see the as.Date function for how to do so). Thankfully, everything is all set up, so let’s plot the personal savings rate over time:\n\nplot(x=economics$date,y=economics$psavert, xlab='Date',ylab='Personal Savings Rate', ty='l')\n\n\n\n\n\n\n\n\nTime series of financial variables like these are very often quite noisy and variable. While there is obviously a general global trend, where the line between ‘trend’ and ‘noise’ is drawn is debatable and there is no single clear answer for these data. Fitting a smoother would help us identify a clear smooth trend, but using different levels of smoothing will give us quite different results.\n\n## fit the model, note we have to convert our 'date' to numbers here\nlfit &lt;- loess(psavert~as.numeric(date), data=economics) \nxs &lt;- seq(min(as.numeric(economics$date)), max(as.numeric(economics$date)), length=200)\nlpred &lt;- predict(lfit, data.frame(date=xs), se = TRUE) \n## redraw the plot\nplot(x=economics$date,y=economics$psavert, xlab='Date',ylab='Personal Savings Rate', ty='l')\nlines(x=xs, y=lpred$fit, col='red',lwd=4) \n\n\n\n\n\n\n\n\nThis extracts the overall trend quite cleanly! Note that we can easily customise the width and type of the smoothing curve\n\n## fit the model, note we have to convert our 'date' to numbers here\nlfit &lt;- loess(psavert~as.numeric(date), data=economics) \nxs &lt;- seq(min(as.numeric(economics$date)), max(as.numeric(economics$date)), length=200)\nlpred &lt;- predict(lfit, data.frame(date=xs), se = TRUE) \n## redraw the plot\nplot(x=economics$date,y=economics$psavert, xlab='Date',ylab='Personal Savings Rate', ty='l')\nlines(x=xs, y=lpred$fit, col='red',lwd=1.5, lty=3) \n\n\n\n\n\n\n\n\nIf we shrink the span, we will fit more closely to the lesser peaks in the data - but is this signal or just noise?\n\n## reduce span to get a more localised fit\nlfit2 &lt;- loess(psavert~as.numeric(date), data=economics,span=0.1) \nlpred2 &lt;- predict(lfit2, data.frame(date=xs), se = TRUE) \n## redraw the plot\nplot(x=economics$date,y=economics$psavert, xlab='Date',ylab='Personal Savings Rate', ty='l')\nlines(x=xs, y=lpred$fit, col='red',lwd=4) \nlines(x=xs, y=lpred2$fit, col='green',lwd=4) \n\n\n\n\n\n\n\n\nThe variation around the trend is quite irregular here, and unless we have any other information to help us explain these features they are difficult to explain. We’ll return to this idea of breaking down a time series into different scales of detail later on.\n\n\nLet us apply the above to another time series, exploring the evolution of income equality over time.\n Download data: income\nThe income data set contains the mean annual income in the USA from 1913 to 2014. The values are given for two separate groups: the top 1% of earners (U01), and the lower 90% of earners (L90). This gives us two time series to explore - the main questions of interest is how have these time series changed relative to each other, and who has done better over the past 100 years? (though we can probably guess the answer).\n\npar(mfrow=c(2,1))\nplot(y=income$L90,x=income$year,ty='l')\nplot(y=income$U01,x=income$year,ty='l')\n\n\n\n\n\n\n\n\nL90 values are a lot smaller, unsurprisingly; L90 appears flat until 1940, then starts to grow steadily, perhaps levelling out after 2010. U01 is also fairly flat until the 40s then slowly increases, and increases more rapidly after 1980\n\nplot(y=income$U01,x=income$year,ty='l', ylim=c(min(income$L90),max(income$U01)))\nlines(y=income$L90,x=income$year,col='red')\n\n\n\n\n\n\n\n\nL90 becomes almost flat due to huge differences in scale!\nA common problem when series have very different scales is that the variation of the larger series obscures anything going on in the other. A standard approach to this is to set a time point as a baseline (often the first point), and then calculate and plot the relative changes from that baseline value. If we take our first year as baseline, then we can have a scale-free measure of growth to compare.\nThe plot of the standardised data can be obtained with the following code:\n\nincome$L90rel &lt;- 100*(income$L90/income$L90[1]-1)\nincome$U01rel &lt;- 100*(income$U01/income$U01[1]-1)\nplot(y=income$U01rel, x=income$year,ty='l')\nlines(y=income$L90rel, x=income$year,ty='l',col='red')\n\n\n\n\n\n\n\n\nHere we see the growth relative to the 1913 levels more clearly: both groups changed similarly up to mid 1940s (end of WWI), after this point the income of L90 grew more rapidly until the 1980s, where the U01 income increased dramatically and by the early 2000s the U01 group had income growth larger than the L90 (despite starting from a higher baseline!)\nWhen we have two associated time series, it can be interesting to see how they vary jointly over time. For this we can use a form of scatterplot. However, given the dependence of both variables on time it is importance to connect the points so we can see where the series is coming from and heading to.\n\n\n\nReturn to the original value of the L90 and U01, and draw a scatterplot with L90 vertically against U01 horizontally. Use plot type ty='b' (b for ‘both’) to connect your points with lines. What features do you see?\nTo help explain this plot, let’s label the points with the years. The following code will add text labels next to each point, run it and then revisit your plot. See the help on text (type ?text) for details. text(x=income$U01,y=income$L90, labels=income$year, cex=0.5, pos=4)\nCan you identify the years corresponding to the clumps of points? What major events took place during these periods that may explain what we see?\nCan you create a factor variable with four levels corresponding to: “Before 1936”, “1936-1960”, “1961-1983”, “After 1983”? You can use the cut function here to split a numerical variable up into a categorical factor.\nUse this new factor to colour the points on your plot. What were the main changes in income growth during each of these periods?\n\n\n\n\nClick for solution\n\n\ngroups &lt;- cut(income$year, breaks=c(1900,1936,1960,1983,2020),\n              labels=c())\nplot(x=income$U01, y=income$L90, ty='b',pch=16, col=groups)\ntext(x=income$U01,y=income$L90, labels=income$year, cex=0.5, pos=4, col=(1:4)[groups])\n\n\n\n\n\n\n\n\nIncome for both groups struggles to grow much until the 30s, income growth stalls for all following WW2, and takes until the 60s to recover. L90 income grows steadily until the 70s, after which it grows much more slowly and the U01’s income grows substantially in the 80s/90s. Another stutter in income affects mostly the U01 in the 2000/2010s (financial crisis).\nIn theory, it should be possible for incomes to rise for everyone at the same time — for the gains of economic growth to be broadly distributed year after year. But the takeaway from these graphs is that since World War II, that’s never really happened in the U.S.\n\nNote that the analysis of multiple time series is a huge topic of its own: here we are just scratching the surface, but if you are interested to learn more about this I can point you to some references.\n\n\n\nWhen exploring a time series, we often notice that it follows a general trend (given by its smoothing), it often has a periodic, or seasonal, behaviour, and then there is some residual noise that might be a little or a lot, depending on the time series.\nLet us see an example using a famous data set.\n Download data: co2\nThe co2 data contains the average monthly concentrations of CO2 from 1959 recorded at the Mauna Loa observatory in Hawaii. Environmental time series like this one often have a very strong and regular pattern to them.\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l')\n\n\n\n\n\n\n\n\nThere’s a clear trend here, with a regular wiggle around it\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020))\n\n\n\n\n\n\n\n\nthe wiggle seems to repeat every year, peaking in summer and falling in winter.\nFor time series with a strong seasonal component it can be useful to look at a Seasonal Decomposition of Time Series by Loess, or STL. Essentially, we define a regular time series such as this in terms of three components:\n\nThe trend - the long-term smooth evolution\nThe seasonal component - a regular smooth variation about the trend\nThe residuals - random noise on top of everything.\n\nTo do a time series decomposition, we’ll need to turn the data into a ts (time series) object so it is recognised by R.\n\nco2ts &lt;- ts(data = co2$co2,\n          start = co2[1, 1:2], \n          end = co2[nrow(co2), 1:2],\n          frequency = 12)\n\nThen we can apply the time series decomposition\n\n decomp &lt;- stl(co2ts, s.window = \"periodic\")\n\nand plot the results:\n\nplot(decomp, col = 'blue')\n\n\n\n\n\n\n\n\nThe top panel gives the original data, the second panel gives the estimated regular periodic effect around the overall trend, the third panel shows the overall trend and the final panel shows the residuals (as data - trend - seasonal). The residuals are quite small, suggesting this decomposition describes the data well.\nTo visualise the time series and the trend in the same plot, we can run the following code:\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l')\n## draw a red line for the trend\nlines(y=decomp$time.series[,2],x=co2$decimaldate,col='red')\n\n\n\n\n\n\n\n\nWe can also add a blue line with the trend plus seasonal component, to check if this is a good fit for the whole time series.\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020)) ## use x-axis limits to zoom in\n\n\n\n\n\n\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020), ylim=c(380,420))\nlines(y=decomp$time.series[,2],x=co2$decimaldate,col='red')\nlines(y=decomp$time.series[,1]+decomp$time.series[,2],\n      x=co2$decimaldate,col='blue')\n\n\n\n\n\n\n\n\nWe see some imperfections, but overall this looks good! Decomposing seasonal time series like this can be very effective, however it can be difficult to find such ideally-suited time series with such regular behaviour outside of highly-structured situations.\n\n\n\nIn lectures, we saw that there are different kinds of multiple time series, and that visualisation methods for comparing them depend on their nature:\n\nRelated series for the same population - if possible, show the different time series within the same plot to ease comparison. Be careful of units!\nSeries for different subgroups - showing subgroups together helps draw comparisons. Are we interested in the values or the proportions of the subgroups?\nSeries with different scales - may need standardising to a common scale to show together, otherwise separate plots will be needed.\n\nIt is always a good idea to compare not only the full plots of these time series, but also their decomposition. In fact, there is a high chance of spurious correlations when it comes to comparisons between time series!\nLet’s see an example: in the economics data set, look at the two variables:\n\npce - personal consumption expenditures, in billions of dollars\npop - total population, in thousands\n\nTheir corresponding time series, plotted together, look like this\n\npar(cex=0.85,cex.axis=0.85,cex.lab=0.85)\nplot(x=economics$date,y=economics$pce, ty='l',col=\"red\",lwd=2,xlab='',ylab='', ylim=c(min(economics$pce),max(economics$pop)))\nlines(x=economics$date,y=economics$pop, ty='l',col=\"green\",lwd=2,xlab='',ylab='')\nlegend(x='topleft',col=2:5,lwd=2,pch=NA,legend=c('Consumption','Population'))\n\n\n\n\n\n\n\n\nOf course, this plot is a bad visualisation for many reasons: the units are different, the scale is different, and one series flatten the other so badly that we can barely notice the increasing trend of pce. In order to meaningfully compare the two time series, let us standardise the two variables:\n\npop_z &lt;- scale(economics$pop)\npce_z &lt;- scale(economics$pce)\n\nand re-plot the time series:\n\nx &lt;- economics$date\nylim &lt;- range(c(pop_z, pce_z), na.rm = TRUE)\n\nplot(x, pop_z, type = \"l\", ylim = ylim, ylab = \"z-score\", col=\"green\")\nlines(x, pce_z, col = \"red\")\nlegend(x='topleft',col=2:3,lwd=2,pch=NA,legend=c('Consumption','Population'))\n\n\n\n\n\n\n\n\nWe now see that both variables have a strong upward drift, but not quite in the same way. Now, if we compute the correlation of pop versus pce, we get\n\ncor(economics$pop, economics$pce)\n\n[1] 0.9872421\n\n\nso, population and consumption are almost perfectly correlated! However, it is important to understand why it is so: does the very high correlation result from the fact that one variable influences the other or only from the fact that both variables grow over time? In order to establish this, we need to compare the correlation of the different components of the time series.\n\n\n\nCreate two time series objects, one for the time series of economics$pop and one for the time series of economics$pce.\nApply STL decomposition to both economics$pop and economics$pce\nCompare the trends, seasonal components and residuals of the two time series. What do you notice?\nCompute the correlation of trends, seasonal components and residuals. Does this help you establish if the overall correlation is simply the result of a time effect?\n\n\n\n\nClick for solution\n\nHere is the code for the STL decomposition\n\npop_ts &lt;- ts(as.numeric(scale(economics$pop)), frequency = 12)\npce_ts &lt;- ts(as.numeric(scale(economics$pce)), frequency = 12)\n\npop_decomp &lt;- stl(pop_ts, s.window = \"periodic\")\npce_decomp &lt;- stl(pce_ts, s.window = \"periodic\")\n\nHere is the plot of trends\n\nplot(y=pop_decomp$time.series[,2], x=economics$date, ty='l', col=\"green\")\nlines(y=pce_decomp$time.series[,2], x=economics$date, col=\"red\")\n\n\n\n\n\n\n\n\nHere is the plot of seasonal components. Since this is periodic, it makes sense to look at this over a limited period of time\n\nplot(y=pop_decomp$time.series[,1], x=economics$date, ty='l', col=\"green\", xlim=c(1,1000))\nlines(y=pce_decomp$time.series[,1], x=economics$date, col=\"red\")\n\n\n\n\n\n\n\n\nThere is a bit of lagged correlation, but an essentially different seasonal pattern.\nThe plot of residuals is better done as a scatterplot:\n\nplot(y=pop_decomp$time.series[,3], x=pce_decomp$time.series[,3], ty='p', col=\"blue\", pch=16)\n\n\n\n\n\n\n\n\nThis shows no association of residuals. In summary, almost all the correlation between the two time series is induced by time. We can only conclude that both variables have a constant increasing trend with time, and that’s why they are correlated. In other words, this is a spurious correlation.\nAs a sanity check, let us compute the correlation coefficient of the components.\n\ncor(x=pop_decomp$time.series[,1], y=pce_decomp$time.series[,1])\n\n[1] 0.5350119\n\ncor(x=pop_decomp$time.series[,2], y=pce_decomp$time.series[,2])\n\n[1] 0.9873021\n\ncor(x=pop_decomp$time.series[,3], y=pce_decomp$time.series[,3])\n\n[1] 0.1676072\n\n\nThe only meaningful correlation is the one between trends!"
  },
  {
    "objectID": "Workshop3_TimeSeries.html#exercise-1-the-fall-and-rise-of-income-inequality",
    "href": "Workshop3_TimeSeries.html#exercise-1-the-fall-and-rise-of-income-inequality",
    "title": "Workshop 3 - Exploring Time Series Data",
    "section": "",
    "text": "Now is your turn to try to make sense of another time series to explore the evolution of income equality over time.\n Download data: income\nThe income data set contains the mean annual income in the USA from 1913 to 2014. The values are given for two separate groups: the top 1% of earners (U01), and the lower 90% of earners (L90). This gives us two time series to explore - the main questions of interest is how have these time series changed relative to each other, and who has done better over the past 100 years? (though we can probably guess the answer)\n\n\n\nLoad the data and draw two plots, one above the other, for the incomes in the two groups over time.What features do you see?\nThen make a single plot of both variables. You’ll need to be careful with your vertical axis ranges, which you should set using the ylim argument to span both series. Use a different colour for each series. What do you observe?\n\n\n\nClick for solution\n\n\npar(mfrow=c(2,1))\nplot(y=income$L90,x=income$year,ty='l')\nplot(y=income$U01,x=income$year,ty='l')\n\n\n\n\n\n\n\n\nL90 values are a lot smaller, unsurprisingly; L90 appears flat until 1940, then starts to grow steadily, perhaps levelling out after 2010. U01 is also fairly flat until the 40s then slowly increases, and increases more rapidly after 1980\n\nplot(y=income$U01,x=income$year,ty='l', ylim=c(min(income$L90),max(income$U01)))\nlines(y=income$L90,x=income$year,col='red')\n\n\n\n\n\n\n\n\nL90 becomes almost flat due to huge differences in scale!\n\nA common problem when series have very different scales is that the variation of the larger series obscures anything going on in the other. A standard approach to this is to set a time point as a baseline (often the first point), and then calculate and plot the relative changes from that baseline value. If we take our first year as baseline, then we can have a scale-free measure of growth to compare:\n\n\n\n\nStandardise (i.e. divide) both series so they are relative to their first observation. Then subtract 1 and multiply by 100 to create the growth above the baseline on a percentage scale. Save the standardised data to two new variables (or new columns within income).\nNow redraw the plot showing both series of income growth together.\nWhat features do you see? Could you see these features on the previous plots?\n\n\n\nClick for solution\n\nThe plot of the standardised data can be obtained with the following code:\n\nincome$L90rel &lt;- 100*(income$L90/income$L90[1]-1)\nincome$U01rel &lt;- 100*(income$U01/income$U01[1]-1)\nplot(y=income$U01rel, x=income$year,ty='l')\nlines(y=income$L90rel, x=income$year,ty='l',col='red')\n\n\n\n\n\n\n\n\nHere we see the growth relative to the 1913 levels more clearly: both groups changed similarly up to mid 1940s (end of WWI), after this point the income of L90 grew more rapidly until the 1980s, where the U01 income increased dramatically and by the early 2000s the U01 group had income growth larger than the L90 (despite starting from a higher baseline!)\n\nWhen we have two associated time series, it can be interesting to see how they vary jointly over time. For this we can use a form of scatterplot. However, given the dependence of both variables on time it is importance to connect the points so we can see where the series is coming from and heading to.\n\n\n\n\nReturn to the original value of the L90 and U01, and draw a scatterplot with L90 vertically against U01 horizontally. Use plot type ty='b' (b for ‘both’) to connect your points with lines. What features do you see?\nTo help explain this plot, let’s label the points with the years. The following code will add text labels next to each point, run it and then revisit your plot. See the help on text (type ?text) for details. text(x=income$U01,y=income$L90, labels=income$year, cex=0.5, pos=4)\nCan you identify the years corresponding to the clumps of points? What major events took place during these periods that may explain what we see?\nCan you create a factor variable with four levels corresponding to: “Before 1936”, “1936-1960”, “1961-1983”, “After 1983”? You can use the cut function here to split a numerical variable up into a categorical factor.\nUse this new factor to colour the points on your plot. What were the main changes in income growth during each of these periods?\n\n\n\nClick for solution\n\n\ngroups &lt;- cut(income$year, breaks=c(1900,1936,1960,1983,2020),\n              labels=c())\nplot(x=income$U01, y=income$L90, ty='b',pch=16, col=groups)\ntext(x=income$U01,y=income$L90, labels=income$year, cex=0.5, pos=4, col=(1:4)[groups])\n\n\n\n\n\n\n\n\nIncome for both groups struggles to grow much until the 30s, income growth stalls for all following WW2, and takes until the 60s to recover. L90 income grows steadily until the 70s, after which it grows much more slowly and the U01’s income grows substantially in the 80s/90s. Another stutter in income affects mostly the U01 in the 2000/2010s (financial crisis).\nIn theory, it should be possible for incomes to rise for everyone at the same time — for the gains of economic growth to be broadly distributed year after year. But the takeaway from these graphs is that since World War II, that’s never really happened in the U.S."
  },
  {
    "objectID": "Workshop3_TimeSeries.html#exercise-2-co2-concentrations-at-mauna-loa",
    "href": "Workshop3_TimeSeries.html#exercise-2-co2-concentrations-at-mauna-loa",
    "title": "Workshop 3 - Exploring Time Series Data",
    "section": "",
    "text": "Download data: co2\nThe co2 data contains the average monthly concentrations of CO2 from 1959 recorded at the Mauna Loa observatory in Hawaii. Environmental time series like this one often have a very strong and regular pattern to them.\n\n\n\nLoad the data and make a line plot of the average monthly CO2 concentration (co2) over time.\nWhat features do you see?\nRedraw the plot, drawing only the data for 2010. How do the data behave?\n\n\n\nClick for solution\n\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l')\n\n\n\n\n\n\n\n\nThere’s a clear trend here, with a regular wiggle around it\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020))\n\n\n\n\n\n\n\n\nthe wiggle seems to repeat every year, peaking in summer and falling in winter.\nFor time series with a strong seasonal component it can be useful to look at a Seasonal Decomposition of Time Series by Loess, or STL. Essentially, we define a regular time series such as this in terms of three components:\n\nThe trend - the long-term smooth evolution\nThe seasonal component - a regular smooth variation about the trend\nThe residuals - random noise on top of everything.\n\nTo do a time series decomposition, we’ll need to turn the data into a ts (time series) object so it is recognised by R.\n\nco2ts &lt;- ts(data = co2$co2,\n          start = co2[1, 1:2], \n          end = co2[nrow(co2), 1:2],\n          frequency = 12)\n\nThen we can apply the time series decomposition\n\n decomp &lt;- stl(co2ts, s.window = \"periodic\")\n\nand plot the results:\n\nplot(decomp, col = 'blue')\n\n\n\n\n\nRun these commands now and draw the final plot.\nWhat is being shown here? Can you explain and interpret the different components?\nThe results of the decomposition will be saved in a decomp object in a time series object called time.series, with a column for each component.\nDraw a line plot of the time series, and add the trend function on top in red using the values in decomp.\nTry adding the trend+seasonal component to the plot in another colour. Does the model fit look to be good?\nZoom into 2010-2020 and redraw the same plot with the components. How does it look now?\n\n\n\nClick for solution\n\n\nco2ts &lt;- ts(data = co2$co2,\n            start = co2[1, 1:2], # columns 1,2 include the year, month\n            end = co2[nrow(co2), 1:2], # final year, month\n            frequency = 12) # 12 months per year\ndecomp &lt;- stl(co2ts, s.window = \"periodic\")\nplot(decomp)\n\n\n\n\n\n\n\n\nThe top panel gives the original data, the second panel gives the estimated regular periodic effect around the overall trend, the third panel shows the overall trend and the final panel shows the residuals (as data - trend - seasonal). The residuals are quite small, suggesting this decomposition describes the data well.\nTo visualise the time series and the trend in the same plot, we can run the following code:\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l')\n## draw a red line for the trend\nlines(y=decomp$time.series[,2],x=co2$decimaldate,col='red')\n## and a blue line for trend + seasonal\nlines(y=decomp$time.series[,1]+decomp$time.series[,2],\n      x=co2$decimaldate,col='blue')\n\n\n\n\n\n\n\n\nThe blue line is a good fit for the time series. Let’s look a little closer\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020)) ## use x-axis limits to zoom in\n\n\n\n\n\n\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020), ylim=c(380,420))\nlines(y=decomp$time.series[,2],x=co2$decimaldate,col='red')\nlines(y=decomp$time.series[,1]+decomp$time.series[,2],\n      x=co2$decimaldate,col='blue')\n\n\n\n\n\n\n\n\nWe start to see some imperfections, but overall it still looks good. De-trending seasonal time series like this can be very effective, however it can be difficult to find such ideally-suited time series with such regular behaviour outside of highly-structured situations."
  },
  {
    "objectID": "Workshop3_TimeSeries.html#exercise-3-airline-flights-from-nyc---optional",
    "href": "Workshop3_TimeSeries.html#exercise-3-airline-flights-from-nyc---optional",
    "title": "Workshop 3 - Exploring Time Series Data",
    "section": "",
    "text": "The nycflights13 library contains a huge amount of data for all flights departing New York City in 2013. We’ll just focus on a simple summary - the number of departures per day.\n Download data: flights\n\n\n\nPlot the line graph of the number of flights (n) over time (date).\nWhat features do you see?\nTry smoothing the data with loess, and add your smoothed trend to the plot. You may want to use the decimalised date (e.g. date format written as 2013.1, 2013.2, etc.) as x in your model here.\nRedraw the plot, but only show the first three weeks of data (either extract the first 4 weeks of observations, or adjust your axis limits to only show this period). What do you conclude about the behaviour of this time series? Is there a regular seasonal component?\n\n\n\nClick for solution\n\n\nplot(x=flights$date, y=flights$n, xlab='Date', ylab='Number of Flights', ty='l')\n\n\n\n\n\n\n\n\nNow, let us fit our smoothing:\n\nflights$decimal_date &lt;- flights$year + (1/12)*flights$month + (1/365)*flights$day\nlfit &lt;- loess(n~as.numeric(decimal_date), data=flights) \nxs &lt;- seq(min(flights$decimal_date), max(as.numeric(flights$decimal_date)), length=200)\nlpred &lt;- predict(lfit, data.frame(decimal_date=xs), se = TRUE)\n\nand redraw the plot\n\nplot(x=flights$decimal_date, y=flights$n, xlab='Date', ylab='Number of Flights', ty='l')\nlines(x=xs, y=lpred$fit, col='red',lwd=4) \n\n\n\n\n\n\n\n\nThe smoothing shows a slight quadratic trend (i.e. it follows the path of a parabola), but not too far from being uniform. There is a clear periodic trend showing drastic fall in the number of flights at regular intervals. To see the period more clearly, we need to zoom in a shorter timeframe:\n\nflights$decimal_date &lt;- flights$year+flights$month*(1/12)+flights$day*(1/365)\nplot(x=flights$decimal_date, y=flights$n, xlab='Date', ylab='Number of Flights', ty='l', xlim=c(2013+1/12, 2013+2/12))\n\n\n\n\n\n\n\n\nIt looks like the fall happens every week! Might it be because there are less flights on the weekend than on weekdays? In task 3b we check if this idea is correct.\n\n\n\n\n\nDivide the data set into groups, according to the day of the week (hint: use the weekdays() function applied to the date variable).\nDraw a linegraph of time series of number of departures by date. Which day has the least traffic?\nDoes this help to explain the seasonal pattern in the original data?\n\n\n\nClick for solution\n\n\nflights$weekday &lt;- weekdays(flights$date)\n\nHere is an example of solution using ggplot (for illustration only! You are not expected to know how to use ggplot but some of you might have seen this in other courses).\nWe can either visualise the groups separately:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nggplot(flights, aes(x = date, y = n)) +\n  geom_line() +\n  facet_wrap(~ weekday, scales = \"free_y\") + \n  theme_minimal() +\n  labs(title = \"Daily Departures by Day of the Week\",\n       x = \"Date\",\n       y = \"Number of Departures\")\n\n\n\n\n\n\n\n\nor together in a single plot for comparison (Saturdays are clearly the less traveled days and so the corresponding time series is highlighted in red)\n\nggplot(flights, aes(x = date, y = n, color = ifelse(weekday == \"Saturday\", \"Saturday\", \"Other\"))) +\n  geom_line() +\n  scale_color_manual(values = c(\"Saturday\" = \"red\", \"Other\" = \"black\")) +\n  theme_minimal() +\n  labs(title = \"Daily Departures (2013)\",\n       x = \"Date\",\n       y = \"Number of Departures\",\n       color = \"Day of the week\")\n\n\n\n\n\n\n\n\nThe weekly pattern is explained by lower numbers of flights on Saturdays! There are still five unexplained sudden falls in flight numbers. What are those?\n\nThis is one example of a well-structured and regular time series where we can apply the decomposition technique. We have regular variation according to the day of the week, and the pattern repeats every week. First, we setup the data as a time series object using that information:\n\nflts &lt;- ts(flights$n, frequency=7,start=0)\n\nWe can then use the stl function to decompose the series into its components:\n\ndecomp &lt;- stl(flts, s.window='periodic')\n\n\n\n\n\nPlot the decomp object to view the decomposition. What features do you see?\nDoes the decomposed trend look like your smoothed trend? Which do you think explains the data best? Can you explain any differences?\nThe trend function appears to exhibit a number of substantial drops in traffic. Let’s look closer:\n\nPlot the decomposed trend from decomp$time.series[,2] against the date variable in the original data set.\nCan you identify the dates of any of these downward spikes in traffic? Hint: you can install the package lubridate and use the function abline(v=ymd('2013-12-25'),col='red') to draw a vertical line on Christmas Day.\n\n\n\n\nClick for solution\n\n\n plot(decomp)\n\n\n\n\n\n\n\n\nThe decomposition seems to be different from our smoothed trend above. This is due to the effect of variations that are not periodic, in this case the variation in flights number that do not occur on Saturdays. The decomposition is finer and allows us to pinpoint drops in traffic that are not periodic.\nLet’s have a closer look at these:\n\nplot(y = decomp$time.series[,2], x = flights$date, ty='l')\n\n\n\n\n\n\n\n\nand let’s draw lines in correspondence of main festivities in US\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nplot(y = decomp$time.series[,2], x = flights$date, ty='l')\nabline(v = ymd('2013-12-25'),col='red') #Christmas\nabline(v = ymd('2013-10-31'),col='red') #Halloween\nabline(v = ymd('2013-11-28'),col='red') #Thanksgiving\nabline(v = ymd('2013-07-04'),col='red') #Independence Day\nabline(v = ymd('2013-09-02'),col='red') #Labor Day\nabline(v = ymd('2013-05-27'),col='red') #Memorial Day\n\n\n\n\n\n\n\n\nThese explain the reasons of drops in flights, as during festivities people are less inclined to travel.\n\nTime series of human activity are often very cyclical, with many predictable features. Whilst perhaps not hugely surprising, part of the job of an exploratory analysis is to check whether what might think should be obvious actually is obvious in the data!"
  },
  {
    "objectID": "Workshop3_TimeSeries.html#example-the-fall-and-rise-of-income-inequality",
    "href": "Workshop3_TimeSeries.html#example-the-fall-and-rise-of-income-inequality",
    "title": "Workshop 3 - Exploring Time Series Data",
    "section": "",
    "text": "Let us apply the above to another time series, exploring the evolution of income equality over time.\n Download data: income\nThe income data set contains the mean annual income in the USA from 1913 to 2014. The values are given for two separate groups: the top 1% of earners (U01), and the lower 90% of earners (L90). This gives us two time series to explore - the main questions of interest is how have these time series changed relative to each other, and who has done better over the past 100 years? (though we can probably guess the answer).\n\npar(mfrow=c(2,1))\nplot(y=income$L90,x=income$year,ty='l')\nplot(y=income$U01,x=income$year,ty='l')\n\n\n\n\n\n\n\n\nL90 values are a lot smaller, unsurprisingly; L90 appears flat until 1940, then starts to grow steadily, perhaps levelling out after 2010. U01 is also fairly flat until the 40s then slowly increases, and increases more rapidly after 1980\n\nplot(y=income$U01,x=income$year,ty='l', ylim=c(min(income$L90),max(income$U01)))\nlines(y=income$L90,x=income$year,col='red')\n\n\n\n\n\n\n\n\nL90 becomes almost flat due to huge differences in scale!\nA common problem when series have very different scales is that the variation of the larger series obscures anything going on in the other. A standard approach to this is to set a time point as a baseline (often the first point), and then calculate and plot the relative changes from that baseline value. If we take our first year as baseline, then we can have a scale-free measure of growth to compare.\nThe plot of the standardised data can be obtained with the following code:\n\nincome$L90rel &lt;- 100*(income$L90/income$L90[1]-1)\nincome$U01rel &lt;- 100*(income$U01/income$U01[1]-1)\nplot(y=income$U01rel, x=income$year,ty='l')\nlines(y=income$L90rel, x=income$year,ty='l',col='red')\n\n\n\n\n\n\n\n\nHere we see the growth relative to the 1913 levels more clearly: both groups changed similarly up to mid 1940s (end of WWI), after this point the income of L90 grew more rapidly until the 1980s, where the U01 income increased dramatically and by the early 2000s the U01 group had income growth larger than the L90 (despite starting from a higher baseline!)\nWhen we have two associated time series, it can be interesting to see how they vary jointly over time. For this we can use a form of scatterplot. However, given the dependence of both variables on time it is importance to connect the points so we can see where the series is coming from and heading to.\n\n\n\nReturn to the original value of the L90 and U01, and draw a scatterplot with L90 vertically against U01 horizontally. Use plot type ty='b' (b for ‘both’) to connect your points with lines. What features do you see?\nTo help explain this plot, let’s label the points with the years. The following code will add text labels next to each point, run it and then revisit your plot. See the help on text (type ?text) for details. text(x=income$U01,y=income$L90, labels=income$year, cex=0.5, pos=4)\nCan you identify the years corresponding to the clumps of points? What major events took place during these periods that may explain what we see?\nCan you create a factor variable with four levels corresponding to: “Before 1936”, “1936-1960”, “1961-1983”, “After 1983”? You can use the cut function here to split a numerical variable up into a categorical factor.\nUse this new factor to colour the points on your plot. What were the main changes in income growth during each of these periods?\n\n\n\n\nClick for solution\n\n\ngroups &lt;- cut(income$year, breaks=c(1900,1936,1960,1983,2020),\n              labels=c())\nplot(x=income$U01, y=income$L90, ty='b',pch=16, col=groups)\ntext(x=income$U01,y=income$L90, labels=income$year, cex=0.5, pos=4, col=(1:4)[groups])\n\n\n\n\n\n\n\n\nIncome for both groups struggles to grow much until the 30s, income growth stalls for all following WW2, and takes until the 60s to recover. L90 income grows steadily until the 70s, after which it grows much more slowly and the U01’s income grows substantially in the 80s/90s. Another stutter in income affects mostly the U01 in the 2000/2010s (financial crisis).\nIn theory, it should be possible for incomes to rise for everyone at the same time — for the gains of economic growth to be broadly distributed year after year. But the takeaway from these graphs is that since World War II, that’s never really happened in the U.S.\n\nNote that the analysis of multiple time series is a huge topic of its own: here we are just scratching the surface, but if you are interested to learn more about this I can point you to some references."
  },
  {
    "objectID": "Workshop3_TimeSeries.html#decomposing-time-series",
    "href": "Workshop3_TimeSeries.html#decomposing-time-series",
    "title": "Workshop 3 - Exploring Time Series Data",
    "section": "",
    "text": "When exploring a time series, we often notice that it follows a general trend (given by its smoothing), it often has a periodic, or seasonal, behaviour, and then there is some residual noise that might be a little or a lot, depending on the time series.\nLet us see an example using a famous data set.\n Download data: co2\nThe co2 data contains the average monthly concentrations of CO2 from 1959 recorded at the Mauna Loa observatory in Hawaii. Environmental time series like this one often have a very strong and regular pattern to them.\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l')\n\n\n\n\n\n\n\n\nThere’s a clear trend here, with a regular wiggle around it\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020))\n\n\n\n\n\n\n\n\nthe wiggle seems to repeat every year, peaking in summer and falling in winter.\nFor time series with a strong seasonal component it can be useful to look at a Seasonal Decomposition of Time Series by Loess, or STL. Essentially, we define a regular time series such as this in terms of three components:\n\nThe trend - the long-term smooth evolution\nThe seasonal component - a regular smooth variation about the trend\nThe residuals - random noise on top of everything.\n\nTo do a time series decomposition, we’ll need to turn the data into a ts (time series) object so it is recognised by R.\n\nco2ts &lt;- ts(data = co2$co2,\n          start = co2[1, 1:2], \n          end = co2[nrow(co2), 1:2],\n          frequency = 12)\n\nThen we can apply the time series decomposition\n\n decomp &lt;- stl(co2ts, s.window = \"periodic\")\n\nand plot the results:\n\nplot(decomp, col = 'blue')\n\n\n\n\n\n\n\n\nThe top panel gives the original data, the second panel gives the estimated regular periodic effect around the overall trend, the third panel shows the overall trend and the final panel shows the residuals (as data - trend - seasonal). The residuals are quite small, suggesting this decomposition describes the data well.\nTo visualise the time series and the trend in the same plot, we can run the following code:\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l')\n## draw a red line for the trend\nlines(y=decomp$time.series[,2],x=co2$decimaldate,col='red')\n\n\n\n\n\n\n\n\nWe can also add a blue line with the trend plus seasonal component, to check if this is a good fit for the whole time series.\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020)) ## use x-axis limits to zoom in\n\n\n\n\n\n\n\nplot(y=co2$co2,x=co2$decimaldate,ty='l', xlim=c(2010,2020), ylim=c(380,420))\nlines(y=decomp$time.series[,2],x=co2$decimaldate,col='red')\nlines(y=decomp$time.series[,1]+decomp$time.series[,2],\n      x=co2$decimaldate,col='blue')\n\n\n\n\n\n\n\n\nWe see some imperfections, but overall this looks good! Decomposing seasonal time series like this can be very effective, however it can be difficult to find such ideally-suited time series with such regular behaviour outside of highly-structured situations."
  },
  {
    "objectID": "Workshop3_TimeSeries.html#multiple-time-series",
    "href": "Workshop3_TimeSeries.html#multiple-time-series",
    "title": "Workshop 3 - Exploring Time Series Data",
    "section": "",
    "text": "In lectures, we saw that there are different kinds of multiple time series, and that visualisation methods for comparing them depend on their nature:\n\nRelated series for the same population - if possible, show the different time series within the same plot to ease comparison. Be careful of units!\nSeries for different subgroups - showing subgroups together helps draw comparisons. Are we interested in the values or the proportions of the subgroups?\nSeries with different scales - may need standardising to a common scale to show together, otherwise separate plots will be needed.\n\nIt is always a good idea to compare not only the full plots of these time series, but also their decomposition. In fact, there is a high chance of spurious correlations when it comes to comparisons between time series!\nLet’s see an example: in the economics data set, look at the two variables:\n\npce - personal consumption expenditures, in billions of dollars\npop - total population, in thousands\n\nTheir corresponding time series, plotted together, look like this\n\npar(cex=0.85,cex.axis=0.85,cex.lab=0.85)\nplot(x=economics$date,y=economics$pce, ty='l',col=\"red\",lwd=2,xlab='',ylab='', ylim=c(min(economics$pce),max(economics$pop)))\nlines(x=economics$date,y=economics$pop, ty='l',col=\"green\",lwd=2,xlab='',ylab='')\nlegend(x='topleft',col=2:5,lwd=2,pch=NA,legend=c('Consumption','Population'))\n\n\n\n\n\n\n\n\nOf course, this plot is a bad visualisation for many reasons: the units are different, the scale is different, and one series flatten the other so badly that we can barely notice the increasing trend of pce. In order to meaningfully compare the two time series, let us standardise the two variables:\n\npop_z &lt;- scale(economics$pop)\npce_z &lt;- scale(economics$pce)\n\nand re-plot the time series:\n\nx &lt;- economics$date\nylim &lt;- range(c(pop_z, pce_z), na.rm = TRUE)\n\nplot(x, pop_z, type = \"l\", ylim = ylim, ylab = \"z-score\", col=\"green\")\nlines(x, pce_z, col = \"red\")\nlegend(x='topleft',col=2:3,lwd=2,pch=NA,legend=c('Consumption','Population'))\n\n\n\n\n\n\n\n\nWe now see that both variables have a strong upward drift, but not quite in the same way. Now, if we compute the correlation of pop versus pce, we get\n\ncor(economics$pop, economics$pce)\n\n[1] 0.9872421\n\n\nso, population and consumption are almost perfectly correlated! However, it is important to understand why it is so: does the very high correlation result from the fact that one variable influences the other or only from the fact that both variables grow over time? In order to establish this, we need to compare the correlation of the different components of the time series.\n\n\n\nCreate two time series objects, one for the time series of economics$pop and one for the time series of economics$pce.\nApply STL decomposition to both economics$pop and economics$pce\nCompare the trends, seasonal components and residuals of the two time series. What do you notice?\nCompute the correlation of trends, seasonal components and residuals. Does this help you establish if the overall correlation is simply the result of a time effect?\n\n\n\n\nClick for solution\n\nHere is the code for the STL decomposition\n\npop_ts &lt;- ts(as.numeric(scale(economics$pop)), frequency = 12)\npce_ts &lt;- ts(as.numeric(scale(economics$pce)), frequency = 12)\n\npop_decomp &lt;- stl(pop_ts, s.window = \"periodic\")\npce_decomp &lt;- stl(pce_ts, s.window = \"periodic\")\n\nHere is the plot of trends\n\nplot(y=pop_decomp$time.series[,2], x=economics$date, ty='l', col=\"green\")\nlines(y=pce_decomp$time.series[,2], x=economics$date, col=\"red\")\n\n\n\n\n\n\n\n\nHere is the plot of seasonal components. Since this is periodic, it makes sense to look at this over a limited period of time\n\nplot(y=pop_decomp$time.series[,1], x=economics$date, ty='l', col=\"green\", xlim=c(1,1000))\nlines(y=pce_decomp$time.series[,1], x=economics$date, col=\"red\")\n\n\n\n\n\n\n\n\nThere is a bit of lagged correlation, but an essentially different seasonal pattern.\nThe plot of residuals is better done as a scatterplot:\n\nplot(y=pop_decomp$time.series[,3], x=pce_decomp$time.series[,3], ty='p', col=\"blue\", pch=16)\n\n\n\n\n\n\n\n\nThis shows no association of residuals. In summary, almost all the correlation between the two time series is induced by time. We can only conclude that both variables have a constant increasing trend with time, and that’s why they are correlated. In other words, this is a spurious correlation.\nAs a sanity check, let us compute the correlation coefficient of the components.\n\ncor(x=pop_decomp$time.series[,1], y=pce_decomp$time.series[,1])\n\n[1] 0.5350119\n\ncor(x=pop_decomp$time.series[,2], y=pce_decomp$time.series[,2])\n\n[1] 0.9873021\n\ncor(x=pop_decomp$time.series[,3], y=pce_decomp$time.series[,3])\n\n[1] 0.1676072\n\n\nThe only meaningful correlation is the one between trends!"
  },
  {
    "objectID": "Lecture4b_MissingData.html",
    "href": "Lecture4b_MissingData.html",
    "title": "Lecture 4b - Missing Data Analysis",
    "section": "",
    "text": "Good analyses depend on good data.\n\n\nMissing values in data sets can pose problems as many statistical methods require a complete set of data. Small numbers of missing values can often just be ignored, if the same is large enough. In other cases, the missing values can be replaced or estimated (“imputed”). Data can be missing for many reasons:\n\nIt wasn’t recorded, there was an error, it was lost or incorrectly transcribed\nThe value didn’t fit into one of the pre-defined categories\nIn a survey, the answer was “don’t know” or the interviewee didn’t answer\nThe individual item/person could not be found\nAnd many others!\n\nA key question is often whether the data values are missing purely by chance, or whether the “missingness” is related to some other aspect of the problem is also important. For example, in a medical trial a patient could be too unwell to attend an appointment and so some of their data will be missing. However, if their absence is due to their treatment or the trial then the absence of these values is actually informative as the patient is adversely affected, even though we don’t have the numbers to go with it.\nWhile we won’t go into much depth on missing data analysis (this is a subject of its own), we will look at some mechanisms to visualise missing data and to help us detect patterns or features.\nFirst, we can make some simple overview plots of the occurrence of missing values in the data. To do this, let’s revisit the data on the Olympic athletes. If we look at the first few rows of data, we note that the symbol NA appears where we might expect data. This indicates that the value is missing.\n\nlibrary(VGAMdata)\n\nLoading required package: VGAM\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\n\nAttaching package: 'VGAMdata'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dlaplace, dtriangle, laplace, plaplace, ptriangle, qlaplace,\n    qtriangle, rlaplace, rtriangle\n\ndata(oly12)\nhead(oly12)[,1:7]\n\n              Name                    Country Age Height Weight Sex        DOB\n1         Lamusi A People's Republic of China  23   1.70     60   M 1989-02-06\n2       A G Kruger   United States of America  33   1.93    125   M       &lt;NA&gt;\n3   Jamale Aarrass                     France  30   1.87     76   M       &lt;NA&gt;\n4 Abdelhak Aatakni                    Morocco  24     NA     NA   M 1988-09-02\n5  Maria Abakumova         Russian Federation  26   1.78     85   F       &lt;NA&gt;\n6        Luc Abalo                     France  27   1.82     80   M 1984-06-09\n\n\nHere we explore the level of “missingness” by variable for the London Olympic athletes:\n\nlibrary(VGAMdata)\ndata(oly12)\nlibrary(naniar)\ngg_miss_var(oly12,show_pct = TRUE)\n\n\n\n\n\n\n\n\nThe plot is quite simple, each variable is displayed down the left axis and a line and point have been drawn to indicate the percentage of observations of that variable that are missing. Here we see that almost 60% of the DOB (date of birth) data is missing! This is followed by Weight and Height. The other variables are complete and not missing any data.\nWe could also consider missingness for each case in the data, rather than the variables, giving the following plot with a similar interpretation:\n\ngg_miss_case(oly12, order_cases = TRUE)\n\n\n\n\n\n\n\n\nHere, we set order_cases = TRUE so that the cases are ordered with those missing the most data at the top, and the least at the bottom. For this plot, the x-axis represents the number of measurements that are missing for that data item. We can see that a small fraction of the data are missing 3 values (the previous plot tells us that these must be DOB, Height and Weight), a slightly larger fraction are missing 2 values, and the majority of the data are missing one value - again this is almost certainly DOB. Only a minority of data cases are complete.\nWhen confronted with missing data in a data set, it can be tempting to simply discard any data cases which are missing values. If we were to do that for this problem, we would throw away the majority of the data!\nA combination of these ideas can be performed in a similar way to a heatmap, giving an excellent first look at a data set:\n\nlibrary(visdat)\nvis_dat(oly12)\n\n\n\n\n\n\n\n\nHere we have variables as columns and observations as rows, and each cell is coloured according to the type of data (numeric, integer, factor=categorical, date) or shaded grey for missing (NA). This is good for a quick overview of the data. A simpler version that just highlights the missing values is:\n\nvis_miss(oly12)\n\n\n\n\n\n\n\n\nValues can be missing one more that one variable at once. In some cases, which values are missing is purely random, but in others there can be patterns in the missingness. For instance, the dataset could be constructed by taking variables from multiple sources, and if an individual is missing entries in one of those sources then all of the variables from that source will be missing. Exploring these patterns of missingness can reveal interesting findings, and identify issues with the data collection.\nWe can explore patterns of missing values graphically with plots like the following\n\nlibrary(mice)\nmd.pattern(oly12)\n\n\n\n\n\n\n\n\nHere we plot the missing data patterns found in the Olympic athletes data. It has identified 8 patterns of missingness, indicated by the rows of the grid. The columns of the grid represent the variables. Where data are complete we have a blue square, and where the data is missing we have a red square. The first row has no red squares and represents the observations that are completed - we have 3648 of these, as indicated in the label to the left. The next row is missing only DOB, as indicated by the red square in the final column, and we have 5390 such cases. At the bottom, we find 289 cases which are missing all of DOB, Height, and Weight.\n\n\nThe simplest and most crudest mechanism is to discard the missing data - the na.omit function will do this. However, this is only justifiable only if a small number or proportion of values are missing. The na.omitfunction will discard entire data points if only one it’s values is missing - be careful!\nOften, unless the missingness affects a great many variables and a substantial portion of the data then it is better and safer to simply ‘work around’ the missing parts of the data. More sophisticated solutions impute the missing values from the information available to fill in the holes and create a complete data set - see impute in the mice package.\nAs we have seen, exploring the patterns in the missingness can reveal problems in data collection, or issues with recording particular variables. It can also be worth exploring whether the presence of missing values depends on other variables in the data - e.g. are we missing patient data because the patient is too ill to attend an appointment (and it this related to their treatment).\n\n\n\n\nThe way we deal with missing values depends on our aim and on the missing mechanism Many visualisation methods can work around missing values and just ignore them * The simplest (and most risky) mechanism is to discard the missing data - na.omit * Justifiable only if MCAR and a small proportion of values are missing Be careful as na.omit will discard all the entire observations even if only one of it’s values is missing More sophisticated solutions impute the missing values and replace them with estimates based on the other information available\nIn single imputation, missing values are replaced separately. It can be of two kinds: 1. Nonpredictive imputation Averaging techniques replace missing values with a summary statistics (usually mean, median or mode) For time series, replacing missing values with the nearest observation (LOCF, NOCB) is more appropriate 2. Predictive imputation Replaces missing values with predicted scores. These might arise from a regression equation, or nonparametric techniques such as k-Nearest Neighbours.\nImputation by mean (or mode or median) works by replacing all the missing values with the mean (or mode or median) of their variable. Mean imputation can only be used with numeric data, categorical data requires mode.\nPros: It’s easy and fast, and preserves the means of data Cons: Reduces data variance; doesn’t account for the uncertainty in the imputations; lacks accuracy.\nTime series data may contain time trends or seasonality, all of which should be addressed when imputing missing data. Carrying the last observation forward (LOCF) or carrying the next observation backward (NOCB) use last observed data value as a replacement for missing data. Linear interpolation is sometimes used when many data points are missing.\nPros: Easy implementation, effective on time series. Cons: Can results in biased estimates, affects modeling, may incorrectly suggest stability of time data.\nIn regression imputation, the existing variables are used to make a prediction, then the predicted value is substituted as if an actual obtained value.\nAssume that the variable Y has missing values and the variable X has no missing values. Then we can estimate a linear model \\[Y = \\beta_1 X + \\beta_2\\] so that the imputed value for the missing i-th observation is\n\\[\\hat{y_i} = \\hat{\\beta}_1 x_i + \\hat{\\beta}_2\\]\nPros: Uses information from the observed data, gives better results than averaging methods. Cons: over-estimates model fit and correlation estimates, weakens variance.\nK-nearest neighbours is an algorithm used for simple classification or regression. It uses feature similarity to predict the values of new data points. K-NN imputation works by finding the K closest neighbours to the observation with missing data and then imputing based on the non-missing values in the neighbourhood.\nPros: More accurate that most imputation methods; works with categorical data. Cons: Computationally expensive; sensitive to outliers; choice of K is not evident (bias-variance tradeoff).\n\n\nIn the London Olympics data, DOB missing is MCAR. Pairwise deletion seems to be the best approach for this variable.\n\n\n\nLet us create a toy data frame with missing values:\n\nset.seed(123)\nn &lt;- 100\ndat &lt;- data.frame(\n  y = rnorm(n),\n  x1 = rnorm(n),\n  x2 = rnorm(n)\n)\n\ndat$y[sample(n, 20)] &lt;- NA\n\nThe code for mean/median imputation then is\n\ndat$y_mean &lt;- ifelse(is.na(dat$y),\n                     mean(dat$y, na.rm = TRUE),\n                     dat$y)\n\ndat$y_median &lt;- ifelse(is.na(dat$y),\n                       median(dat$y, na.rm = TRUE),\n                       dat$y)\n\nIf we assume our dataset is a time series, then we can apply LOCF, NOCB or linear interpolation using the zoo package\n\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\ndat$y_locf  &lt;- na.locf(dat$y, na.rm = FALSE)\ndat$y_nocb  &lt;- na.locf(dat$y, fromLast = TRUE, na.rm = FALSE)\ndat$y_interp &lt;- na.approx(dat$y, na.rm = FALSE)\n\nRegression interpolation can be of deterministic type\n\nfit &lt;- lm(y ~ x1 + x2, data = dat)\ny_hat &lt;- predict(fit, newdata = dat)\n\ndat$y_reg &lt;- ifelse(is.na(dat$y), y_hat, dat$y)\n\nor in alternative, it can be of stochastic type (i.e. noise is added to the linear model). This has the effect of taking into account the fact that the model may not be a perfect fit.\n\nsigma &lt;- summary(fit)$sigma\ndat$y_reg_noise &lt;- ifelse(is.na(dat$y),\n                          y_hat + rnorm(sum(is.na(dat$y)), 0, sigma),\n                          dat$y)\n\nk-Nearest Neighbour imputation can be done using the VIM package\n\nlibrary(VIM)\n\ndat_knn &lt;- kNN(dat, variable = \"y\", k = 5)\ndat$y_knn &lt;- dat_knn$y\n\nor using the DMwR2 package\n\nlibrary(DMwR2)\n\ndat$y_knn2 &lt;- knnImputation(dat[, c(\"y\", \"x1\", \"x2\")], k = 5)$y\n\n\n\n\n\nMultiple imputation (MI) replaces each missing value with several plausible draws, creating multiple completed datasets. Each dataset is analyzed separately, and results are combined using Rubin’s rules, which propagate uncertainty due to missingness into standard errors and confidence intervals.\nMultiple imputation is a wide and complex subject. If you want to know more about it, there are many resources out there. I recommend the Handbook of missing data.\nThe goal of MI is not to recover “true” missing values, but rather to: * Preserve the joint distribution of the data * Preserve relationships between variables (means, variances, correlations, regression slopes, …) * Correctly reflect uncertainty due to missingness in downstream inference.\nMI returns a predicted distribution of missing values given the observed values.\nMI is an appropriate method in the following situations. Use multiple imputation when:\n\nMAR is plausible (missingness depends on observed data)\nYou care about valid inference (SEs, p-values, CIs, …)\nMissingness is non-trivial (not just a few percent)\nYou can reasonably model relationships among variables\n\nBut be careful: multiple imputation is not a cure for MNAR without extra assumptions!\nOne of the most popular algorithm for multiple imputation is Multiple Imputation by Chained Equations (MICE). This begins by creating several completed versions of the dataset through an iterative imputation process. For each variable with missing values, an appropriate conditional model is specified (for example, linear regression for continuous variables or logistic regression for binary variables). The algorithm cycles through these models repeatedly, drawing parameters and imputing missing values stochastically, until convergence is reached. Repeating this process \\(M\\) times yields \\(M\\) imputed datasets that reflect uncertainty about the missing data under the Missing At Random (MAR) assumption.\nEach imputed dataset is then analyzed separately using the same statistical model of interest. This produces \\(M\\) sets of parameter estimates and standard errors, which are subsequently combined using Rubin’s rules. Pooling accounts for both within-dataset variability and between-imputation variability, ensuring that standard errors and confidence intervals correctly reflect uncertainty due to missingness. Inference is therefore based on the pooled results rather than on any single imputed dataset.\nAfter fitting the analysis model to each of the \\(M\\) imputed datasets, Rubin’s rules are used to combine the results into a single set of estimates. Let \\(\\hat{\\theta_1}, \\dots ,\\hat{\\theta_M}\\) denote the parameter estimates from each imputed dataset, with corresponding variance estimates \\(V_1, \\dots ,V_M\\). The pooled point estimate is given by \\[\\bar{\\theta}=\\frac{1}{M} \\sum_{j=1}^M \\hat{\\theta_j}\\] which represents the average of the estimates across imputations. Uncertainty is decomposed into the within-imputation variance \\[\\bar{U}=\\frac{1}{M} \\sum_{j=1}^M U_j\\] and the between-imputation variance \\[B=\\frac{1}{M-1} \\sum_{j=1}^M (\\hat{\\theta_j}-\\bar{\\theta})^2.\\]\nThe total variance of the pooled estimate combines these two sources of uncertainty and is defined as \\[T=\\bar{U} + \\left(1+ \\frac{1}{M}\\right) B.\\]\nThe square root of \\(T\\) provides the standard error used for inference, ensuring that uncertainty due to missing data is properly reflected. As the number of imputations \\(M\\) increases, the term \\(\\frac{1}{M}\\) becomes negligible, and the total variance stabilizes. Rubin’s rules are valid under the assumption that the imputation model is correctly specified and that missingness is Missing At Random (MAR), allowing standard confidence intervals and hypothesis tests to be constructed from the pooled results.\nAn example of MICE will be covered in Practical 4, with the AirQuality data.\n\ndata(airquality)\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nThe missing pattern of this dataset looks appropriate for multiple imputation: data is missing from more than one variable, but there are no data points that have missing values for all variables.\n\nmd.pattern(airquality)\n\n\n\n\n\n\n\n\n    Wind Temp Month Day Solar.R Ozone   \n111    1    1     1   1       1     1  0\n35     1    1     1   1       1     0  1\n5      1    1     1   1       0     1  1\n2      1    1     1   1       0     0  2\n       0    0     0   0       7    37 44\n\n\nA marginplot shows that predictive methods shall be effective:\n\nmarginplot(airquality[, c(\"Solar.R\", \"Ozone\")], col = c(2:3), \n           cex.numbers = 1.2, pch = 19)\n\n\n\n\n\n\n\n\nThe estimated densities of the mean-imputed variables are quite different from the original ones.\n\nimp_mean &lt;- mice(airquality, method = \"mean\",m = 1,maxit = 1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\ndensityplot(imp_mean)\n\n\n\n\n\n\n\n\nThe estimated densities of the mice-imputed variables (M=10) exhibit some variability.\n\nimp_pred &lt;- mice(airquality, method = \"norm\",m = 10, maxit = 1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  1   2  Ozone  Solar.R\n  1   3  Ozone  Solar.R\n  1   4  Ozone  Solar.R\n  1   5  Ozone  Solar.R\n  1   6  Ozone  Solar.R\n  1   7  Ozone  Solar.R\n  1   8  Ozone  Solar.R\n  1   9  Ozone  Solar.R\n  1   10  Ozone  Solar.R\n\ndensityplot(imp_pred)\n\n\n\n\n\n\n\n\nIn order to perform MICE, the first step is to generate the number M of imputed datasets.\n\nimp &lt;- mice(airquality, method = \"pmm\", m = 10)\n\nThe output of mice() is a structured object that contains: * the original incomplete data, * all imputed values, * metadata needed to reproduce, analyze, and diagnose the imputations.\nTo extract the complete imputed data, we use complete(). We can extract complete data for a single dataset (e.g. the first one) or for all of them.\n\ndat1 &lt;- complete(imp, 1)  \ndat2 &lt;- complete(imp, c(1,2))  \nall_dat &lt;- complete(imp, \"all\") \n\nTo fit models across imputations, we use with(). This applies the same analysis to the M imputed datasets, then returns an object with M sets of estimates.\n\nfit &lt;- with(imp, lm(Ozone ~ Temp + Wind))\n\nFinally, to combine results, we use pool(). This pools parameter estimates and standard errors, applying Rubin’s rules automatically.\n\npooled &lt;- pool(fit)\nsummary(pooled)\n\n         term   estimate  std.error statistic       df      p.value\n1 (Intercept) -67.032104 23.0990182 -2.901946 60.06948 5.177607e-03\n2        Temp   1.781996  0.2486444  7.166846 57.36561 1.624303e-09\n3        Wind  -2.919563  0.6367257 -4.585275 78.37782 1.690700e-05"
  },
  {
    "objectID": "Lecture4b_MissingData.html#missing-data",
    "href": "Lecture4b_MissingData.html#missing-data",
    "title": "Lecture 4b - Missing Data Analysis",
    "section": "",
    "text": "Missing values in data sets can pose problems as many statistical methods require a complete set of data. Small numbers of missing values can often just be ignored, if the same is large enough. In other cases, the missing values can be replaced or estimated (“imputed”). Data can be missing for many reasons:\n\nIt wasn’t recorded, there was an error, it was lost or incorrectly transcribed\nThe value didn’t fit into one of the pre-defined categories\nIn a survey, the answer was “don’t know” or the interviewee didn’t answer\nThe individual item/person could not be found\nAnd many others!\n\nA key question is often whether the data values are missing purely by chance, or whether the “missingness” is related to some other aspect of the problem is also important. For example, in a medical trial a patient could be too unwell to attend an appointment and so some of their data will be missing. However, if their absence is due to their treatment or the trial then the absence of these values is actually informative as the patient is adversely affected, even though we don’t have the numbers to go with it.\nWhile we won’t go into much depth on missing data analysis (this is a subject of its own), we will look at some mechanisms to visualise missing data and to help us detect patterns or features.\nFirst, we can make some simple overview plots of the occurrence of missing values in the data. To do this, let’s revisit the data on the Olympic athletes. If we look at the first few rows of data, we note that the symbol NA appears where we might expect data. This indicates that the value is missing.\n\nlibrary(VGAMdata)\n\nLoading required package: VGAM\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\n\nAttaching package: 'VGAMdata'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dlaplace, dtriangle, laplace, plaplace, ptriangle, qlaplace,\n    qtriangle, rlaplace, rtriangle\n\ndata(oly12)\nhead(oly12)[,1:7]\n\n              Name                    Country Age Height Weight Sex        DOB\n1         Lamusi A People's Republic of China  23   1.70     60   M 1989-02-06\n2       A G Kruger   United States of America  33   1.93    125   M       &lt;NA&gt;\n3   Jamale Aarrass                     France  30   1.87     76   M       &lt;NA&gt;\n4 Abdelhak Aatakni                    Morocco  24     NA     NA   M 1988-09-02\n5  Maria Abakumova         Russian Federation  26   1.78     85   F       &lt;NA&gt;\n6        Luc Abalo                     France  27   1.82     80   M 1984-06-09\n\n\nHere we explore the level of “missingness” by variable for the London Olympic athletes:\n\nlibrary(VGAMdata)\ndata(oly12)\nlibrary(naniar)\ngg_miss_var(oly12,show_pct = TRUE)\n\n\n\n\n\n\n\n\nThe plot is quite simple, each variable is displayed down the left axis and a line and point have been drawn to indicate the percentage of observations of that variable that are missing. Here we see that almost 60% of the DOB (date of birth) data is missing! This is followed by Weight and Height. The other variables are complete and not missing any data.\nWe could also consider missingness for each case in the data, rather than the variables, giving the following plot with a similar interpretation:\n\ngg_miss_case(oly12, order_cases = TRUE)\n\n\n\n\n\n\n\n\nHere, we set order_cases = TRUE so that the cases are ordered with those missing the most data at the top, and the least at the bottom. For this plot, the x-axis represents the number of measurements that are missing for that data item. We can see that a small fraction of the data are missing 3 values (the previous plot tells us that these must be DOB, Height and Weight), a slightly larger fraction are missing 2 values, and the majority of the data are missing one value - again this is almost certainly DOB. Only a minority of data cases are complete.\nWhen confronted with missing data in a data set, it can be tempting to simply discard any data cases which are missing values. If we were to do that for this problem, we would throw away the majority of the data!\nA combination of these ideas can be performed in a similar way to a heatmap, giving an excellent first look at a data set:\n\nlibrary(visdat)\nvis_dat(oly12)\n\n\n\n\n\n\n\n\nHere we have variables as columns and observations as rows, and each cell is coloured according to the type of data (numeric, integer, factor=categorical, date) or shaded grey for missing (NA). This is good for a quick overview of the data. A simpler version that just highlights the missing values is:\n\nvis_miss(oly12)\n\n\n\n\n\n\n\n\nValues can be missing one more that one variable at once. In some cases, which values are missing is purely random, but in others there can be patterns in the missingness. For instance, the dataset could be constructed by taking variables from multiple sources, and if an individual is missing entries in one of those sources then all of the variables from that source will be missing. Exploring these patterns of missingness can reveal interesting findings, and identify issues with the data collection.\nWe can explore patterns of missing values graphically with plots like the following\n\nlibrary(mice)\nmd.pattern(oly12)\n\n\n\n\n\n\n\n\nHere we plot the missing data patterns found in the Olympic athletes data. It has identified 8 patterns of missingness, indicated by the rows of the grid. The columns of the grid represent the variables. Where data are complete we have a blue square, and where the data is missing we have a red square. The first row has no red squares and represents the observations that are completed - we have 3648 of these, as indicated in the label to the left. The next row is missing only DOB, as indicated by the red square in the final column, and we have 5390 such cases. At the bottom, we find 289 cases which are missing all of DOB, Height, and Weight.\n\n\nThe simplest and most crudest mechanism is to discard the missing data - the na.omit function will do this. However, this is only justifiable only if a small number or proportion of values are missing. The na.omitfunction will discard entire data points if only one it’s values is missing - be careful!\nOften, unless the missingness affects a great many variables and a substantial portion of the data then it is better and safer to simply ‘work around’ the missing parts of the data. More sophisticated solutions impute the missing values from the information available to fill in the holes and create a complete data set - see impute in the mice package.\nAs we have seen, exploring the patterns in the missingness can reveal problems in data collection, or issues with recording particular variables. It can also be worth exploring whether the presence of missing values depends on other variables in the data - e.g. are we missing patient data because the patient is too ill to attend an appointment (and it this related to their treatment)."
  },
  {
    "objectID": "Lecture4b_MissingData.html#single-imputation",
    "href": "Lecture4b_MissingData.html#single-imputation",
    "title": "Lecture 4b - Missing Data Analysis",
    "section": "",
    "text": "The way we deal with missing values depends on our aim and on the missing mechanism Many visualisation methods can work around missing values and just ignore them * The simplest (and most risky) mechanism is to discard the missing data - na.omit * Justifiable only if MCAR and a small proportion of values are missing Be careful as na.omit will discard all the entire observations even if only one of it’s values is missing More sophisticated solutions impute the missing values and replace them with estimates based on the other information available\nIn single imputation, missing values are replaced separately. It can be of two kinds: 1. Nonpredictive imputation Averaging techniques replace missing values with a summary statistics (usually mean, median or mode) For time series, replacing missing values with the nearest observation (LOCF, NOCB) is more appropriate 2. Predictive imputation Replaces missing values with predicted scores. These might arise from a regression equation, or nonparametric techniques such as k-Nearest Neighbours.\nImputation by mean (or mode or median) works by replacing all the missing values with the mean (or mode or median) of their variable. Mean imputation can only be used with numeric data, categorical data requires mode.\nPros: It’s easy and fast, and preserves the means of data Cons: Reduces data variance; doesn’t account for the uncertainty in the imputations; lacks accuracy.\nTime series data may contain time trends or seasonality, all of which should be addressed when imputing missing data. Carrying the last observation forward (LOCF) or carrying the next observation backward (NOCB) use last observed data value as a replacement for missing data. Linear interpolation is sometimes used when many data points are missing.\nPros: Easy implementation, effective on time series. Cons: Can results in biased estimates, affects modeling, may incorrectly suggest stability of time data.\nIn regression imputation, the existing variables are used to make a prediction, then the predicted value is substituted as if an actual obtained value.\nAssume that the variable Y has missing values and the variable X has no missing values. Then we can estimate a linear model \\[Y = \\beta_1 X + \\beta_2\\] so that the imputed value for the missing i-th observation is\n\\[\\hat{y_i} = \\hat{\\beta}_1 x_i + \\hat{\\beta}_2\\]\nPros: Uses information from the observed data, gives better results than averaging methods. Cons: over-estimates model fit and correlation estimates, weakens variance.\nK-nearest neighbours is an algorithm used for simple classification or regression. It uses feature similarity to predict the values of new data points. K-NN imputation works by finding the K closest neighbours to the observation with missing data and then imputing based on the non-missing values in the neighbourhood.\nPros: More accurate that most imputation methods; works with categorical data. Cons: Computationally expensive; sensitive to outliers; choice of K is not evident (bias-variance tradeoff).\n\n\nIn the London Olympics data, DOB missing is MCAR. Pairwise deletion seems to be the best approach for this variable.\n\n\n\nLet us create a toy data frame with missing values:\n\nset.seed(123)\nn &lt;- 100\ndat &lt;- data.frame(\n  y = rnorm(n),\n  x1 = rnorm(n),\n  x2 = rnorm(n)\n)\n\ndat$y[sample(n, 20)] &lt;- NA\n\nThe code for mean/median imputation then is\n\ndat$y_mean &lt;- ifelse(is.na(dat$y),\n                     mean(dat$y, na.rm = TRUE),\n                     dat$y)\n\ndat$y_median &lt;- ifelse(is.na(dat$y),\n                       median(dat$y, na.rm = TRUE),\n                       dat$y)\n\nIf we assume our dataset is a time series, then we can apply LOCF, NOCB or linear interpolation using the zoo package\n\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\ndat$y_locf  &lt;- na.locf(dat$y, na.rm = FALSE)\ndat$y_nocb  &lt;- na.locf(dat$y, fromLast = TRUE, na.rm = FALSE)\ndat$y_interp &lt;- na.approx(dat$y, na.rm = FALSE)\n\nRegression interpolation can be of deterministic type\n\nfit &lt;- lm(y ~ x1 + x2, data = dat)\ny_hat &lt;- predict(fit, newdata = dat)\n\ndat$y_reg &lt;- ifelse(is.na(dat$y), y_hat, dat$y)\n\nor in alternative, it can be of stochastic type (i.e. noise is added to the linear model). This has the effect of taking into account the fact that the model may not be a perfect fit.\n\nsigma &lt;- summary(fit)$sigma\ndat$y_reg_noise &lt;- ifelse(is.na(dat$y),\n                          y_hat + rnorm(sum(is.na(dat$y)), 0, sigma),\n                          dat$y)\n\nk-Nearest Neighbour imputation can be done using the VIM package\n\nlibrary(VIM)\n\ndat_knn &lt;- kNN(dat, variable = \"y\", k = 5)\ndat$y_knn &lt;- dat_knn$y\n\nor using the DMwR2 package\n\nlibrary(DMwR2)\n\ndat$y_knn2 &lt;- knnImputation(dat[, c(\"y\", \"x1\", \"x2\")], k = 5)$y"
  },
  {
    "objectID": "Lecture4b_MissingData.html#multiple-imputation",
    "href": "Lecture4b_MissingData.html#multiple-imputation",
    "title": "Lecture 4b - Missing Data Analysis",
    "section": "",
    "text": "Multiple imputation (MI) replaces each missing value with several plausible draws, creating multiple completed datasets. Each dataset is analyzed separately, and results are combined using Rubin’s rules, which propagate uncertainty due to missingness into standard errors and confidence intervals.\nMultiple imputation is a wide and complex subject. If you want to know more about it, there are many resources out there. I recommend the Handbook of missing data.\nThe goal of MI is not to recover “true” missing values, but rather to: * Preserve the joint distribution of the data * Preserve relationships between variables (means, variances, correlations, regression slopes, …) * Correctly reflect uncertainty due to missingness in downstream inference.\nMI returns a predicted distribution of missing values given the observed values.\nMI is an appropriate method in the following situations. Use multiple imputation when:\n\nMAR is plausible (missingness depends on observed data)\nYou care about valid inference (SEs, p-values, CIs, …)\nMissingness is non-trivial (not just a few percent)\nYou can reasonably model relationships among variables\n\nBut be careful: multiple imputation is not a cure for MNAR without extra assumptions!\nOne of the most popular algorithm for multiple imputation is Multiple Imputation by Chained Equations (MICE). This begins by creating several completed versions of the dataset through an iterative imputation process. For each variable with missing values, an appropriate conditional model is specified (for example, linear regression for continuous variables or logistic regression for binary variables). The algorithm cycles through these models repeatedly, drawing parameters and imputing missing values stochastically, until convergence is reached. Repeating this process \\(M\\) times yields \\(M\\) imputed datasets that reflect uncertainty about the missing data under the Missing At Random (MAR) assumption.\nEach imputed dataset is then analyzed separately using the same statistical model of interest. This produces \\(M\\) sets of parameter estimates and standard errors, which are subsequently combined using Rubin’s rules. Pooling accounts for both within-dataset variability and between-imputation variability, ensuring that standard errors and confidence intervals correctly reflect uncertainty due to missingness. Inference is therefore based on the pooled results rather than on any single imputed dataset.\nAfter fitting the analysis model to each of the \\(M\\) imputed datasets, Rubin’s rules are used to combine the results into a single set of estimates. Let \\(\\hat{\\theta_1}, \\dots ,\\hat{\\theta_M}\\) denote the parameter estimates from each imputed dataset, with corresponding variance estimates \\(V_1, \\dots ,V_M\\). The pooled point estimate is given by \\[\\bar{\\theta}=\\frac{1}{M} \\sum_{j=1}^M \\hat{\\theta_j}\\] which represents the average of the estimates across imputations. Uncertainty is decomposed into the within-imputation variance \\[\\bar{U}=\\frac{1}{M} \\sum_{j=1}^M U_j\\] and the between-imputation variance \\[B=\\frac{1}{M-1} \\sum_{j=1}^M (\\hat{\\theta_j}-\\bar{\\theta})^2.\\]\nThe total variance of the pooled estimate combines these two sources of uncertainty and is defined as \\[T=\\bar{U} + \\left(1+ \\frac{1}{M}\\right) B.\\]\nThe square root of \\(T\\) provides the standard error used for inference, ensuring that uncertainty due to missing data is properly reflected. As the number of imputations \\(M\\) increases, the term \\(\\frac{1}{M}\\) becomes negligible, and the total variance stabilizes. Rubin’s rules are valid under the assumption that the imputation model is correctly specified and that missingness is Missing At Random (MAR), allowing standard confidence intervals and hypothesis tests to be constructed from the pooled results.\nAn example of MICE will be covered in Practical 4, with the AirQuality data.\n\ndata(airquality)\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nThe missing pattern of this dataset looks appropriate for multiple imputation: data is missing from more than one variable, but there are no data points that have missing values for all variables.\n\nmd.pattern(airquality)\n\n\n\n\n\n\n\n\n    Wind Temp Month Day Solar.R Ozone   \n111    1    1     1   1       1     1  0\n35     1    1     1   1       1     0  1\n5      1    1     1   1       0     1  1\n2      1    1     1   1       0     0  2\n       0    0     0   0       7    37 44\n\n\nA marginplot shows that predictive methods shall be effective:\n\nmarginplot(airquality[, c(\"Solar.R\", \"Ozone\")], col = c(2:3), \n           cex.numbers = 1.2, pch = 19)\n\n\n\n\n\n\n\n\nThe estimated densities of the mean-imputed variables are quite different from the original ones.\n\nimp_mean &lt;- mice(airquality, method = \"mean\",m = 1,maxit = 1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\ndensityplot(imp_mean)\n\n\n\n\n\n\n\n\nThe estimated densities of the mice-imputed variables (M=10) exhibit some variability.\n\nimp_pred &lt;- mice(airquality, method = \"norm\",m = 10, maxit = 1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  1   2  Ozone  Solar.R\n  1   3  Ozone  Solar.R\n  1   4  Ozone  Solar.R\n  1   5  Ozone  Solar.R\n  1   6  Ozone  Solar.R\n  1   7  Ozone  Solar.R\n  1   8  Ozone  Solar.R\n  1   9  Ozone  Solar.R\n  1   10  Ozone  Solar.R\n\ndensityplot(imp_pred)\n\n\n\n\n\n\n\n\nIn order to perform MICE, the first step is to generate the number M of imputed datasets.\n\nimp &lt;- mice(airquality, method = \"pmm\", m = 10)\n\nThe output of mice() is a structured object that contains: * the original incomplete data, * all imputed values, * metadata needed to reproduce, analyze, and diagnose the imputations.\nTo extract the complete imputed data, we use complete(). We can extract complete data for a single dataset (e.g. the first one) or for all of them.\n\ndat1 &lt;- complete(imp, 1)  \ndat2 &lt;- complete(imp, c(1,2))  \nall_dat &lt;- complete(imp, \"all\") \n\nTo fit models across imputations, we use with(). This applies the same analysis to the M imputed datasets, then returns an object with M sets of estimates.\n\nfit &lt;- with(imp, lm(Ozone ~ Temp + Wind))\n\nFinally, to combine results, we use pool(). This pools parameter estimates and standard errors, applying Rubin’s rules automatically.\n\npooled &lt;- pool(fit)\nsummary(pooled)\n\n         term   estimate  std.error statistic       df      p.value\n1 (Intercept) -67.032104 23.0990182 -2.901946 60.06948 5.177607e-03\n2        Temp   1.781996  0.2486444  7.166846 57.36561 1.624303e-09\n3        Wind  -2.919563  0.6367257 -4.585275 78.37782 1.690700e-05"
  },
  {
    "objectID": "Lecture4a_CompDataQual.html",
    "href": "Lecture4a_CompDataQual.html",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "",
    "text": "We commonly need to compare different sets of data, and to explore and (try to) explain any differences.\nWith formal statistical comparisons, we have an explicit statement of what is to be compared in the form of hypotheses.\nA formal two-sample t-test comes with a lot of baggage and assumptions (Normality, equal variances), and only detects differences in means.\nThere are many features of the data that cannot be (easily) formally compared in such a way.\nThis should not stop us visualising the data, and plots can help us easily assess if our statistical assumptions are satisfied.\nWith graphical comparisons, you decide what is important depending on what you see and on what your expectations are.\nWith any graphic, we unconsciously make many comparisons and pick out unusual features or patterns.\nGraphical comparisons have a lot of flexibility, though there is less objectivity - statistical comparisons should still be made to investigate what we find.\nOne person’s “it is obvious from the plot that…” may be another’s “that could have arisen by chance and should be checked.”\n\nExample: Swiss Banknotes\nThe banknotes data are six measurements on each of 100 genuine and 100 forged Swiss banknotes.\nCarrying out t-tests for differences in means give different significances depending on the variables tested. Comparing genuine and forged notes using the Diagonal or Right measurements gives p-values of \\(&lt;2\\times 10^{-16}\\). This tells us that there is overwhelmingly strong evidence that the mean of these measurements is different in the different groups.\nWe can compare the distributions of these variables for the two groups by drawing comparable histograms.\n\n\nLoading required package: cluster\n\n\n\n\n\n\n\n\n\nNote that we have drawn the Diagonal variable in the left column, and the Right variabl in the right, and indicated the two groups of banknotes (Genuine vs Fake) with two histograms in each of the columns. The comparison to be made here is between the two histograms in each column. To make this\n\nWe use the same axis limits within each column, as they are plotting data for the same variable\nWe use the same break points of bars\nDraw the different groups vertically in the column. This means our axes and histogram bars will line up perfectly between the pairs of plots.\n\nThese simple adjustments ensure the similarities and differences between the plots can be easily, and that the comparisons are fair. If we overlooked using common axis, then the groups would not appear to be as different as they are and we would be distorting the presentation of the data. We should always seek to perform fair comparisons so that any differences we find should be due to the factor or feature we are interested in. In practice, there are many different way to ensure fairness of comparisons and they will vary from problem to problem.\n\nComparable populations - samples tell us about the populations they’re drawn from. Data need to come from sufficiently similar populations, otherwise we may find substantial differences just because the population is different.\nComparable variables - variable definitions can change over time. Ensure a common definition is used, and that any data that does not is either corrected appropriately or considered separately.\nComparable sources - data from different sources can be collected in different ways and by different rules. Different mechanisms of data gathering can generate different data and results, e.g. a question on a survey could be posed differently and prompt different responses.\nComparable groups - the individuals in each group should be similar enough that the only difference is the factor of interest. This is particularly true in medical applications, where you want to carefully control or account for any effects of age, gender, etc and so must have balanced representation of these different groups.\nComparable conditions - if the effects of a particular factor are to be investigated, the effects all other possible influences should be eliminated.\nStandardising data - sometimes data are standardised to enable comparison.\n\n\n\nMichelson’s data for the speed of light was gathered in 1879. Here we can compare the values to a known ‘true’ value.\nAs this data is attempting to measure a known value, we have a “gold standard” to compare against. We can easily indicate this graphically by adding reference lines to our plots.\nThe abline function can add vertical, horizontal or arbitrary lines to any standard plot:\n\nabline(h=10) - horizontal line at \\(y=10\\)\nabline(v=10) - vertical line at \\(x=10\\)\nabline(a=1,b=2) - straight line \\(y=1+2x\\)\n\nWe can customise the line by adding options:\n\nlty - line type, integer\nlwd - line width, integer\ncol - colour\n\n\nlibrary(HistData)\ndata(Michelson)\nhist(Michelson$velocity,xlab='Speed of light in km/sec (minus 299,000)',breaks=seq(600,1100,by=20),col=3,main='')\nabline(v=734.5, col=2,lty=2,lwd=3)\n\n\n\n\n\n\n\n\nDespite being gathered in 1879, the range of values does actually include the actual speed of light! But the variability in the measurements is quite large, and a 95% confidence interval for the mean would not include it!\n\n\n\nLet’s compare the miles per gallon of 32 American cars from 1973-4, to 93 cars in 1993. Here’s we’re combining two different datasets. As “miles per gallon” (MPG) is fairly simply defined, we will assume that the MPG variables in the two data sets are appropriately comparable.\nTo compare the MPG for the new versus old cars, we could draw:\n\nSide-by-side boxplots - compare the summaries on a common scale\nTwo histograms - if we use common axes ranges and bars, this would help compare the data distributions\n\nWhat might we expect to see here? We might expect that more modern cars would be more efficient and might have higher MPG vaules than the old ones.\nFirst, the boxplots:\n\nlibrary(MASS)\ndata(Cars93)\nlibrary(datasets)\ndata(mtcars)\nboxplot(mtcars$mpg, Cars93$MPG.city,col=3:2)\n\n\n\n\n\n\n\n\nBoxplots drawn like this automatically use a common axis scale, making comparison easier. We can see that the older cars (left) appear to have slightly lower MPG, but with a greater spread. The more modern cars (right) have slightly better and more consistent MPG, with some very efficient case appearing as outliers. Both distributions appear slightly skewed, with more lower values than higher.\nWe could draw the default histograms:\n\npar(mfrow=c(2,1))\nhist(mtcars$mpg,col=3,main='',xlab='mpg for 32 cars from 1973-4')\nhist(Cars93$MPG.city,col=2,main='',xlab='mpg in city driving for 93 cars from 1993')\n\n\n\n\n\n\n\n\nNote how the difference between the groups has been lost, as the individual plots are scaled to the ranges of the data and draw different numbers of bars. If we instead follow the advice from above, we get the following plots:\n\npar(mfrow=c(2,1))\nlibrary(MASS)\ndata(Cars93)\nlibrary(datasets)\ndata(mtcars)\nhist(mtcars$mpg,breaks=seq(10,50),col=3,main='',xlab='mpg for 32 cars from 1973-4')\nhist(Cars93$MPG.city,breaks=seq(10,50),col=2,main='',xlab='mpg in city driving for 93 cars from 1993')\n\n\n\n\n\n\n\n\n\nNote the same horizontal scale is used to facilitate comparison.\nPositioning histograms above each other makes identifying changes in location easier.\nThe modern cars appear to have slightly better performance.\nNote that the 1993 cars are for city driving, which is more demanding. The 1973 car data doesn’t distinguish.\nWe might also expect different results for European cars vs US cars.\n\n\n\n\nPerhaps the most common type of comparison is to compare subgroups in our data. Typically, these groups are defined by the values of a categorical variable.\n Download data: olives\nThe olives data contain data measuring the levels of various fatty acids in olives produced in different Italian regions and areas. * Area - a factor with levels Calabria, Coast-Sardinia, East-Liguria, Inland-Sardinia, North-Apulia, Sicily, South-Apulia, Umbria, West-Liguria * Region - a factor with levels North, Sardinia, South * palmitic, palmitoleic, stearic, oleic, linoleic, linolenic, arachidic, eicosenoic - all numeric measurements\nA working hypothesis is that olives grown in different parts of Italy will have different levels of fatty acids.\nLet’s focus on one measurement - palmitic. We can try what we did before and draw a column of three histograms, with common ranges and bars\n\npar(mfrow=c(3,1))\nhist(olives$palmitic[olives$Region==\"North\"], main='North',xlab='palmitic', col=2,breaks=seq(600,1800,by=50))\nhist(olives$palmitic[olives$Region==\"South\"], main='South',xlab='palmitic', col=3,breaks=seq(600,1800,by=50))\nhist(olives$palmitic[olives$Region==\"Sardinia\"], main='Sardinia',xlab='palmitic', col=4,breaks=seq(600,1800,by=50))\n\n\n\n\n\n\n\n\nThe North and Sardinia groups appear to behave relatively similarly, but the olives from the North have higher levels of this fatty acid.\nWe can visualise this with a boxplot too, which has a helpful shorthand syntax for plotting the effect of a categorical variables (Region) on a continuous one (palmitic)\n\nboxplot(palmitic~Region, data=olives, col=2:4)\n\n\n\n\n\n\n\n\nThis gives us a similar high-level overview of the main features from the histograms, though without some of the finer detail. It is also debatable whether we would consider many of the points indicated as outliers as being genuine given the histograms.\nIt can be beneficial to combine mutiple views and plots of the data, to get a fuller picture of the data.\n\n\n\n\n\n\n\n\n\nUnfortunately, drawing columns of histograms is only useful when you have a small number of groups to compare. Drawing columns of histograms is less feasible when the number of levels is large. For those cases, boxplots work particularly well to get an impression of the bigger picture. We can then follow this up with a closer look at the histograms of variables of interest.\nFor example, the variation in palmitic by the Area of origin reveals some substantially different behaviour in the different areas.\n\nboxplot(palmitic~Area,data=olives)\n\n\n\n\n\n\n\n\nWe could even use colour to indicate the other categorical variable, Region.\n\n\n\n\n\n\n\n\n\n\nSeparating categories of Area horizontally, and using colour to indicate Region allows for exploration of both variables at once.\nWe see differences in levels and variability of fatty acid content, with more variation observed in the areas in the South region.\n\n\n\nScatterplots can be used in similar ways to explore and compare the behaviour of sub-groups within the data in relation to two numerical variables. For instance, we can see how palmitic and palmitoleic behave and colour by Area to get:\n\nplot(x=olives$palmitic, y=olives$palmitoleic, col=olives$Area)\n\n\n\n\n\n\n\n\nWhile we can certainly identify some of the groups, not all a clearly visible or easy to distinguish from the other points. This is a problem when using colour to indicate many subgroups. Additionally, some colours like yellow are simply harder to see which also impacts our ability to interpret these results.\nA remedy to this issue is to draw a separate mini-scatterplot for each subgroup:\n\nlibrary(lattice)\nxyplot(palmitoleic~palmitic|Area, data=olives,pch=16)\n\n\n\n\n\n\n\n\nThis simplifies the problem of having to distinguish the differently coloured points, and produces a scatterplot of the data for each subgroup. It automatically uses a common x- and y-axis to ensure comparability in scale and location of the plotted points.\n\n\n\n\nAs we have been above, sometimes combining multiple different plots together can help emphasise key data features:\n\nSeveral plots of the same type for different groups - e.g. lattice/trellis/matrix plots, and scatterplot matrices are good for making comparisons\nSeveral plots of the same type for the same variable(s) - e.g. histograms with different bin widths. Groups and mode will stand out more on some plots than others.\nSeveral different plots for the same variable(s) - box plots, histograms, and density estimates emphasise different data features.\nSeveral plots of the same type for the different but comparable variable(s) - e.g histograms of exam marks for different subjects\n\n\n\nThese data show an overall summary of the measurements on 43 varieties of coffee beans of two varieties: Arabica and Robusta.\n\n\n\n\n\n\n\n\n\n\nClearly, most beans are Arabica.\nThe PCP shows clearly different behaviour between varieties, notably on Fat and Caffeine.\nThe scatterplot shows these differences more definitively.\n\nFinding the right combination of graphics also requires exploration and experimentation.\n\n\n\n\n\nGraph size - each graph should be the same size and aspect ratio for comparison\nCommon scaling - plots of the same/similar variables should use the same scales and axis ranges\nAlignment - displaying multiple plots in a column helps distinguish variation and shape, but in a row it is easier to compare levels of peaks and troughs.\nSingle or multiple plots - plotting all the data together is good unless the groups overlap; plotting the groups separately can help distinguish groups, unless there are a large number of groups.\nColour and shape - using colour, size and shape help distinguish different groups within a graph"
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#example-comparing-to-a-standard",
    "href": "Lecture4a_CompDataQual.html#example-comparing-to-a-standard",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "",
    "text": "Michelson’s data for the speed of light was gathered in 1879. Here we can compare the values to a known ‘true’ value.\nAs this data is attempting to measure a known value, we have a “gold standard” to compare against. We can easily indicate this graphically by adding reference lines to our plots.\nThe abline function can add vertical, horizontal or arbitrary lines to any standard plot:\n\nabline(h=10) - horizontal line at \\(y=10\\)\nabline(v=10) - vertical line at \\(x=10\\)\nabline(a=1,b=2) - straight line \\(y=1+2x\\)\n\nWe can customise the line by adding options:\n\nlty - line type, integer\nlwd - line width, integer\ncol - colour\n\n\nlibrary(HistData)\ndata(Michelson)\nhist(Michelson$velocity,xlab='Speed of light in km/sec (minus 299,000)',breaks=seq(600,1100,by=20),col=3,main='')\nabline(v=734.5, col=2,lty=2,lwd=3)\n\n\n\n\n\n\n\n\nDespite being gathered in 1879, the range of values does actually include the actual speed of light! But the variability in the measurements is quite large, and a 95% confidence interval for the mean would not include it!"
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#example-comparing-new-data-to-old",
    "href": "Lecture4a_CompDataQual.html#example-comparing-new-data-to-old",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "",
    "text": "Let’s compare the miles per gallon of 32 American cars from 1973-4, to 93 cars in 1993. Here’s we’re combining two different datasets. As “miles per gallon” (MPG) is fairly simply defined, we will assume that the MPG variables in the two data sets are appropriately comparable.\nTo compare the MPG for the new versus old cars, we could draw:\n\nSide-by-side boxplots - compare the summaries on a common scale\nTwo histograms - if we use common axes ranges and bars, this would help compare the data distributions\n\nWhat might we expect to see here? We might expect that more modern cars would be more efficient and might have higher MPG vaules than the old ones.\nFirst, the boxplots:\n\nlibrary(MASS)\ndata(Cars93)\nlibrary(datasets)\ndata(mtcars)\nboxplot(mtcars$mpg, Cars93$MPG.city,col=3:2)\n\n\n\n\n\n\n\n\nBoxplots drawn like this automatically use a common axis scale, making comparison easier. We can see that the older cars (left) appear to have slightly lower MPG, but with a greater spread. The more modern cars (right) have slightly better and more consistent MPG, with some very efficient case appearing as outliers. Both distributions appear slightly skewed, with more lower values than higher.\nWe could draw the default histograms:\n\npar(mfrow=c(2,1))\nhist(mtcars$mpg,col=3,main='',xlab='mpg for 32 cars from 1973-4')\nhist(Cars93$MPG.city,col=2,main='',xlab='mpg in city driving for 93 cars from 1993')\n\n\n\n\n\n\n\n\nNote how the difference between the groups has been lost, as the individual plots are scaled to the ranges of the data and draw different numbers of bars. If we instead follow the advice from above, we get the following plots:\n\npar(mfrow=c(2,1))\nlibrary(MASS)\ndata(Cars93)\nlibrary(datasets)\ndata(mtcars)\nhist(mtcars$mpg,breaks=seq(10,50),col=3,main='',xlab='mpg for 32 cars from 1973-4')\nhist(Cars93$MPG.city,breaks=seq(10,50),col=2,main='',xlab='mpg in city driving for 93 cars from 1993')\n\n\n\n\n\n\n\n\n\nNote the same horizontal scale is used to facilitate comparison.\nPositioning histograms above each other makes identifying changes in location easier.\nThe modern cars appear to have slightly better performance.\nNote that the 1993 cars are for city driving, which is more demanding. The 1973 car data doesn’t distinguish.\nWe might also expect different results for European cars vs US cars."
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#example-comparing-subgroups",
    "href": "Lecture4a_CompDataQual.html#example-comparing-subgroups",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "",
    "text": "Perhaps the most common type of comparison is to compare subgroups in our data. Typically, these groups are defined by the values of a categorical variable.\n Download data: olives\nThe olives data contain data measuring the levels of various fatty acids in olives produced in different Italian regions and areas. * Area - a factor with levels Calabria, Coast-Sardinia, East-Liguria, Inland-Sardinia, North-Apulia, Sicily, South-Apulia, Umbria, West-Liguria * Region - a factor with levels North, Sardinia, South * palmitic, palmitoleic, stearic, oleic, linoleic, linolenic, arachidic, eicosenoic - all numeric measurements\nA working hypothesis is that olives grown in different parts of Italy will have different levels of fatty acids.\nLet’s focus on one measurement - palmitic. We can try what we did before and draw a column of three histograms, with common ranges and bars\n\npar(mfrow=c(3,1))\nhist(olives$palmitic[olives$Region==\"North\"], main='North',xlab='palmitic', col=2,breaks=seq(600,1800,by=50))\nhist(olives$palmitic[olives$Region==\"South\"], main='South',xlab='palmitic', col=3,breaks=seq(600,1800,by=50))\nhist(olives$palmitic[olives$Region==\"Sardinia\"], main='Sardinia',xlab='palmitic', col=4,breaks=seq(600,1800,by=50))\n\n\n\n\n\n\n\n\nThe North and Sardinia groups appear to behave relatively similarly, but the olives from the North have higher levels of this fatty acid.\nWe can visualise this with a boxplot too, which has a helpful shorthand syntax for plotting the effect of a categorical variables (Region) on a continuous one (palmitic)\n\nboxplot(palmitic~Region, data=olives, col=2:4)\n\n\n\n\n\n\n\n\nThis gives us a similar high-level overview of the main features from the histograms, though without some of the finer detail. It is also debatable whether we would consider many of the points indicated as outliers as being genuine given the histograms.\nIt can be beneficial to combine mutiple views and plots of the data, to get a fuller picture of the data.\n\n\n\n\n\n\n\n\n\nUnfortunately, drawing columns of histograms is only useful when you have a small number of groups to compare. Drawing columns of histograms is less feasible when the number of levels is large. For those cases, boxplots work particularly well to get an impression of the bigger picture. We can then follow this up with a closer look at the histograms of variables of interest.\nFor example, the variation in palmitic by the Area of origin reveals some substantially different behaviour in the different areas.\n\nboxplot(palmitic~Area,data=olives)\n\n\n\n\n\n\n\n\nWe could even use colour to indicate the other categorical variable, Region.\n\n\n\n\n\n\n\n\n\n\nSeparating categories of Area horizontally, and using colour to indicate Region allows for exploration of both variables at once.\nWe see differences in levels and variability of fatty acid content, with more variation observed in the areas in the South region.\n\n\n\nScatterplots can be used in similar ways to explore and compare the behaviour of sub-groups within the data in relation to two numerical variables. For instance, we can see how palmitic and palmitoleic behave and colour by Area to get:\n\nplot(x=olives$palmitic, y=olives$palmitoleic, col=olives$Area)\n\n\n\n\n\n\n\n\nWhile we can certainly identify some of the groups, not all a clearly visible or easy to distinguish from the other points. This is a problem when using colour to indicate many subgroups. Additionally, some colours like yellow are simply harder to see which also impacts our ability to interpret these results.\nA remedy to this issue is to draw a separate mini-scatterplot for each subgroup:\n\nlibrary(lattice)\nxyplot(palmitoleic~palmitic|Area, data=olives,pch=16)\n\n\n\n\n\n\n\n\nThis simplifies the problem of having to distinguish the differently coloured points, and produces a scatterplot of the data for each subgroup. It automatically uses a common x- and y-axis to ensure comparability in scale and location of the plotted points."
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#combining-different-views-of-the-data",
    "href": "Lecture4a_CompDataQual.html#combining-different-views-of-the-data",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "",
    "text": "As we have been above, sometimes combining multiple different plots together can help emphasise key data features:\n\nSeveral plots of the same type for different groups - e.g. lattice/trellis/matrix plots, and scatterplot matrices are good for making comparisons\nSeveral plots of the same type for the same variable(s) - e.g. histograms with different bin widths. Groups and mode will stand out more on some plots than others.\nSeveral different plots for the same variable(s) - box plots, histograms, and density estimates emphasise different data features.\nSeveral plots of the same type for the different but comparable variable(s) - e.g histograms of exam marks for different subjects\n\n\n\nThese data show an overall summary of the measurements on 43 varieties of coffee beans of two varieties: Arabica and Robusta.\n\n\n\n\n\n\n\n\n\n\nClearly, most beans are Arabica.\nThe PCP shows clearly different behaviour between varieties, notably on Fat and Caffeine.\nThe scatterplot shows these differences more definitively.\n\nFinding the right combination of graphics also requires exploration and experimentation."
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#graphical-principles-for-comparisons",
    "href": "Lecture4a_CompDataQual.html#graphical-principles-for-comparisons",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "",
    "text": "Graph size - each graph should be the same size and aspect ratio for comparison\nCommon scaling - plots of the same/similar variables should use the same scales and axis ranges\nAlignment - displaying multiple plots in a column helps distinguish variation and shape, but in a row it is easier to compare levels of peaks and troughs.\nSingle or multiple plots - plotting all the data together is good unless the groups overlap; plotting the groups separately can help distinguish groups, unless there are a large number of groups.\nColour and shape - using colour, size and shape help distinguish different groups within a graph"
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#missing-values",
    "href": "Lecture4a_CompDataQual.html#missing-values",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "Missing values",
    "text": "Missing values\nMissing values in data sets can pose problems as many statistical methods require a complete set of data. Small numbers of missing values can often just be ignored, if the same is large enough. In other cases, the missing values can be replaced or estimated (“imputed”). Data can be missing for many reasons:\n\nIt wasn’t recorded, there was an error, it was lost or incorrectly transcribed\nThe value didn’t fit into one of the pre-defined categories\nIn a survey, the answer was “don’t know” or the interviewee didn’t answer\nThe individual item/person could not be found\n\nA key question is often whether the data values are missing purely by chance, or whether the “missingness” is related to some other aspect of the problem is also important. For example, in a medical trial a patient could be too unwell to attend an appointment and so some of their data will be missing. However, if their absence is due to their treatment or the trial then the absence of these values is actually informative as the patient is adversely affected, even though we don’t have the numbers to go with it.\nWhile we won’t go into much depth on missing data analysis (this is a subject of its own), we will look at some mechanisms to visualise missing data and to help us detect patterns or features.\nFirst, we can make some simple overview plots of the occurrence of missing values in the data. To do this, let’s revisit the data on the Olympic athletes. If we look at the first few rows of data, we note that the symbol NA appears where we might expect data. This indicates that the value is missing.\n\nlibrary(VGAMdata)\n\nLoading required package: VGAM\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\n\nAttaching package: 'VGAMdata'\n\n\nThe following objects are masked from 'package:VGAM':\n\n    dlaplace, dtriangle, laplace, plaplace, ptriangle, qlaplace,\n    qtriangle, rlaplace, rtriangle\n\ndata(oly12)\nhead(oly12)[,1:7]\n\n              Name                    Country Age Height Weight Sex        DOB\n1         Lamusi A People's Republic of China  23   1.70     60   M 1989-02-06\n2       A G Kruger   United States of America  33   1.93    125   M       &lt;NA&gt;\n3   Jamale Aarrass                     France  30   1.87     76   M       &lt;NA&gt;\n4 Abdelhak Aatakni                    Morocco  24     NA     NA   M 1988-09-02\n5  Maria Abakumova         Russian Federation  26   1.78     85   F       &lt;NA&gt;\n6        Luc Abalo                     France  27   1.82     80   M 1984-06-09\n\n\nHere we explore the level of “missingness” by variable for the London Olympic athletes:\n\nlibrary(VGAMdata)\ndata(oly12)\nlibrary(naniar)\ngg_miss_var(oly12,show_pct = TRUE)\n\n\n\n\n\n\n\n\nThe plot is quite simple, each variable is displayed down the left axis and a line and point have been drawn to indicate the percentage of observations of that variable that are missing. Here we see that almost 60% of the DOB (date of birth) data is missing! This is followed by Weight and Height. The other variables are complete and not missing any data.\nWe could also consider missingness for each case in the data, rather than the variables, giving the following plot with a similar interpretation:\n\ngg_miss_case(oly12, order_cases = TRUE)\n\n\n\n\n\n\n\n\nHere, we set order_cases = TRUE so that the cases are ordered with those missing the most data at the top, and the least at the bottom. For this plot, the x-axis represents the number of measurements that are missing for that data item. We can see that a small fraction of the data are missing 3 values (the previous plot tells us that these must be DOB, Height and Weight), a slightly larger fraction are missing 2 values, and the majority of the data are missing one value - again this is almost certainly DOB. Only a minority of data cases are complete.\nWhen confronted with missing data in a data set, it can be tempting to simply discard any data cases which are missing values. If we were to do that for this problem, we would throw away the majority of the data!\nA combination of these ideas can be performed in a similar way to a heatmap, giving an excellent first look at a data set:\n\nlibrary(visdat)\nvis_dat(oly12)\n\n\n\n\n\n\n\n\nHere we have variables as columns and observations as rows, and each cell is coloured according to the type of data (numeric, integer, factor=categorical, date) or shaded grey for missing (NA). This is good for a quick overview of the data. A simpler version that just highlights the missing values is:\n\nvis_miss(oly12)\n\n\n\n\n\n\n\n\nValues can be missing one more that one variable at once. In some cases, which values are missing is purely random, but in others there can be patterns in the missingness. For instance, the dataset could be constructed by taking variables from multiple sources, and if an individual is missing entries in one of those sources then all of the variables from that source will be missing. Exploring these patterns of missingness can reveal interesting findings, and identify issues with the data collection.\nWe can explore patterns of missing values graphically with plots like the following\n\nlibrary(mice)\nmd.pattern(oly12)\n\n\n\n\n\n\n\n\nHere we plot the missing data patterns found in the Olympic athletes data. It has identified 8 patterns of missingness, indicated by the rows of the grid. The columns of the grid represent the variables. Where data are complete we have a blue square, and where the data is missing we have a red square. The first row has no red squares and represents the observations that are completed - we have 3648 of these, as indicated in the label to the left. The next row is missing only DOB, as indicated by the red square in the final column, and we have 5390 such cases. At the bottom, we find 289 cases which are missing all of DOB, Height, and Weight.\n\nDealing with missing values\nThe simplest and most crudest mechanism is to discard the missing data - the na.omit function will do this. However, this is only justifiable only if a small number or proportion of values are missing. The na.omitfunction will discard entire data points if only one it’s values is missing - be careful!\nOften, unless the missingness affects a great many variables and a substantial portion of the data then it is better and safer to simply ‘work around’ the missing parts of the data. More sophisticated solutions impute the missing values from the information available to fill in the holes and create a complete data set - see impute in the mice package.\nAs we have seen, exploring the patterns in the missingness can reveal problems in data collection, or issues with recording particular variables. It can also be worth exploring whether the presence of missing values depends on other variables in the data - e.g. are we missing patient data because the patient is too ill to attend an appointment (and it this related to their treatment)."
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#outliers",
    "href": "Lecture4a_CompDataQual.html#outliers",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "Outliers",
    "text": "Outliers\nAn outlier is a data point that is far away from the bulk of the data. Outliers can arise due to\n\nerrors in data gathering/recording\ngenuine extreme values – e.g. water levels during a freak storm\nrare or unusual occurrences\ncombining data from different sources - different sources may have different variable definitions that need to be carefully reconciled.\n\nIdentification of outliers is important as:\n\nErrors in the data should be corrected or erroneous data marked as missing\nExtremes and unusual values are usually worth investigating\nMany statistical methods will work poorly in the presence of outliers.\n\nIn one dimension, an outlier is easy to graphically detect potential outliers as points far from the rest of the data. The usual rule is to mark potential outliers as being any points more than \\(1.5\\times IQR\\) beyond the upper and lower quartiles of the data. This corresponds to any data outside the whiskers of a box plot. However, this method struggles when the data sets are large as you naturally accumulated points outside of this range in bigger data sets as you see more observations.\n\nExample: US Crime Statistics\n\nlibrary(TeachingDemos)\ndata(USCrimes)\n\nThe USCrimes dataset contains historical crime statistics for all US states from 1960-2010. Let’s look at the 2010 Murder rate:\n\nusc2010 &lt;- data.frame(USCrimes[,\"2010\",])\nboxplot(usc2010$MurderRate)\n\n\n\n\n\n\n\n\nThe boxplot has flagged two unusual values. To identify the values, we have to go back to the data. The highest murder rate corresponds for Washington DC and is dramatically far above the others. The second largest is for Louisiana. We can repeat this process for all the individual variables, but it is time consuming to try and identify every individual outlier.\nIn general, the best strategy is to start with the most extreme points - e.g. Washington DC in the plot above - and work in towards the rest of the data.\nMost outliers are outliers on at least one univariate dimension, sometimes several. The more dimensions there are, the more likely it is we will find outliers on some variable or another - so the presence of outliers is, in fact, not that unusual and to be expected for bigger data sets. Unfortunately, the simple rule for identifying an outlier doesn’t really translate to many variables. Scatterplots and Parallel Coordinate Plots can help identify outliers on multiple variables.\nScatterplots can help identify unusual observations on pairs of variables:\n\nplot(x=usc2010$TheftRate,y=usc2010$VehicleTheftRate, pch=16,\n     xlab='Theft rate per 100,000', ylab='Vehicle theft rate per 100,000')\n\n\n\n\n\n\n\n\nWe can see another clear outlier in the top right corner - this is an outlier on both variables. Again, this is Washington DC. However, we do notice a pair of points with Theft rate of ~1600 and Vehicle theft of ~400. These points seem far away from the bulk of the rest of the points - the vehicle theft is unusually high for states with this level of theft. Are they outliers too?\nWe can apply any of our multivariate visualisations to look for unusual points across many variables. For example, the parallel coordinate plot quite easily singles out Washington DC for a high crime rate in most categories:"
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#example-diamonds",
    "href": "Lecture4a_CompDataQual.html#example-diamonds",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "Example: Diamonds",
    "text": "Example: Diamonds\n\nlibrary(ggplot2)\ndata(diamonds)\n\nThe diamonds data contain information on the prices and attributes of over 50,000 round cut diamonds. This is quite a large data set, so we would expect to see many outliers.\n\n\n\n\n\n\n\n\n\nAll of these variables are showing outliers to some degree or another! Let’s focus on the diamond width (y) against depth (z) in a scatterplot\n\nplot(x=diamonds$y, y=diamonds$z, pch=16, xlab='width', ylab='depth')\n\n\n\n\n\n\n\n\nWe can see there are clearly some points with implausible values, far away from the main data cloud. If we add transparency, we can see whether these outlying points correspond to one observation or many:\n\nlibrary(scales)\nplot(x=diamonds$y, y=diamonds$z, pch=16, xlab='width', ylab='depth', col=alpha('black',0.1))\n\n\n\n\n\n\n\n\nThe distant cases are clearly low in number, and the bulk of the data is centred on small values between 5 and 10 mm. We can see that there are two extremely large values of width at about 32 and 59. Bearing mind these are diamonds and these dimensions correspond to widths of 3cm and almost 6cm, these diamonds would be huge - but strangely their depth is normal. This suggests that this is probably an error and maybe they should be 3.2 and 5.9 instead. Similarly, we have one diamond with a depth of 32mm but a width of 5mm.\nAt the other end of the scale, we seem to have a non-trivial number of diamonds with 0 width and 0 depth, and some with positive width but 0 depth. These are probably indications that the values are not known and they should be treated as missing.\nWe could discard these unusual points, and re-focus our attention on the main data cloud:\n\nlibrary(scales)\nplot(x=diamonds$y, y=diamonds$z, pch=16, xlab='width', ylab='depth', col=alpha('black',0.1), xlim=c(3,11), ylim=c(1,7))\n\n\n\n\n\n\n\n\nThe plot is no longer dominated by the extremes and unusual values. However, zooming in like this does highlight some more points that may need closer scrutiny.\n\nDealing with outliers\nIdentifying outliers is relatively straightforward, but what to do next is less obvious. Clear outliers should be inspected and reviewed - are they genuinely surprising values or errors? Values which are clearly errors are often discarded (if they can’t be corrected) or treated as missing. ‘Ordinary’ outliers may be large but genuine - how to handle these depends on what you plan to do with the data next and what the goal is. If you’re analysing data on rainfall and flooding then you will need to pay much more attention to the unusually large values!\nA possible outlier strategy could go something like this: 1. Plot the 1-D distribution. Identify any extremes for possible errors that could be corrected. 1. Examine the values of any identified ‘extremes’ on other variables, using scatterplots and parallel coordinate plots. Consider removing cases which are outlying on more than one dimension. 1. Repeat for ‘ordinary’ outliers. 1. If the data has categorical or grouping variables, consider repeating for each group in the data."
  },
  {
    "objectID": "Lecture4a_CompDataQual.html#modelling-testing",
    "href": "Lecture4a_CompDataQual.html#modelling-testing",
    "title": "Lecture 4a - Comparisons and Data Quality",
    "section": "Modelling & Testing",
    "text": "Modelling & Testing\n\nComparing means - the standard t-test can be applied. Even if the standard assumptions are not ideally met, very low p-values are an indication of some form of difference.\nMore complex comparisons - either require more specialised tests, or visualising results from more specialised models (e.g. regressions).\nComparing rates - for a binary (two-category) dependent variable, logistic regression is a good approach.\nNonparametric methods - can be applied to problems where the ‘standard assumptions’ do not hold.\nMultiple testing - testing for several kinds of differences at once can easily and mistakenly lead to false positive results."
  },
  {
    "objectID": "Lab4_MissingData.html",
    "href": "Lab4_MissingData.html",
    "title": "Practical 4 - Missing Data Analysis",
    "section": "",
    "text": "This practical is designed to enhance your understanding of missing data mechanism and imputation techniques.\n\n\nBy the end of the lab, you will have acquired the following skills:\n\n\nPerform missing data analysis with listwise deletion\n\n\nDistinguish between different types of single imputation and how to check their effectiveness\n\n\nPerform multiple imputation using the mice algorithm\n\n\nAssess effectiveness of multiple imputation on a different range of data sets\n\n\n\n\n\nData sets with missing values are very common: missing data is a crucial issue in many disciplines such as social and medical sciences: questionnaire respondents do not answer every question; countries do not collect statistics every year; archives can be incomplete; subjects may drop out of longitudinal studies, and so on.\nThere are many packages for dealing with missing data in R. These include Amelia, missForest, Hmisc, mi and mice. We will focus on the use of mice, as this is the one discussed in lectures.\nYou will need to install the following packages:\n\nmice for missing data analysis\nVIM for the Visualisation and Imputation of Missing values\n\n\ninstall.packages(c(\"mice\",\"VIM\"))\n\nWe will work through an example of dealing with missing data using the nhanes (national health and nutrition examination survey) data. The data contains 25 observations on 4 variables. We will explore the data and use both single and multiple imputation techniques.\n\n\n\nWe first load the required packages and load the data set.\n\nlibrary(mice)\nlibrary(VIM)\ndata(nhanes)\nhead(nhanes)\n\n  age  bmi hyp chl\n1   1   NA  NA  NA\n2   2 22.7   1 187\n3   1   NA   1 187\n4   3   NA  NA  NA\n5   1 20.4   1 113\n6   3   NA  NA 184\n\nstr(nhanes)\n\n'data.frame':   25 obs. of  4 variables:\n $ age: num  1 2 1 3 1 3 1 1 2 2 ...\n $ bmi: num  NA 22.7 NA NA 20.4 NA 22.5 30.1 22 NA ...\n $ hyp: num  NA 1 1 NA 1 NA 1 1 1 NA ...\n $ chl: num  NA 187 187 NA 113 184 118 187 238 NA ...\n\n\nWe can explore the missing data pattern using md.pattern.\n\nmd.pattern(nhanes)\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27\n\n\n1’s and 0’s under each variable represent their presence and missing state respectively. There are 13 (out of 25) rows that are complete. There is one row for which only bmi is missing, and there are seven rows for which only age is known.\n\nmd.pairs(nhanes)\n\n$rr\n    age bmi hyp chl\nage  25  16  17  15\nbmi  16  16  16  13\nhyp  17  16  17  14\nchl  15  13  14  15\n\n$rm\n    age bmi hyp chl\nage   0   9   8  10\nbmi   0   0   0   3\nhyp   0   1   0   3\nchl   0   2   1   0\n\n$mr\n    age bmi hyp chl\nage   0   0   0   0\nbmi   9   0   1   2\nhyp   8   0   0   1\nchl  10   3   3   0\n\n$mm\n    age bmi hyp chl\nage   0   0   0   0\nbmi   0   9   8   7\nhyp   0   8   8   7\nchl   0   7   7  10\n\n\nThe matrices above represent the number of observations per patterns for all pairs of variables: * The rr matrix represents the number of observations where both pairs of values are observed (recorded) * The mm matrix the observations where both variables are missing * The rm matrix the observations where row variables are observed, columns missing * The mr matrix is the transpose of the rm matrix\nThe proportion of missing values can be visualized using VIM\n\naggr(nhanes, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(nhanes),\n cex.axis=.7, gap=3, ylab=c(\"Proportion of missingness\",\"Missingness Pattern\"))\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n Variable Count\n      chl  0.40\n      bmi  0.36\n      hyp  0.32\n      age  0.00\n\n\nThe plot showed that almost 52% of the samples are not missing any information. We have made a convenient color choice using col=mdc(1:2), a transparent blue color for the observed data, and a transparent red color for the imputed data. You can change this as you want, for example col=mdc(3:4).\nWe can also make a margin plot. It plots two variables at the same time and the plot can be used to explore the distribution of missing data and the observed data. Let us look at bmi and chl.\n\nmarginplot(nhanes[, c(\"chl\", \"bmi\")], col = mdc(1:2), \n           cex.numbers = 1.2, pch = 19)\n\n\n\n\n\n\n\n\nThere are 13 blue points for which both bmi and chl were observed. The three red dots in the left margin correspond to the records for which bmi is observed and chl is missing. The bottom margin contains two red points with observed chl and missing bmi. The red dot at the intersection of the bottom and left margin indicates that there are records for which both bmi and chl are missing. There are 7 records in which both are missing. 10 (bottom margin): Indicates the number of missing bmi values (when chl is observed).\nThe nice thing about a margin plot, is that for each variable it includes a boxplot of the two variables observed\nThere are other graphs implemented in VIM package for visualizing missing data. Please see Supportive Graphic Methods at your leisure for more details.\n\n\n\n\nSuppose we want to fit the following linear regression model to the data, lm(bmi~ age+chl+hyp). The lm() function will automatically remove the missing values before fitting the model.\n\nfit.c &lt;- lm(bmi~ age+chl+hyp, data=nhanes) \nsummary(fit.c)\n\n\nCall:\nlm(formula = bmi ~ age + chl + hyp, data = nhanes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9482 -1.1453  0.1893  2.1127  3.6352 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 18.26967    3.82835   4.772  0.00101 **\nage         -5.78652    1.56876  -3.689  0.00501 **\nchl          0.08045    0.02337   3.443  0.00736 **\nhyp          2.10468    2.38189   0.884  0.39989   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.099 on 9 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.6588,    Adjusted R-squared:  0.545 \nF-statistic: 5.792 on 3 and 9 DF,  p-value: 0.01738\n\n\nNote that in the output it says “12 observations deleted due to missingness”. This is, of course, acceptable only if the missing data is MCAR and if it is a small proportion of the whole dataset. But this is not the case here (12/25 is a big proportion), so we want to look for better alternatives.\n\n\n\n\nThe mice package provides a plethora of methods for single and multiple imputation\n\nmethods(mice)\n\n [1] mice.impute.2l.bin              mice.impute.2l.lmer            \n [3] mice.impute.2l.norm             mice.impute.2l.pan             \n [5] mice.impute.2lonly.mean         mice.impute.2lonly.norm        \n [7] mice.impute.2lonly.pmm          mice.impute.cart               \n [9] mice.impute.jomoImpute          mice.impute.lasso.logreg       \n[11] mice.impute.lasso.norm          mice.impute.lasso.select.logreg\n[13] mice.impute.lasso.select.norm   mice.impute.lda                \n[15] mice.impute.logreg              mice.impute.logreg.boot        \n[17] mice.impute.mean                mice.impute.midastouch         \n[19] mice.impute.mnar.logreg         mice.impute.mnar.norm          \n[21] mice.impute.mpmm                mice.impute.norm               \n[23] mice.impute.norm.boot           mice.impute.norm.nob           \n[25] mice.impute.norm.predict        mice.impute.panImpute          \n[27] mice.impute.passive             mice.impute.pmm                \n[29] mice.impute.polr                mice.impute.polyreg            \n[31] mice.impute.quadratic           mice.impute.rf                 \n[33] mice.impute.ri                  mice.impute.sample             \n[35] mice.mids                       mice.theme                     \nsee '?methods' for accessing help and source code\n\n\nLet’s explore some of these. When using mice, both single and multiple imputation follows the pattern:\n\\[\\begin{equation*}\nmice() ----&gt; with() ----&gt; pool()\n\\end{equation*}\\]\n\nmice() imputes each missing value with a plausible value (simulates a value to fill-in the missing one) until all missing values are imputed and dataset is completed. When specified, the process is repeated multiple times and all the complete(d)/imputed datasets are stored in the output.\nwith() analyses each of the completed data sets separately based on the analysis model you want.\npool() combines (pools) all the results together based on Rubin’s Rules (see lecture notes on missing data).\n\n\n\nRecall that mean imputation consists in replacing missing values on a certain variable by the mean of the available cases.\n\n\n\n\nRun the mice() function on the nhanes dataset, specifying the option method = \"mean\" and m=1\n\n\nRun the with() command to fit the linear regression bmi~ age+chl+hyp using the imputed dataset.\n\n\nPool together the parameter estimates using pool() and print out a summary of the output.\n\n\nDoes it make sense to pool the results together in this case? Why/why not?\n\n\nIf you don’t know where to start with this task, look at how the mice() —-&gt; with() —-&gt; pool() sequence is performed in the lecture notes and try to mimic that.\n\n\n\nClick for solution\n\n\nimp_mean &lt;- mice(nhanes, method = \"mean\",m = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n\nmodel_mean &lt;- with(imp_mean,lm(bmi~ age+chl+hyp))\nsummary(pool(model_mean))\n\nWarning in pool(model_mean): Number of multiple imputations m = 1. No pooling\ndone.\n\n\n\nCall:\nlm(formula = bmi ~ age + chl + hyp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3120 -1.5890  0.5018  1.7622  5.9258 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 19.63677    3.42047   5.741 1.07e-05 ***\nage         -2.09083    0.80161  -2.608   0.0164 *  \nchl          0.05143    0.01894   2.715   0.0130 *  \nhyp          0.61726    1.86730   0.331   0.7442    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.87 on 21 degrees of freedom\nMultiple R-squared:  0.3509,    Adjusted R-squared:  0.2582 \nF-statistic: 3.785 on 3 and 21 DF,  p-value: 0.02578\n\n\nNotice the warning; Warning: Number of multiple imputations m = 1. No pooling done. This is because we are running a single imputation, but pooling makes sense only for multiple imputations. In fact, doing\n\nsummary(model_mean)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic   p.value  nobs df.residual\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 (Intercept)  19.6       3.42       5.74  0.0000107    25          21\n2 age          -2.09      0.802     -2.61  0.0164       25          21\n3 chl           0.0514    0.0189     2.71  0.0130       25          21\n4 hyp           0.617     1.87       0.331 0.744        25          21\n\n\nreturns the same values, with no need for pooling.\n\nThe completed dataset can be obtained using complete()\n\ndata_mean &lt;- complete(imp_mean)\n\nWe can check that fitting a linear regression using the completed dataset agrees with the linear model above\n\nres_mean &lt;- lm(bmi~ age+chl+hyp, data = data_mean)\nsummary(res_mean)\n\n\nCall:\nlm(formula = bmi ~ age + chl + hyp, data = data_mean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3120 -1.5890  0.5018  1.7622  5.9258 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 19.63677    3.42047   5.741 1.07e-05 ***\nage         -2.09083    0.80161  -2.608   0.0164 *  \nchl          0.05143    0.01894   2.715   0.0130 *  \nhyp          0.61726    1.86730   0.331   0.7442    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.87 on 21 degrees of freedom\nMultiple R-squared:  0.3509,    Adjusted R-squared:  0.2582 \nF-statistic: 3.785 on 3 and 21 DF,  p-value: 0.02578\n\n\nOne can access the method used for imputation as follows:\n\nimp_mean$method\n\n   age    bmi    hyp    chl \n    \"\" \"mean\" \"mean\" \"mean\" \n\n\nQuite reassuring, as we have used mean imputation.\nWe can visualize the density of the mean imputed data set\n\ndensityplot(imp_mean)\n\n\n\n\n\n\n\n\nThis compares the observed and imputed densities. It seems that the densities are quite different. Perhaps mean imputation is not ideal for the data.\n\n\n\nRegression imputation uses a regression model to estimate the missing values. It can do better than mean imputation because it incorporates knowledge of other variables with the idea of producing more accurate imputations.\n\n\n\n\nPerform single imputation using mice() with an option method = \"norm.predict\n\n\nUse this imputed dataset to fit a regression of the variable bmi using covariates age+chl+hyp as above. How do the coefficient compare with respect to mean imputation?\n\n\nDraw a densityplot of the imputed dataset. Has this method changed the distribution by much?\n\n\n\n\n\nClick for solution\n\n\nimp_reg &lt;- mice(nhanes, method = \"norm.predict\", m = 1, maxit = 1,\n                seed = 123, print = FALSE)\nmodel_reg &lt;- with(imp_reg,lm(bmi~ age+chl+hyp))\nsummary(model_reg)  \n\n# A tibble: 4 × 7\n  term        estimate std.error statistic      p.value  nobs df.residual\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 (Intercept)  16.6       2.26        7.36 0.000000307     25          21\n2 age          -6.07      0.678      -8.95 0.0000000130    25          21\n3 chl           0.0924    0.0134      6.89 0.000000826     25          21\n4 hyp           1.95      1.47        1.33 0.199           25          21\n\n\nIf you used the pool() function, you receive the same warning as above: there is no pooling done as we have only imputed one data set (\\(m=1\\)). However, the output is correct and the same as the one so this is not a problem (but again, the cleanest code would be to just input summary(model_reg)).\nWe can extract the data completed by the imputation method using complete function. Regression model can then be fitted to this data. You can see that the result is the same as above.\n\nreg_dat &lt;- complete(imp_reg )# extract the completed data set\nsummary(lm(bmi~ age+chl+hyp, data = reg_dat))# model fitting and summary\n\n\nCall:\nlm(formula = bmi ~ age + chl + hyp, data = reg_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4307 -0.9303  0.2356  1.2856  3.7733 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.63816    2.26161   7.357 3.07e-07 ***\nage         -6.07200    0.67837  -8.951 1.30e-08 ***\nchl          0.09239    0.01341   6.891 8.26e-07 ***\nhyp          1.94892    1.47043   1.325    0.199    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.203 on 21 degrees of freedom\nMultiple R-squared:  0.8192,    Adjusted R-squared:  0.7934 \nF-statistic: 31.71 on 3 and 21 DF,  p-value: 5.494e-08\n\n\nThe fit of the imputation method can be compared using densityplot function as we did with the mean imputation.\n\ndensityplot(imp_reg)\n\n\n\n\n\n\n\n\nIf there is agreement, the two densities will almost superimpose. This is not the case here and as such both mean and regression imputation are not particularly suitable for this data.\n\n\n\n\nPredictive mean matching (PMM) can be used as a single imputation method. It works by replacing missing values with observed values from “similar” cases, where similarity is defined through a predictive model. For a variable with missing data, a regression model is first fitted using the observed cases, and predicted values are computed for both observed and missing observations. For each missing value, a small set of observed cases with predicted values closest to the missing case is identified, and one of their observed values is selected (often at random) and used as the imputation. Because PMM imputes only values that were actually observed, it preserves the variable’s original scale and distribution and avoids implausible extrapolation, while still exploiting relationships with covariates.\n\n\n\n\nPerform single imputation using mice() with the option method=\"pmm\"\n\n\nCompare the density plots of bmi for mean, regression, and PMM imputation. Ensure that the three plots are placed side by side\n\n\nFit a regression model to each imputed dataset and compare the coefficients. Which method produces the most plausible results?\n\n\nHint: In order to draw the plots side by side you can use par() or check the package gridextra\n\n\n\nClick for solution\n\n\nimp_pmm &lt;- mice(nhanes, method = \"pmm\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\n\n\nlibrary(gridExtra)\n\np1 &lt;- densityplot(imp_pmm)[1]\np2 &lt;- densityplot(imp_reg)[1]\np3 &lt;- densityplot(imp_mean)[1]\n\ngrid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\n\n\n\n\nIf we want to label the plot, we can do the following\n\np1 &lt;- update(densityplot(imp_pmm)[1], main = \"imp_pmm\")\np2 &lt;- update(densityplot(imp_reg)[1], main = \"imp_reg\")\np3 &lt;- update(densityplot(imp_mean)[1], main = \"imp_mean\")\ngrid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can now step up to multiple imputation. This is done by setting the value of m to the desired number of parallel imputations that we want to make. For example, to create 10 imputed data sets (m=10) via the mice package we can proceed as follows\n\nmice_nhanes &lt;- mice(nhanes, m=10, seed=123)\n\n\nsummary(mice_nhanes)\n\nClass: mids\nNumber of multiple imputations:  10 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\n\nWe have 10 imputed data sets (\\(m=10\\)). The default number of iteration (5) is used, and the three variables (age, chl and hypertension) were imputed using predictive mean matching.\nThe so-called predictor matrix has 0/1 entries. This is a way to specify the set of predictors to be used for each target column. For example, all the variables in the nhanes data set are used to impute chl variable (except chl itself, of course).\nWe can override the default pmm imputation method by setting a vector of methods:\n\nmice_nhanes2 &lt;- mice(nhanes, m=10, seed=123, method =c(\"\",\"norm.nob\",\"norm\", \"pmm\") )\n\n\nmice_nhanes2$method\n\n       age        bmi        hyp        chl \n        \"\" \"norm.nob\"     \"norm\"      \"pmm\" \n\n\n\nThis implies that we have imputed chl based on Bayesian linear regression (norm) and bmi using Linear regression ignoring model error (norm.nob). Obviously, age is not imputed.\nWhat if we want to impute bmi without using the age variable? To do this, we can either delete age from the data set or modify the predictor matrix. We can extract the predictor matrix as follows:\n\npred &lt;- mice_nhanes$predictorMatrix\npred\n\nTo impute bmi without the age variable, we can modify the predictor matrix to exclude age as a predictor for bmi\n\npred[\"bmi\", \"age\"] &lt;- 0\nprint(pred) #Checks the updated predictor matrix\n\n    age bmi hyp chl\nage   0   1   1   1\nbmi   0   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\nNow that age is excluded from the covariates, we can use the modified predictor matrix for the imputation process as follows:\n\nmice_nhanes4 &lt;- mice(nhanes, m = 10, seed = 123, predictorMatrix = pred)\nsummary(mice_nhanes4)\n\n\nWe can inspect the distribution of original and imputed data using xyplot:\n\nxyplot(mice_nhanes, bmi ~ chl | .imp, pch = 20, cex = 1.4)\n\n\n\n\n\n\n\n\nWhat we would like to see is that the position of the red points (imputed) does not distort the features of the scatterplot with only the blue points (observed). The matching shape tells us that the imputed values are indeed plausible values. The density plot used for single imputation can also be used here:\n\ndensityplot(mice_nhanes)\n\n\n\n\n\n\n\n\n\n\nThe next step in the analysis is to use the with() function. This will now fit 10 separate linear regression models to each of the imputed data sets:\n\nmodel_final &lt;- with(mice_nhanes,lm(bmi~ age+chl+hyp))\nmodel_final\n\nWe cannot make sense of the 10 separate analyses (results) except we combine them in some way. That is where the Rubin's Rules seen in lectures enter into play. The parameter estimates are averaged over the 10 results. The standard errors are combined by averaging within-imputation variance and between-imputation variance (see lectures). This responds to the need to take into account two sources of variability: the variability within each imputed data sets and variability between the 10 imputed data sets.\n\n\n\n\n\n\nCalculate the average of the estimated parameters ((Intercept), age, chl, hyp) over the 10 analyses.\n\n\nCompare the estimates with the output of summary(pool(model_final)). What can you see?\n\n\n\n\n\nClick for solution\n\nTo manually extract the coefficients from a given model, we have to look inside the model_final output\n\nmodel_final$analyses[[1]]$coefficients\n\n(Intercept)         age         chl         hyp \n21.84881821 -2.57565537  0.04382633  0.26258315 \n\n\nAlternatively, one can use the output of the summary to build a data set that has all the estimates in a unique variable called estimate\n\ncoeff &lt;- summary(model_final)\n\nThen, we compute the average for the observations belonging to the estimates of the same coefficients\n\nmean(coeff$estimate[coeff$term == \"(Intercept)\"])\n\n[1] 20.25634\n\nmean(coeff$estimate[coeff$term == \"age\"])\n\n[1] -3.702813\n\nmean(coeff$estimate[coeff$term == \"chl\"])\n\n[1] 0.05643052\n\nmean(coeff$estimate[coeff$term == \"hyp\"])\n\n[1] 1.574474\n\n\nEven quicker, we can do all at the same time using aggregate\n\naggregate(estimate ~ term, data = coeff, mean)\n\n         term    estimate\n1 (Intercept) 20.25633745\n2         age -3.70281280\n3         chl  0.05643052\n4         hyp  1.57447405\n\n\nEven if we are able to compute the average for the terms, the computation of the standard errors is not straightforward. Instead, we shall use the pool function in mice to summarize the 10 results:\n\nsummary(pool(model_final))\n\n         term    estimate  std.error  statistic        df      p.value\n1 (Intercept) 20.25633745 3.57914955  5.6595393 14.453645 5.220259e-05\n2         age -3.70281280 1.27110597 -2.9130638  9.428827 1.642428e-02\n3         chl  0.05643052 0.01901666  2.9674261 14.851278 9.670916e-03\n4         hyp  1.57447405 2.07581749  0.7584839 10.772710 4.644325e-01\n\n\n\nNote that the estimates coincide with those manually calculated above!\n\n\n\nWe have seen that the random forest imputation method is implemented in mice. Imputation using random forests fills in missing values by training a random forest model on the observed data for each variable with missing values. Then using the model’s predictions to replace the missing entries. Because random forests can capture nonlinear relationships and interactions among variables without requiring parametric assumptions, this approach often produces more realistic imputations than mean or regression-based methods.\nWe see here an example of simple imputation using random forests. However, keep in mind that if random forests are to be used for inference, then simple imputation with random forests leads to overconfidence (too small variance), so it is better to use multiple imputation in that case.\n\n\n\n\nImpute the nhanes dataset using random forests. Use only one model and three trees.\n\n\nCreate a density plot and assess the imputation.\n\n\nNext, increase the number of trees to 100 and generate another density plot. Does the number of trees affect the density plots?\n\n\n\n\n\nClick for solution\n\nThe random forest method is calle using method = \"rf\" and the number of trees is specified as ntree = 3\n\nimp_rf_3 &lt;- mice(nhanes, m=1, seed=123, method = \"rf\", ntree = 3)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n\n\n\ndensityplot(imp_rf_3)\n\n\n\n\n\n\n\n\nBy changing the number of trees to 100 we compute\n\nimp_rf_100 &lt;- mice(nhanes, m=1, seed=123, method = \"rf\", ntree = 100)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n\n\nand we can redraw the density plots\n\ndensityplot(imp_rf_100)\n\n\n\n\n\n\n\n\nIncreasing the number of trees doesn’t seem to improve the accuracy of distributional features. This is because increasing the number of trees improves predictive stability, but not distributions, so density agreement between observed and imputed data is not guaranteed to improve. It may even get worse!\n\n\n\n\n\nIn this exercise, you will work through an example of dealing with missing data using the airquality data. This contains 153 observations of 6 variables. We will explore the data and use both single and multiple imputation techniques\n\ndata(airquality)\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n\nThe first step is to explore the missing data pattern.\n\n\n\n\nUse md.pattern() and md.pairs() to explore the missing data pattern. What do you see?\n\n\nDraw a margin plot of the two variables with missing data. Which features emerge from the plot?\n\n\nCan you try to make some hypotheses about the reason data is missing as well as about the missing data mechanisms?\n\n\n\n\n\nClick for solution\n\n\nmd.pattern(airquality)\n\n\n\n\n\n\n\n\n    Wind Temp Month Day Solar.R Ozone   \n111    1    1     1   1       1     1  0\n35     1    1     1   1       1     0  1\n5      1    1     1   1       0     1  1\n2      1    1     1   1       0     0  2\n       0    0     0   0       7    37 44\n\nmd.pairs(airquality)\n\n$rr\n        Ozone Solar.R Wind Temp Month Day\nOzone     116     111  116  116   116 116\nSolar.R   111     146  146  146   146 146\nWind      116     146  153  153   153 153\nTemp      116     146  153  153   153 153\nMonth     116     146  153  153   153 153\nDay       116     146  153  153   153 153\n\n$rm\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       5    0    0     0   0\nSolar.R    35       0    0    0     0   0\nWind       37       7    0    0     0   0\nTemp       37       7    0    0     0   0\nMonth      37       7    0    0     0   0\nDay        37       7    0    0     0   0\n\n$mr\n        Ozone Solar.R Wind Temp Month Day\nOzone       0      35   37   37    37  37\nSolar.R     5       0    7    7     7   7\nWind        0       0    0    0     0   0\nTemp        0       0    0    0     0   0\nMonth       0       0    0    0     0   0\nDay         0       0    0    0     0   0\n\n$mm\n        Ozone Solar.R Wind Temp Month Day\nOzone      37       2    0    0     0   0\nSolar.R     2       7    0    0     0   0\nWind        0       0    0    0     0   0\nTemp        0       0    0    0     0   0\nMonth       0       0    0    0     0   0\nDay         0       0    0    0     0   0\n\n\nWe can see that there are 111 complete cases (as shown in the left and right columns). A zero in the last column indicates there are no missing values. The second row indicates that there are 35 patterns with only one missing value (Ozone). In the third row we have 5 patterns with only one missing value, which is on Solar.R. Only two patterns have two missing values, which are on Solar.R and Ozone.\nThe last row tells us how many values are missing from each variable (column). No value is missing on Wind, Temp, Month and Day, 7 observations are missing on Solar.R and 37 missing on Ozone. Overall, 44 observations are missing out of 153.\n\nmarginplot(airquality[, c(\"Solar.R\", \"Ozone\")], col = mdc(1:2), cex.numbers = 1.2, pch = 19)\n\n\n\n\n\n\n\n\nThe blue box plots summarize the distribution of observed data given the other variable is observed, and red box plots summarize the distribution of observed data given the other variable is missing.\nThe five red dots in the left margin correspond to the records for which Ozone is observed and Solar.R is missing. Notice that the points are drawn at the known values of Ozone. Similarly, the bottom margin contains red points with observed Solar.R and missing Ozone. There are 7 observations in which Solar.R is missing and 2 records in which both are missing (records 5 & 27).\nAs the red and blue distributions are not very different, the marginplot does not provide strong evidence for MAR mechanism, but it can not be ruled out either.\nSome plausible explanations for missingness are:\n\nInstrument or recording failures (MCAR) Measurements of Ozone and Solar.R relied on monitoring equipment that could malfunction, be improperly calibrated, or fail intermittently, leading to missing readings on certain days. If the malfunctioning is related to observable weather conditions, then the mechanism is MAR.\nWeather-related constraints (plausible MAR) Some meteorological conditions (e.g. heavy cloud cover, storms) can prevent reliable measurement of solar radiation and indirectly affect ozone monitoring, resulting in missing values.\nOperational gaps (MCAR or MAR) Data may not have been collected every day due to staffing limitations, maintenance, or reporting delays, especially earlier on, when automated monitoring was less robust. If gaps are more likely during specific months or seasons then the mechanism is MAR.\nDependence on observed variables (MAR) Missingness in Ozone or Solar.R appears related to seasonal patterns, temperature, and wind, which are observed variables—making a Missing At Random (MAR) mechanism plausible.\n\nThere is no indication to believe that high or low ozone values are selectively unrecorded after conditioning on observed weather, so MNAR is implausible.\n\nSuppose we want to fit the linear regression model to the airquality data, lm(Ozone ~ Wind + Temp + Solar.R). With complete case analysis (the default option), there would be 42 deleted observations. This leads to bias in the analysis if the missingness process is not MCAR. Even if the data is MCAR, the fit of the model may be quite poor. We’d rather fit the linear regression using multiple imputation by chained equations (MICE).\n\n\n\n\nUse mice to perform multiple imputation of the airquality data. Choose your preferred method, m and maxit values.\n\n\nNow, run the same command, but change it in such a way that the variables Day and Month are not used in the imputation process.\n\n\nFinally, change the code in such a way that the imputation methods for Ozone is set to norm and for Solar.R is set to norm.nob (linear regression ignoring model error).\n\n\nCompare the density plots in the three cases. How does the picture change in the three cases?\n\n\n\n\n\nClick for solution\n\nWe can fit a multiple imputation with Bayesian linear regression and 5 imputed datasets.\n\nmice_air1 &lt;- mice(airquality, method = \"pmm\",m = 5)\n\nTo exclude the Day and Month variable from the imputation, we change the predictorMatrix\n\npred_air &lt;- mice_air1$predictorMatrix\npred_air[, c(\"Month\",\"Day\")] &lt;- c(0,0)\npred_air\n\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       1    1    1     0   0\nSolar.R     1       0    1    1     0   0\nWind        1       1    0    1     0   0\nTemp        1       1    1    0     0   0\nMonth       1       1    1    1     0   0\nDay         1       1    1    1     0   0\n\n\nthen we run the multiple imputation with the new matrix\n\nmice_air2 &lt;- mice(airquality, method = \"pmm\",m = 5, predictorMatrix = pred_air)\n\nFinally, we can differentiate the imputation method by inputing a vector of methods instead of a single method\n\nmice_air3&lt;- mice(airquality, m = 5, \n                 method =c(\"norm\",\"norm.nob\",\"\", \"\", \"\",\"\"),\n                   predictorMatrix = pred_air)\n\n\np1 &lt;- densityplot(mice_air1)[1]\np2 &lt;- densityplot(mice_air2)[1]\np3 &lt;- densityplot(mice_air3)[1]\n\nq1 &lt;- densityplot(mice_air1)[2]\nq2 &lt;- densityplot(mice_air2)[2]\nq3 &lt;- densityplot(mice_air3)[2]\n\ngrid.arrange(p1, p2, p3, q1, q2, q3, ncol = 3, nrow = 2)\n\n\n\n\n\n\n\n\nIn terms of density estimation, pmm seems to be the best choice. Excluding Day and Month makes sense both conceptually and in terms of visual results.\n\nRemember that creating the different imputed data sets is just the first step of MI. Carrying the analysis and pooling the results is required for the process to make sense. This is the content of your final task:\n\n\n\n\nExtract the best set of imputations from Task 3b. Then, use the with() function in to fit separate linear regression models to each of the imputed data sets\n\n\nThen, use the pool() function to summarize the 10 results.\n\n\nCompare the results with those obtained by fitting the linear model only on complete cases. Which estimates differ and why? Which SEs are larger?\n\n\n\n\n\nClick for solution\n\n\nmodel_final &lt;- with(mice_air2,lm(Ozone ~ Wind + Temp + Solar.R))\n\nThis is what we get as estimate from the multiple imputation\n\nsummary(pool(model_final))\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -74.43442155 26.79030126 -2.778409 19.65930 1.172048e-02\n2        Wind  -2.93234227  0.70977278 -4.131382 32.20605 2.395603e-04\n3        Temp   1.73105798  0.29198912  5.928502 21.74834 6.032614e-06\n4     Solar.R   0.06028454  0.02301146  2.619761 85.61616 1.040772e-02\n\n\nThis is what we get from complete case analysis\n\nfit_cc &lt;- lm(Ozone ~ Wind + Temp + Solar.R,\n             data = airquality,\n             na.action = na.omit)\nsummary(fit_cc)\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nSolar.R       0.05982    0.02319   2.580  0.01124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficients are not widely different in the two cases, but we get a better p-value for the estimate of the intercept. In other situations, the difference can be more marked."
  },
  {
    "objectID": "Lab4_MissingData.html#missing-data",
    "href": "Lab4_MissingData.html#missing-data",
    "title": "Practical 4 - Missing Data Analysis",
    "section": "",
    "text": "Data sets with missing values are very common: missing data is a crucial issue in many disciplines such as social and medical sciences: questionnaire respondents do not answer every question; countries do not collect statistics every year; archives can be incomplete; subjects may drop out of longitudinal studies, and so on.\nThere are many packages for dealing with missing data in R. These include Amelia, missForest, Hmisc, mi and mice. We will focus on the use of mice, as this is the one discussed in lectures.\nYou will need to install the following packages:\n\nmice for missing data analysis\nVIM for the Visualisation and Imputation of Missing values\n\n\ninstall.packages(c(\"mice\",\"VIM\"))\n\nWe will work through an example of dealing with missing data using the nhanes (national health and nutrition examination survey) data. The data contains 25 observations on 4 variables. We will explore the data and use both single and multiple imputation techniques."
  },
  {
    "objectID": "Lab4_MissingData.html#data-exploration",
    "href": "Lab4_MissingData.html#data-exploration",
    "title": "Practical 4 - Missing Data Analysis",
    "section": "",
    "text": "We first load the required packages and load the data set.\n\nlibrary(mice)\nlibrary(VIM)\ndata(nhanes)\nhead(nhanes)\n\n  age  bmi hyp chl\n1   1   NA  NA  NA\n2   2 22.7   1 187\n3   1   NA   1 187\n4   3   NA  NA  NA\n5   1 20.4   1 113\n6   3   NA  NA 184\n\nstr(nhanes)\n\n'data.frame':   25 obs. of  4 variables:\n $ age: num  1 2 1 3 1 3 1 1 2 2 ...\n $ bmi: num  NA 22.7 NA NA 20.4 NA 22.5 30.1 22 NA ...\n $ hyp: num  NA 1 1 NA 1 NA 1 1 1 NA ...\n $ chl: num  NA 187 187 NA 113 184 118 187 238 NA ...\n\n\nWe can explore the missing data pattern using md.pattern.\n\nmd.pattern(nhanes)\n\n\n\n\n\n\n\n\n   age hyp bmi chl   \n13   1   1   1   1  0\n3    1   1   1   0  1\n1    1   1   0   1  1\n1    1   0   0   1  2\n7    1   0   0   0  3\n     0   8   9  10 27\n\n\n1’s and 0’s under each variable represent their presence and missing state respectively. There are 13 (out of 25) rows that are complete. There is one row for which only bmi is missing, and there are seven rows for which only age is known.\n\nmd.pairs(nhanes)\n\n$rr\n    age bmi hyp chl\nage  25  16  17  15\nbmi  16  16  16  13\nhyp  17  16  17  14\nchl  15  13  14  15\n\n$rm\n    age bmi hyp chl\nage   0   9   8  10\nbmi   0   0   0   3\nhyp   0   1   0   3\nchl   0   2   1   0\n\n$mr\n    age bmi hyp chl\nage   0   0   0   0\nbmi   9   0   1   2\nhyp   8   0   0   1\nchl  10   3   3   0\n\n$mm\n    age bmi hyp chl\nage   0   0   0   0\nbmi   0   9   8   7\nhyp   0   8   8   7\nchl   0   7   7  10\n\n\nThe matrices above represent the number of observations per patterns for all pairs of variables: * The rr matrix represents the number of observations where both pairs of values are observed (recorded) * The mm matrix the observations where both variables are missing * The rm matrix the observations where row variables are observed, columns missing * The mr matrix is the transpose of the rm matrix\nThe proportion of missing values can be visualized using VIM\n\naggr(nhanes, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(nhanes),\n cex.axis=.7, gap=3, ylab=c(\"Proportion of missingness\",\"Missingness Pattern\"))\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n Variable Count\n      chl  0.40\n      bmi  0.36\n      hyp  0.32\n      age  0.00\n\n\nThe plot showed that almost 52% of the samples are not missing any information. We have made a convenient color choice using col=mdc(1:2), a transparent blue color for the observed data, and a transparent red color for the imputed data. You can change this as you want, for example col=mdc(3:4).\nWe can also make a margin plot. It plots two variables at the same time and the plot can be used to explore the distribution of missing data and the observed data. Let us look at bmi and chl.\n\nmarginplot(nhanes[, c(\"chl\", \"bmi\")], col = mdc(1:2), \n           cex.numbers = 1.2, pch = 19)\n\n\n\n\n\n\n\n\nThere are 13 blue points for which both bmi and chl were observed. The three red dots in the left margin correspond to the records for which bmi is observed and chl is missing. The bottom margin contains two red points with observed chl and missing bmi. The red dot at the intersection of the bottom and left margin indicates that there are records for which both bmi and chl are missing. There are 7 records in which both are missing. 10 (bottom margin): Indicates the number of missing bmi values (when chl is observed).\nThe nice thing about a margin plot, is that for each variable it includes a boxplot of the two variables observed\nThere are other graphs implemented in VIM package for visualizing missing data. Please see Supportive Graphic Methods at your leisure for more details.\n\n\n\n\nSuppose we want to fit the following linear regression model to the data, lm(bmi~ age+chl+hyp). The lm() function will automatically remove the missing values before fitting the model.\n\nfit.c &lt;- lm(bmi~ age+chl+hyp, data=nhanes) \nsummary(fit.c)\n\n\nCall:\nlm(formula = bmi ~ age + chl + hyp, data = nhanes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9482 -1.1453  0.1893  2.1127  3.6352 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 18.26967    3.82835   4.772  0.00101 **\nage         -5.78652    1.56876  -3.689  0.00501 **\nchl          0.08045    0.02337   3.443  0.00736 **\nhyp          2.10468    2.38189   0.884  0.39989   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.099 on 9 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.6588,    Adjusted R-squared:  0.545 \nF-statistic: 5.792 on 3 and 9 DF,  p-value: 0.01738\n\n\nNote that in the output it says “12 observations deleted due to missingness”. This is, of course, acceptable only if the missing data is MCAR and if it is a small proportion of the whole dataset. But this is not the case here (12/25 is a big proportion), so we want to look for better alternatives."
  },
  {
    "objectID": "Lab4_MissingData.html#exercise-1-single-imputation",
    "href": "Lab4_MissingData.html#exercise-1-single-imputation",
    "title": "Practical 4 - Missing Data Analysis",
    "section": "",
    "text": "The mice package provides a plethora of methods for single and multiple imputation\n\nmethods(mice)\n\n [1] mice.impute.2l.bin              mice.impute.2l.lmer            \n [3] mice.impute.2l.norm             mice.impute.2l.pan             \n [5] mice.impute.2lonly.mean         mice.impute.2lonly.norm        \n [7] mice.impute.2lonly.pmm          mice.impute.cart               \n [9] mice.impute.jomoImpute          mice.impute.lasso.logreg       \n[11] mice.impute.lasso.norm          mice.impute.lasso.select.logreg\n[13] mice.impute.lasso.select.norm   mice.impute.lda                \n[15] mice.impute.logreg              mice.impute.logreg.boot        \n[17] mice.impute.mean                mice.impute.midastouch         \n[19] mice.impute.mnar.logreg         mice.impute.mnar.norm          \n[21] mice.impute.mpmm                mice.impute.norm               \n[23] mice.impute.norm.boot           mice.impute.norm.nob           \n[25] mice.impute.norm.predict        mice.impute.panImpute          \n[27] mice.impute.passive             mice.impute.pmm                \n[29] mice.impute.polr                mice.impute.polyreg            \n[31] mice.impute.quadratic           mice.impute.rf                 \n[33] mice.impute.ri                  mice.impute.sample             \n[35] mice.mids                       mice.theme                     \nsee '?methods' for accessing help and source code\n\n\nLet’s explore some of these. When using mice, both single and multiple imputation follows the pattern:\n\\[\\begin{equation*}\nmice() ----&gt; with() ----&gt; pool()\n\\end{equation*}\\]\n\nmice() imputes each missing value with a plausible value (simulates a value to fill-in the missing one) until all missing values are imputed and dataset is completed. When specified, the process is repeated multiple times and all the complete(d)/imputed datasets are stored in the output.\nwith() analyses each of the completed data sets separately based on the analysis model you want.\npool() combines (pools) all the results together based on Rubin’s Rules (see lecture notes on missing data).\n\n\n\nRecall that mean imputation consists in replacing missing values on a certain variable by the mean of the available cases.\n\n\n\n\nRun the mice() function on the nhanes dataset, specifying the option method = \"mean\" and m=1\n\n\nRun the with() command to fit the linear regression bmi~ age+chl+hyp using the imputed dataset.\n\n\nPool together the parameter estimates using pool() and print out a summary of the output.\n\n\nDoes it make sense to pool the results together in this case? Why/why not?\n\n\nIf you don’t know where to start with this task, look at how the mice() —-&gt; with() —-&gt; pool() sequence is performed in the lecture notes and try to mimic that.\n\n\n\nClick for solution\n\n\nimp_mean &lt;- mice(nhanes, method = \"mean\",m = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n\nmodel_mean &lt;- with(imp_mean,lm(bmi~ age+chl+hyp))\nsummary(pool(model_mean))\n\nWarning in pool(model_mean): Number of multiple imputations m = 1. No pooling\ndone.\n\n\n\nCall:\nlm(formula = bmi ~ age + chl + hyp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3120 -1.5890  0.5018  1.7622  5.9258 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 19.63677    3.42047   5.741 1.07e-05 ***\nage         -2.09083    0.80161  -2.608   0.0164 *  \nchl          0.05143    0.01894   2.715   0.0130 *  \nhyp          0.61726    1.86730   0.331   0.7442    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.87 on 21 degrees of freedom\nMultiple R-squared:  0.3509,    Adjusted R-squared:  0.2582 \nF-statistic: 3.785 on 3 and 21 DF,  p-value: 0.02578\n\n\nNotice the warning; Warning: Number of multiple imputations m = 1. No pooling done. This is because we are running a single imputation, but pooling makes sense only for multiple imputations. In fact, doing\n\nsummary(model_mean)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic   p.value  nobs df.residual\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 (Intercept)  19.6       3.42       5.74  0.0000107    25          21\n2 age          -2.09      0.802     -2.61  0.0164       25          21\n3 chl           0.0514    0.0189     2.71  0.0130       25          21\n4 hyp           0.617     1.87       0.331 0.744        25          21\n\n\nreturns the same values, with no need for pooling.\n\nThe completed dataset can be obtained using complete()\n\ndata_mean &lt;- complete(imp_mean)\n\nWe can check that fitting a linear regression using the completed dataset agrees with the linear model above\n\nres_mean &lt;- lm(bmi~ age+chl+hyp, data = data_mean)\nsummary(res_mean)\n\n\nCall:\nlm(formula = bmi ~ age + chl + hyp, data = data_mean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3120 -1.5890  0.5018  1.7622  5.9258 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 19.63677    3.42047   5.741 1.07e-05 ***\nage         -2.09083    0.80161  -2.608   0.0164 *  \nchl          0.05143    0.01894   2.715   0.0130 *  \nhyp          0.61726    1.86730   0.331   0.7442    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.87 on 21 degrees of freedom\nMultiple R-squared:  0.3509,    Adjusted R-squared:  0.2582 \nF-statistic: 3.785 on 3 and 21 DF,  p-value: 0.02578\n\n\nOne can access the method used for imputation as follows:\n\nimp_mean$method\n\n   age    bmi    hyp    chl \n    \"\" \"mean\" \"mean\" \"mean\" \n\n\nQuite reassuring, as we have used mean imputation.\nWe can visualize the density of the mean imputed data set\n\ndensityplot(imp_mean)\n\n\n\n\n\n\n\n\nThis compares the observed and imputed densities. It seems that the densities are quite different. Perhaps mean imputation is not ideal for the data.\n\n\n\nRegression imputation uses a regression model to estimate the missing values. It can do better than mean imputation because it incorporates knowledge of other variables with the idea of producing more accurate imputations.\n\n\n\n\nPerform single imputation using mice() with an option method = \"norm.predict\n\n\nUse this imputed dataset to fit a regression of the variable bmi using covariates age+chl+hyp as above. How do the coefficient compare with respect to mean imputation?\n\n\nDraw a densityplot of the imputed dataset. Has this method changed the distribution by much?\n\n\n\n\n\nClick for solution\n\n\nimp_reg &lt;- mice(nhanes, method = \"norm.predict\", m = 1, maxit = 1,\n                seed = 123, print = FALSE)\nmodel_reg &lt;- with(imp_reg,lm(bmi~ age+chl+hyp))\nsummary(model_reg)  \n\n# A tibble: 4 × 7\n  term        estimate std.error statistic      p.value  nobs df.residual\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 (Intercept)  16.6       2.26        7.36 0.000000307     25          21\n2 age          -6.07      0.678      -8.95 0.0000000130    25          21\n3 chl           0.0924    0.0134      6.89 0.000000826     25          21\n4 hyp           1.95      1.47        1.33 0.199           25          21\n\n\nIf you used the pool() function, you receive the same warning as above: there is no pooling done as we have only imputed one data set (\\(m=1\\)). However, the output is correct and the same as the one so this is not a problem (but again, the cleanest code would be to just input summary(model_reg)).\nWe can extract the data completed by the imputation method using complete function. Regression model can then be fitted to this data. You can see that the result is the same as above.\n\nreg_dat &lt;- complete(imp_reg )# extract the completed data set\nsummary(lm(bmi~ age+chl+hyp, data = reg_dat))# model fitting and summary\n\n\nCall:\nlm(formula = bmi ~ age + chl + hyp, data = reg_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4307 -0.9303  0.2356  1.2856  3.7733 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 16.63816    2.26161   7.357 3.07e-07 ***\nage         -6.07200    0.67837  -8.951 1.30e-08 ***\nchl          0.09239    0.01341   6.891 8.26e-07 ***\nhyp          1.94892    1.47043   1.325    0.199    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.203 on 21 degrees of freedom\nMultiple R-squared:  0.8192,    Adjusted R-squared:  0.7934 \nF-statistic: 31.71 on 3 and 21 DF,  p-value: 5.494e-08\n\n\nThe fit of the imputation method can be compared using densityplot function as we did with the mean imputation.\n\ndensityplot(imp_reg)\n\n\n\n\n\n\n\n\nIf there is agreement, the two densities will almost superimpose. This is not the case here and as such both mean and regression imputation are not particularly suitable for this data.\n\n\n\n\nPredictive mean matching (PMM) can be used as a single imputation method. It works by replacing missing values with observed values from “similar” cases, where similarity is defined through a predictive model. For a variable with missing data, a regression model is first fitted using the observed cases, and predicted values are computed for both observed and missing observations. For each missing value, a small set of observed cases with predicted values closest to the missing case is identified, and one of their observed values is selected (often at random) and used as the imputation. Because PMM imputes only values that were actually observed, it preserves the variable’s original scale and distribution and avoids implausible extrapolation, while still exploiting relationships with covariates.\n\n\n\n\nPerform single imputation using mice() with the option method=\"pmm\"\n\n\nCompare the density plots of bmi for mean, regression, and PMM imputation. Ensure that the three plots are placed side by side\n\n\nFit a regression model to each imputed dataset and compare the coefficients. Which method produces the most plausible results?\n\n\nHint: In order to draw the plots side by side you can use par() or check the package gridextra\n\n\n\nClick for solution\n\n\nimp_pmm &lt;- mice(nhanes, method = \"pmm\", m = 1, maxit = 1)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n\n\n\nlibrary(gridExtra)\n\np1 &lt;- densityplot(imp_pmm)[1]\np2 &lt;- densityplot(imp_reg)[1]\np3 &lt;- densityplot(imp_mean)[1]\n\ngrid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\n\n\n\n\nIf we want to label the plot, we can do the following\n\np1 &lt;- update(densityplot(imp_pmm)[1], main = \"imp_pmm\")\np2 &lt;- update(densityplot(imp_reg)[1], main = \"imp_reg\")\np3 &lt;- update(densityplot(imp_mean)[1], main = \"imp_mean\")\ngrid.arrange(p1, p2, p3, ncol = 3)"
  },
  {
    "objectID": "Lab4_MissingData.html#exercise-2-multiple-imputation",
    "href": "Lab4_MissingData.html#exercise-2-multiple-imputation",
    "title": "Practical 4 - Missing Data Analysis",
    "section": "",
    "text": "We can now step up to multiple imputation. This is done by setting the value of m to the desired number of parallel imputations that we want to make. For example, to create 10 imputed data sets (m=10) via the mice package we can proceed as follows\n\nmice_nhanes &lt;- mice(nhanes, m=10, seed=123)\n\n\nsummary(mice_nhanes)\n\nClass: mids\nNumber of multiple imputations:  10 \nImputation methods:\n  age   bmi   hyp   chl \n   \"\" \"pmm\" \"pmm\" \"pmm\" \nPredictorMatrix:\n    age bmi hyp chl\nage   0   1   1   1\nbmi   1   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\n\nWe have 10 imputed data sets (\\(m=10\\)). The default number of iteration (5) is used, and the three variables (age, chl and hypertension) were imputed using predictive mean matching.\nThe so-called predictor matrix has 0/1 entries. This is a way to specify the set of predictors to be used for each target column. For example, all the variables in the nhanes data set are used to impute chl variable (except chl itself, of course).\nWe can override the default pmm imputation method by setting a vector of methods:\n\nmice_nhanes2 &lt;- mice(nhanes, m=10, seed=123, method =c(\"\",\"norm.nob\",\"norm\", \"pmm\") )\n\n\nmice_nhanes2$method\n\n       age        bmi        hyp        chl \n        \"\" \"norm.nob\"     \"norm\"      \"pmm\" \n\n\n\nThis implies that we have imputed chl based on Bayesian linear regression (norm) and bmi using Linear regression ignoring model error (norm.nob). Obviously, age is not imputed.\nWhat if we want to impute bmi without using the age variable? To do this, we can either delete age from the data set or modify the predictor matrix. We can extract the predictor matrix as follows:\n\npred &lt;- mice_nhanes$predictorMatrix\npred\n\nTo impute bmi without the age variable, we can modify the predictor matrix to exclude age as a predictor for bmi\n\npred[\"bmi\", \"age\"] &lt;- 0\nprint(pred) #Checks the updated predictor matrix\n\n    age bmi hyp chl\nage   0   1   1   1\nbmi   0   0   1   1\nhyp   1   1   0   1\nchl   1   1   1   0\n\n\nNow that age is excluded from the covariates, we can use the modified predictor matrix for the imputation process as follows:\n\nmice_nhanes4 &lt;- mice(nhanes, m = 10, seed = 123, predictorMatrix = pred)\nsummary(mice_nhanes4)\n\n\nWe can inspect the distribution of original and imputed data using xyplot:\n\nxyplot(mice_nhanes, bmi ~ chl | .imp, pch = 20, cex = 1.4)\n\n\n\n\n\n\n\n\nWhat we would like to see is that the position of the red points (imputed) does not distort the features of the scatterplot with only the blue points (observed). The matching shape tells us that the imputed values are indeed plausible values. The density plot used for single imputation can also be used here:\n\ndensityplot(mice_nhanes)\n\n\n\n\n\n\n\n\n\n\nThe next step in the analysis is to use the with() function. This will now fit 10 separate linear regression models to each of the imputed data sets:\n\nmodel_final &lt;- with(mice_nhanes,lm(bmi~ age+chl+hyp))\nmodel_final\n\nWe cannot make sense of the 10 separate analyses (results) except we combine them in some way. That is where the Rubin's Rules seen in lectures enter into play. The parameter estimates are averaged over the 10 results. The standard errors are combined by averaging within-imputation variance and between-imputation variance (see lectures). This responds to the need to take into account two sources of variability: the variability within each imputed data sets and variability between the 10 imputed data sets.\n\n\n\n\n\n\nCalculate the average of the estimated parameters ((Intercept), age, chl, hyp) over the 10 analyses.\n\n\nCompare the estimates with the output of summary(pool(model_final)). What can you see?\n\n\n\n\n\nClick for solution\n\nTo manually extract the coefficients from a given model, we have to look inside the model_final output\n\nmodel_final$analyses[[1]]$coefficients\n\n(Intercept)         age         chl         hyp \n21.84881821 -2.57565537  0.04382633  0.26258315 \n\n\nAlternatively, one can use the output of the summary to build a data set that has all the estimates in a unique variable called estimate\n\ncoeff &lt;- summary(model_final)\n\nThen, we compute the average for the observations belonging to the estimates of the same coefficients\n\nmean(coeff$estimate[coeff$term == \"(Intercept)\"])\n\n[1] 20.25634\n\nmean(coeff$estimate[coeff$term == \"age\"])\n\n[1] -3.702813\n\nmean(coeff$estimate[coeff$term == \"chl\"])\n\n[1] 0.05643052\n\nmean(coeff$estimate[coeff$term == \"hyp\"])\n\n[1] 1.574474\n\n\nEven quicker, we can do all at the same time using aggregate\n\naggregate(estimate ~ term, data = coeff, mean)\n\n         term    estimate\n1 (Intercept) 20.25633745\n2         age -3.70281280\n3         chl  0.05643052\n4         hyp  1.57447405\n\n\nEven if we are able to compute the average for the terms, the computation of the standard errors is not straightforward. Instead, we shall use the pool function in mice to summarize the 10 results:\n\nsummary(pool(model_final))\n\n         term    estimate  std.error  statistic        df      p.value\n1 (Intercept) 20.25633745 3.57914955  5.6595393 14.453645 5.220259e-05\n2         age -3.70281280 1.27110597 -2.9130638  9.428827 1.642428e-02\n3         chl  0.05643052 0.01901666  2.9674261 14.851278 9.670916e-03\n4         hyp  1.57447405 2.07581749  0.7584839 10.772710 4.644325e-01\n\n\n\nNote that the estimates coincide with those manually calculated above!\n\n\n\nWe have seen that the random forest imputation method is implemented in mice. Imputation using random forests fills in missing values by training a random forest model on the observed data for each variable with missing values. Then using the model’s predictions to replace the missing entries. Because random forests can capture nonlinear relationships and interactions among variables without requiring parametric assumptions, this approach often produces more realistic imputations than mean or regression-based methods.\nWe see here an example of simple imputation using random forests. However, keep in mind that if random forests are to be used for inference, then simple imputation with random forests leads to overconfidence (too small variance), so it is better to use multiple imputation in that case.\n\n\n\n\nImpute the nhanes dataset using random forests. Use only one model and three trees.\n\n\nCreate a density plot and assess the imputation.\n\n\nNext, increase the number of trees to 100 and generate another density plot. Does the number of trees affect the density plots?\n\n\n\n\n\nClick for solution\n\nThe random forest method is calle using method = \"rf\" and the number of trees is specified as ntree = 3\n\nimp_rf_3 &lt;- mice(nhanes, m=1, seed=123, method = \"rf\", ntree = 3)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n\n\n\ndensityplot(imp_rf_3)\n\n\n\n\n\n\n\n\nBy changing the number of trees to 100 we compute\n\nimp_rf_100 &lt;- mice(nhanes, m=1, seed=123, method = \"rf\", ntree = 100)\n\n\n iter imp variable\n  1   1  bmi  hyp  chl\n  2   1  bmi  hyp  chl\n  3   1  bmi  hyp  chl\n  4   1  bmi  hyp  chl\n  5   1  bmi  hyp  chl\n\n\nand we can redraw the density plots\n\ndensityplot(imp_rf_100)\n\n\n\n\n\n\n\n\nIncreasing the number of trees doesn’t seem to improve the accuracy of distributional features. This is because increasing the number of trees improves predictive stability, but not distributions, so density agreement between observed and imputed data is not guaranteed to improve. It may even get worse!"
  },
  {
    "objectID": "Lab4_MissingData.html#exercise-3-air-quality",
    "href": "Lab4_MissingData.html#exercise-3-air-quality",
    "title": "Practical 4 - Missing Data Analysis",
    "section": "",
    "text": "In this exercise, you will work through an example of dealing with missing data using the airquality data. This contains 153 observations of 6 variables. We will explore the data and use both single and multiple imputation techniques\n\ndata(airquality)\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n\nThe first step is to explore the missing data pattern.\n\n\n\n\nUse md.pattern() and md.pairs() to explore the missing data pattern. What do you see?\n\n\nDraw a margin plot of the two variables with missing data. Which features emerge from the plot?\n\n\nCan you try to make some hypotheses about the reason data is missing as well as about the missing data mechanisms?\n\n\n\n\n\nClick for solution\n\n\nmd.pattern(airquality)\n\n\n\n\n\n\n\n\n    Wind Temp Month Day Solar.R Ozone   \n111    1    1     1   1       1     1  0\n35     1    1     1   1       1     0  1\n5      1    1     1   1       0     1  1\n2      1    1     1   1       0     0  2\n       0    0     0   0       7    37 44\n\nmd.pairs(airquality)\n\n$rr\n        Ozone Solar.R Wind Temp Month Day\nOzone     116     111  116  116   116 116\nSolar.R   111     146  146  146   146 146\nWind      116     146  153  153   153 153\nTemp      116     146  153  153   153 153\nMonth     116     146  153  153   153 153\nDay       116     146  153  153   153 153\n\n$rm\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       5    0    0     0   0\nSolar.R    35       0    0    0     0   0\nWind       37       7    0    0     0   0\nTemp       37       7    0    0     0   0\nMonth      37       7    0    0     0   0\nDay        37       7    0    0     0   0\n\n$mr\n        Ozone Solar.R Wind Temp Month Day\nOzone       0      35   37   37    37  37\nSolar.R     5       0    7    7     7   7\nWind        0       0    0    0     0   0\nTemp        0       0    0    0     0   0\nMonth       0       0    0    0     0   0\nDay         0       0    0    0     0   0\n\n$mm\n        Ozone Solar.R Wind Temp Month Day\nOzone      37       2    0    0     0   0\nSolar.R     2       7    0    0     0   0\nWind        0       0    0    0     0   0\nTemp        0       0    0    0     0   0\nMonth       0       0    0    0     0   0\nDay         0       0    0    0     0   0\n\n\nWe can see that there are 111 complete cases (as shown in the left and right columns). A zero in the last column indicates there are no missing values. The second row indicates that there are 35 patterns with only one missing value (Ozone). In the third row we have 5 patterns with only one missing value, which is on Solar.R. Only two patterns have two missing values, which are on Solar.R and Ozone.\nThe last row tells us how many values are missing from each variable (column). No value is missing on Wind, Temp, Month and Day, 7 observations are missing on Solar.R and 37 missing on Ozone. Overall, 44 observations are missing out of 153.\n\nmarginplot(airquality[, c(\"Solar.R\", \"Ozone\")], col = mdc(1:2), cex.numbers = 1.2, pch = 19)\n\n\n\n\n\n\n\n\nThe blue box plots summarize the distribution of observed data given the other variable is observed, and red box plots summarize the distribution of observed data given the other variable is missing.\nThe five red dots in the left margin correspond to the records for which Ozone is observed and Solar.R is missing. Notice that the points are drawn at the known values of Ozone. Similarly, the bottom margin contains red points with observed Solar.R and missing Ozone. There are 7 observations in which Solar.R is missing and 2 records in which both are missing (records 5 & 27).\nAs the red and blue distributions are not very different, the marginplot does not provide strong evidence for MAR mechanism, but it can not be ruled out either.\nSome plausible explanations for missingness are:\n\nInstrument or recording failures (MCAR) Measurements of Ozone and Solar.R relied on monitoring equipment that could malfunction, be improperly calibrated, or fail intermittently, leading to missing readings on certain days. If the malfunctioning is related to observable weather conditions, then the mechanism is MAR.\nWeather-related constraints (plausible MAR) Some meteorological conditions (e.g. heavy cloud cover, storms) can prevent reliable measurement of solar radiation and indirectly affect ozone monitoring, resulting in missing values.\nOperational gaps (MCAR or MAR) Data may not have been collected every day due to staffing limitations, maintenance, or reporting delays, especially earlier on, when automated monitoring was less robust. If gaps are more likely during specific months or seasons then the mechanism is MAR.\nDependence on observed variables (MAR) Missingness in Ozone or Solar.R appears related to seasonal patterns, temperature, and wind, which are observed variables—making a Missing At Random (MAR) mechanism plausible.\n\nThere is no indication to believe that high or low ozone values are selectively unrecorded after conditioning on observed weather, so MNAR is implausible.\n\nSuppose we want to fit the linear regression model to the airquality data, lm(Ozone ~ Wind + Temp + Solar.R). With complete case analysis (the default option), there would be 42 deleted observations. This leads to bias in the analysis if the missingness process is not MCAR. Even if the data is MCAR, the fit of the model may be quite poor. We’d rather fit the linear regression using multiple imputation by chained equations (MICE).\n\n\n\n\nUse mice to perform multiple imputation of the airquality data. Choose your preferred method, m and maxit values.\n\n\nNow, run the same command, but change it in such a way that the variables Day and Month are not used in the imputation process.\n\n\nFinally, change the code in such a way that the imputation methods for Ozone is set to norm and for Solar.R is set to norm.nob (linear regression ignoring model error).\n\n\nCompare the density plots in the three cases. How does the picture change in the three cases?\n\n\n\n\n\nClick for solution\n\nWe can fit a multiple imputation with Bayesian linear regression and 5 imputed datasets.\n\nmice_air1 &lt;- mice(airquality, method = \"pmm\",m = 5)\n\nTo exclude the Day and Month variable from the imputation, we change the predictorMatrix\n\npred_air &lt;- mice_air1$predictorMatrix\npred_air[, c(\"Month\",\"Day\")] &lt;- c(0,0)\npred_air\n\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       1    1    1     0   0\nSolar.R     1       0    1    1     0   0\nWind        1       1    0    1     0   0\nTemp        1       1    1    0     0   0\nMonth       1       1    1    1     0   0\nDay         1       1    1    1     0   0\n\n\nthen we run the multiple imputation with the new matrix\n\nmice_air2 &lt;- mice(airquality, method = \"pmm\",m = 5, predictorMatrix = pred_air)\n\nFinally, we can differentiate the imputation method by inputing a vector of methods instead of a single method\n\nmice_air3&lt;- mice(airquality, m = 5, \n                 method =c(\"norm\",\"norm.nob\",\"\", \"\", \"\",\"\"),\n                   predictorMatrix = pred_air)\n\n\np1 &lt;- densityplot(mice_air1)[1]\np2 &lt;- densityplot(mice_air2)[1]\np3 &lt;- densityplot(mice_air3)[1]\n\nq1 &lt;- densityplot(mice_air1)[2]\nq2 &lt;- densityplot(mice_air2)[2]\nq3 &lt;- densityplot(mice_air3)[2]\n\ngrid.arrange(p1, p2, p3, q1, q2, q3, ncol = 3, nrow = 2)\n\n\n\n\n\n\n\n\nIn terms of density estimation, pmm seems to be the best choice. Excluding Day and Month makes sense both conceptually and in terms of visual results.\n\nRemember that creating the different imputed data sets is just the first step of MI. Carrying the analysis and pooling the results is required for the process to make sense. This is the content of your final task:\n\n\n\n\nExtract the best set of imputations from Task 3b. Then, use the with() function in to fit separate linear regression models to each of the imputed data sets\n\n\nThen, use the pool() function to summarize the 10 results.\n\n\nCompare the results with those obtained by fitting the linear model only on complete cases. Which estimates differ and why? Which SEs are larger?\n\n\n\n\n\nClick for solution\n\n\nmodel_final &lt;- with(mice_air2,lm(Ozone ~ Wind + Temp + Solar.R))\n\nThis is what we get as estimate from the multiple imputation\n\nsummary(pool(model_final))\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -74.43442155 26.79030126 -2.778409 19.65930 1.172048e-02\n2        Wind  -2.93234227  0.70977278 -4.131382 32.20605 2.395603e-04\n3        Temp   1.73105798  0.29198912  5.928502 21.74834 6.032614e-06\n4     Solar.R   0.06028454  0.02301146  2.619761 85.61616 1.040772e-02\n\n\nThis is what we get from complete case analysis\n\nfit_cc &lt;- lm(Ozone ~ Wind + Temp + Solar.R,\n             data = airquality,\n             na.action = na.omit)\nsummary(fit_cc)\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nSolar.R       0.05982    0.02319   2.580  0.01124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficients are not widely different in the two cases, but we get a better p-value for the estimate of the intercept. In other situations, the difference can be more marked."
  },
  {
    "objectID": "Workshop4_AdvancedFunctions.html",
    "href": "Workshop4_AdvancedFunctions.html",
    "title": "Workshop 4 - More functions for EDA",
    "section": "",
    "text": "In this workshop, we tie together some of the topics seen so far, and explore more functions that help us with exploratory data analysis.\n\n\nBy the end of the workshop, you will have seen how to:\n\n\nManipulate dataframes using dplyr\n\n\nUse the grammar of graphics required to draw plots with ggplot\n\n\nPerform exploratory data analysis on a data set of multiple time series with missing data\n\n\n\nPackages needed (if you want to replicate this outside the NCC) are tidyverse, naniar, and scales.\n\nlibrary(tidyverse)   # ggplot2, dplyr, tidyr, readr\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(naniar)      # missing data visualization\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n\n Download data: gap_miss\n\n\nRows: 29,393\nColumns: 8\n$ country    &lt;fct&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Argentina\",…\n$ year       &lt;dbl&gt; 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800,…\n$ pop        &lt;int&gt; 3280000, 400000, 2500000, 1570000, 534000, 200000, 3000000,…\n$ lifeExp    &lt;dbl&gt; 28.2, 35.4, 28.8, 27.0, 33.2, 34.0, 34.4, NA, 25.5, 40.0, 3…\n$ gdpPercap  &lt;int&gt; 603, 667, 715, 618, 1640, 817, 1850, 1240, 876, 2390, 597, …\n$ fertility  &lt;dbl&gt; 7.00, 4.60, 6.99, 6.93, 6.80, 6.50, 5.10, 7.03, 6.70, 4.85,…\n$ infantMort &lt;dbl&gt; 469, 375, 460, 486, 402, 391, 387, 440, 508, 322, 430, 405,…\n$ continent  &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, As…\n\n\nThe data contain values for population, life expectancy, GDP per capita, population, fertility and infant mortality, every years, from 1900 to 2020. The variables are:\n\ncountry - the country, factor with 142 levels\ncontinent - the continent, factor with 5 levels\nyear - year of observation ranges from 1900 to 2020\nlifeExp - life expectancy at birth, in years\npop - population\ngdpPercap - GDP per capita (US$, inflation-adjusted)\ninfantMort - Death of children under 5 years of age per 1000 live births\nfertility - The number of children that would be born to each woman with prevailing age-specific fertility rates\n\nThis data set is quite complex, as we have five time series observed for multiple countries (i.e. a categorical variable with many levels).\nTo get a feeling for the data, let’s focus on a particular picture of the state of the world in one year - let’s take 2000.\n\n# subset the data\ngm.2000 &lt;- gap_miss[gap_miss$year==2000,]\n# a pairs plot to look for associations\npairs(gm.2000[,3:7],pch=16,col=gm.2000$continent)\n\n\n\n\n\n\n\n## some strong correlations, and mix of linear/nonlinear relationships\n## seems to be an association with continent\n## some outliers in population - India and China\nrownames(gm.2000) &lt;- gm.2000$country\n\n\nWe soon note that 10% of the lifeExp data is missing. Missingness doesn’t seem to follow a precise pattern based on observable variables, but we need more exploration.\n\ncolSums(is.na(gap_miss))\n\n   country       year        pop    lifeExp  gdpPercap  fertility infantMort \n         0          0          0       2939          0          0          0 \n continent \n         0 \n\nvis_miss(gap_miss)\n\n\n\n\n\n\n\n\n\n\n\nThe package ggplot2 is built on the Grammar of Graphics. Every plot is made of layers, and almost every call looks like:\nggplot(data, aes(...)) + geom_...() + labs_...() + scale_...() + theme_...().\nNot all these layers are necessary! The first layer creates a grid: not very interesting\n\ngap_2000 &lt;- gap_miss |&gt; filter(year == 2000)\nggplot(gap_2000, aes(lifeExp))\n\n\n\n\n\n\n\n\nBy using the fisrt two layers, we can already create some basics visualisation, such as histograms\n\nggplot(gap_2000, aes(lifeExp)) +\ngeom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe can now add settings and a new layer that specifies labels\n\nggplot(gap_2000, aes(x = lifeExp)) +\n  geom_histogram(bins = 30, na.rm = TRUE) +\n  labs(\n    title = \"Distribution of Life Expectancy (Year 2000)\",\n    x = \"Life expectancy (years)\",\n    y = \"Number of observations\"\n  )\n\n\n\n\n\n\n\n\nIf we want to explore the distributions of different subgroups, we need to make sure that the second variables is included in the aesthetic, and we change the geometry to geom_density.\n\nggplot(gap_2000, aes(x = lifeExp, fill = continent)) +\n  geom_density(alpha = 0.4, na.rm = TRUE)\n\n\n\n\n\n\n\n\nggplot automatically splits the data by continent, computes one density per group and assigns colors. Quite impressive compared to base R!\nAnother option for comparing continents is to use faceting\n\nggplot(gap_2000, aes(x = lifeExp)) +\n  geom_histogram(bins = 25, na.rm = TRUE) +\n  facet_wrap(~ continent) +\n  labs(title = \"Life Expectancy by Continent\")\n\n\n\n\n\n\n\n\nAnd the beauty of it is that it makes it intuitive to superimpose multiple layers. Let’s try to draw a scatterplot of (log-transformed) gdpPercap versus lifeExp with colouring depending on continent and a smoothing line for each group\n\nggplot(gap_2000, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(alpha = 0.5, na.rm = TRUE) +\n  geom_smooth(se = FALSE, na.rm = TRUE) +\n  scale_x_log10() +\n  labs(\n    title = \"Life Expectancy vs GDP per Capita\",\n    x = \"GDP per capita (log scale)\",\n    y = \"Life expectancy\"\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow it’s your turn:\nstarting from the following basic code\n\ncountries_sel &lt;- c(\"France\", \"Germany\", \"United States\",\"China\", \"India\", \"Brazil\")\n\ngap_select &lt;- gap_miss %&gt;% filter(country %in% countries_sel)\n\nggplot(gap_select, aes(x = year, y = lifeExp)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\nsee if you can perform the following tasks.\nYou can look for inspiration at this section of the R graph gallery.\n\n\n\n\nModify the code so that different countries are shown in different colors. Make points slightly transparent to reduce overplotting.\n\n\nInstead of a scatterplot, try to change the geometry in order to make a linechart Try to see if you can make the line thicker.\n\n\nImprove the plot by giving it a clear title and axis labels, using a cleaner theme and making the y-axis easier to read.\n\n\n\n\n\nClick for (a possible) solution\n\n\nggplot(gap_select, aes(x = year, y = lifeExp, color = country)) +\n  geom_point(alpha = 0.4, na.rm = TRUE)\n\n\n\n\n\n\n\n\n\nggplot(gap_select, aes(x = year, y = lifeExp, color = country)) +\n  geom_line(linewidth = 1.2,na.rm = TRUE)\n\n\n\n\n\n\n\n\n\nggplot(gap_select, aes(x = year, y = lifeExp, color = country)) +\n  stat_summary(fun = mean, geom = \"line\", linewidth = 1, na.rm = TRUE) +\n  labs(\n    title = \"Average Life Expectancy Over Time by Country\",\n    x = \"Year\",\n    y = \"Life expectancy (years)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we made some practice, let’s look at the distribution of life expectancy: as this is dependent on time, we need to explore this in a fixed year if we want to do it meaningfully!\n\nggplot(gap_2000, aes(lifeExp)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", na.rm = TRUE) +\n  labs(\n    title = \"Distribution of Life Expectancy in 2007\",\n    x = \"Life expectancy\",\n    y = \"Number of countries\"\n  )\n\n\n\n\n\n\n\n\nWe can also explore this distribution across several years, using the methods we already know for plotting subgroups\n\nyears_sel &lt;- c(1952, 1977, 2007)\n\ngap_sel &lt;- gap_miss |&gt; filter(year %in% years_sel)\n\nggplot(gap_sel, aes(lifeExp)) +\n  geom_histogram(bins = 25, fill='steelblue3' ,na.rm = TRUE) +\n  facet_wrap(~ year, scales = \"free_y\") +\n  labs(title = \"Life Expectancy Distributions Across Selected Years\")\n\n\n\n\n\n\n\n\nand we can strenghten exploration of the evolution of life expectancy by using density plots\n\nggplot(gap_sel, aes(lifeExp, color = factor(year))) +\n  geom_density(na.rm = TRUE) +\n  labs(title = \"Life Expectancy Density by Year\")\n\n\n\n\n\n\n\n\nTo plot the above, we just picked three years at random. We might explore this more in depth by plotting some statistics over time. Here’s how you plot the time series of mean and median life expectancy\n\ngap_miss |&gt;\n  group_by(year) |&gt;\n  summarise(\n    mean_lifeExp = mean(lifeExp, na.rm = TRUE),\n    median_lifeExp = median(lifeExp, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(year, mean_lifeExp)) +\n  geom_line() +\n  geom_line(aes(y = median_lifeExp), linetype = \"dashed\") +\n  labs(title = \"Mean and Median Life Expectancy Over Time\")\n\n\n\n\n\n\n\n\nIf we had more time, we would explore all the variables like this. Then, we would proceed to explore relationship and association between variables, remembering that we are in the special case of a time series (which will require us to decompose the time series to meaningfully address association).\nHowever, let’s use the rest of our time to focus on missingness. We know some standard plots to explore missingness. The tidyverse allows us to produce some nice variations with minimal code\n\ngap_miss |&gt;\n  mutate(miss_lifeExp = is.na(lifeExp)) |&gt;\n  ggplot(aes(year, fill = miss_lifeExp)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~ continent) +\n  labs(title = \"Proportion of Missing Life Expectancy Over Time\")\n\n\n\n\n\n\n\n\n\n\n\n\nImpute missing values using a time-series appropriated method\n\n\nVisualise the time series of mean life expectancy with imputed data\n\n\nRepeat the visualisation of point 2, but this time showing the separate time series of means (or medians) for each continent, using different colours\n\n\n\nPart 1 is clearly the hardest, especially if you can’t use the zoo library (NCC doesn’t have it by default), so I’ll include code for a possible method.\n\ninterp_linear &lt;- function(x, y) {\n  approx(x, y, xout = x)$y\n}\n\ngap_imp &lt;- gap_miss %&gt;%\n  arrange(country, year) %&gt;%\n  group_by(country) %&gt;%\n  mutate(lifeExp_imp = interp_linear(year, lifeExp)) %&gt;%\n  ungroup()\n\nCan you see what the code is doing?\n\n\nClick for solution\n\n\ngap_imp |&gt;\n  group_by(year) |&gt;\n  summarise(\n    mean_lifeExp = mean(lifeExp, na.rm = TRUE),\n    median_lifeExp = median(lifeExp, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(year, mean_lifeExp)) +\n  geom_line() +\n  geom_line(aes(y = median_lifeExp), linetype = \"dashed\") +\n  labs(title = \"Mean and Median Life Expectancy Over Time\")\n\n\n\n\n\n\n\n\n\ngap_imp |&gt;\n  group_by(year, continent) |&gt;\n  summarise(\n    mean_lifeExp = mean(lifeExp_imp, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  ggplot(aes(year, mean_lifeExp, color = continent)) +\n  geom_line() +\n  labs(title = \"Mean Life Expectancy Over Time by Continent\")"
  },
  {
    "objectID": "Workshop4_AdvancedFunctions.html#case-study-gapminder-data",
    "href": "Workshop4_AdvancedFunctions.html#case-study-gapminder-data",
    "title": "Workshop 4 - More functions for EDA",
    "section": "",
    "text": "Download data: gap_miss\n\n\nRows: 29,393\nColumns: 8\n$ country    &lt;fct&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Angola\", \"Argentina\",…\n$ year       &lt;dbl&gt; 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800, 1800,…\n$ pop        &lt;int&gt; 3280000, 400000, 2500000, 1570000, 534000, 200000, 3000000,…\n$ lifeExp    &lt;dbl&gt; 28.2, 35.4, 28.8, 27.0, 33.2, 34.0, 34.4, NA, 25.5, 40.0, 3…\n$ gdpPercap  &lt;int&gt; 603, 667, 715, 618, 1640, 817, 1850, 1240, 876, 2390, 597, …\n$ fertility  &lt;dbl&gt; 7.00, 4.60, 6.99, 6.93, 6.80, 6.50, 5.10, 7.03, 6.70, 4.85,…\n$ infantMort &lt;dbl&gt; 469, 375, 460, 486, 402, 391, 387, 440, 508, 322, 430, 405,…\n$ continent  &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, As…\n\n\nThe data contain values for population, life expectancy, GDP per capita, population, fertility and infant mortality, every years, from 1900 to 2020. The variables are:\n\ncountry - the country, factor with 142 levels\ncontinent - the continent, factor with 5 levels\nyear - year of observation ranges from 1900 to 2020\nlifeExp - life expectancy at birth, in years\npop - population\ngdpPercap - GDP per capita (US$, inflation-adjusted)\ninfantMort - Death of children under 5 years of age per 1000 live births\nfertility - The number of children that would be born to each woman with prevailing age-specific fertility rates\n\nThis data set is quite complex, as we have five time series observed for multiple countries (i.e. a categorical variable with many levels).\nTo get a feeling for the data, let’s focus on a particular picture of the state of the world in one year - let’s take 2000.\n\n# subset the data\ngm.2000 &lt;- gap_miss[gap_miss$year==2000,]\n# a pairs plot to look for associations\npairs(gm.2000[,3:7],pch=16,col=gm.2000$continent)\n\n\n\n\n\n\n\n## some strong correlations, and mix of linear/nonlinear relationships\n## seems to be an association with continent\n## some outliers in population - India and China\nrownames(gm.2000) &lt;- gm.2000$country\n\n\nWe soon note that 10% of the lifeExp data is missing. Missingness doesn’t seem to follow a precise pattern based on observable variables, but we need more exploration.\n\ncolSums(is.na(gap_miss))\n\n   country       year        pop    lifeExp  gdpPercap  fertility infantMort \n         0          0          0       2939          0          0          0 \n continent \n         0 \n\nvis_miss(gap_miss)"
  },
  {
    "objectID": "Workshop4_AdvancedFunctions.html#making-graphics-with-ggplot",
    "href": "Workshop4_AdvancedFunctions.html#making-graphics-with-ggplot",
    "title": "Workshop 4 - More functions for EDA",
    "section": "",
    "text": "The package ggplot2 is built on the Grammar of Graphics. Every plot is made of layers, and almost every call looks like:\nggplot(data, aes(...)) + geom_...() + labs_...() + scale_...() + theme_...().\nNot all these layers are necessary! The first layer creates a grid: not very interesting\n\ngap_2000 &lt;- gap_miss |&gt; filter(year == 2000)\nggplot(gap_2000, aes(lifeExp))\n\n\n\n\n\n\n\n\nBy using the fisrt two layers, we can already create some basics visualisation, such as histograms\n\nggplot(gap_2000, aes(lifeExp)) +\ngeom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe can now add settings and a new layer that specifies labels\n\nggplot(gap_2000, aes(x = lifeExp)) +\n  geom_histogram(bins = 30, na.rm = TRUE) +\n  labs(\n    title = \"Distribution of Life Expectancy (Year 2000)\",\n    x = \"Life expectancy (years)\",\n    y = \"Number of observations\"\n  )\n\n\n\n\n\n\n\n\nIf we want to explore the distributions of different subgroups, we need to make sure that the second variables is included in the aesthetic, and we change the geometry to geom_density.\n\nggplot(gap_2000, aes(x = lifeExp, fill = continent)) +\n  geom_density(alpha = 0.4, na.rm = TRUE)\n\n\n\n\n\n\n\n\nggplot automatically splits the data by continent, computes one density per group and assigns colors. Quite impressive compared to base R!\nAnother option for comparing continents is to use faceting\n\nggplot(gap_2000, aes(x = lifeExp)) +\n  geom_histogram(bins = 25, na.rm = TRUE) +\n  facet_wrap(~ continent) +\n  labs(title = \"Life Expectancy by Continent\")\n\n\n\n\n\n\n\n\nAnd the beauty of it is that it makes it intuitive to superimpose multiple layers. Let’s try to draw a scatterplot of (log-transformed) gdpPercap versus lifeExp with colouring depending on continent and a smoothing line for each group\n\nggplot(gap_2000, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(alpha = 0.5, na.rm = TRUE) +\n  geom_smooth(se = FALSE, na.rm = TRUE) +\n  scale_x_log10() +\n  labs(\n    title = \"Life Expectancy vs GDP per Capita\",\n    x = \"GDP per capita (log scale)\",\n    y = \"Life expectancy\"\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNow it’s your turn:\nstarting from the following basic code\n\ncountries_sel &lt;- c(\"France\", \"Germany\", \"United States\",\"China\", \"India\", \"Brazil\")\n\ngap_select &lt;- gap_miss %&gt;% filter(country %in% countries_sel)\n\nggplot(gap_select, aes(x = year, y = lifeExp)) +\n  geom_point(na.rm = TRUE)\n\n\n\n\n\n\n\n\nsee if you can perform the following tasks.\nYou can look for inspiration at this section of the R graph gallery.\n\n\n\n\nModify the code so that different countries are shown in different colors. Make points slightly transparent to reduce overplotting.\n\n\nInstead of a scatterplot, try to change the geometry in order to make a linechart Try to see if you can make the line thicker.\n\n\nImprove the plot by giving it a clear title and axis labels, using a cleaner theme and making the y-axis easier to read.\n\n\n\n\n\nClick for (a possible) solution\n\n\nggplot(gap_select, aes(x = year, y = lifeExp, color = country)) +\n  geom_point(alpha = 0.4, na.rm = TRUE)\n\n\n\n\n\n\n\n\n\nggplot(gap_select, aes(x = year, y = lifeExp, color = country)) +\n  geom_line(linewidth = 1.2,na.rm = TRUE)\n\n\n\n\n\n\n\n\n\nggplot(gap_select, aes(x = year, y = lifeExp, color = country)) +\n  stat_summary(fun = mean, geom = \"line\", linewidth = 1, na.rm = TRUE) +\n  labs(\n    title = \"Average Life Expectancy Over Time by Country\",\n    x = \"Year\",\n    y = \"Life expectancy (years)\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Workshop4_AdvancedFunctions.html#one-dimensional-exploration",
    "href": "Workshop4_AdvancedFunctions.html#one-dimensional-exploration",
    "title": "Workshop 4 - More functions for EDA",
    "section": "",
    "text": "Now that we made some practice, let’s look at the distribution of life expectancy: as this is dependent on time, we need to explore this in a fixed year if we want to do it meaningfully!\n\nggplot(gap_2000, aes(lifeExp)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", na.rm = TRUE) +\n  labs(\n    title = \"Distribution of Life Expectancy in 2007\",\n    x = \"Life expectancy\",\n    y = \"Number of countries\"\n  )\n\n\n\n\n\n\n\n\nWe can also explore this distribution across several years, using the methods we already know for plotting subgroups\n\nyears_sel &lt;- c(1952, 1977, 2007)\n\ngap_sel &lt;- gap_miss |&gt; filter(year %in% years_sel)\n\nggplot(gap_sel, aes(lifeExp)) +\n  geom_histogram(bins = 25, fill='steelblue3' ,na.rm = TRUE) +\n  facet_wrap(~ year, scales = \"free_y\") +\n  labs(title = \"Life Expectancy Distributions Across Selected Years\")\n\n\n\n\n\n\n\n\nand we can strenghten exploration of the evolution of life expectancy by using density plots\n\nggplot(gap_sel, aes(lifeExp, color = factor(year))) +\n  geom_density(na.rm = TRUE) +\n  labs(title = \"Life Expectancy Density by Year\")\n\n\n\n\n\n\n\n\nTo plot the above, we just picked three years at random. We might explore this more in depth by plotting some statistics over time. Here’s how you plot the time series of mean and median life expectancy\n\ngap_miss |&gt;\n  group_by(year) |&gt;\n  summarise(\n    mean_lifeExp = mean(lifeExp, na.rm = TRUE),\n    median_lifeExp = median(lifeExp, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(year, mean_lifeExp)) +\n  geom_line() +\n  geom_line(aes(y = median_lifeExp), linetype = \"dashed\") +\n  labs(title = \"Mean and Median Life Expectancy Over Time\")\n\n\n\n\n\n\n\n\nIf we had more time, we would explore all the variables like this. Then, we would proceed to explore relationship and association between variables, remembering that we are in the special case of a time series (which will require us to decompose the time series to meaningfully address association).\nHowever, let’s use the rest of our time to focus on missingness. We know some standard plots to explore missingness. The tidyverse allows us to produce some nice variations with minimal code\n\ngap_miss |&gt;\n  mutate(miss_lifeExp = is.na(lifeExp)) |&gt;\n  ggplot(aes(year, fill = miss_lifeExp)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~ continent) +\n  labs(title = \"Proportion of Missing Life Expectancy Over Time\")\n\n\n\n\n\n\n\n\n\n\n\n\nImpute missing values using a time-series appropriated method\n\n\nVisualise the time series of mean life expectancy with imputed data\n\n\nRepeat the visualisation of point 2, but this time showing the separate time series of means (or medians) for each continent, using different colours\n\n\n\nPart 1 is clearly the hardest, especially if you can’t use the zoo library (NCC doesn’t have it by default), so I’ll include code for a possible method.\n\ninterp_linear &lt;- function(x, y) {\n  approx(x, y, xout = x)$y\n}\n\ngap_imp &lt;- gap_miss %&gt;%\n  arrange(country, year) %&gt;%\n  group_by(country) %&gt;%\n  mutate(lifeExp_imp = interp_linear(year, lifeExp)) %&gt;%\n  ungroup()\n\nCan you see what the code is doing?\n\n\nClick for solution\n\n\ngap_imp |&gt;\n  group_by(year) |&gt;\n  summarise(\n    mean_lifeExp = mean(lifeExp, na.rm = TRUE),\n    median_lifeExp = median(lifeExp, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(year, mean_lifeExp)) +\n  geom_line() +\n  geom_line(aes(y = median_lifeExp), linetype = \"dashed\") +\n  labs(title = \"Mean and Median Life Expectancy Over Time\")\n\n\n\n\n\n\n\n\n\ngap_imp |&gt;\n  group_by(year, continent) |&gt;\n  summarise(\n    mean_lifeExp = mean(lifeExp_imp, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  ggplot(aes(year, mean_lifeExp, color = continent)) +\n  geom_line() +\n  labs(title = \"Mean Life Expectancy Over Time by Continent\")"
  },
  {
    "objectID": "Workshop5_PCA.html",
    "href": "Workshop5_PCA.html",
    "title": "Workshop 1 Principal Component Analysis",
    "section": "",
    "text": "Principal component analysis (PCA) is a technique that produces a smaller set of uncorrelated variables from a larger set of correlated variables. This allows us to visualize, explore, and discover hidden features of the data set. It is also very useful to perform a special type of multilinear regression (see Principal Components Regression). Here we explore how to perform PCA using R, how to read and visualize the output, as well as good practices and how to avoid pitfalls. For a more theoretical viewpoint on PCA, you are referred to the lecture notes.\nIn R, PCA is performed using either princomp() or prcomp() (the latter being the preferred method)."
  },
  {
    "objectID": "Workshop5_PCA.html#iris-data",
    "href": "Workshop5_PCA.html#iris-data",
    "title": "Workshop 1 Principal Component Analysis",
    "section": "Iris data",
    "text": "Iris data\nLet us put this into practice with the Iris data set. You get the data in R by typing data(iris) in the console (the data set is also on Blackboard in a csv file iris.csv).\nWe begin by reading the data into R and printing out the first six rows of the data.\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nWe can also look at the summary statistics of the numerical variables:\n\nsummary(iris[,-5])\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n\n\nFinally, we can plot a scatter plot matrix using pairs.panels function in the psych package:\n\n#install.packages('psych')\nlibrary(psych)\npairs.panels(iris[,-5],\n             method = \"pearson\", # correlation method\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # show density plots\n             ellipses = TRUE # show correlation ellipses\n             )\n\n\n\n\n\n\n\n\nWhat can you say about the plot? Do the variables share sufficient information to warrant redundancy?\nYou can see that the Petal variables are highly correlated (correlation =0.96). They are also highly correlated with the Sepal length variable. This implies that the three variables share redundant information, so the use of PCA can remove this redundancy, thereby reducing the dimension of the data.\nLet us explore how the function prcomp() can help us with dimension reduction:\n\ndat &lt;- iris[,-5]\npr.out &lt;- prcomp(dat, scale = TRUE)\nnames(pr.out)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\nsummary(pr.out)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\nWe find here five quantities of interest. The most important are sdev and rotation.\n\npr.out$rotation\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n\nThis matrix has four columns, called \\(PC1\\), \\(PC2\\), and so on. They represent the principal components of our data set, i.e. a new basis of the data space that explains better the variability of the data. The coefficients (i.e. the numbers in each column) are called the loadings, so you will find the terminology loading vector of the principal component PC1 to mean the vector underlying PC1.\nObserve: the loading vectors are pairwise orthogonal, and in fact orthonormal (in addition to being orthogonal, each one has Euclidean norm 1). Let us check this:\n\nPC &lt;- function(i){pr.out$rotation[,i]}\nPC(1)%*%PC(2)\n\n             [,1]\n[1,] 9.714451e-17\n\nPC(1)%*%PC(1)\n\n     [,1]\n[1,]    1\n\nPC(4)%*%PC(4)\n\n     [,1]\n[1,]    1\n\n\nHow exactly are principal components better in capturing the data variability? By design (see lectures), \\(PC1\\) spans the direction that captures the biggest proportion of variance in the observations, \\(PC2\\) is chosen among all the vectors that are orthonormal to \\(PC1\\) as one spanning the direction that captures the most variance, and so on. The constraint that these vectors are pairwise orthonomal reduces a lot the possible choices.\nWe can think about this geometrically: \\(PC1\\) is the closest line to the data points, \\(\\langle PC1, PC2 \\rangle\\) the closest plane to the data points, \\(\\langle PC1, PC2, PC3 \\rangle\\) the closest 3-dimensional vector space, and so on… More on this in the lectures.\nTo retain: the most important component is \\(PC1\\): it is the direction where the data set is more variable.\nHow do we measure this importance?\n\npr.var &lt;- pr.out$sdev^2\npr.var\n\n[1] 2.91849782 0.91403047 0.14675688 0.02071484\n\nsum(pr.var)\n\n[1] 4\n\n\nWe have then discovered how the quantities given under Importance of components are calculated: they are obtained from the variance of the principal components. We can make a screeplot to visualize this better.\n\nplot(pr.out, col = \"steelblue\", main=\"Importance of Iris PC\")\nlines(x = 1:4, pr.var, type=\"b\", pch=19, col = \"red\")\n\n\n\n\n\n\n\n\nThe component \\(PC1\\) accounts for a large proportion of variance (72.96%), \\(PC2\\) for some of it, and the other directions are essentially variance free. Geometrically, this means that the points are very close to the plane \\(\\langle PC1, PC2 \\rangle\\). It then makes sense to analyze and visualize our data set using this plane. How?\nThe most natural thing to do is to look at the position of the data points when they get projected onto this plane. The coefficients of the observations in the basis \\((PC1, PC2, PC3, PC4)\\) are called the scores. They make up the feature denoted by x in the output of prcomp()\n\npr.out$x\nsummary(pr.out$x)\n\nNote that the scores of all principal components have mean zero. Why?\n\nTask: Using linear algebra to compute scores\nWe can compute the scores of a single observation, for example the one corresponding to the first row of our data matrix, by observing that they are obtained by multiplication with the inverse of the rotation matrix. You need to recall how to make a change of basis in linear algebra to make sense of this. The output should look like the following:\n\npr.out$x[1,]\n\n        PC1         PC2         PC3         PC4 \n-2.25714118 -0.47842383  0.12727962  0.02408751 \n\n\n\n\nSolution to Task\n\n\n\n\n\n\nTo understand how principal components modify our datascape, we can look at the scatterplot matrix obtained by replacing the original data set with the data set of principal component scores.\n\npairs.panels(pr.out$x,\n             method = \"pearson\", # correlation method\n             hist.col = \"#00AFBB\",#hexadecimal colour code\n             density = TRUE,  # show density plots\n             ellipses = TRUE # show correlation ellipses\n             )\n\n\n\n\n\n\n\n\nThe variables are now uncorrelated! This is not a coincidence, as we have seen in the lectures.\nTo visualize the scores of the first two principal components we don’t have to compute them one by one. The biplot() command can be used, but we can achieve a better visualization using autoplot() (from the ggfortify library)\n\n#install.packages('ggfortify')\nlibrary(ggfortify)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following objects are masked from 'package:psych':\n\n    %+%, alpha\n\nautoplot(pr.out, data = iris, colour = \"Species\") #, label = TRUE\n\n\n\n\n\n\n\n\nUsing the option loadings, we can add to this plot the projections of the original variables (Sepal Length, Sepal Width, etc.) on the plane \\(\\langle PC1, PC2 \\rangle\\). This is a good way to visualize the loading vectors.\n\nautoplot(pr.out, data = iris, colour = 'Species',\n         loadings = TRUE, loadings.colour = 'blue',\n         loadings.label = TRUE, loadings.label.size = 3)\n\n\n\n\n\n\n\n\nHere we can see how much the original variables contribute to the two first principal components. Petal length, Petal width and Sepal length are the features contributing the most to \\(PC1\\). Sepal width is the feature that mostly contribute to \\(PC2\\).\n\nA clearer way to see how each of the original variables contribute to the new dimensions using the figures below:\n\nlibrary(\"factoextra\")\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n# Contributions of variables to PC1\nfviz_contrib(pr.out, choice = \"var\", axes = 1, top = 4)\n\n\n\n\n\n\n\n\n\n# Contributions of variables to PC2\nfviz_contrib(pr.out, choice = \"var\", axes = 2, top = 4)\n\n\n\n\n\n\n\n\nThe plot below simultaneously shows the strength of each of the original variables on both PC1 and PC2.\n\nfviz_contrib(pr.out, choice = \"var\", axes = 1:2, top = 4)\n\n\n\n\n\n\n\n\n\n\nWe can produce a variety of biplots. Below is another one.\n\nfviz_pca_biplot(pr.out, \n                col.ind = iris$Species, palette = \"jco\", \n                addEllipses = TRUE, label = \"var\",\n                col.var = \"black\", repel = TRUE,\n                legend.title = \"Species\")"
  },
  {
    "objectID": "Workshop5_PCA.html#the-importance-of-scaling",
    "href": "Workshop5_PCA.html#the-importance-of-scaling",
    "title": "Workshop 1 Principal Component Analysis",
    "section": "The importance of scaling",
    "text": "The importance of scaling\nIn the previous example, note that we have applied PCA to the scaled iris data (through the command `scale=TRUE’).\nWhat happens if we don’t do this?\nFirst, we get different loading vectors:\n\ndat &lt;- iris[,-5]\npr2.out &lt;- prcomp(dat)\nsummary(pr.out)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\nsummary(pr2.out)\n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000\n\n\nNote that the first principal component becomes much more prominent: it explains a much larger proportion of the variance. Why?\nBecause the feature with larger variance weights much more now!\nWe can verify this by comparing the standard deviations of the features:\n\napply(dat,2,sd)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n\n\nPetal Length is significantly more variable than the others.\nWe can clearly see that the results are skewed more towards the features with larger variance (Petal Length) when we look at the loadings plot:\n\nautoplot(pr2.out, data = iris, colour = \"Species\", loadings = TRUE, loadings.colour = 'blue', loadings.label = TRUE, loadings.label.size = 3)\n\n\n\n\n\n\n\n\nHere we were lucky because all features were expressed in the same unit (cm), but it may get worse: by changing units, variance changes too: for example if Sepal Length were to be expressed in meters, that would significantly reduce its standard deviation!\nWe want PCA to be independent of rescaling, so scaling the variables will help us with this issue. If the units are the same and the variances close to each other it is less problematic to not scale the data. This has the advantage of avoiding a rather clumsy manipulation of the data.\nIs there any situation in which it is advisable not to normalize? Yes! When the variance of the original features is important, or if one wants to give more importance to those features that have more variability. We’ll see an example in the lecture.\n\n\n\nStep-by-step analysis of iris data- Eigendecomposition\n\n\nAll that we have done so far was based on the use of a ready made use of an already packaged function, prcomp(), in R. We can use our linear algebra skills to carry out the analysis.\nLet \\(\\Sigma\\) be the covariance matrix of data set \\(x\\) with ordered eigenvalue-eigenvector pairs \\((\\lambda_j, \\phi_j), j = 1, \\ldots, p\\). Then\n\nthe principal components are \\[z_{j} =\\phi_{j}^{T}x =  \\phi_{j1}x_{1} + \\phi_{j2}x_{2} + \\ldots \\phi_{jp}x_{p}\\]\n\\(\\mbox{var}(z_{j}) = \\lambda_{j}\\)\n\\(\\mbox{Covariance}(z_{j},z_{i}) =  \\phi_{j}^{T} \\Sigma \\phi_{i} = 0.\\)\n\n\nNow,\n\ndata(iris)\niris2 &lt;- scale(iris[,-5])\nS &lt;- cov(iris2)# now a correlation matrix\nmy_var = eigen(S)$values # variance\npc_loading = eigen(S)$vectors #loading score\npc_names &lt;- paste0(\"PC\", 1:ncol(iris2))\nrownames(pc_loading) &lt;- colnames(iris2)\ncolnames(pc_loading) &lt;- pc_names\n\nmy_var\n\n[1] 2.91849782 0.91403047 0.14675688 0.02071484\n\npc_loading\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n\nCompare this with pr.out$rotation\nPrincipal Components are the factor loading:\n\n\\(z_1 = 0.521* Sepal.Length  + (-0.269)* Sepal.Width + 0.580*Petal.Length  + 0.565* Petal.Width\\)\n\n\\(z_2 = -0.377* Sepal.Length  + (-0.923)* Sepal.Width + (-0.025)*Petal.Length  + (-0.067)* Petal.Width\\)\n\n\\(z_3 = 0.720* Sepal.Length  + (-0.244)* Sepal.Width + (-0.142)*Petal.Length  + (-0.634)* Petal.Width\\)\n\n\\(z_4 = 0.261* Sepal.Length  + (-0.124)* Sepal.Width + (-0.801)*Petal.Length  + (0.524)* Petal.Width\\)\n\n\n\n\npc_score = as.matrix(iris2)%*%pc_loading\npc_score[1:3,]\n\n           PC1        PC2        PC3        PC4\n[1,] -2.257141 -0.4784238  0.1272796 0.02408751\n[2,] -2.074013  0.6718827  0.2338255 0.10266284\n[3,] -2.356335  0.3407664 -0.0440539 0.02828231\n\n\nWe can compute the proportion of variance explained\n\nvariance &lt;- my_var*100/sum(my_var)\n# Cumulative variances\ncumvar &lt;- cumsum(variance)\neig2&lt;- data.frame(eig = my_var, Percent.variance = variance,\n                  cumvariance = cumvar)\neig2\n\n         eig Percent.variance cumvariance\n1 2.91849782       72.9624454    72.96245\n2 0.91403047       22.8507618    95.81321\n3 0.14675688        3.6689219    99.48213\n4 0.02071484        0.5178709   100.00000\n\n\n\nbarplot(eig2[, 2], names.arg=1:nrow(eig2),#use the names of the row for each bar \n        main = \"Variances\",\n        xlab = \"Dimensions\",\n        ylab = \"Percentage of variances\",\n        col =\"steelblue\")\n# Add connected line segments to the plot\nlines(x = 1:nrow(eig2), eig2[, 2], \n      type=\"b\", pch=19, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nTask: Using SVD\n\n\nNow, as a small exercise, try to obtain the factor loadings using Singular Value Decomposition (SVD), which is the method that the prcomp() function in R is based on. Compare your loadings with those obtained via SVD.\n\n\nSolution to Task:"
  },
  {
    "objectID": "Workshop5_PCA.html#reconstruction-error",
    "href": "Workshop5_PCA.html#reconstruction-error",
    "title": "Workshop 1 Principal Component Analysis",
    "section": "Reconstruction Error",
    "text": "Reconstruction Error\nWe can reconstruct the data matrix using the principal components and verify the reconstruction error. Assuming we retained all four PCs, we can compute the reconstruction errors using the following code:\n\nreconstructed_data &lt;- sweep(\n  sweep(pr.out$x %*% t(pr.out$rotation), \n        2, attr(iris2, \"scaled:scale\"), \"*\"), \n  2, attr(iris2, \"scaled:center\"), \"+\")\nreconstruction_error &lt;- sum((iris[, -5] - reconstructed_data)^2)\nreconstruction_error\n\n[1] 3.145436e-28\n\n\nAs you can see, the reconstruction error is very small (close to 0) because all the principal components are used. This indicates that no variance is lost during the reconstruction. If fewer components are retained, the error will reflect the information loss due to dimensionality reduction.\n\n\n\nTask: Reconstruction Error\n\nUpdate the code above to compute the reconstruction errors when 2 and 3 PCs are retained. Use the following steps as a guide:\n\n# Retain fewer PCs (e.g., 2)\nk &lt;- 2\nreconstructed_data_k &lt;- sweep(sweep(pr.out$x[, 1:k] %*% t(pr.out$rotation[, 1:k]), \n                                    2, attr(iris2, \"scaled:scale\"), \"*\"), \n                               2, attr(iris2, \"scaled:center\"), \"+\")\nreconstruction_error_k &lt;- sum((iris[, -5] - reconstructed_data_k)^2)\nreconstruction_error_k\n\n[1] 21.32238\n\n\nCompare the reconstruction errors when 2 and 3 PCs are retained. How do these errors relate to the cumulative variance explained by the retained components? Can you explain the results?"
  },
  {
    "objectID": "Lecture5a_PCA.html",
    "href": "Lecture5a_PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The main two types of statistical learning are: * Supervised learning, which is “learning with a teacher”: the student present an estimate response for each observation, the teacher provides the correct answer and the student improves their models. * Unsupervised learning, which is “learning without a teacher”. In this respect, it is more challenging (no training, no validation, no simple goal). But sometimes you can’t avoid using it!\nUnsupervised learning is often overlooked, but it has many applications: recommender systems, document search, fake image analysis, risk management, …\n\n\n\nDimensionality reduction methods aim to reduce the number of variables, retaining much of the properties of the original data. They are used to * Improve storage and computational complexity * Prevent overfitting in data analysis * Reduce noise and detecting anomalies The most important technique for dimensionality reduction is principal component analysis (PCA).\n\n\n\nNow that we are backed by the theory, let’s explore once again how to perform PCA with R. We will analyze the US Senate data sets, reduce its dimension in an appropriate way and see what this analysis tells us about the evolution of political parties in the US.\n\n\nYou can download the data sets from Blackboard as .csv files in the folder USsenate.\nWe first start by loading the datasets in our environment together with the libraries.\n\n#install.packages(\"factoextra\")\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata07 &lt;- read.csv(\"senate-2007.csv\")\ndata17 &lt;- read.csv(\"senate-2017.csv\")\n\nWe then select only the numeric features:\n\ndata07N &lt;- data07 %&gt;%\n  select_if(is.numeric)\ndata17N &lt;- data17 %&gt;%\n  select_if(is.numeric)\n\nFirst of all, note that you can’t check the correlation matrix from a plot matrix, as it is just too big! We can try to visualize the matrix with a color plot:\n\nSigma07 &lt;- cov(data07N)\nSigma17 &lt;- cov(data17N)\npar(mfrow=c(1, 2))\nimage(t(Sigma07), main = \"Covariance matrix 07\")\nimage(t(Sigma17), main = \"Covariance matrix 17\")\n\n\n\n\n\n\n\n\nThere is at least some redundancy in the data, which makes it worthwhile to try PCA. In the second plot this is more evident, so we might guess that PCA will be even more effective there.\n/ Let us get the principal components using prcomp(). There is an argument to be made in favor of not to scaling the data: high variance in an observation means a higher level of disagreement in senators votes, while low variance means that most of them voted in the same way. If we care more about those issues that are more debated (e.g. gun laws) and less about those on which everyone agrees (e.g. raising senators’ salary) then it is preferable not to scale the data.\n\npr.out07 &lt;- prcomp(data07N)\npr.out17 &lt;- prcomp(data17N)\n\nThe output contains many objects\n\nnames(pr.out07)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nWe can check now the importance of principal components in the two cases\n\nsummary(pr.out07)\n\nImportance of components:\n                           PC1     PC2     PC3    PC4     PC5    PC6     PC7\nStandard deviation     12.9042 3.71408 3.03882 2.4098 2.19140 2.0549 1.91235\nProportion of Variance  0.5562 0.04607 0.03084 0.0194 0.01604 0.0141 0.01221\nCumulative Proportion   0.5562 0.60225 0.63310 0.6525 0.66853 0.6826 0.69485\n                           PC8     PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     1.81034 1.79620 1.77976 1.7214 1.67003 1.60616 1.56163\nProportion of Variance 0.01095 0.01078 0.01058 0.0099 0.00932 0.00862 0.00815\nCumulative Proportion  0.70580 0.71657 0.72715 0.7370 0.74637 0.75498 0.76313\n                          PC15    PC16    PC17    PC18   PC19    PC20    PC21\nStandard deviation     1.53252 1.51194 1.48791 1.47152 1.4476 1.40269 1.39002\nProportion of Variance 0.00784 0.00764 0.00739 0.00723 0.0070 0.00657 0.00645\nCumulative Proportion  0.77097 0.77861 0.78600 0.79324 0.8002 0.80681 0.81326\n                          PC22    PC23    PC24    PC25    PC26   PC27    PC28\nStandard deviation     1.35985 1.32609 1.32011 1.28057 1.27026 1.2595 1.22948\nProportion of Variance 0.00618 0.00587 0.00582 0.00548 0.00539 0.0053 0.00505\nCumulative Proportion  0.81944 0.82531 0.83113 0.83661 0.84200 0.8473 0.85234\n                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\nStandard deviation     1.22124 1.21861 1.19118 1.16494 1.14058 1.12554 1.10577\nProportion of Variance 0.00498 0.00496 0.00474 0.00453 0.00435 0.00423 0.00408\nCumulative Proportion  0.85733 0.86229 0.86703 0.87156 0.87590 0.88013 0.88422\n                          PC36    PC37    PC38    PC39   PC40    PC41    PC42\nStandard deviation     1.09615 1.07222 1.06166 1.04658 1.0378 1.02996 1.00640\nProportion of Variance 0.00401 0.00384 0.00376 0.00366 0.0036 0.00354 0.00338\nCumulative Proportion  0.88823 0.89207 0.89584 0.89949 0.9031 0.90664 0.91002\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.99843 0.98258 0.98042 0.96616 0.95144 0.94407 0.91291\nProportion of Variance 0.00333 0.00322 0.00321 0.00312 0.00302 0.00298 0.00278\nCumulative Proportion  0.91335 0.91657 0.91978 0.92290 0.92592 0.92890 0.93168\n                          PC50    PC51    PC52    PC53    PC54    PC55    PC56\nStandard deviation     0.90311 0.89371 0.87048 0.86888 0.85340 0.84171 0.82562\nProportion of Variance 0.00272 0.00267 0.00253 0.00252 0.00243 0.00237 0.00228\nCumulative Proportion  0.93441 0.93708 0.93961 0.94213 0.94456 0.94693 0.94920\n                          PC57    PC58    PC59    PC60   PC61    PC62   PC63\nStandard deviation     0.82342 0.81051 0.80683 0.79840 0.7925 0.77049 0.7552\nProportion of Variance 0.00226 0.00219 0.00217 0.00213 0.0021 0.00198 0.0019\nCumulative Proportion  0.95147 0.95366 0.95584 0.95797 0.9601 0.96205 0.9639\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.74195 0.73172 0.72879 0.70627 0.70032 0.68822 0.67301\nProportion of Variance 0.00184 0.00179 0.00177 0.00167 0.00164 0.00158 0.00151\nCumulative Proportion  0.96579 0.96758 0.96935 0.97102 0.97266 0.97424 0.97575\n                         PC71   PC72   PC73    PC74    PC75    PC76    PC77\nStandard deviation     0.6712 0.6699 0.6478 0.63796 0.63142 0.61631 0.61038\nProportion of Variance 0.0015 0.0015 0.0014 0.00136 0.00133 0.00127 0.00124\nCumulative Proportion  0.9773 0.9788 0.9802 0.98152 0.98285 0.98412 0.98536\n                          PC78    PC79    PC80   PC81    PC82    PC83    PC84\nStandard deviation     0.59675 0.58376 0.56136 0.5481 0.53674 0.52440 0.51468\nProportion of Variance 0.00119 0.00114 0.00105 0.0010 0.00096 0.00092 0.00088\nCumulative Proportion  0.98655 0.98769 0.98874 0.9898 0.99071 0.99163 0.99251\n                          PC85   PC86    PC87    PC88    PC89    PC90    PC91\nStandard deviation     0.50177 0.4885 0.48019 0.45426 0.43499 0.43203 0.40844\nProportion of Variance 0.00084 0.0008 0.00077 0.00069 0.00063 0.00062 0.00056\nCumulative Proportion  0.99335 0.9941 0.99492 0.99561 0.99624 0.99686 0.99742\n                          PC92    PC93    PC94    PC95    PC96    PC97    PC98\nStandard deviation     0.39534 0.36208 0.35420 0.31679 0.31342 0.29432 0.27198\nProportion of Variance 0.00052 0.00044 0.00042 0.00034 0.00033 0.00029 0.00025\nCumulative Proportion  0.99794 0.99838 0.99880 0.99914 0.99946 0.99975 1.00000\n                           PC99\nStandard deviation     1.36e-14\nProportion of Variance 0.00e+00\nCumulative Proportion  1.00e+00\n\nsummary(pr.out17)\n\nImportance of components:\n                           PC1    PC2     PC3     PC4     PC5    PC6     PC7\nStandard deviation     12.0503 2.9027 1.82664 1.70235 1.29058 1.2370 1.19967\nProportion of Variance  0.7686 0.0446 0.01766 0.01534 0.00882 0.0081 0.00762\nCumulative Proportion   0.7686 0.8132 0.83089 0.84623 0.85505 0.8631 0.87077\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.11374 1.07762 1.05381 1.03398 1.01148 0.95085 0.91687\nProportion of Variance 0.00657 0.00615 0.00588 0.00566 0.00542 0.00479 0.00445\nCumulative Proportion  0.87733 0.88348 0.88936 0.89502 0.90043 0.90522 0.90967\n                          PC15    PC16    PC17    PC18    PC19   PC20    PC21\nStandard deviation     0.89851 0.86111 0.84270 0.81861 0.81776 0.7896 0.77534\nProportion of Variance 0.00427 0.00393 0.00376 0.00355 0.00354 0.0033 0.00318\nCumulative Proportion  0.91394 0.91787 0.92163 0.92517 0.92871 0.9320 0.93520\n                         PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.7404 0.73203 0.71876 0.70269 0.67188 0.65235 0.64574\nProportion of Variance 0.0029 0.00284 0.00273 0.00261 0.00239 0.00225 0.00221\nCumulative Proportion  0.9381 0.94093 0.94367 0.94628 0.94867 0.95092 0.95313\n                          PC29    PC30    PC31    PC32   PC33    PC34    PC35\nStandard deviation     0.63316 0.62168 0.60849 0.60280 0.5667 0.56380 0.55342\nProportion of Variance 0.00212 0.00205 0.00196 0.00192 0.0017 0.00168 0.00162\nCumulative Proportion  0.95525 0.95730 0.95926 0.96118 0.9629 0.96456 0.96619\n                          PC36   PC37    PC38    PC39    PC40    PC41    PC42\nStandard deviation     0.55231 0.5329 0.52293 0.51992 0.50170 0.49733 0.48640\nProportion of Variance 0.00161 0.0015 0.00145 0.00143 0.00133 0.00131 0.00125\nCumulative Proportion  0.96780 0.9693 0.97075 0.97218 0.97351 0.97482 0.97608\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.48504 0.48082 0.46988 0.46212 0.44674 0.43645 0.43040\nProportion of Variance 0.00125 0.00122 0.00117 0.00113 0.00106 0.00101 0.00098\nCumulative Proportion  0.97732 0.97855 0.97971 0.98084 0.98190 0.98291 0.98389\n                          PC50    PC51    PC52   PC53    PC54    PC55    PC56\nStandard deviation     0.41828 0.40480 0.39884 0.3895 0.38140 0.37360 0.36564\nProportion of Variance 0.00093 0.00087 0.00084 0.0008 0.00077 0.00074 0.00071\nCumulative Proportion  0.98482 0.98568 0.98652 0.9873 0.98810 0.98884 0.98954\n                          PC57    PC58    PC59    PC60    PC61    PC62    PC63\nStandard deviation     0.35721 0.35284 0.34250 0.32554 0.32372 0.32072 0.30358\nProportion of Variance 0.00068 0.00066 0.00062 0.00056 0.00055 0.00054 0.00049\nCumulative Proportion  0.99022 0.99088 0.99150 0.99206 0.99262 0.99316 0.99365\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.30149 0.28769 0.28589 0.27252 0.26746 0.25620 0.25116\nProportion of Variance 0.00048 0.00044 0.00043 0.00039 0.00038 0.00035 0.00033\nCumulative Proportion  0.99413 0.99457 0.99500 0.99539 0.99577 0.99612 0.99645\n                          PC71   PC72    PC73    PC74    PC75    PC76   PC77\nStandard deviation     0.24160 0.2385 0.22971 0.21421 0.20824 0.20091 0.1944\nProportion of Variance 0.00031 0.0003 0.00028 0.00024 0.00023 0.00021 0.0002\nCumulative Proportion  0.99676 0.9971 0.99734 0.99759 0.99781 0.99803 0.9982\n                          PC78    PC79    PC80    PC81    PC82    PC83    PC84\nStandard deviation     0.19135 0.18550 0.17552 0.16872 0.16317 0.15708 0.14630\nProportion of Variance 0.00019 0.00018 0.00016 0.00015 0.00014 0.00013 0.00011\nCumulative Proportion  0.99842 0.99860 0.99877 0.99892 0.99906 0.99919 0.99930\n                          PC85   PC86    PC87    PC88    PC89    PC90    PC91\nStandard deviation     0.14214 0.1355 0.12813 0.12415 0.11872 0.10075 0.09239\nProportion of Variance 0.00011 0.0001 0.00009 0.00008 0.00007 0.00005 0.00005\nCumulative Proportion  0.99941 0.9995 0.99959 0.99968 0.99975 0.99980 0.99985\n                          PC92    PC93    PC94    PC95    PC96    PC97\nStandard deviation     0.08683 0.07978 0.07389 0.06650 0.05529 0.04076\nProportion of Variance 0.00004 0.00003 0.00003 0.00002 0.00002 0.00001\nCumulative Proportion  0.99989 0.99992 0.99995 0.99998 0.99999 1.00000\n                            PC98      PC99\nStandard deviation     1.156e-14 1.105e-15\nProportion of Variance 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00\n\n\nIn the first case, one needs 19 principal components to capture \\(\\geq 80\\%\\) of variance, in the second case, it is sufficient to retain two of them! This is particularly striking, as our original set of variables contains more than 250 items! In both cases, \\(PC1\\) captures a lot of variance, so it is something interesting to look at.\nTo convince ourselves of this, we can make a screeplot\n\nrequire(gridExtra)\n\nLoading required package: gridExtra\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nplot1 &lt;- fviz_eig(pr.out07)\nplot2 &lt;- fviz_eig(pr.out17)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nNote that, even in the first case, there is a big discrepancy between PC1 and the other principal components. In this case one can make an exception to the “80% rule” by noticing two things:\n\nOriginal variables are a lot.\nThere is a clear elbow in the screeplot: this is the point where the edges begin to flatten, and it occurs right after PC1.\n\nAccording to this elbow method, we can make an argument for retaining only the first principal component.\nEven when only one principal component is deemed to be sufficient, it is worthwhile to visualize scores on a biplot, that can be slightly squeezed on the horizontal axis, if needed.\n\nplot1 &lt;- fviz_pca_ind(pr.out07, label=FALSE)\nplot2 &lt;- fviz_pca_ind(pr.out17, label=FALSE)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nWe see a clear divide between senators on the left and on the right of the principal component. What could this represent?\n\nplot1 &lt;- fviz_pca_ind(pr.out07, col.ind = data07$Party, label=FALSE)\nplot2 &lt;- fviz_pca_ind(pr.out17, col.ind = data17$Party, label=FALSE)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nThe first principal component is a very good representation of party affiliation! From the historic comparison of the two plots, we can see that the divide between Democrats and Republicans has grown a lot between 2007 and 2017, this is an indication that polarization of senators along party lines is a phenomenon that predates the election of Donald Trump (2016)."
  },
  {
    "objectID": "Lecture5a_PCA.html#pca-using-princomp",
    "href": "Lecture5a_PCA.html#pca-using-princomp",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Now that we are backed by the theory, let’s explore once again how to perform PCA with R. We will analyze the US Senate data sets, reduce its dimension in an appropriate way and see what this analysis tells us about the evolution of political parties in the US.\n\n\nYou can download the data sets from Blackboard as .csv files in the folder USsenate.\nWe first start by loading the datasets in our environment together with the libraries.\n\n#install.packages(\"factoextra\")\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata07 &lt;- read.csv(\"senate-2007.csv\")\ndata17 &lt;- read.csv(\"senate-2017.csv\")\n\nWe then select only the numeric features:\n\ndata07N &lt;- data07 %&gt;%\n  select_if(is.numeric)\ndata17N &lt;- data17 %&gt;%\n  select_if(is.numeric)\n\nFirst of all, note that you can’t check the correlation matrix from a plot matrix, as it is just too big! We can try to visualize the matrix with a color plot:\n\nSigma07 &lt;- cov(data07N)\nSigma17 &lt;- cov(data17N)\npar(mfrow=c(1, 2))\nimage(t(Sigma07), main = \"Covariance matrix 07\")\nimage(t(Sigma17), main = \"Covariance matrix 17\")\n\n\n\n\n\n\n\n\nThere is at least some redundancy in the data, which makes it worthwhile to try PCA. In the second plot this is more evident, so we might guess that PCA will be even more effective there.\n/ Let us get the principal components using prcomp(). There is an argument to be made in favor of not to scaling the data: high variance in an observation means a higher level of disagreement in senators votes, while low variance means that most of them voted in the same way. If we care more about those issues that are more debated (e.g. gun laws) and less about those on which everyone agrees (e.g. raising senators’ salary) then it is preferable not to scale the data.\n\npr.out07 &lt;- prcomp(data07N)\npr.out17 &lt;- prcomp(data17N)\n\nThe output contains many objects\n\nnames(pr.out07)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nWe can check now the importance of principal components in the two cases\n\nsummary(pr.out07)\n\nImportance of components:\n                           PC1     PC2     PC3    PC4     PC5    PC6     PC7\nStandard deviation     12.9042 3.71408 3.03882 2.4098 2.19140 2.0549 1.91235\nProportion of Variance  0.5562 0.04607 0.03084 0.0194 0.01604 0.0141 0.01221\nCumulative Proportion   0.5562 0.60225 0.63310 0.6525 0.66853 0.6826 0.69485\n                           PC8     PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     1.81034 1.79620 1.77976 1.7214 1.67003 1.60616 1.56163\nProportion of Variance 0.01095 0.01078 0.01058 0.0099 0.00932 0.00862 0.00815\nCumulative Proportion  0.70580 0.71657 0.72715 0.7370 0.74637 0.75498 0.76313\n                          PC15    PC16    PC17    PC18   PC19    PC20    PC21\nStandard deviation     1.53252 1.51194 1.48791 1.47152 1.4476 1.40269 1.39002\nProportion of Variance 0.00784 0.00764 0.00739 0.00723 0.0070 0.00657 0.00645\nCumulative Proportion  0.77097 0.77861 0.78600 0.79324 0.8002 0.80681 0.81326\n                          PC22    PC23    PC24    PC25    PC26   PC27    PC28\nStandard deviation     1.35985 1.32609 1.32011 1.28057 1.27026 1.2595 1.22948\nProportion of Variance 0.00618 0.00587 0.00582 0.00548 0.00539 0.0053 0.00505\nCumulative Proportion  0.81944 0.82531 0.83113 0.83661 0.84200 0.8473 0.85234\n                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\nStandard deviation     1.22124 1.21861 1.19118 1.16494 1.14058 1.12554 1.10577\nProportion of Variance 0.00498 0.00496 0.00474 0.00453 0.00435 0.00423 0.00408\nCumulative Proportion  0.85733 0.86229 0.86703 0.87156 0.87590 0.88013 0.88422\n                          PC36    PC37    PC38    PC39   PC40    PC41    PC42\nStandard deviation     1.09615 1.07222 1.06166 1.04658 1.0378 1.02996 1.00640\nProportion of Variance 0.00401 0.00384 0.00376 0.00366 0.0036 0.00354 0.00338\nCumulative Proportion  0.88823 0.89207 0.89584 0.89949 0.9031 0.90664 0.91002\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.99843 0.98258 0.98042 0.96616 0.95144 0.94407 0.91291\nProportion of Variance 0.00333 0.00322 0.00321 0.00312 0.00302 0.00298 0.00278\nCumulative Proportion  0.91335 0.91657 0.91978 0.92290 0.92592 0.92890 0.93168\n                          PC50    PC51    PC52    PC53    PC54    PC55    PC56\nStandard deviation     0.90311 0.89371 0.87048 0.86888 0.85340 0.84171 0.82562\nProportion of Variance 0.00272 0.00267 0.00253 0.00252 0.00243 0.00237 0.00228\nCumulative Proportion  0.93441 0.93708 0.93961 0.94213 0.94456 0.94693 0.94920\n                          PC57    PC58    PC59    PC60   PC61    PC62   PC63\nStandard deviation     0.82342 0.81051 0.80683 0.79840 0.7925 0.77049 0.7552\nProportion of Variance 0.00226 0.00219 0.00217 0.00213 0.0021 0.00198 0.0019\nCumulative Proportion  0.95147 0.95366 0.95584 0.95797 0.9601 0.96205 0.9639\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.74195 0.73172 0.72879 0.70627 0.70032 0.68822 0.67301\nProportion of Variance 0.00184 0.00179 0.00177 0.00167 0.00164 0.00158 0.00151\nCumulative Proportion  0.96579 0.96758 0.96935 0.97102 0.97266 0.97424 0.97575\n                         PC71   PC72   PC73    PC74    PC75    PC76    PC77\nStandard deviation     0.6712 0.6699 0.6478 0.63796 0.63142 0.61631 0.61038\nProportion of Variance 0.0015 0.0015 0.0014 0.00136 0.00133 0.00127 0.00124\nCumulative Proportion  0.9773 0.9788 0.9802 0.98152 0.98285 0.98412 0.98536\n                          PC78    PC79    PC80   PC81    PC82    PC83    PC84\nStandard deviation     0.59675 0.58376 0.56136 0.5481 0.53674 0.52440 0.51468\nProportion of Variance 0.00119 0.00114 0.00105 0.0010 0.00096 0.00092 0.00088\nCumulative Proportion  0.98655 0.98769 0.98874 0.9898 0.99071 0.99163 0.99251\n                          PC85   PC86    PC87    PC88    PC89    PC90    PC91\nStandard deviation     0.50177 0.4885 0.48019 0.45426 0.43499 0.43203 0.40844\nProportion of Variance 0.00084 0.0008 0.00077 0.00069 0.00063 0.00062 0.00056\nCumulative Proportion  0.99335 0.9941 0.99492 0.99561 0.99624 0.99686 0.99742\n                          PC92    PC93    PC94    PC95    PC96    PC97    PC98\nStandard deviation     0.39534 0.36208 0.35420 0.31679 0.31342 0.29432 0.27198\nProportion of Variance 0.00052 0.00044 0.00042 0.00034 0.00033 0.00029 0.00025\nCumulative Proportion  0.99794 0.99838 0.99880 0.99914 0.99946 0.99975 1.00000\n                           PC99\nStandard deviation     1.36e-14\nProportion of Variance 0.00e+00\nCumulative Proportion  1.00e+00\n\nsummary(pr.out17)\n\nImportance of components:\n                           PC1    PC2     PC3     PC4     PC5    PC6     PC7\nStandard deviation     12.0503 2.9027 1.82664 1.70235 1.29058 1.2370 1.19967\nProportion of Variance  0.7686 0.0446 0.01766 0.01534 0.00882 0.0081 0.00762\nCumulative Proportion   0.7686 0.8132 0.83089 0.84623 0.85505 0.8631 0.87077\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.11374 1.07762 1.05381 1.03398 1.01148 0.95085 0.91687\nProportion of Variance 0.00657 0.00615 0.00588 0.00566 0.00542 0.00479 0.00445\nCumulative Proportion  0.87733 0.88348 0.88936 0.89502 0.90043 0.90522 0.90967\n                          PC15    PC16    PC17    PC18    PC19   PC20    PC21\nStandard deviation     0.89851 0.86111 0.84270 0.81861 0.81776 0.7896 0.77534\nProportion of Variance 0.00427 0.00393 0.00376 0.00355 0.00354 0.0033 0.00318\nCumulative Proportion  0.91394 0.91787 0.92163 0.92517 0.92871 0.9320 0.93520\n                         PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.7404 0.73203 0.71876 0.70269 0.67188 0.65235 0.64574\nProportion of Variance 0.0029 0.00284 0.00273 0.00261 0.00239 0.00225 0.00221\nCumulative Proportion  0.9381 0.94093 0.94367 0.94628 0.94867 0.95092 0.95313\n                          PC29    PC30    PC31    PC32   PC33    PC34    PC35\nStandard deviation     0.63316 0.62168 0.60849 0.60280 0.5667 0.56380 0.55342\nProportion of Variance 0.00212 0.00205 0.00196 0.00192 0.0017 0.00168 0.00162\nCumulative Proportion  0.95525 0.95730 0.95926 0.96118 0.9629 0.96456 0.96619\n                          PC36   PC37    PC38    PC39    PC40    PC41    PC42\nStandard deviation     0.55231 0.5329 0.52293 0.51992 0.50170 0.49733 0.48640\nProportion of Variance 0.00161 0.0015 0.00145 0.00143 0.00133 0.00131 0.00125\nCumulative Proportion  0.96780 0.9693 0.97075 0.97218 0.97351 0.97482 0.97608\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.48504 0.48082 0.46988 0.46212 0.44674 0.43645 0.43040\nProportion of Variance 0.00125 0.00122 0.00117 0.00113 0.00106 0.00101 0.00098\nCumulative Proportion  0.97732 0.97855 0.97971 0.98084 0.98190 0.98291 0.98389\n                          PC50    PC51    PC52   PC53    PC54    PC55    PC56\nStandard deviation     0.41828 0.40480 0.39884 0.3895 0.38140 0.37360 0.36564\nProportion of Variance 0.00093 0.00087 0.00084 0.0008 0.00077 0.00074 0.00071\nCumulative Proportion  0.98482 0.98568 0.98652 0.9873 0.98810 0.98884 0.98954\n                          PC57    PC58    PC59    PC60    PC61    PC62    PC63\nStandard deviation     0.35721 0.35284 0.34250 0.32554 0.32372 0.32072 0.30358\nProportion of Variance 0.00068 0.00066 0.00062 0.00056 0.00055 0.00054 0.00049\nCumulative Proportion  0.99022 0.99088 0.99150 0.99206 0.99262 0.99316 0.99365\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.30149 0.28769 0.28589 0.27252 0.26746 0.25620 0.25116\nProportion of Variance 0.00048 0.00044 0.00043 0.00039 0.00038 0.00035 0.00033\nCumulative Proportion  0.99413 0.99457 0.99500 0.99539 0.99577 0.99612 0.99645\n                          PC71   PC72    PC73    PC74    PC75    PC76   PC77\nStandard deviation     0.24160 0.2385 0.22971 0.21421 0.20824 0.20091 0.1944\nProportion of Variance 0.00031 0.0003 0.00028 0.00024 0.00023 0.00021 0.0002\nCumulative Proportion  0.99676 0.9971 0.99734 0.99759 0.99781 0.99803 0.9982\n                          PC78    PC79    PC80    PC81    PC82    PC83    PC84\nStandard deviation     0.19135 0.18550 0.17552 0.16872 0.16317 0.15708 0.14630\nProportion of Variance 0.00019 0.00018 0.00016 0.00015 0.00014 0.00013 0.00011\nCumulative Proportion  0.99842 0.99860 0.99877 0.99892 0.99906 0.99919 0.99930\n                          PC85   PC86    PC87    PC88    PC89    PC90    PC91\nStandard deviation     0.14214 0.1355 0.12813 0.12415 0.11872 0.10075 0.09239\nProportion of Variance 0.00011 0.0001 0.00009 0.00008 0.00007 0.00005 0.00005\nCumulative Proportion  0.99941 0.9995 0.99959 0.99968 0.99975 0.99980 0.99985\n                          PC92    PC93    PC94    PC95    PC96    PC97\nStandard deviation     0.08683 0.07978 0.07389 0.06650 0.05529 0.04076\nProportion of Variance 0.00004 0.00003 0.00003 0.00002 0.00002 0.00001\nCumulative Proportion  0.99989 0.99992 0.99995 0.99998 0.99999 1.00000\n                            PC98      PC99\nStandard deviation     1.156e-14 1.105e-15\nProportion of Variance 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00\n\n\nIn the first case, one needs 19 principal components to capture \\(\\geq 80\\%\\) of variance, in the second case, it is sufficient to retain two of them! This is particularly striking, as our original set of variables contains more than 250 items! In both cases, \\(PC1\\) captures a lot of variance, so it is something interesting to look at.\nTo convince ourselves of this, we can make a screeplot\n\nrequire(gridExtra)\n\nLoading required package: gridExtra\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nplot1 &lt;- fviz_eig(pr.out07)\nplot2 &lt;- fviz_eig(pr.out17)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nNote that, even in the first case, there is a big discrepancy between PC1 and the other principal components. In this case one can make an exception to the “80% rule” by noticing two things:\n\nOriginal variables are a lot.\nThere is a clear elbow in the screeplot: this is the point where the edges begin to flatten, and it occurs right after PC1.\n\nAccording to this elbow method, we can make an argument for retaining only the first principal component.\nEven when only one principal component is deemed to be sufficient, it is worthwhile to visualize scores on a biplot, that can be slightly squeezed on the horizontal axis, if needed.\n\nplot1 &lt;- fviz_pca_ind(pr.out07, label=FALSE)\nplot2 &lt;- fviz_pca_ind(pr.out17, label=FALSE)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nWe see a clear divide between senators on the left and on the right of the principal component. What could this represent?\n\nplot1 &lt;- fviz_pca_ind(pr.out07, col.ind = data07$Party, label=FALSE)\nplot2 &lt;- fviz_pca_ind(pr.out17, col.ind = data17$Party, label=FALSE)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nThe first principal component is a very good representation of party affiliation! From the historic comparison of the two plots, we can see that the divide between Democrats and Republicans has grown a lot between 2007 and 2017, this is an indication that polarization of senators along party lines is a phenomenon that predates the election of Donald Trump (2016)."
  },
  {
    "objectID": "Lab5_PCA.html",
    "href": "Lab5_PCA.html",
    "title": "Practical 5 - Principal Component Analysis",
    "section": "",
    "text": "This computer practical consists of three exercises.\n\nIn exercise 1, you will use factoextra package to extract more information and visualise the results obtained by principal component analysis on the USArrests dataset;\nIn exercise 2, you will examine how the strength of existing redundancy in data affects the PCA results;\nIn exercise 3, you will get a taste of principal component regression.\n\n\n\n\n\nFirst load the following package into your workspace for visualisation.\nlibrary(factoextra)\nIn this first part, you will analyze data from the database USArrests, which is already in R.\nFor each of the \\(n = 50\\) states in the United States, the data set contains the number of arrests per 100,000 residents made in 1973 for each of three types of violent crimes: Assault, Murder, and Rape. Moreover, the data set records also for each state the percent of the population living in urban areas.\n\n\n\nPerform principal component analysis on USArrests and answer the following questions:\n\nIs PCA a tool that can be successfully applied to this data set? Why?\nDo you need to scale the data? Why?\nWhat are the loading vectors, the scores, and the importance of the principal components?\n\n\nSolution to Task 1a\n\n\nClick for solution\n\n\n\nPCA is justified by the fact that the crime rates variables (especially assault and murder) are highly correlated, as shown by the correlation matrix:\n\ncor(USArrests)\n\n             Murder   Assault   UrbanPop      Rape\nMurder   1.00000000 0.8018733 0.06957262 0.5635788\nAssault  0.80187331 1.0000000 0.25887170 0.6652412\nUrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\nRape     0.56357883 0.6652412 0.41134124 1.0000000\n\n\nIt is therefore appropriate to perform dimension reduction via PCA.\nAs we did in the workshop, we can perform PCA using the prcomp() command. We need to scale the data because the units are not the same for each variable, and the variance of one variable (Assault) is significantly higher than the rest. This makes sense, as assault is on average much more common than the other two types of crime (see summary table).\n\nsummary(USArrests)\napply(USArrests, 2, sd)\npr.out &lt;- prcomp(USArrests, scale =TRUE)\n\nThe loading vectors are the columns of the rotation matrix\n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n\nThe scores are obtained as follows\n\npr.out$x\n\nand the importance of the principal components can be computed from pr.out$sd as in the workshop, or more easily from the summary:\n\nsummary(pr.out)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.5749 0.9949 0.59713 0.41645\nProportion of Variance 0.6201 0.2474 0.08914 0.04336\nCumulative Proportion  0.6201 0.8675 0.95664 1.00000\n\n\nWe clearly see here that \\(PC1\\) encodes 62% of variance, so it does not retain enough information. However, \\(PC1\\) and \\(PC2\\) together encode 86.75% of variance in the data, enough to give an informative picture.\n\n\n\nThe library factoextra has a simple command that can be used to make a nice scree plot:\n\nlibrary(factoextra)\nfviz_screeplot(pr.out, addlabels = TRUE)\n\n\n\n\n\n\n\n\nThis is an effective way to visualize the importance of the principal components.\nOther useful functions for visualization in the factoextra package are fviz_pca_ind, fviz_pca_var and fviz_pca_biplot.\n\n\n\nExplore the functions above (e.g. by typing ?fviz_pca_ind) and describe the kind of plots that you can obtain using them.\n\nSolution to Task 1b\n\n\nClick for solution\n\nLet us start with fviz_pca_ind(). This command plots the scores of observations of the first two principal components. This is informative, as we saw that \\(PC1\\) and \\(PC2\\) explain a big proportion (&gt; 80%) of the variance.\n\nfviz_pca_ind(pr.out, axes = c(1, 2), repel = TRUE,\n  col.ind = \"blue\"\n)\n\n\n\n\n\n\n\n\n\n\nWe can compare this to the autoplot() function and see that it is just another way of creating the same visualization. The advantage of fviz_pca_ind() is that it is somewhat more flexibile if you want to change options.\n\n#install.packages('ggfortify')\nlibrary(ggfortify)\n\nautoplot(pr.out, data = USArrests, label = TRUE) #, label = TRUE\n\n\n\n\n\n\n\n\nThe command fviz_pca_var() plots the loadings of the first two principal components.\n\nfviz_pca_var(pr.out, axes = c(1, 2), repel = TRUE,\n  col.var = \"red\"\n)\n\n\n\n\n\n\n\n\nIn the case of USArrests, we can see that the length of the four vectors are very similar. This means that the first two principal components represent quite fairly the original variables. As expected, the crime variables are the one that mostly contribute to the first principal component, while the UrbanPop variable is predominantly contributing to the second principal component.\nFinally, the fviz_pca_biplot() plots together scores and loading.\n\nfviz_pca_biplot(pr.out, axes = c(1, 2), repel = TRUE,\n  col.var = \"red\"\n)\n\n\n\n\n\n\n\n\nThis is very helpful to interpret the data: based only on the plot of the scores, one could think that a higher \\(PC1\\) score corresponds to higher crime rates. The biplot shows instead that the states on the left of the plot are those with higher crime rates.\n\n\nYou can plot the contribution of the variables to PC1 as follows:\n\nfviz_contrib(pr.out, choice = \"var\", axes = 1, top = 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModify the code to plot the contribution of the variables to PC2.\n\n\nSolution to Task 1c\n\n\nClick for solution\n\n\n\n\n# Contributions of variables to PC2\nfviz_contrib(pr.out, choice = \"var\", axes = 2, top = 10)\n\n\n\n\n\n\n\n\n\nThe library factoextra has commands to quickly extract PCA results for variables. These results include correlation between variables and principal components (cor), quality of representation (cos2), and contributions (contrib). They can be obtained respectively as follows:\n\nvar &lt;- get_pca_var(pr.out)\nvar$cor\n\n              Dim.1      Dim.2      Dim.3       Dim.4\nMurder   -0.8439764 -0.4160354  0.2037600  0.27037052\nAssault  -0.9184432 -0.1870211  0.1601192 -0.30959159\nUrbanPop -0.4381168  0.8683282  0.2257242  0.05575330\nRape     -0.8558394  0.1664602 -0.4883190  0.03707412\n\nvar$cos2\n\n             Dim.1     Dim.2      Dim.3       Dim.4\nMurder   0.7122962 0.1730854 0.04151814 0.073100217\nAssault  0.8435380 0.0349769 0.02563817 0.095846950\nUrbanPop 0.1919463 0.7539938 0.05095143 0.003108430\nRape     0.7324611 0.0277090 0.23845544 0.001374491\n\nvar$contrib\n\n             Dim.1     Dim.2     Dim.3     Dim.4\nMurder   28.718825 17.487524 11.643977 42.149674\nAssault  34.010315  3.533859  7.190358 55.265468\nUrbanPop  7.739016 76.179065 14.289594  1.792325\nRape     29.531844  2.799553 66.876071  0.792533\n\n\nIf you have a hard time making sense of these numbers, don’t worry: there will be more on this in the lectures.\n\n\n\n\n\n\nGenerate n = 200 observations and p=30 variables from a multivariate normal distribution with correlation matrix given as \\(corr(x_{i},x_{j}) = \\rho^{|i-j|}\\). You can create the covariance matrix using\n\ncovmat &lt;- function(rho, p) {\n  rho^(abs(outer(seq(p), seq(p), \"-\")))\n}\n\nand use the mvrnorm function in MASS package to generate the multivariate Gaussian data.\n\n\n\n\nTake \\(\\rho=0.95\\) and carry out PCA. For reproducibility, set the pseudo-number generator seed = 123. How many PCs are sufficient to retain at least 80% of variance? Comment on your general observation. (Hint: Use the following codes to generate the data called dd.\n\n\nlibrary(MASS)\nset.seed(123)\np = 30\nn = 200\ncov.e = covmat(0.95, p)\nmean.e = rep(0, p)\ndd &lt;- mvrnorm(n, mean.e, cov.e)\n\n\n\nSolution to Task 2a\n\n\nClick for solution\n\n\nprr &lt;- prcomp(dd)\nsummary(prr)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     4.1415 2.2933 1.3464 1.00341 0.77935 0.63838 0.48520\nProportion of Variance 0.6179 0.1895 0.0653 0.03627 0.02188 0.01468 0.00848\nCumulative Proportion  0.6179 0.8073 0.8726 0.90886 0.93074 0.94542 0.95390\n                           PC8     PC9    PC10    PC11    PC12    PC13   PC14\nStandard deviation     0.41615 0.40216 0.37139 0.29564 0.29060 0.26536 0.2469\nProportion of Variance 0.00624 0.00583 0.00497 0.00315 0.00304 0.00254 0.0022\nCumulative Proportion  0.96014 0.96597 0.97093 0.97408 0.97712 0.97966 0.9819\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.24380 0.22571 0.21583 0.20360 0.18959 0.18087 0.17804\nProportion of Variance 0.00214 0.00184 0.00168 0.00149 0.00129 0.00118 0.00114\nCumulative Proportion  0.98400 0.98583 0.98751 0.98900 0.99030 0.99148 0.99262\n                         PC22    PC23    PC24    PC25    PC26    PC27   PC28\nStandard deviation     0.1744 0.16711 0.16001 0.15574 0.15240 0.14561 0.1390\nProportion of Variance 0.0011 0.00101 0.00092 0.00087 0.00084 0.00076 0.0007\nCumulative Proportion  0.9937 0.99472 0.99564 0.99652 0.99735 0.99812 0.9988\n                          PC29    PC30\nStandard deviation     0.12848 0.12825\nProportion of Variance 0.00059 0.00059\nCumulative Proportion  0.99941 1.00000\n\nplot(summary(prr)$importance[2,], type=\"b\", xlab=\"PCs\", ylab=\"Variability explained\")\n\n\n\n\n\n\n\n\nThe summary shows that 2 components are enough to keep 80% of the variability in data.\n\n\n\n\n\n\n\n\nTake \\(\\rho=0.2\\) and carry out PCA. As done before, use the pseudo-number generator, seed = 123 for reproducibility. How many PCs are sufficient to retain at least 80% of variance? Compare your results with the case of \\(\\rho=0.95\\).\n\n\nSolution to task 2b\n\n\nClick for solution\n\n\nlibrary(MASS)\nset.seed(123)\np = 30\nn = 200\ncov.e = covmat(0.2, p)\nmean.e = rep(0, p)\ndd &lt;- mvrnorm(n, mean.e, cov.e)\n\nprr &lt;- prcomp(dd)\nsummary(prr)\n\nImportance of components:\n                           PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.46267 1.33775 1.32938 1.28976 1.22917 1.17781 1.17183\nProportion of Variance 0.07223 0.06042 0.05966 0.05616 0.05101 0.04683 0.04636\nCumulative Proportion  0.07223 0.13265 0.19231 0.24847 0.29948 0.34631 0.39267\n                           PC8     PC9   PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.16283 1.13430 1.1074 1.05365 1.03779 1.01332 0.99861\nProportion of Variance 0.04565 0.04344 0.0414 0.03748 0.03636 0.03467 0.03367\nCumulative Proportion  0.43832 0.48176 0.5232 0.56065 0.59701 0.63167 0.66534\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.96032 0.92220 0.91021 0.88108 0.85584 0.83619 0.81532\nProportion of Variance 0.03114 0.02871 0.02797 0.02621 0.02473 0.02361 0.02244\nCumulative Proportion  0.69648 0.72519 0.75316 0.77937 0.80410 0.82770 0.85015\n                          PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.78841 0.75942 0.74078 0.71840 0.70589 0.69243 0.66375\nProportion of Variance 0.02099 0.01947 0.01853 0.01742 0.01682 0.01619 0.01487\nCumulative Proportion  0.87113 0.89060 0.90913 0.92655 0.94338 0.95956 0.97444\n                          PC29    PC30\nStandard deviation     0.62502 0.60544\nProportion of Variance 0.01319 0.01238\nCumulative Proportion  0.98762 1.00000\n\nplot(summary(prr)$importance[2,], type=\"b\", xlab=\"PCs\", ylab=\"Variability explained\")\n\n\n\n\n\n\n\n\nThe summary shows that 19 components are needed to keep 80% of the variability in data. This shows that the higher the correlation, the higher the redundancy in the data in which case PCA becomes more useful and gives more clear interpretation.\n\n\n\n\n\n\n\n\n\n\nOne of the many uses of PCA is principal component regression (PCR). We will get a taste of how this works on the Iris data.\nWe have seen that Sepal length is highly correlated with Petal length and Petal width. Suppose that we want to carryout regression analysis between Sepal length (response variable) and the three remaining variables (covariates). Then we can use PCA to improve our analysis.\n\n\n\nPerform the PCA and find the scores of the principal components of the covariates.\n\nSolution to task 3a\n\n\nClick for solution\n\n\niris.pca &lt;- prcomp(iris[,2:4], scale=TRUE)\n\nThe rotation matrix is given by\n\niris.pca$rotation\n\n                    PC1        PC2         PC3\nSepal.Width  -0.4181177 -0.9067335  0.05488053\nPetal.Length  0.6482670 -0.2555198  0.71725833\nPetal.Width   0.6363391 -0.3354757 -0.69464280\n\n\nand the scores by\n\niris.pca$x\n\n              PC1          PC2          PC3\n  [1,] -2.1248387 -0.139743260  0.008370721\n  [2,] -1.6451988  0.900407772 -0.054584971\n  [3,] -1.8737775  0.498821958 -0.070033694\n  [4,] -1.7044039  0.677902966 -0.001362832\n  [5,] -2.2207666 -0.347773466  0.020961859\n  [6,] -2.2314161 -1.103311779 -0.001635822\n  [7,] -1.9454276  0.024274998 -0.095352466\n  [8,] -1.9921879  0.053812348  0.036410583\n  [9,] -1.5492708  1.108437978 -0.067176109\n [10,] -1.7878870  0.721914915  0.089769216\n [11,] -2.2799718 -0.570278271  0.074183997\n [12,] -1.9554651  0.039337749  0.077041583\n [13,] -1.7286818  0.944419721  0.036547077\n [14,] -1.8388502  0.987843518 -0.085345923\n [15,] -2.6779242 -1.150945093 -0.009935588\n [16,] -2.7845016 -2.114513613 -0.019942131\n [17,] -2.3783073 -1.045413383 -0.164159822\n [18,] -2.0413556 -0.183755209 -0.082761327\n [19,] -2.2189712 -0.851269624  0.076905088\n [20,] -2.2924168 -0.822320426 -0.004356912\n [21,] -1.9187423  0.024863150  0.117672583\n [22,] -2.1130058 -0.658302169 -0.108080099\n [23,] -2.3676579 -0.289875070 -0.141562142\n [24,] -1.5723652  0.100857509 -0.168314699\n [25,] -1.8452967 -0.004086048  0.198934583\n [26,] -1.5717531  0.871458574  0.026677030\n [27,] -1.7884990 -0.048686149 -0.105222513\n [28,] -2.0881159 -0.154217859  0.049001721\n [29,] -2.0289107  0.068286947 -0.004220418\n [30,] -1.7636091  0.455398161  0.051859306\n [31,] -1.6676811  0.663428368  0.039268168\n [32,] -1.8252218 -0.034211550 -0.145853513\n [33,] -2.7471668 -1.358387147  0.215680599\n [34,] -2.7963345 -1.595954703  0.096508689\n [35,] -1.7044039  0.677902966 -0.001362832\n [36,] -1.9105003  0.513296557 -0.110664695\n [37,] -2.1615615 -0.125268661 -0.032260280\n [38,] -2.3042497 -0.303761517  0.112093907\n [39,] -1.6819216  0.914882371 -0.095215971\n [40,] -1.9921879  0.053812348  0.036410583\n [41,] -2.0780784 -0.169280610 -0.123392328\n [42,] -0.9269427  2.327081865 -0.274485987\n [43,] -1.8737775  0.498821958 -0.070033694\n [44,] -1.7174609 -0.344740253 -0.274895471\n [45,] -2.0620425 -0.924230771  0.067035040\n [46,] -1.5617157  0.856395823 -0.145717019\n [47,] -2.3391770 -0.792783076  0.127406136\n [48,] -1.8370547  0.484347359 -0.029402694\n [49,] -2.2799718 -0.570278271  0.074183997\n [50,] -1.9329827  0.276317153 -0.016811556\n [51,]  0.3765942 -0.521457794  0.217835736\n [52,]  0.3866316 -0.536520545  0.045441688\n [53,]  0.6294508 -0.386388735  0.195374550\n [54,]  0.8994033  1.496148204 -0.088769462\n [55,]  0.8070663  0.281125681  0.035708135\n [56,]  0.6033775  0.383624178  0.177341231\n [57,]  0.4476323 -0.817511898  0.048162778\n [58,]  0.2959666  1.521476037 -0.087199181\n [59,]  0.5441723  0.161119373  0.230563369\n [60,]  0.5624516  0.634490029 -0.170167957\n [61,]  0.7531242  2.324647664 -0.056301734\n [62,]  0.4683191 -0.077036336 -0.101633589\n [63,]  0.7448822  1.836214257  0.172035544\n [64,]  0.6643781  0.102632825  0.180062321\n [65,]  0.1769442  0.305865362 -0.175746633\n [66,]  0.3623537 -0.270003791  0.083351597\n [67,]  0.5784876 -0.120460133  0.020259411\n [68,]  0.3019651  0.781588627  0.275622236\n [69,]  1.3459114  1.543781517 -0.080469695\n [70,]  0.5038585  1.182586288  0.078045911\n [71,]  0.7472491 -0.711980189 -0.106061456\n [72,]  0.4197634  0.455997173 -0.025813770\n [73,]  1.2050187  0.861792503  0.119827721\n [74,]  0.5933400  0.398686929  0.349735279\n [75,]  0.4340039  0.204543170  0.108670369\n [76,]  0.4582817 -0.061973585  0.070760459\n [77,]  0.7970289  0.296188432  0.208102183\n [78,]  0.9290676 -0.280857025  0.041150316\n [79,]  0.6744155  0.087570074  0.007668273\n [80,]  0.1775563  1.076466427  0.019245096\n [81,]  0.5630637  1.405091093  0.024823772\n [82,]  0.4428579  1.463577641  0.075324820\n [83,]  0.3954856  0.722513927  0.012096139\n [84,]  1.1700913  0.372770943  0.135139949\n [85,]  0.5784876 -0.120460133  0.020259411\n [86,]  0.2782587 -0.996592907 -0.020508084\n [87,]  0.5560052 -0.357439537  0.114112550\n [88,]  1.0462946  1.438249808  0.073754539\n [89,]  0.2646303  0.025462161  0.039999506\n [90,]  0.7075474  1.080087791 -0.063587185\n [91,]  0.6750276  0.858171138  0.202660002\n [92,]  0.5317273 -0.090922783  0.152022459\n [93,]  0.5281364  0.916069534  0.040136001\n [94,]  0.3918946  1.729506244 -0.099790319\n [95,]  0.5891370  0.635078181  0.042857092\n [96,]  0.2178701  0.054999511  0.171762555\n [97,]  0.3972811  0.219017768  0.068039368\n [98,]  0.4340039  0.204543170  0.108670369\n [99,]  0.1733533  1.312857679 -0.287633091\n[100,]  0.4564862  0.441522574  0.014817230\n[101,]  1.6763760 -1.401789225 -0.243822651\n[102,]  1.4205404  0.240735097 -0.138256195\n[103,]  1.5935050 -0.587176212  0.042301126\n[104,]  1.3288155 -0.203686362  0.181213131\n[105,]  1.6402652 -0.616713562 -0.089461922\n[106,]  1.8505646 -0.688498405  0.326718127\n[107,]  1.2250935  0.831667001 -0.224960376\n[108,]  1.5858751 -0.305008555  0.465630132\n[109,]  1.7859730  0.599485265  0.212110578\n[110,]  1.4253148 -2.040354443 -0.165418236\n[111,]  1.0243836 -0.843427884 -0.166432551\n[112,]  1.4939861  0.211785899 -0.056994194\n[113,]  1.4466138 -0.529277816 -0.120222875\n[114,]  1.6591566  0.627258159 -0.295201519\n[115,]  1.7420276 -0.187354854 -0.581325296\n[116,]  1.3482783 -1.004412928 -0.358566695\n[117,]  1.1961647 -0.397241969  0.153173269\n[118,]  1.2033466 -2.411226602  0.376946186\n[119,]  2.5114110  0.012174726  0.215982479\n[120,]  1.5295254  1.471408523  0.122685306\n[121,]  1.4951695 -1.062311324 -0.196042694\n[122,]  1.3346499  0.017642139 -0.298059105\n[123,]  1.9956603 -0.242900642  0.433298899\n[124,]  1.2636118  0.313696243 -0.128386147\n[125,]  1.2322754 -1.182317633 -0.001187460\n[126,]  1.1879227 -0.885675376  0.381510547\n[127,]  1.1309610  0.120140636 -0.156426009\n[128,]  0.9758279 -0.310394375 -0.090612732\n[129,]  1.6751925 -0.127692002 -0.104774151\n[130,]  1.1393670 -0.352641868  0.457330366\n[131,]  1.6918405 -0.112041099  0.280644946\n[132,]  0.9262121 -2.279778908  0.437317281\n[133,]  1.7586756 -0.171703951 -0.195906199\n[134,]  0.9906803  0.208752686  0.238863136\n[135,]  1.2826673  0.596452053  0.507967908\n[136,]  1.8339167 -0.704149308 -0.058700970\n[137,]  1.3500738 -1.507909087 -0.302623466\n[138,]  1.1002367 -0.605272175  0.165764407\n[139,]  0.9391051 -0.295919776 -0.131243732\n[140,]  1.3139630 -0.722833423 -0.148262737\n[141,]  1.6378577 -0.883818468 -0.340396881\n[142,]  1.3707607 -0.767433524 -0.452419834\n[143,]  1.4205404  0.240735097 -0.138256195\n[144,]  1.5686151 -1.091260522 -0.114780694\n[145,]  1.5662076 -1.358365428 -0.365715652\n[146,]  1.5034114 -0.573877917 -0.424379972\n[147,]  1.5756736  0.671270108 -0.204069471\n[148,]  1.2529623 -0.441842070 -0.150983828\n[149,]  1.1931451 -1.434947940 -0.292753418\n[150,]  1.0492735 -0.339343573 -0.009350732\n\n\n\n\n\n\n\n\nRun a linear regression by using the lm() command. This carries out the principal component regression (PCR, not PCA!). Use just the first 2 principal components as covariates in the linear regression model. As hinted before, the response variable should be Sepal length.\n\nSolution to task 3b\n\n\nClick for solution\n\n\nZ = iris.pca$x[,1:2] # select the first two PCs\niris.lm &lt;- lm(iris$Sepal.Length~Z)\niris.lm\n\n\nCall:\nlm(formula = iris$Sepal.Length ~ Z)\n\nCoefficients:\n(Intercept)         ZPC1         ZPC2  \n     5.8433       0.4230      -0.4348  \n\n\n\n\n\nNow, you can transform this coefficient vector to the scale of the original variables as follows (might look mysterious for now: come back and see this again after the lecture!). This gives the final PCR estimator, whose dimension is equal to the total number of original covariates.\n\niris.pca$rotation[,1:2]%*%matrix(coef(iris.lm)[-1], ncol=1)\n\n                  [,1]\nSepal.Width  0.2173767\nPetal.Length 0.3853085\nPetal.Width  0.4150270\n\n\n\n\n\n\n\nIn practice, we do not need to do PCR ourselves in this way, as this is implemented in the pls R package. Carryout PCR using the pcr function in R (check  HERE) and compare the results.\n\nSolution to task 3c\n\n\nClick for solution\n\n\nlibrary(pls)\niris.pcr &lt;- pcr(Sepal.Length~ Sepal.Width+Petal.Length+Petal.Width, 2, \n                scale=TRUE, data=iris) # 2 stands for number of principal components used in pcr,\ncoef(iris.pcr)\n\n, , 2 comps\n\n             Sepal.Length\nSepal.Width     0.2173767\nPetal.Length    0.3853085\nPetal.Width     0.4150270"
  },
  {
    "objectID": "Lab5_PCA.html#exercise-1-factoextra",
    "href": "Lab5_PCA.html#exercise-1-factoextra",
    "title": "Practical 5 - Principal Component Analysis",
    "section": "",
    "text": "First load the following package into your workspace for visualisation.\nlibrary(factoextra)\nIn this first part, you will analyze data from the database USArrests, which is already in R.\nFor each of the \\(n = 50\\) states in the United States, the data set contains the number of arrests per 100,000 residents made in 1973 for each of three types of violent crimes: Assault, Murder, and Rape. Moreover, the data set records also for each state the percent of the population living in urban areas.\n\n\n\nPerform principal component analysis on USArrests and answer the following questions:\n\nIs PCA a tool that can be successfully applied to this data set? Why?\nDo you need to scale the data? Why?\nWhat are the loading vectors, the scores, and the importance of the principal components?\n\n\nSolution to Task 1a\n\n\nClick for solution\n\n\n\nPCA is justified by the fact that the crime rates variables (especially assault and murder) are highly correlated, as shown by the correlation matrix:\n\ncor(USArrests)\n\n             Murder   Assault   UrbanPop      Rape\nMurder   1.00000000 0.8018733 0.06957262 0.5635788\nAssault  0.80187331 1.0000000 0.25887170 0.6652412\nUrbanPop 0.06957262 0.2588717 1.00000000 0.4113412\nRape     0.56357883 0.6652412 0.41134124 1.0000000\n\n\nIt is therefore appropriate to perform dimension reduction via PCA.\nAs we did in the workshop, we can perform PCA using the prcomp() command. We need to scale the data because the units are not the same for each variable, and the variance of one variable (Assault) is significantly higher than the rest. This makes sense, as assault is on average much more common than the other two types of crime (see summary table).\n\nsummary(USArrests)\napply(USArrests, 2, sd)\npr.out &lt;- prcomp(USArrests, scale =TRUE)\n\nThe loading vectors are the columns of the rotation matrix\n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n\nThe scores are obtained as follows\n\npr.out$x\n\nand the importance of the principal components can be computed from pr.out$sd as in the workshop, or more easily from the summary:\n\nsummary(pr.out)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.5749 0.9949 0.59713 0.41645\nProportion of Variance 0.6201 0.2474 0.08914 0.04336\nCumulative Proportion  0.6201 0.8675 0.95664 1.00000\n\n\nWe clearly see here that \\(PC1\\) encodes 62% of variance, so it does not retain enough information. However, \\(PC1\\) and \\(PC2\\) together encode 86.75% of variance in the data, enough to give an informative picture.\n\n\n\nThe library factoextra has a simple command that can be used to make a nice scree plot:\n\nlibrary(factoextra)\nfviz_screeplot(pr.out, addlabels = TRUE)\n\n\n\n\n\n\n\n\nThis is an effective way to visualize the importance of the principal components.\nOther useful functions for visualization in the factoextra package are fviz_pca_ind, fviz_pca_var and fviz_pca_biplot.\n\n\n\nExplore the functions above (e.g. by typing ?fviz_pca_ind) and describe the kind of plots that you can obtain using them.\n\nSolution to Task 1b\n\n\nClick for solution\n\nLet us start with fviz_pca_ind(). This command plots the scores of observations of the first two principal components. This is informative, as we saw that \\(PC1\\) and \\(PC2\\) explain a big proportion (&gt; 80%) of the variance.\n\nfviz_pca_ind(pr.out, axes = c(1, 2), repel = TRUE,\n  col.ind = \"blue\"\n)\n\n\n\n\n\n\n\n\n\n\nWe can compare this to the autoplot() function and see that it is just another way of creating the same visualization. The advantage of fviz_pca_ind() is that it is somewhat more flexibile if you want to change options.\n\n#install.packages('ggfortify')\nlibrary(ggfortify)\n\nautoplot(pr.out, data = USArrests, label = TRUE) #, label = TRUE\n\n\n\n\n\n\n\n\nThe command fviz_pca_var() plots the loadings of the first two principal components.\n\nfviz_pca_var(pr.out, axes = c(1, 2), repel = TRUE,\n  col.var = \"red\"\n)\n\n\n\n\n\n\n\n\nIn the case of USArrests, we can see that the length of the four vectors are very similar. This means that the first two principal components represent quite fairly the original variables. As expected, the crime variables are the one that mostly contribute to the first principal component, while the UrbanPop variable is predominantly contributing to the second principal component.\nFinally, the fviz_pca_biplot() plots together scores and loading.\n\nfviz_pca_biplot(pr.out, axes = c(1, 2), repel = TRUE,\n  col.var = \"red\"\n)\n\n\n\n\n\n\n\n\nThis is very helpful to interpret the data: based only on the plot of the scores, one could think that a higher \\(PC1\\) score corresponds to higher crime rates. The biplot shows instead that the states on the left of the plot are those with higher crime rates.\n\n\nYou can plot the contribution of the variables to PC1 as follows:\n\nfviz_contrib(pr.out, choice = \"var\", axes = 1, top = 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModify the code to plot the contribution of the variables to PC2.\n\n\nSolution to Task 1c\n\n\nClick for solution\n\n\n\n\n# Contributions of variables to PC2\nfviz_contrib(pr.out, choice = \"var\", axes = 2, top = 10)\n\n\n\n\n\n\n\n\n\nThe library factoextra has commands to quickly extract PCA results for variables. These results include correlation between variables and principal components (cor), quality of representation (cos2), and contributions (contrib). They can be obtained respectively as follows:\n\nvar &lt;- get_pca_var(pr.out)\nvar$cor\n\n              Dim.1      Dim.2      Dim.3       Dim.4\nMurder   -0.8439764 -0.4160354  0.2037600  0.27037052\nAssault  -0.9184432 -0.1870211  0.1601192 -0.30959159\nUrbanPop -0.4381168  0.8683282  0.2257242  0.05575330\nRape     -0.8558394  0.1664602 -0.4883190  0.03707412\n\nvar$cos2\n\n             Dim.1     Dim.2      Dim.3       Dim.4\nMurder   0.7122962 0.1730854 0.04151814 0.073100217\nAssault  0.8435380 0.0349769 0.02563817 0.095846950\nUrbanPop 0.1919463 0.7539938 0.05095143 0.003108430\nRape     0.7324611 0.0277090 0.23845544 0.001374491\n\nvar$contrib\n\n             Dim.1     Dim.2     Dim.3     Dim.4\nMurder   28.718825 17.487524 11.643977 42.149674\nAssault  34.010315  3.533859  7.190358 55.265468\nUrbanPop  7.739016 76.179065 14.289594  1.792325\nRape     29.531844  2.799553 66.876071  0.792533\n\n\nIf you have a hard time making sense of these numbers, don’t worry: there will be more on this in the lectures."
  },
  {
    "objectID": "Lab5_PCA.html#exercise-2",
    "href": "Lab5_PCA.html#exercise-2",
    "title": "Practical 5 - Principal Component Analysis",
    "section": "",
    "text": "Generate n = 200 observations and p=30 variables from a multivariate normal distribution with correlation matrix given as \\(corr(x_{i},x_{j}) = \\rho^{|i-j|}\\). You can create the covariance matrix using\n\ncovmat &lt;- function(rho, p) {\n  rho^(abs(outer(seq(p), seq(p), \"-\")))\n}\n\nand use the mvrnorm function in MASS package to generate the multivariate Gaussian data.\n\n\n\n\nTake \\(\\rho=0.95\\) and carry out PCA. For reproducibility, set the pseudo-number generator seed = 123. How many PCs are sufficient to retain at least 80% of variance? Comment on your general observation. (Hint: Use the following codes to generate the data called dd.\n\n\nlibrary(MASS)\nset.seed(123)\np = 30\nn = 200\ncov.e = covmat(0.95, p)\nmean.e = rep(0, p)\ndd &lt;- mvrnorm(n, mean.e, cov.e)\n\n\n\nSolution to Task 2a\n\n\nClick for solution\n\n\nprr &lt;- prcomp(dd)\nsummary(prr)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     4.1415 2.2933 1.3464 1.00341 0.77935 0.63838 0.48520\nProportion of Variance 0.6179 0.1895 0.0653 0.03627 0.02188 0.01468 0.00848\nCumulative Proportion  0.6179 0.8073 0.8726 0.90886 0.93074 0.94542 0.95390\n                           PC8     PC9    PC10    PC11    PC12    PC13   PC14\nStandard deviation     0.41615 0.40216 0.37139 0.29564 0.29060 0.26536 0.2469\nProportion of Variance 0.00624 0.00583 0.00497 0.00315 0.00304 0.00254 0.0022\nCumulative Proportion  0.96014 0.96597 0.97093 0.97408 0.97712 0.97966 0.9819\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.24380 0.22571 0.21583 0.20360 0.18959 0.18087 0.17804\nProportion of Variance 0.00214 0.00184 0.00168 0.00149 0.00129 0.00118 0.00114\nCumulative Proportion  0.98400 0.98583 0.98751 0.98900 0.99030 0.99148 0.99262\n                         PC22    PC23    PC24    PC25    PC26    PC27   PC28\nStandard deviation     0.1744 0.16711 0.16001 0.15574 0.15240 0.14561 0.1390\nProportion of Variance 0.0011 0.00101 0.00092 0.00087 0.00084 0.00076 0.0007\nCumulative Proportion  0.9937 0.99472 0.99564 0.99652 0.99735 0.99812 0.9988\n                          PC29    PC30\nStandard deviation     0.12848 0.12825\nProportion of Variance 0.00059 0.00059\nCumulative Proportion  0.99941 1.00000\n\nplot(summary(prr)$importance[2,], type=\"b\", xlab=\"PCs\", ylab=\"Variability explained\")\n\n\n\n\n\n\n\n\nThe summary shows that 2 components are enough to keep 80% of the variability in data.\n\n\n\n\n\n\n\n\nTake \\(\\rho=0.2\\) and carry out PCA. As done before, use the pseudo-number generator, seed = 123 for reproducibility. How many PCs are sufficient to retain at least 80% of variance? Compare your results with the case of \\(\\rho=0.95\\).\n\n\nSolution to task 2b\n\n\nClick for solution\n\n\nlibrary(MASS)\nset.seed(123)\np = 30\nn = 200\ncov.e = covmat(0.2, p)\nmean.e = rep(0, p)\ndd &lt;- mvrnorm(n, mean.e, cov.e)\n\nprr &lt;- prcomp(dd)\nsummary(prr)\n\nImportance of components:\n                           PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     1.46267 1.33775 1.32938 1.28976 1.22917 1.17781 1.17183\nProportion of Variance 0.07223 0.06042 0.05966 0.05616 0.05101 0.04683 0.04636\nCumulative Proportion  0.07223 0.13265 0.19231 0.24847 0.29948 0.34631 0.39267\n                           PC8     PC9   PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.16283 1.13430 1.1074 1.05365 1.03779 1.01332 0.99861\nProportion of Variance 0.04565 0.04344 0.0414 0.03748 0.03636 0.03467 0.03367\nCumulative Proportion  0.43832 0.48176 0.5232 0.56065 0.59701 0.63167 0.66534\n                          PC15    PC16    PC17    PC18    PC19    PC20    PC21\nStandard deviation     0.96032 0.92220 0.91021 0.88108 0.85584 0.83619 0.81532\nProportion of Variance 0.03114 0.02871 0.02797 0.02621 0.02473 0.02361 0.02244\nCumulative Proportion  0.69648 0.72519 0.75316 0.77937 0.80410 0.82770 0.85015\n                          PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.78841 0.75942 0.74078 0.71840 0.70589 0.69243 0.66375\nProportion of Variance 0.02099 0.01947 0.01853 0.01742 0.01682 0.01619 0.01487\nCumulative Proportion  0.87113 0.89060 0.90913 0.92655 0.94338 0.95956 0.97444\n                          PC29    PC30\nStandard deviation     0.62502 0.60544\nProportion of Variance 0.01319 0.01238\nCumulative Proportion  0.98762 1.00000\n\nplot(summary(prr)$importance[2,], type=\"b\", xlab=\"PCs\", ylab=\"Variability explained\")\n\n\n\n\n\n\n\n\nThe summary shows that 19 components are needed to keep 80% of the variability in data. This shows that the higher the correlation, the higher the redundancy in the data in which case PCA becomes more useful and gives more clear interpretation."
  },
  {
    "objectID": "Lab5_PCA.html#exercise-3",
    "href": "Lab5_PCA.html#exercise-3",
    "title": "Practical 5 - Principal Component Analysis",
    "section": "",
    "text": "One of the many uses of PCA is principal component regression (PCR). We will get a taste of how this works on the Iris data.\nWe have seen that Sepal length is highly correlated with Petal length and Petal width. Suppose that we want to carryout regression analysis between Sepal length (response variable) and the three remaining variables (covariates). Then we can use PCA to improve our analysis.\n\n\n\nPerform the PCA and find the scores of the principal components of the covariates.\n\nSolution to task 3a\n\n\nClick for solution\n\n\niris.pca &lt;- prcomp(iris[,2:4], scale=TRUE)\n\nThe rotation matrix is given by\n\niris.pca$rotation\n\n                    PC1        PC2         PC3\nSepal.Width  -0.4181177 -0.9067335  0.05488053\nPetal.Length  0.6482670 -0.2555198  0.71725833\nPetal.Width   0.6363391 -0.3354757 -0.69464280\n\n\nand the scores by\n\niris.pca$x\n\n              PC1          PC2          PC3\n  [1,] -2.1248387 -0.139743260  0.008370721\n  [2,] -1.6451988  0.900407772 -0.054584971\n  [3,] -1.8737775  0.498821958 -0.070033694\n  [4,] -1.7044039  0.677902966 -0.001362832\n  [5,] -2.2207666 -0.347773466  0.020961859\n  [6,] -2.2314161 -1.103311779 -0.001635822\n  [7,] -1.9454276  0.024274998 -0.095352466\n  [8,] -1.9921879  0.053812348  0.036410583\n  [9,] -1.5492708  1.108437978 -0.067176109\n [10,] -1.7878870  0.721914915  0.089769216\n [11,] -2.2799718 -0.570278271  0.074183997\n [12,] -1.9554651  0.039337749  0.077041583\n [13,] -1.7286818  0.944419721  0.036547077\n [14,] -1.8388502  0.987843518 -0.085345923\n [15,] -2.6779242 -1.150945093 -0.009935588\n [16,] -2.7845016 -2.114513613 -0.019942131\n [17,] -2.3783073 -1.045413383 -0.164159822\n [18,] -2.0413556 -0.183755209 -0.082761327\n [19,] -2.2189712 -0.851269624  0.076905088\n [20,] -2.2924168 -0.822320426 -0.004356912\n [21,] -1.9187423  0.024863150  0.117672583\n [22,] -2.1130058 -0.658302169 -0.108080099\n [23,] -2.3676579 -0.289875070 -0.141562142\n [24,] -1.5723652  0.100857509 -0.168314699\n [25,] -1.8452967 -0.004086048  0.198934583\n [26,] -1.5717531  0.871458574  0.026677030\n [27,] -1.7884990 -0.048686149 -0.105222513\n [28,] -2.0881159 -0.154217859  0.049001721\n [29,] -2.0289107  0.068286947 -0.004220418\n [30,] -1.7636091  0.455398161  0.051859306\n [31,] -1.6676811  0.663428368  0.039268168\n [32,] -1.8252218 -0.034211550 -0.145853513\n [33,] -2.7471668 -1.358387147  0.215680599\n [34,] -2.7963345 -1.595954703  0.096508689\n [35,] -1.7044039  0.677902966 -0.001362832\n [36,] -1.9105003  0.513296557 -0.110664695\n [37,] -2.1615615 -0.125268661 -0.032260280\n [38,] -2.3042497 -0.303761517  0.112093907\n [39,] -1.6819216  0.914882371 -0.095215971\n [40,] -1.9921879  0.053812348  0.036410583\n [41,] -2.0780784 -0.169280610 -0.123392328\n [42,] -0.9269427  2.327081865 -0.274485987\n [43,] -1.8737775  0.498821958 -0.070033694\n [44,] -1.7174609 -0.344740253 -0.274895471\n [45,] -2.0620425 -0.924230771  0.067035040\n [46,] -1.5617157  0.856395823 -0.145717019\n [47,] -2.3391770 -0.792783076  0.127406136\n [48,] -1.8370547  0.484347359 -0.029402694\n [49,] -2.2799718 -0.570278271  0.074183997\n [50,] -1.9329827  0.276317153 -0.016811556\n [51,]  0.3765942 -0.521457794  0.217835736\n [52,]  0.3866316 -0.536520545  0.045441688\n [53,]  0.6294508 -0.386388735  0.195374550\n [54,]  0.8994033  1.496148204 -0.088769462\n [55,]  0.8070663  0.281125681  0.035708135\n [56,]  0.6033775  0.383624178  0.177341231\n [57,]  0.4476323 -0.817511898  0.048162778\n [58,]  0.2959666  1.521476037 -0.087199181\n [59,]  0.5441723  0.161119373  0.230563369\n [60,]  0.5624516  0.634490029 -0.170167957\n [61,]  0.7531242  2.324647664 -0.056301734\n [62,]  0.4683191 -0.077036336 -0.101633589\n [63,]  0.7448822  1.836214257  0.172035544\n [64,]  0.6643781  0.102632825  0.180062321\n [65,]  0.1769442  0.305865362 -0.175746633\n [66,]  0.3623537 -0.270003791  0.083351597\n [67,]  0.5784876 -0.120460133  0.020259411\n [68,]  0.3019651  0.781588627  0.275622236\n [69,]  1.3459114  1.543781517 -0.080469695\n [70,]  0.5038585  1.182586288  0.078045911\n [71,]  0.7472491 -0.711980189 -0.106061456\n [72,]  0.4197634  0.455997173 -0.025813770\n [73,]  1.2050187  0.861792503  0.119827721\n [74,]  0.5933400  0.398686929  0.349735279\n [75,]  0.4340039  0.204543170  0.108670369\n [76,]  0.4582817 -0.061973585  0.070760459\n [77,]  0.7970289  0.296188432  0.208102183\n [78,]  0.9290676 -0.280857025  0.041150316\n [79,]  0.6744155  0.087570074  0.007668273\n [80,]  0.1775563  1.076466427  0.019245096\n [81,]  0.5630637  1.405091093  0.024823772\n [82,]  0.4428579  1.463577641  0.075324820\n [83,]  0.3954856  0.722513927  0.012096139\n [84,]  1.1700913  0.372770943  0.135139949\n [85,]  0.5784876 -0.120460133  0.020259411\n [86,]  0.2782587 -0.996592907 -0.020508084\n [87,]  0.5560052 -0.357439537  0.114112550\n [88,]  1.0462946  1.438249808  0.073754539\n [89,]  0.2646303  0.025462161  0.039999506\n [90,]  0.7075474  1.080087791 -0.063587185\n [91,]  0.6750276  0.858171138  0.202660002\n [92,]  0.5317273 -0.090922783  0.152022459\n [93,]  0.5281364  0.916069534  0.040136001\n [94,]  0.3918946  1.729506244 -0.099790319\n [95,]  0.5891370  0.635078181  0.042857092\n [96,]  0.2178701  0.054999511  0.171762555\n [97,]  0.3972811  0.219017768  0.068039368\n [98,]  0.4340039  0.204543170  0.108670369\n [99,]  0.1733533  1.312857679 -0.287633091\n[100,]  0.4564862  0.441522574  0.014817230\n[101,]  1.6763760 -1.401789225 -0.243822651\n[102,]  1.4205404  0.240735097 -0.138256195\n[103,]  1.5935050 -0.587176212  0.042301126\n[104,]  1.3288155 -0.203686362  0.181213131\n[105,]  1.6402652 -0.616713562 -0.089461922\n[106,]  1.8505646 -0.688498405  0.326718127\n[107,]  1.2250935  0.831667001 -0.224960376\n[108,]  1.5858751 -0.305008555  0.465630132\n[109,]  1.7859730  0.599485265  0.212110578\n[110,]  1.4253148 -2.040354443 -0.165418236\n[111,]  1.0243836 -0.843427884 -0.166432551\n[112,]  1.4939861  0.211785899 -0.056994194\n[113,]  1.4466138 -0.529277816 -0.120222875\n[114,]  1.6591566  0.627258159 -0.295201519\n[115,]  1.7420276 -0.187354854 -0.581325296\n[116,]  1.3482783 -1.004412928 -0.358566695\n[117,]  1.1961647 -0.397241969  0.153173269\n[118,]  1.2033466 -2.411226602  0.376946186\n[119,]  2.5114110  0.012174726  0.215982479\n[120,]  1.5295254  1.471408523  0.122685306\n[121,]  1.4951695 -1.062311324 -0.196042694\n[122,]  1.3346499  0.017642139 -0.298059105\n[123,]  1.9956603 -0.242900642  0.433298899\n[124,]  1.2636118  0.313696243 -0.128386147\n[125,]  1.2322754 -1.182317633 -0.001187460\n[126,]  1.1879227 -0.885675376  0.381510547\n[127,]  1.1309610  0.120140636 -0.156426009\n[128,]  0.9758279 -0.310394375 -0.090612732\n[129,]  1.6751925 -0.127692002 -0.104774151\n[130,]  1.1393670 -0.352641868  0.457330366\n[131,]  1.6918405 -0.112041099  0.280644946\n[132,]  0.9262121 -2.279778908  0.437317281\n[133,]  1.7586756 -0.171703951 -0.195906199\n[134,]  0.9906803  0.208752686  0.238863136\n[135,]  1.2826673  0.596452053  0.507967908\n[136,]  1.8339167 -0.704149308 -0.058700970\n[137,]  1.3500738 -1.507909087 -0.302623466\n[138,]  1.1002367 -0.605272175  0.165764407\n[139,]  0.9391051 -0.295919776 -0.131243732\n[140,]  1.3139630 -0.722833423 -0.148262737\n[141,]  1.6378577 -0.883818468 -0.340396881\n[142,]  1.3707607 -0.767433524 -0.452419834\n[143,]  1.4205404  0.240735097 -0.138256195\n[144,]  1.5686151 -1.091260522 -0.114780694\n[145,]  1.5662076 -1.358365428 -0.365715652\n[146,]  1.5034114 -0.573877917 -0.424379972\n[147,]  1.5756736  0.671270108 -0.204069471\n[148,]  1.2529623 -0.441842070 -0.150983828\n[149,]  1.1931451 -1.434947940 -0.292753418\n[150,]  1.0492735 -0.339343573 -0.009350732\n\n\n\n\n\n\n\n\nRun a linear regression by using the lm() command. This carries out the principal component regression (PCR, not PCA!). Use just the first 2 principal components as covariates in the linear regression model. As hinted before, the response variable should be Sepal length.\n\nSolution to task 3b\n\n\nClick for solution\n\n\nZ = iris.pca$x[,1:2] # select the first two PCs\niris.lm &lt;- lm(iris$Sepal.Length~Z)\niris.lm\n\n\nCall:\nlm(formula = iris$Sepal.Length ~ Z)\n\nCoefficients:\n(Intercept)         ZPC1         ZPC2  \n     5.8433       0.4230      -0.4348  \n\n\n\n\n\nNow, you can transform this coefficient vector to the scale of the original variables as follows (might look mysterious for now: come back and see this again after the lecture!). This gives the final PCR estimator, whose dimension is equal to the total number of original covariates.\n\niris.pca$rotation[,1:2]%*%matrix(coef(iris.lm)[-1], ncol=1)\n\n                  [,1]\nSepal.Width  0.2173767\nPetal.Length 0.3853085\nPetal.Width  0.4150270\n\n\n\n\n\n\n\nIn practice, we do not need to do PCR ourselves in this way, as this is implemented in the pls R package. Carryout PCR using the pcr function in R (check  HERE) and compare the results.\n\nSolution to task 3c\n\n\nClick for solution\n\n\nlibrary(pls)\niris.pcr &lt;- pcr(Sepal.Length~ Sepal.Width+Petal.Length+Petal.Width, 2, \n                scale=TRUE, data=iris) # 2 stands for number of principal components used in pcr,\ncoef(iris.pcr)\n\n, , 2 comps\n\n             Sepal.Length\nSepal.Width     0.2173767\nPetal.Length    0.3853085\nPetal.Width     0.4150270"
  },
  {
    "objectID": "Lecture5a_PCA.html#unsupervised-learning",
    "href": "Lecture5a_PCA.html#unsupervised-learning",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The main two types of statistical learning are: * Supervised learning, which is “learning with a teacher”: the student present an estimate response for each observation, the teacher provides the correct answer and the student improves their models. * Unsupervised learning, which is “learning without a teacher”. In this respect, it is more challenging (no training, no validation, no simple goal). But sometimes you can’t avoid using it!\nUnsupervised learning is often overlooked, but it has many applications: recommender systems, document search, fake image analysis, risk management, …"
  },
  {
    "objectID": "Lecture5a_PCA.html#dimensionality-reduction",
    "href": "Lecture5a_PCA.html#dimensionality-reduction",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Dimensionality reduction methods aim to reduce the number of variables, retaining much of the properties of the original data. They are used to * Improve storage and computational complexity * Prevent overfitting in data analysis * Reduce noise and detecting anomalies The most important technique for dimensionality reduction is principal component analysis (PCA)."
  },
  {
    "objectID": "Lecture5_PCA.html",
    "href": "Lecture5_PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The main two types of statistical learning are: * Supervised learning, which is “learning with a teacher”: the student present an estimate response for each observation, the teacher provides the correct answer and the student improves their models. * Unsupervised learning, which is “learning without a teacher”. In this respect, it is more challenging (no training, no validation, no simple goal). But sometimes you can’t avoid using it!\nUnsupervised learning is often overlooked, but it has many applications: recommender systems, document search, fake image analysis, risk management, …\n\n\n\nDimensionality reduction methods aim to reduce the number of variables, retaining much of the properties of the original data. They are used to * Improve storage and computational complexity * Prevent overfitting in data analysis * Reduce noise and detecting anomalies The most important technique for dimensionality reduction is principal component analysis (PCA).\nFor a description of how to define the principal components and the related quantities, please refer to the slides or to the very readable chapter 12 of An introduction to statistical learning (by James, Witten, Hastie and Tibshirani).\nHere below, we discuss how to use PCA for dimensionality reduction.\n\n\n\nWe want to establish a technique to select the more relevant \\(PC_k\\), to minimize loss of information when we reduce the dimension. So our main question is how to measure the amount of structure captured by each \\(PC_k\\).\nThe proportion of variance is what we need to use here. Recall that the variance explained by \\(\\phi_k\\) is\n[_k = _k^T _k,]\nwhich is exactly the \\(k\\)-th eigenvalue of \\(\\mathbf{\\Sigma}\\).\nThe proportion of variance explained by \\(\\phi_k\\) is then\n[ .]\nA good dimension reduction technique must retain at least 80% of the original variance, as a rule of thumb. In some situations, this might be replaced by elbow rule.\n\n\nThe procedure always works well in theory, but presents some issues when looking at data in real life.\n\nIf the original variables are highly uncorrelated, we need too many principal components to explain at least 80% of variance\nBy changing units of measurement, the variances also change, for the very same data!\nEven when the units are the same, the standard deviation of the original variables can vary a lot (e.g. Iris data): the variables with higher standard deviation will be more represented in the first PCs.\n\nIssue 1 is structural, so no solution can be found. Issues 2 and 3 can be solved by scaling the data set.\n\n\n\nLet \\(\\mathbf{X}\\) be the matrix of a data set. Scaling \\(\\mathbf{X}\\) means replacing each column \\(\\mathbf{X}\\) of \\(\\mathbf{X}\\) with\n[ ,] where \\(\\mathbf{\\mu} = \\begin{pmatrix}\n\\mu \\\\\n\\mu\\\\\n\\vdots \\\\\n\\mu\n\\end{pmatrix}\\) is the mean vector of \\(\\mathbf{X}\\) and \\(sd\\) is the standard deviation of \\(\\mathbf{X}\\).\nThe covariance matrix of the scaled data is called the correlation matrix of the original data.\nOther relevant notions for PCA are:\n\nCorrelation between PC and original variables: these are obtained as dot products of the loadings against the PCs\nQuality of representation (cos2):\nAbsolute contributions (contrib):\n\n\n\n\n\nNow that we are backed by the theory, let’s explore once again how to perform PCA with R. We will analyze the US Senate data sets, reduce its dimension in an appropriate way and see what this analysis tells us about the evolution of political parties in the US.\n\n\nYou can download the data sets from Blackboard as .csv files.\nWe first start by loading the datasets in our environment together with the libraries.\n\n#install.packages(\"factoextra\")\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata07 &lt;- read.csv(\"senate-2007.csv\")\ndata17 &lt;- read.csv(\"senate-2017.csv\")\n\nWe then select only the numeric features:\n\ndata07N &lt;- data07 %&gt;%\n  select_if(is.numeric)\ndata17N &lt;- data17 %&gt;%\n  select_if(is.numeric)\n\nFirst of all, note that you can’t check the correlation matrix from a plot matrix, as it is just too big! We can try to visualize the matrix with a color plot:\n\nSigma07 &lt;- cov(data07N)\nSigma17 &lt;- cov(data17N)\npar(mfrow=c(1, 2))\nimage(t(Sigma07), main = \"Covariance matrix 07\")\nimage(t(Sigma17), main = \"Covariance matrix 17\")\n\n\n\n\n\n\n\n\nThere is at least some redundancy in the data, which makes it worthwhile to try PCA. In the second plot this is more evident, so we might guess that PCA will be even more effective there.\n/ Let us get the principal components using prcomp(). There is an argument to be made in favor of not to scaling the data: high variance in an observation means a higher level of disagreement in senators votes, while low variance means that most of them voted in the same way. If we care more about those issues that are more debated (e.g. gun laws) and less about those on which everyone agrees (e.g. raising senators’ salary) then it is preferable not to scale the data.\n\npr.out07 &lt;- prcomp(data07N)\npr.out17 &lt;- prcomp(data17N)\n\nThe output contains many objects\n\nnames(pr.out07)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nWe can check now the importance of principal components in the two cases\n\nsummary(pr.out07)\n\nImportance of components:\n                           PC1     PC2     PC3    PC4     PC5    PC6     PC7\nStandard deviation     12.9042 3.71408 3.03882 2.4098 2.19140 2.0549 1.91235\nProportion of Variance  0.5562 0.04607 0.03084 0.0194 0.01604 0.0141 0.01221\nCumulative Proportion   0.5562 0.60225 0.63310 0.6525 0.66853 0.6826 0.69485\n                           PC8     PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     1.81034 1.79620 1.77976 1.7214 1.67003 1.60616 1.56163\nProportion of Variance 0.01095 0.01078 0.01058 0.0099 0.00932 0.00862 0.00815\nCumulative Proportion  0.70580 0.71657 0.72715 0.7370 0.74637 0.75498 0.76313\n                          PC15    PC16    PC17    PC18   PC19    PC20    PC21\nStandard deviation     1.53252 1.51194 1.48791 1.47152 1.4476 1.40269 1.39002\nProportion of Variance 0.00784 0.00764 0.00739 0.00723 0.0070 0.00657 0.00645\nCumulative Proportion  0.77097 0.77861 0.78600 0.79324 0.8002 0.80681 0.81326\n                          PC22    PC23    PC24    PC25    PC26   PC27    PC28\nStandard deviation     1.35985 1.32609 1.32011 1.28057 1.27026 1.2595 1.22948\nProportion of Variance 0.00618 0.00587 0.00582 0.00548 0.00539 0.0053 0.00505\nCumulative Proportion  0.81944 0.82531 0.83113 0.83661 0.84200 0.8473 0.85234\n                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\nStandard deviation     1.22124 1.21861 1.19118 1.16494 1.14058 1.12554 1.10577\nProportion of Variance 0.00498 0.00496 0.00474 0.00453 0.00435 0.00423 0.00408\nCumulative Proportion  0.85733 0.86229 0.86703 0.87156 0.87590 0.88013 0.88422\n                          PC36    PC37    PC38    PC39   PC40    PC41    PC42\nStandard deviation     1.09615 1.07222 1.06166 1.04658 1.0378 1.02996 1.00640\nProportion of Variance 0.00401 0.00384 0.00376 0.00366 0.0036 0.00354 0.00338\nCumulative Proportion  0.88823 0.89207 0.89584 0.89949 0.9031 0.90664 0.91002\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.99843 0.98258 0.98042 0.96616 0.95144 0.94407 0.91291\nProportion of Variance 0.00333 0.00322 0.00321 0.00312 0.00302 0.00298 0.00278\nCumulative Proportion  0.91335 0.91657 0.91978 0.92290 0.92592 0.92890 0.93168\n                          PC50    PC51    PC52    PC53    PC54    PC55    PC56\nStandard deviation     0.90311 0.89371 0.87048 0.86888 0.85340 0.84171 0.82562\nProportion of Variance 0.00272 0.00267 0.00253 0.00252 0.00243 0.00237 0.00228\nCumulative Proportion  0.93441 0.93708 0.93961 0.94213 0.94456 0.94693 0.94920\n                          PC57    PC58    PC59    PC60   PC61    PC62   PC63\nStandard deviation     0.82342 0.81051 0.80683 0.79840 0.7925 0.77049 0.7552\nProportion of Variance 0.00226 0.00219 0.00217 0.00213 0.0021 0.00198 0.0019\nCumulative Proportion  0.95147 0.95366 0.95584 0.95797 0.9601 0.96205 0.9639\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.74195 0.73172 0.72879 0.70627 0.70032 0.68822 0.67301\nProportion of Variance 0.00184 0.00179 0.00177 0.00167 0.00164 0.00158 0.00151\nCumulative Proportion  0.96579 0.96758 0.96935 0.97102 0.97266 0.97424 0.97575\n                         PC71   PC72   PC73    PC74    PC75    PC76    PC77\nStandard deviation     0.6712 0.6699 0.6478 0.63796 0.63142 0.61631 0.61038\nProportion of Variance 0.0015 0.0015 0.0014 0.00136 0.00133 0.00127 0.00124\nCumulative Proportion  0.9773 0.9788 0.9802 0.98152 0.98285 0.98412 0.98536\n                          PC78    PC79    PC80   PC81    PC82    PC83    PC84\nStandard deviation     0.59675 0.58376 0.56136 0.5481 0.53674 0.52440 0.51468\nProportion of Variance 0.00119 0.00114 0.00105 0.0010 0.00096 0.00092 0.00088\nCumulative Proportion  0.98655 0.98769 0.98874 0.9898 0.99071 0.99163 0.99251\n                          PC85   PC86    PC87    PC88    PC89    PC90    PC91\nStandard deviation     0.50177 0.4885 0.48019 0.45426 0.43499 0.43203 0.40844\nProportion of Variance 0.00084 0.0008 0.00077 0.00069 0.00063 0.00062 0.00056\nCumulative Proportion  0.99335 0.9941 0.99492 0.99561 0.99624 0.99686 0.99742\n                          PC92    PC93    PC94    PC95    PC96    PC97    PC98\nStandard deviation     0.39534 0.36208 0.35420 0.31679 0.31342 0.29432 0.27198\nProportion of Variance 0.00052 0.00044 0.00042 0.00034 0.00033 0.00029 0.00025\nCumulative Proportion  0.99794 0.99838 0.99880 0.99914 0.99946 0.99975 1.00000\n                           PC99\nStandard deviation     1.36e-14\nProportion of Variance 0.00e+00\nCumulative Proportion  1.00e+00\n\nsummary(pr.out17)\n\nImportance of components:\n                           PC1    PC2     PC3     PC4     PC5    PC6     PC7\nStandard deviation     12.0503 2.9027 1.82664 1.70235 1.29058 1.2370 1.19967\nProportion of Variance  0.7686 0.0446 0.01766 0.01534 0.00882 0.0081 0.00762\nCumulative Proportion   0.7686 0.8132 0.83089 0.84623 0.85505 0.8631 0.87077\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.11374 1.07762 1.05381 1.03398 1.01148 0.95085 0.91687\nProportion of Variance 0.00657 0.00615 0.00588 0.00566 0.00542 0.00479 0.00445\nCumulative Proportion  0.87733 0.88348 0.88936 0.89502 0.90043 0.90522 0.90967\n                          PC15    PC16    PC17    PC18    PC19   PC20    PC21\nStandard deviation     0.89851 0.86111 0.84270 0.81861 0.81776 0.7896 0.77534\nProportion of Variance 0.00427 0.00393 0.00376 0.00355 0.00354 0.0033 0.00318\nCumulative Proportion  0.91394 0.91787 0.92163 0.92517 0.92871 0.9320 0.93520\n                         PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.7404 0.73203 0.71876 0.70269 0.67188 0.65235 0.64574\nProportion of Variance 0.0029 0.00284 0.00273 0.00261 0.00239 0.00225 0.00221\nCumulative Proportion  0.9381 0.94093 0.94367 0.94628 0.94867 0.95092 0.95313\n                          PC29    PC30    PC31    PC32   PC33    PC34    PC35\nStandard deviation     0.63316 0.62168 0.60849 0.60280 0.5667 0.56380 0.55342\nProportion of Variance 0.00212 0.00205 0.00196 0.00192 0.0017 0.00168 0.00162\nCumulative Proportion  0.95525 0.95730 0.95926 0.96118 0.9629 0.96456 0.96619\n                          PC36   PC37    PC38    PC39    PC40    PC41    PC42\nStandard deviation     0.55231 0.5329 0.52293 0.51992 0.50170 0.49733 0.48640\nProportion of Variance 0.00161 0.0015 0.00145 0.00143 0.00133 0.00131 0.00125\nCumulative Proportion  0.96780 0.9693 0.97075 0.97218 0.97351 0.97482 0.97608\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.48504 0.48082 0.46988 0.46212 0.44674 0.43645 0.43040\nProportion of Variance 0.00125 0.00122 0.00117 0.00113 0.00106 0.00101 0.00098\nCumulative Proportion  0.97732 0.97855 0.97971 0.98084 0.98190 0.98291 0.98389\n                          PC50    PC51    PC52   PC53    PC54    PC55    PC56\nStandard deviation     0.41828 0.40480 0.39884 0.3895 0.38140 0.37360 0.36564\nProportion of Variance 0.00093 0.00087 0.00084 0.0008 0.00077 0.00074 0.00071\nCumulative Proportion  0.98482 0.98568 0.98652 0.9873 0.98810 0.98884 0.98954\n                          PC57    PC58    PC59    PC60    PC61    PC62    PC63\nStandard deviation     0.35721 0.35284 0.34250 0.32554 0.32372 0.32072 0.30358\nProportion of Variance 0.00068 0.00066 0.00062 0.00056 0.00055 0.00054 0.00049\nCumulative Proportion  0.99022 0.99088 0.99150 0.99206 0.99262 0.99316 0.99365\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.30149 0.28769 0.28589 0.27252 0.26746 0.25620 0.25116\nProportion of Variance 0.00048 0.00044 0.00043 0.00039 0.00038 0.00035 0.00033\nCumulative Proportion  0.99413 0.99457 0.99500 0.99539 0.99577 0.99612 0.99645\n                          PC71   PC72    PC73    PC74    PC75    PC76   PC77\nStandard deviation     0.24160 0.2385 0.22971 0.21421 0.20824 0.20091 0.1944\nProportion of Variance 0.00031 0.0003 0.00028 0.00024 0.00023 0.00021 0.0002\nCumulative Proportion  0.99676 0.9971 0.99734 0.99759 0.99781 0.99803 0.9982\n                          PC78    PC79    PC80    PC81    PC82    PC83    PC84\nStandard deviation     0.19135 0.18550 0.17552 0.16872 0.16317 0.15708 0.14630\nProportion of Variance 0.00019 0.00018 0.00016 0.00015 0.00014 0.00013 0.00011\nCumulative Proportion  0.99842 0.99860 0.99877 0.99892 0.99906 0.99919 0.99930\n                          PC85   PC86    PC87    PC88    PC89    PC90    PC91\nStandard deviation     0.14214 0.1355 0.12813 0.12415 0.11872 0.10075 0.09239\nProportion of Variance 0.00011 0.0001 0.00009 0.00008 0.00007 0.00005 0.00005\nCumulative Proportion  0.99941 0.9995 0.99959 0.99968 0.99975 0.99980 0.99985\n                          PC92    PC93    PC94    PC95    PC96    PC97\nStandard deviation     0.08683 0.07978 0.07389 0.06650 0.05529 0.04076\nProportion of Variance 0.00004 0.00003 0.00003 0.00002 0.00002 0.00001\nCumulative Proportion  0.99989 0.99992 0.99995 0.99998 0.99999 1.00000\n                            PC98      PC99\nStandard deviation     1.156e-14 1.105e-15\nProportion of Variance 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00\n\n\nIn the first case, one needs 19 principal components to capture \\(\\geq 80\\%\\) of variance, in the second case, it is sufficient to retain two of them! This is particularly striking, as our original set of variables contains more than 250 items! In both cases, \\(PC1\\) captures a lot of variance, so it is something interesting to look at.\nTo convince ourselves of this, we can make a screeplot\n\nrequire(gridExtra)\n\nLoading required package: gridExtra\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nplot1 &lt;- fviz_eig(pr.out07)\nplot2 &lt;- fviz_eig(pr.out17)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nNote that, even in the first case, there is a big discrepancy between PC1 and the other principal components. In this case one can make an exception to the “80% rule” by noticing two things:\n\nOriginal variables are a lot.\nThere is a clear elbow in the screeplot: this is the point where the edges begin to flatten, and it occurs right after PC1.\n\nAccording to this elbow method, we can make an argument for retaining only the first principal component.\nEven when only one principal component is deemed to be sufficient, it is worthwhile to visualize scores on a biplot, that can be slightly squeezed on the horizontal axis, if needed.\n\nplot1 &lt;- fviz_pca_ind(pr.out07, label=FALSE)\nplot2 &lt;- fviz_pca_ind(pr.out17, label=FALSE)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nWe see a clear divide between senators on the left and on the right of the principal component. What could this represent?\n\nplot1 &lt;- fviz_pca_ind(pr.out07, col.ind = data07$Party, label=FALSE)\nplot2 &lt;- fviz_pca_ind(pr.out17, col.ind = data17$Party, label=FALSE)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nThe first principal component is a very good representation of party affiliation! From the historic comparison of the two plots, we can see that the divide between Democrats and Republicans has grown a lot between 2007 and 2017, this is an indication that polarization of senators along party lines is a phenomenon that predates the election of Donald Trump (2016)."
  },
  {
    "objectID": "Lecture5_PCA.html#unsupervised-learning",
    "href": "Lecture5_PCA.html#unsupervised-learning",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "The main two types of statistical learning are: * Supervised learning, which is “learning with a teacher”: the student present an estimate response for each observation, the teacher provides the correct answer and the student improves their models. * Unsupervised learning, which is “learning without a teacher”. In this respect, it is more challenging (no training, no validation, no simple goal). But sometimes you can’t avoid using it!\nUnsupervised learning is often overlooked, but it has many applications: recommender systems, document search, fake image analysis, risk management, …"
  },
  {
    "objectID": "Lecture5_PCA.html#dimensionality-reduction",
    "href": "Lecture5_PCA.html#dimensionality-reduction",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Dimensionality reduction methods aim to reduce the number of variables, retaining much of the properties of the original data. They are used to * Improve storage and computational complexity * Prevent overfitting in data analysis * Reduce noise and detecting anomalies The most important technique for dimensionality reduction is principal component analysis (PCA).\nFor a description of how to define the principal components and the related quantities, please refer to the slides or to the very readable chapter 12 of An introduction to statistical learning (by James, Witten, Hastie and Tibshirani).\nHere below, we discuss how to use PCA for dimensionality reduction."
  },
  {
    "objectID": "Lecture5_PCA.html#proportion-of-variance",
    "href": "Lecture5_PCA.html#proportion-of-variance",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "We want to establish a technique to select the more relevant \\(PC_k\\), to minimize loss of information when we reduce the dimension. So our main question is how to measure the amount of structure captured by each \\(PC_k\\).\nThe proportion of variance is what we need to use here. Recall that the variance explained by \\(\\phi_k\\) is\n[_k = _k^T _k,]\nwhich is exactly the \\(k\\)-th eigenvalue of \\(\\mathbf{\\Sigma}\\).\nThe proportion of variance explained by \\(\\phi_k\\) is then\n[ .]\nA good dimension reduction technique must retain at least 80% of the original variance, as a rule of thumb. In some situations, this might be replaced by elbow rule.\n\n\nThe procedure always works well in theory, but presents some issues when looking at data in real life.\n\nIf the original variables are highly uncorrelated, we need too many principal components to explain at least 80% of variance\nBy changing units of measurement, the variances also change, for the very same data!\nEven when the units are the same, the standard deviation of the original variables can vary a lot (e.g. Iris data): the variables with higher standard deviation will be more represented in the first PCs.\n\nIssue 1 is structural, so no solution can be found. Issues 2 and 3 can be solved by scaling the data set.\n\n\n\nLet \\(\\mathbf{X}\\) be the matrix of a data set. Scaling \\(\\mathbf{X}\\) means replacing each column \\(\\mathbf{X}\\) of \\(\\mathbf{X}\\) with\n[ ,] where \\(\\mathbf{\\mu} = \\begin{pmatrix}\n\\mu \\\\\n\\mu\\\\\n\\vdots \\\\\n\\mu\n\\end{pmatrix}\\) is the mean vector of \\(\\mathbf{X}\\) and \\(sd\\) is the standard deviation of \\(\\mathbf{X}\\).\nThe covariance matrix of the scaled data is called the correlation matrix of the original data.\nOther relevant notions for PCA are:\n\nCorrelation between PC and original variables: these are obtained as dot products of the loadings against the PCs\nQuality of representation (cos2):\nAbsolute contributions (contrib):"
  },
  {
    "objectID": "Lecture5_PCA.html#pca-using-prcomp",
    "href": "Lecture5_PCA.html#pca-using-prcomp",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Now that we are backed by the theory, let’s explore once again how to perform PCA with R. We will analyze the US Senate data sets, reduce its dimension in an appropriate way and see what this analysis tells us about the evolution of political parties in the US.\n\n\nYou can download the data sets from Blackboard as .csv files.\nWe first start by loading the datasets in our environment together with the libraries.\n\n#install.packages(\"factoextra\")\nlibrary(factoextra)\n\nLoading required package: ggplot2\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndata07 &lt;- read.csv(\"senate-2007.csv\")\ndata17 &lt;- read.csv(\"senate-2017.csv\")\n\nWe then select only the numeric features:\n\ndata07N &lt;- data07 %&gt;%\n  select_if(is.numeric)\ndata17N &lt;- data17 %&gt;%\n  select_if(is.numeric)\n\nFirst of all, note that you can’t check the correlation matrix from a plot matrix, as it is just too big! We can try to visualize the matrix with a color plot:\n\nSigma07 &lt;- cov(data07N)\nSigma17 &lt;- cov(data17N)\npar(mfrow=c(1, 2))\nimage(t(Sigma07), main = \"Covariance matrix 07\")\nimage(t(Sigma17), main = \"Covariance matrix 17\")\n\n\n\n\n\n\n\n\nThere is at least some redundancy in the data, which makes it worthwhile to try PCA. In the second plot this is more evident, so we might guess that PCA will be even more effective there.\n/ Let us get the principal components using prcomp(). There is an argument to be made in favor of not to scaling the data: high variance in an observation means a higher level of disagreement in senators votes, while low variance means that most of them voted in the same way. If we care more about those issues that are more debated (e.g. gun laws) and less about those on which everyone agrees (e.g. raising senators’ salary) then it is preferable not to scale the data.\n\npr.out07 &lt;- prcomp(data07N)\npr.out17 &lt;- prcomp(data17N)\n\nThe output contains many objects\n\nnames(pr.out07)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\nWe can check now the importance of principal components in the two cases\n\nsummary(pr.out07)\n\nImportance of components:\n                           PC1     PC2     PC3    PC4     PC5    PC6     PC7\nStandard deviation     12.9042 3.71408 3.03882 2.4098 2.19140 2.0549 1.91235\nProportion of Variance  0.5562 0.04607 0.03084 0.0194 0.01604 0.0141 0.01221\nCumulative Proportion   0.5562 0.60225 0.63310 0.6525 0.66853 0.6826 0.69485\n                           PC8     PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     1.81034 1.79620 1.77976 1.7214 1.67003 1.60616 1.56163\nProportion of Variance 0.01095 0.01078 0.01058 0.0099 0.00932 0.00862 0.00815\nCumulative Proportion  0.70580 0.71657 0.72715 0.7370 0.74637 0.75498 0.76313\n                          PC15    PC16    PC17    PC18   PC19    PC20    PC21\nStandard deviation     1.53252 1.51194 1.48791 1.47152 1.4476 1.40269 1.39002\nProportion of Variance 0.00784 0.00764 0.00739 0.00723 0.0070 0.00657 0.00645\nCumulative Proportion  0.77097 0.77861 0.78600 0.79324 0.8002 0.80681 0.81326\n                          PC22    PC23    PC24    PC25    PC26   PC27    PC28\nStandard deviation     1.35985 1.32609 1.32011 1.28057 1.27026 1.2595 1.22948\nProportion of Variance 0.00618 0.00587 0.00582 0.00548 0.00539 0.0053 0.00505\nCumulative Proportion  0.81944 0.82531 0.83113 0.83661 0.84200 0.8473 0.85234\n                          PC29    PC30    PC31    PC32    PC33    PC34    PC35\nStandard deviation     1.22124 1.21861 1.19118 1.16494 1.14058 1.12554 1.10577\nProportion of Variance 0.00498 0.00496 0.00474 0.00453 0.00435 0.00423 0.00408\nCumulative Proportion  0.85733 0.86229 0.86703 0.87156 0.87590 0.88013 0.88422\n                          PC36    PC37    PC38    PC39   PC40    PC41    PC42\nStandard deviation     1.09615 1.07222 1.06166 1.04658 1.0378 1.02996 1.00640\nProportion of Variance 0.00401 0.00384 0.00376 0.00366 0.0036 0.00354 0.00338\nCumulative Proportion  0.88823 0.89207 0.89584 0.89949 0.9031 0.90664 0.91002\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.99843 0.98258 0.98042 0.96616 0.95144 0.94407 0.91291\nProportion of Variance 0.00333 0.00322 0.00321 0.00312 0.00302 0.00298 0.00278\nCumulative Proportion  0.91335 0.91657 0.91978 0.92290 0.92592 0.92890 0.93168\n                          PC50    PC51    PC52    PC53    PC54    PC55    PC56\nStandard deviation     0.90311 0.89371 0.87048 0.86888 0.85340 0.84171 0.82562\nProportion of Variance 0.00272 0.00267 0.00253 0.00252 0.00243 0.00237 0.00228\nCumulative Proportion  0.93441 0.93708 0.93961 0.94213 0.94456 0.94693 0.94920\n                          PC57    PC58    PC59    PC60   PC61    PC62   PC63\nStandard deviation     0.82342 0.81051 0.80683 0.79840 0.7925 0.77049 0.7552\nProportion of Variance 0.00226 0.00219 0.00217 0.00213 0.0021 0.00198 0.0019\nCumulative Proportion  0.95147 0.95366 0.95584 0.95797 0.9601 0.96205 0.9639\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.74195 0.73172 0.72879 0.70627 0.70032 0.68822 0.67301\nProportion of Variance 0.00184 0.00179 0.00177 0.00167 0.00164 0.00158 0.00151\nCumulative Proportion  0.96579 0.96758 0.96935 0.97102 0.97266 0.97424 0.97575\n                         PC71   PC72   PC73    PC74    PC75    PC76    PC77\nStandard deviation     0.6712 0.6699 0.6478 0.63796 0.63142 0.61631 0.61038\nProportion of Variance 0.0015 0.0015 0.0014 0.00136 0.00133 0.00127 0.00124\nCumulative Proportion  0.9773 0.9788 0.9802 0.98152 0.98285 0.98412 0.98536\n                          PC78    PC79    PC80   PC81    PC82    PC83    PC84\nStandard deviation     0.59675 0.58376 0.56136 0.5481 0.53674 0.52440 0.51468\nProportion of Variance 0.00119 0.00114 0.00105 0.0010 0.00096 0.00092 0.00088\nCumulative Proportion  0.98655 0.98769 0.98874 0.9898 0.99071 0.99163 0.99251\n                          PC85   PC86    PC87    PC88    PC89    PC90    PC91\nStandard deviation     0.50177 0.4885 0.48019 0.45426 0.43499 0.43203 0.40844\nProportion of Variance 0.00084 0.0008 0.00077 0.00069 0.00063 0.00062 0.00056\nCumulative Proportion  0.99335 0.9941 0.99492 0.99561 0.99624 0.99686 0.99742\n                          PC92    PC93    PC94    PC95    PC96    PC97    PC98\nStandard deviation     0.39534 0.36208 0.35420 0.31679 0.31342 0.29432 0.27198\nProportion of Variance 0.00052 0.00044 0.00042 0.00034 0.00033 0.00029 0.00025\nCumulative Proportion  0.99794 0.99838 0.99880 0.99914 0.99946 0.99975 1.00000\n                           PC99\nStandard deviation     1.36e-14\nProportion of Variance 0.00e+00\nCumulative Proportion  1.00e+00\n\nsummary(pr.out17)\n\nImportance of components:\n                           PC1    PC2     PC3     PC4     PC5    PC6     PC7\nStandard deviation     12.0503 2.9027 1.82664 1.70235 1.29058 1.2370 1.19967\nProportion of Variance  0.7686 0.0446 0.01766 0.01534 0.00882 0.0081 0.00762\nCumulative Proportion   0.7686 0.8132 0.83089 0.84623 0.85505 0.8631 0.87077\n                           PC8     PC9    PC10    PC11    PC12    PC13    PC14\nStandard deviation     1.11374 1.07762 1.05381 1.03398 1.01148 0.95085 0.91687\nProportion of Variance 0.00657 0.00615 0.00588 0.00566 0.00542 0.00479 0.00445\nCumulative Proportion  0.87733 0.88348 0.88936 0.89502 0.90043 0.90522 0.90967\n                          PC15    PC16    PC17    PC18    PC19   PC20    PC21\nStandard deviation     0.89851 0.86111 0.84270 0.81861 0.81776 0.7896 0.77534\nProportion of Variance 0.00427 0.00393 0.00376 0.00355 0.00354 0.0033 0.00318\nCumulative Proportion  0.91394 0.91787 0.92163 0.92517 0.92871 0.9320 0.93520\n                         PC22    PC23    PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.7404 0.73203 0.71876 0.70269 0.67188 0.65235 0.64574\nProportion of Variance 0.0029 0.00284 0.00273 0.00261 0.00239 0.00225 0.00221\nCumulative Proportion  0.9381 0.94093 0.94367 0.94628 0.94867 0.95092 0.95313\n                          PC29    PC30    PC31    PC32   PC33    PC34    PC35\nStandard deviation     0.63316 0.62168 0.60849 0.60280 0.5667 0.56380 0.55342\nProportion of Variance 0.00212 0.00205 0.00196 0.00192 0.0017 0.00168 0.00162\nCumulative Proportion  0.95525 0.95730 0.95926 0.96118 0.9629 0.96456 0.96619\n                          PC36   PC37    PC38    PC39    PC40    PC41    PC42\nStandard deviation     0.55231 0.5329 0.52293 0.51992 0.50170 0.49733 0.48640\nProportion of Variance 0.00161 0.0015 0.00145 0.00143 0.00133 0.00131 0.00125\nCumulative Proportion  0.96780 0.9693 0.97075 0.97218 0.97351 0.97482 0.97608\n                          PC43    PC44    PC45    PC46    PC47    PC48    PC49\nStandard deviation     0.48504 0.48082 0.46988 0.46212 0.44674 0.43645 0.43040\nProportion of Variance 0.00125 0.00122 0.00117 0.00113 0.00106 0.00101 0.00098\nCumulative Proportion  0.97732 0.97855 0.97971 0.98084 0.98190 0.98291 0.98389\n                          PC50    PC51    PC52   PC53    PC54    PC55    PC56\nStandard deviation     0.41828 0.40480 0.39884 0.3895 0.38140 0.37360 0.36564\nProportion of Variance 0.00093 0.00087 0.00084 0.0008 0.00077 0.00074 0.00071\nCumulative Proportion  0.98482 0.98568 0.98652 0.9873 0.98810 0.98884 0.98954\n                          PC57    PC58    PC59    PC60    PC61    PC62    PC63\nStandard deviation     0.35721 0.35284 0.34250 0.32554 0.32372 0.32072 0.30358\nProportion of Variance 0.00068 0.00066 0.00062 0.00056 0.00055 0.00054 0.00049\nCumulative Proportion  0.99022 0.99088 0.99150 0.99206 0.99262 0.99316 0.99365\n                          PC64    PC65    PC66    PC67    PC68    PC69    PC70\nStandard deviation     0.30149 0.28769 0.28589 0.27252 0.26746 0.25620 0.25116\nProportion of Variance 0.00048 0.00044 0.00043 0.00039 0.00038 0.00035 0.00033\nCumulative Proportion  0.99413 0.99457 0.99500 0.99539 0.99577 0.99612 0.99645\n                          PC71   PC72    PC73    PC74    PC75    PC76   PC77\nStandard deviation     0.24160 0.2385 0.22971 0.21421 0.20824 0.20091 0.1944\nProportion of Variance 0.00031 0.0003 0.00028 0.00024 0.00023 0.00021 0.0002\nCumulative Proportion  0.99676 0.9971 0.99734 0.99759 0.99781 0.99803 0.9982\n                          PC78    PC79    PC80    PC81    PC82    PC83    PC84\nStandard deviation     0.19135 0.18550 0.17552 0.16872 0.16317 0.15708 0.14630\nProportion of Variance 0.00019 0.00018 0.00016 0.00015 0.00014 0.00013 0.00011\nCumulative Proportion  0.99842 0.99860 0.99877 0.99892 0.99906 0.99919 0.99930\n                          PC85   PC86    PC87    PC88    PC89    PC90    PC91\nStandard deviation     0.14214 0.1355 0.12813 0.12415 0.11872 0.10075 0.09239\nProportion of Variance 0.00011 0.0001 0.00009 0.00008 0.00007 0.00005 0.00005\nCumulative Proportion  0.99941 0.9995 0.99959 0.99968 0.99975 0.99980 0.99985\n                          PC92    PC93    PC94    PC95    PC96    PC97\nStandard deviation     0.08683 0.07978 0.07389 0.06650 0.05529 0.04076\nProportion of Variance 0.00004 0.00003 0.00003 0.00002 0.00002 0.00001\nCumulative Proportion  0.99989 0.99992 0.99995 0.99998 0.99999 1.00000\n                            PC98      PC99\nStandard deviation     1.156e-14 1.105e-15\nProportion of Variance 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00\n\n\nIn the first case, one needs 19 principal components to capture \\(\\geq 80\\%\\) of variance, in the second case, it is sufficient to retain two of them! This is particularly striking, as our original set of variables contains more than 250 items! In both cases, \\(PC1\\) captures a lot of variance, so it is something interesting to look at.\nTo convince ourselves of this, we can make a screeplot\n\nrequire(gridExtra)\n\nLoading required package: gridExtra\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nplot1 &lt;- fviz_eig(pr.out07)\nplot2 &lt;- fviz_eig(pr.out17)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nNote that, even in the first case, there is a big discrepancy between PC1 and the other principal components. In this case one can make an exception to the “80% rule” by noticing two things:\n\nOriginal variables are a lot.\nThere is a clear elbow in the screeplot: this is the point where the edges begin to flatten, and it occurs right after PC1.\n\nAccording to this elbow method, we can make an argument for retaining only the first principal component.\nEven when only one principal component is deemed to be sufficient, it is worthwhile to visualize scores on a biplot, that can be slightly squeezed on the horizontal axis, if needed.\n\nplot1 &lt;- fviz_pca_ind(pr.out07, label=FALSE)\nplot2 &lt;- fviz_pca_ind(pr.out17, label=FALSE)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nWe see a clear divide between senators on the left and on the right of the principal component. What could this represent?\n\nplot1 &lt;- fviz_pca_ind(pr.out07, col.ind = data07$Party, label=FALSE)\nplot2 &lt;- fviz_pca_ind(pr.out17, col.ind = data17$Party, label=FALSE)\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n\n\n\n\n\n\nThe first principal component is a very good representation of party affiliation! From the historic comparison of the two plots, we can see that the divide between Democrats and Republicans has grown a lot between 2007 and 2017, this is an indication that polarization of senators along party lines is a phenomenon that predates the election of Donald Trump (2016)."
  }
]