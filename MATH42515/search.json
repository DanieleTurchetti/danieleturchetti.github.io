[
  {
    "objectID": "Lab1_ExplContVar.html",
    "href": "Lab1_ExplContVar.html",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "This practical will lead you into producing some high quality standard visualisations of single continuous data variables, as seen in lectures.\n\n\nBy the end of the lab, you will have acquired the following skills:\n\n\nPlotting a histogram with hist\n\n\nPlotting a boxplot with boxplot\n\n\nExtracting simple numerical summaries with summary and fivenum\n\n\nCustomising plots with labels, titles, colour, etc.\n\n\nProducing multiple variations of the plots above\n\n\n\n\n\nTo begin with, let’s first see how to use R to produce standard plots of a variable, namely histograms, boxplots, and quantile (or QQ) plots.\nFor illustration, let us use the R built-in mtcars data set, which contains information on the characteristics of 23 cars.\n\ndata(mtcars)\n\n\n\nA histogram consists of parallel vertical bars that graphically shows the frequency distribution of a quantitative variable. The area of each bar is proportional to the frequency of items found in each class. A histogram is useful to look at when we want to see more detail on the full distribution of the data, and features relating to its shape.\nTo plot a histogram, we apply the hist function to the data vector. We can extract the mpg (miles-per-gallon) variable from the mtcars data set using the $ operator, and so we can draw a histogram as follows:\n\nhist(mtcars$mpg)\n\n\n\n\n\n\n\n\nThis seems to suggest that we have a peak somewhere between 15 and 20 mpg, and potentially another peak between 30 and 35 mpg - perhaps suggesting groups of ‘fuel efficient’ and ‘fuel inefficient’ cars.\nOne way to assess if the number of bars in the histogram is appropriate is to show the location of the data points on the horizontal axis. We can add a ‘rug plot’ to our histogram, which marks the positions of the data with lines on the axis:\n\nhist(mtcars$mpg)\nrug(mtcars$mpg) ## Note: the 'rug' function draws on top of an existing histogram\n\n\n\n\n\n\n\n\nNow we can also see where the data fall within the bars!\nThe default settings of hist will determine the number of bars to display algorithmically, and in this case it has drawn only 5. In general, this is probably too few to show any detail, but we don’t have many data points here. Fortunately, the display of the histogram can be adjusted by a number of arguments:\n\nbreaks - allows us to control the number of bars in the histogram. breaks can take a variety of different inputs:\n\nIf breaks is set to a single number, this will be used to (suggest) the number of bars in the histogram.\nIf breaks is set to a vector, the values will be used to indicate the endpoints of the bars of the histogram.\n\nfreq - if TRUE the histogram shows the simple frequencies or counts within each bar; if FALSE then the histogram shows probability densities rather than counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the hist function to draw histograms of miles-per-gallon which match those shown above (don’t worry about the labels).\n\n\n\nClick for solution\n\n\nhist(mtcars$mpg,freq=FALSE,main='freq=FALSE')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=10,main='breaks=10')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=50,main='breaks=50')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=c(10,12,20,30,35), main=\"breaks=c(10,12,20,30,35)\")\n\n\n\n\n\n\n\n\n\n\n\n\nA five-number numerical summary can be computed with the fivenum function, which takes a vector of numbers as input. To add a little more information, the summary function includes the mean of the data for a 6-number summary.\n\n\n\nCompute summaries of the mpg data in the mtcars dataset using fivenum and summary. What is your interpretation of the result?\n\n\nClick for solution\n\n\nfivenum(mtcars$mpg)\n\n[1] 10.40 15.35 19.20 22.80 33.90\n\n\nThe values returned are the sample minimum, lower quartile, median, upper quartile and maximum. We can see that the median across all the cars in the dataset is about 20 miles per gallon. This is pretty terrible by modern standards, but the data are from 1974 and the USA.\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nThe inclusion of the mean can be quite helpful. If the data have an approximately symmetric distribution then the mean and median values should be close, which can be used as a quick check for any potential skewness in the data. Given that the mean is fairly close to the median, there doesn’t appear to be a dramatic amount of skewness in the distribution of MPG.\n\n\n\n\n\nA boxplot provides a graphical view of the median, quartiles, maximum, and minimum of a data set. In many ways, it is simply a direct visualisation of the five number summary constructed above. The whiskers (vertical lines) capture roughly 99% of a normal distribution, and observations outside this range are plotted as points representing outliers (see the figure below).\n\nBoxplots are created for single variables using the boxplot function, but can be used to easily compare many variables or groups within the data. To draw a boxplot of a single variable, multiple variables, or all variables in a data frame, we simply pass the data directly to the boxplot function:\n\nboxplot(mtcars$mpg)\n\n\n\n\n\n\n\n\nAs the boxplot is based on the simple 5-number summary, it lacks the detail of a histogram. However, we can inspect it for features such as symmetry or skewness - a symmetric distribution will give a boxplot with a whiskers of equal length, a centrally-positioned box evenly divided by the median line.\nHere, we see the box is slightly off-centre, suggesting some slight skewness. We also note that there are no obvious outliers.\n\n\nDraw a a boxplot of all the variables in mtcars by passing the entire data frame to the boxplot function. Can you see anything useful in the plot?\n\n\nClick for solution\n\n\nboxplot(mtcars)\n\n\n\n\n\n\n\n\nUnfortunately, because of the different scales of variables we can’t see what’s going on with the smaller variables.\n\nThe boxplot is most useful when comparing how a variable behaves in different groups (i.e., the levels of a categorical variable). For example, we can compare the MPG with the number of engine cylinders\n\ncyl &lt;- factor(mtcars$cyl) # make the 'cyl' variable categorical\nboxplot(mtcars$mpg ~ cyl)\n\n\n\n\n\n\n\n\nWhat do you conclude about fuel efficiency in cars with more engine cylinders?\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data.\n\n\n\n\n\nFrancis Galton famously developed his ideas on correlation and regression using data which included the heights of parents and their children. This galton data set include data on heights for 928 children and their 205 ‘midparents’. Each ‘midparent’ height is the average of the father’s height and 1.08 times the mother’s height (to adjust for the usual gender differences). Similarly, the daughter’s heights have also been multiplied by 1.08. Note that we have one midparent height for each child, so that many midparent heights are repeated.\nThe variables are child and parent for the different heights recorded in inches.\n\n\n\nDownload the galton.Rda data set from the Ultra page and load the file\nDraw a boxplot of both variables in the data set.\nWhat features do you see? How do the heights compare in terms of location and spread?\nDoes this agree with what you expected?\nDraw and compare histograms of the two variables - can you detect any noticeable similarities or differences?\nRedraw your histograms and add a rugplot to each. Does this give you more information?\n\n\n\nClick for solution\n\n\nboxplot(galton)\n\n\n\n\n\n\n\n## parents are less spread than children, both seem to be centred around the \n## same values (about 68.5)\n## both appear symmetric\n## parent displays two outliers\n\n## expectations: similar heights? so yes.\n## not sure would we expect more variability in children than parents, though parents are an average!\n\nhist(galton$child)\nrug(galton$child)\n\n\n\n\n\n\n\nhist(galton$parent)\nrug(galton$parent)\n\n\n\n\n\n\n\n## histograms are clearly symmetric, normal distributions?\n## rugplot shows something strange - all of these data points are lining up on a few values - why?\n## lets look at the first 30 values\ngalton$child[1:30]\n\n [1] 61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 62.2 62.2 63.2 63.2 63.2\n[16] 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2\n\n## is there rounding here? all the values are at integers +0.2...\n## how many unique values are there?\nunique(galton$parent)\n\n [1] 70.5 68.5 65.5 64.5 64.0 67.5 66.5 69.5 71.5 72.5 73.0\n\n## only 11 unique values!\n\n\n\n\n\n\nUsing the par() command, draw histograms of parents and child heights side-by-side on the same plot (you may need to adjust the width of your plot window.)\nDo your plots reveal anything interesting?\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\nhist(galton$parent)\nhist(galton$child)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the difficulties of comparing multiple independent plots is we need to do more work to ensure consistency of presentation. In particular, we should ensure that our histogram intervals and axis ranges are the same for both plots, as the default presentation will change from plot to plot.\nMany high level plotting functions (plot, hist, boxplot, etc.) allow you to include additional options to customise how the plot is drawn (as well as other graphical parameters). We have seen examples of these already with the axis label arguments xlab and ylab, however we can customise the following plot features for finer control of how a plot is drawn.\nAxis limits\nTo control the ranges of the horizontal and vertical axes, we can add the xlim and ylim arguments to our original plotting function To set the horixontal axis limits, we pass a vector of two numbers to represent the lower and upper limits, xlim = c(lower, upper) specifying numerical values for upper and lower, and repeat the same for ylim to customise the vertical axis.\nAxis labels\nTo specify a label for the x- and y-axes we can supply a string to the xlab and ylab arguments. To give a plot a title, we pass the title as a string to the main argument.\nIt is easier to compare the shape of distributions in histograms when they are arranged vertically, they use equal horizontal axis limits, and the same binwidths.\n\n\n\n\nUse par to setup a column of two plots.\nPlot a histogram of child and then parent from galton using:\n\nx-axis limits of 60 to 75\nA bar width of 1 unit\nAn appropriate x-axis label and plot title\n\nDoes the comparison yield any new information that wasn’t conveyed in the boxplot?\nTry reducing your bar widths - what do you find?\n\n\n\nClick for solution\n\n\npar(mfrow=c(2,1))\nhist(galton$parent,xlim=c(60,75),breaks=60:75,xlab='Parent heights', main='')\nhist(galton$child,xlim=c(60,75),breaks=60:75,xlab='Child heights', main='')\n\n\n\n\n\n\n\n## they seem to line up nicely, with the children more spread. Much like the boxplots\n## why are the parents less variable? One set of parents has many children, so \n## there's a lot more variability from many children (between 1 and 15) there\n## due to repetition of the parent values \n## its actually a difference of the order of 1/sqrt(2), which you would expect for\n## the mean of 2 parents!\n\nIn interpreting the data, it is worth noting that:\n\nGalton obtained this data “through the offer of prizes” for the “bext Extracts from their own Family Records”, so the sample is hardly a random one\nthe data are clearly heavily rounded for tabulation\nfamily sizes vary from 1 child up to 15, so that there is a lot of repetition in the midparent heights.\n\nYou might expect that if we had the individual un-adjusted heights and the genders of the parents and children we would find that the height data distributions would be neatly bimodal with one peak for females and one for males. They are not. Apparently, height distributions are rarely like that.\n\n\n\n\n\nData science inevitably involves working with large data sets. The effort involved in preparing and making a large dataset usable for analysis should not be underestimated, but thankfully we’re going to look at a dataset “prepared earlier”. This movies data set (downloadable from Ultra) is reasonably large(ish), containing 24 different attribues of 28819 movies gathered from IMDB. One of the variables is the movie length in minutes, and it is interesting to look at this variable in some detail.\n\n\n\nLoad the movies data set and draw a histogram of the data.\nPlot a histogram of the length variable.\nWhat features can you see?\nAdd a rugplot to the histogram - what problems does this highlight?\nLet’s try a boxplot of the data - do we learn anything more?\nAre there any obvious outliers?\n\n\n\nClick for solution\n\n\nhist(movies$length)\nrug(movies$length)\n\n\n\n\n\n\n\n## all the data stack up at the left end near zero, but the range of the data is huge!\nboxplot(movies$length,horizontal = TRUE)\n\n\n\n\n\n\n\n## this confirms the same as the rugplot, but there are two ridiculously large outliers\n\nClearly, our data are distorted by some particularly egregious outliers.\n\n\n\n\nLook at the outliers more closely:\n\nWhat are the lengths of the two longest movies?\nDoes that seem sensible?\nExtract the subset of the dataframe containing all the variables for both of these movies\nInspect the variable values:\nThe variables r1 to r10 give the percentage of reviews which rated the movie as a 1 up to a 10 out of 10. Are these movies particularly popular?\nWhat are the names of the movies? Do a quick Google search to see if you can find out more.\n\n\n\nClick for solution\n\nThere are various way you can do this, I chose to sort the data and use the head or tail functions to extract the end/start of the list\n\ntail(sort(movies$length))\n\n[1]  647  773  873 1100 2880 5220\n\nhead(sort(movies$length,decreasing=TRUE))\n\n[1] 5220 2880 1100  873  773  647\n\n\nso 5220 and 2880 minutes - that’s 87 and 48 hours, or 14.5 and 8 days respectively. These are obviously not ‘genuine’ movies. Although it’s tempting to dismiss these as simple errors, it is worth checking if possible.\nTo look at all the variables for these two movies, we must subset the data frame using the square brackets\n\nmovies[movies$length&gt;2000,]\n\n                                                 title year length budget\n11937                           Cure for Insomnia, The 1987   5220     NA\n30574 Longest Most Meaningless Movie in the World, The 1970   2880     NA\n      rating votes   r1  r2  r3  r4 r5 r6 r7  r8  r9  r10 mpaa Action Animation\n11937    3.8    59 44.5 4.5 4.5 4.5  0  0  0 4.5 4.5 44.5           0         0\n30574    6.4    15 44.5 0.0 0.0 0.0  0  0  0 0.0 0.0 64.5           0         0\n      Comedy Drama Documentary Romance Short\n11937      0     0           0       0     0\n30574      0     0           0       0     0\n\n\nThe movies seem to be somewhat polarising, either rated 0 or 10, but only by a small number of reviews! Incidentally, this data set is no longer up-to-date and there are some even longer films now (though it’s a mystery why.)\n\nIn this case, the extreme outliers should be ignored, and for exploring the main distribution of movie lengths it makes sense to set some kind of upper limit. Over 99% of the data are less than three hours in length, so let’s restrict ourselve to those.\n\n\n\n\nExtract the all the movies of length at most three hours.\nDraw a histogram - what do you find?\n\n\n\nClick for solution\n\n\nmovies2 &lt;- movies[movies$length&lt;180,]\nhist(movies2$length)\n\n\n\n\n\n\n\n\nAt last, we see some structure! There are two clear peaks here (bi-modality): the first under 20 minutes, the second in [80,100] minutes. The latter group seems about right for the ‘average’ movie.\n\nUseful context:\n\nThe Oscars define a “short film” as anything under 40 minutes.\nAnimated shorts are typically between 5 and 8 minutes long, and are counted as individual movies (so, e.g., every ‘Tom and Jerry’ cartoon has its own entry).\n\n\n\n\n\nRedraw your histogram using a bin-width of 1 minute (you may need to enlarge your plot window).\nWhat do you see? How does the information above help explain the data?\nIs there any heaping in the data? At what values?\n\n\n\nClick for solution\n\nTo get a 1-minute bin width, I made a sequence of integers from 0 to 180 as my breakpoints. If you’re not familiar with this, look at the help for the ‘seq’ function in creating similar sequences.\n\nhist(movies2$length,breaks=0:180)\n\n\n\n\n\n\n\n\nThere’s a lot going on here! * there are still a fair few longer films over 2hrs, but its a minority * the clump of short movies correspond to the defined ‘short film’ and have a clear peak around 7mins. Coincidentally, most short cartoons are 6-8 mins long. * A big pronounced spike in values at exactly 90 minutes. This probably isn’t rounding, but rather that when making and editing movies they will have aimed to produce a film of that length. Perhaps we might have expected an ever sharper peak? * We also see some stacking around this peak, with certain lengths being favoured at 80, 85, 95, 100, etc\n\n\n\n\n\nUsing colour in a plot can be very effective, for example to highlight different groups within the data. Colour is adjusted by setting the col optional arugment to the plotting function, and what R does with that information depends on the value we supply.\n\ncol is assigned a single value: all points on a scatterplot, all bars of a histogram, all boxplots are coloured with the new colour\ncol is a vector:\n\nin a scatterplot, if col is a vector of the same length as the number of data points then each data point is coloured individually\nin a histogram, if col is a vector of the same length as the number of bars then each bar is coloured individually\nin a boxplot, if col is a vector of the same length as the number of boxplots then each boxplot is coloured individually\nif the vector is not of the correct length, it will be replicated until it is and the above rules apply\n\n\nNow that we know how the col argument works, we need to know how to specify colours. Again, there are a number of ways and you can mix and match as appropriate\n\nIntegers: The integers 1:8 are interpreted as colours (black, red, green, blue, …) and can be used as a quick shorthand for a common colour. Type palette() to see the sequence of colours R uses.\nNames: R recognises over 650 named colours is specified as text, e.g.\"steelblue\", \"darkorange\". You can see the list of recognised names by typing colors(), and a document showing the actual colors is available here\nHexadecimal: R can recognise colours specified as hexadecimal RGB codes (as used in HTML etc), so pure red can be specified as \"#ff0000\" and cyan as \"#00ffff\".\nColour functions: R has a number of functions that will generate a number of colours for use in plotting. These functions include rainbow, heat.colors, and terrain.colors and all take the number of desired colours as argument.\n\n\n## Colour example\n## 3 plots in one row\npar(mfrow=c(1,3))\n## colour the cars data by number of gears\nplot(x=mtcars$wt, y=mtcars$mpg, col=mtcars$gear, xlab=\"Weight\", ylab=\"MPG\", \n     main=\"MPG vs Weight\")\n## manually colour boxplots\nboxplot(mpg~cyl, data=mtcars, col=c(\"orange\",\"violet\",\"steelblue3\"),\n        main=\"Car Milage Data\", xlab=\"Number of Cylinders\", \n        ylab=\"Miles Per Gallon\")\n## use a colour function to shade histogram bars\nhist(mtcars$mpg,col=rainbow(5),main='MPG')\n\n\n\n\n\n\n\n\n\n\n\nShow the histograms of length of short movies next to that for ‘non-short’ movies, using a different colour for each histogram.\nExperiment with using the col argument to add colour to your histograms.\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\n## again, we can use subsetting to select a subset of the data. Here we don't need a ',' as we're subsetting a vector instead of a matrix \nhist(movies2$length[movies2$length&lt;=40],col='royalblue',xlab='length',main='Short films')\nhist(movies2$length[movies2$length&gt;40],col='tomato',xlab='length', main='Regular films')\n\n\n\n\n\n\n\n\n\n\n\n\n\n Download data: bundestag\nThese data contain the results of the 2009 elections for the German Bundestag, the first chamber of the German parliament. The contains the number of votes cast for the various political parties, for each state (“Bundesland”). Amongst the German political parties there are two on the left of the political spectrum, the SPD - similar to the UK’s Labour party - and Die Linke (“The Left”), a party even further to the left. Suppose we’re interested in the support for this “Die Linke” party.\n\n\n\nExplore the election support for ‘Die Linke’ party by examining the LINKE1 variable using. Compare the outputs and explain the difference you find:\n\nA histogram, with a rugplot\nA stacked dotplot\nA beewswarm plot, setting horizontal=TRUE\n\n\n\n\nClick for solution\n\n\nhist(bundestag$LINKE1)\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## clear concentration of points around 10k. let's try narrower bins for more detail\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=2000))\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## that's a bit more useful\n\n\nstripchart(bundestag$LINKE1,method='stack',pch=16)\n\n\n\n\n\n\n\n## this looks mostly like the rugplot information on the plot above, but even with stacking this doesn't resemble the histogram\n## Why not? The histogram is grouping nearby points into its bars, but the dotplot/stripchart does not and is treating each unique value as a separate point.\n\n\nlibrary(beeswarm)\nbeeswarm(bundestag$LINKE1,horiz=TRUE)\n\n\n\n\n\n\n\n## the shape of the beewswarm more closely resembles the histogram, albeit rather than stacking points up from the bottom it works from the middle out towards the edges\n\n\nA stem and leaf plot is a technique for displaying the data in a similar fashion to a histogram, while preserving the information ofthe individual numerical values. Where the histogram summarises the data by the counts in its various intervals, the stem and leaf plot retains the original data values up to two significant figures.\n\n\n\n\nDraw a stem and leaf plot of the German election support for ‘Die Linke’ data using the stem() function.\nHow does the stem and leaf plot represent these data? You may want to look at the data value to help understand.\nNow draw a histogram, adjusting the histogram to have axis range and bar width to match the stem and leaf plot.\n\n\n\nClick for solution\n\n\nstem(bundestag$LINKE1)\n\n\n  The decimal point is 4 digit(s) to the right of the |\n\n  0 | \n  0 | 55566666667777777777777888888888888888888888888888888888999999999999+14\n  1 | 00000000000000000000000000000000000001111111111111111111111111111111+51\n  1 | 5555566\n  2 | 122234\n  2 | 588889\n  3 | 01223333344444\n  3 | 556666677777788\n  4 | 0000011233444\n  4 | 6677899\n  5 | \n  5 | \n  6 | 0233\n\n## This is effectively a histogram with bins of width 5000 units. The number indicate the actual data values, with the \"stem\" being the leading digit to the left of the | symbol, and the values of the next digits of the data being indicated to the right of the |\n## We can see some structure now *within* the bars, which is a bit more detail than we get from the histogram.\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=5000))\n\n\n\n\n\n\n\n\nAs with the beeswarm plot, the stem and leaf plot is only suitable for relatively modestly sized data sets due to the fact it is literally writing out all of the data values on the screen!\n\n\n\n\n\n\n\n‘Stripplots’ or ‘Stripcharts’ are very similar to the rugplot we applied to our histograms, and display the individual data points along a single axis. They can be used in much the same way as a boxplot, but rather than showing the data summaries they display everything!\nThe built-in faithful data set contains measurements on the waiting times between the eruptions of the Old Faithful geyser.\n\ndata(faithful)\nstripchart(faithful$waiting,ylab='Waiting Time', pch=16) \n\n\n\n\n\n\n\n\nPlotting symbols\nThe symbols used for points in plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is usually pch=1 which is a hollow circle, in the plot above we changed it to 15 which is a filled circle.\nHowever, when we have a lot of data points concentrated in a small interval, the stripplot suffers from problems of ‘overplotting’ where many points with similar values are drawn on top of each other.\nA partial solution to this is to add random noise (known as ‘jittering’) to spread out the points.\n\nMake a stripplot of the movie length data - how does the overplotting problem manifest here? You may want to compare to your histogram.\n\nA better solution is to stack the dots that fall close together, producing an alternative plot to a histogram - sometimes called a ‘dotplot’ or ‘stacked dotplot’\n\nstripchart(faithful$waiting, method='stack',pch=16)\n\n\n\n\n\n\n\n\n\nTry this out with the movies data.\n\n\n\n\nAn evolution of the stripplot is the ‘beeswarm’ plot, available from the beeswarm package. A bee swarm plot is similar to stripplot, but with various methods to separate nearby points such that each point is visible.\n\nlibrary(beeswarm)\nbeeswarm(faithful$waiting)\n\n\n\n\n\n\n\n\nOne limitation of the beeswarm plot is that the computations to arrange all the points do not scale well with large data sets. Do not try this with the movies data, or you will be waiting for a very long time!\n\n\n\nTo see how this works, let’s look at the Old Faithful data, sorted from smallest to largest.\n\nsort(faithful$waiting)\n\n  [1] 43 45 45 45 46 46 46 46 46 47 47 47 47 48 48 48 49 49 49 49 49 50 50 50 50\n [26] 50 51 51 51 51 51 51 52 52 52 52 52 53 53 53 53 53 53 53 54 54 54 54 54 54\n [51] 54 54 54 55 55 55 55 55 55 56 56 56 56 57 57 57 58 58 58 58 59 59 59 59 59\n [76] 59 59 60 60 60 60 60 60 62 62 62 62 63 63 63 64 64 64 64 65 65 65 66 66 67\n[101] 68 69 69 70 70 70 70 71 71 71 71 71 72 73 73 73 73 73 73 73 74 74 74 74 74\n[126] 74 75 75 75 75 75 75 75 75 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77\n[151] 77 77 77 77 77 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79\n[176] 79 79 79 79 79 80 80 80 80 80 80 80 80 81 81 81 81 81 81 81 81 81 81 81 81\n[201] 81 82 82 82 82 82 82 82 82 82 82 82 82 83 83 83 83 83 83 83 83 83 83 83 83\n[226] 83 83 84 84 84 84 84 84 84 84 84 84 85 85 85 85 85 85 86 86 86 86 86 86 87\n[251] 87 88 88 88 88 88 88 89 89 89 90 90 90 90 90 90 91 92 93 93 94 96\n\n\nNote that the smallest value is 43, followed by three values of 45. A stem and leaf plot of these data looks like this\n\nstem(faithful$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 3\n  4 | 55566666777788899999\n  5 | 00000111111222223333333444444444\n  5 | 555555666677788889999999\n  6 | 00000022223334444\n  6 | 555667899\n  7 | 00001111123333333444444\n  7 | 555555556666666667777777777778888888888888889999999999\n  8 | 000000001111111111111222222222222333333333333334444444444\n  8 | 55555566666677888888999\n  9 | 00000012334\n  9 | 6\n\n\nEach row of this plot is called a ‘stem’ and the values to the right of the ‘|’ symbol are the leaves. Be sure to read where R places the decimal point for the output. For this result, the decimal is placed one digit to the right of the vertical bar. Thus, the first row of the table then consists of data values of the form \\(4x\\), and the only leaf is a \\(3\\) corresponding to the value \\(43\\) in the data. The next stem groups the values \\(45-49\\), and we notice the three observations of \\(45\\) are represented by the \\(555\\) at the start of the second stem.\nNotice that each stem part is representing an interval of width 5, much like a histogram. As usual, R figures out how best to increment the stem part unless you specify otherwise. Finally, notice how the shape of the stem and leaf plot mirrors that of a histogram with interval width 5 - the only difference is that here we can see the values inside the bars.\n\n\n\nThe data come from an old survey of 237 students taking their first statistics course. The dataset is called survey in the package MASS.\n\n\n\nLoad the data with data(survey, package='MASS')\nDraw a histogram of student heights - do you see evidence of bimodality?\nExperiment with different binwidths for the histogram. Which choice do you think is the best for conveying the information in the data?\nCompare male and femal heights using separate histograms with a common scale and binwidths.\n\n\n\n\n\n Download data: diamonds\nThe set diamonds includes information on the weight in carats (carat) and price of 53,940 diamonds.\n\n\n\nIs there anything unusual about the distribution of diamond weights? Which plot do you think shows it best? How might you explain the pattern you find?\nWhat about the distribution of prices? With a bit of detective work you ought to be able to uncover at least one unexpected feature. How you discover it, whether with a histogram, a dotplot, or whatever, is unimportant, the important thing is to find it. Having found it, what plot would you draw to present your results to someone else? Can you think of an explanation for the feature?\n\n\n\n\n\n Download data: zuni\nThe zuni dataset seems quite simple. There are three pieces of information about each of 89 school districts in the US State of New Mexico: the name of the district, the average revenue per pupil in dollars, and the number of pupils. The apparent simplicity hides an interesting story. The data were used to determine how to allocate substantial amounts of money and there were intense legal disgreements about how the law should be interpreted and how the data should be used. Gastwirth was heavily involved and has written informatively about the case from a statistical point of view Gastwirth, 2006 and Gastwirth, 2008.\nOne statistical issue was the rule that before determining whether district revenues were sufficiently equal, the largest and smallest 5% of the data should first be deleted.\n\n\n\nAre the lowest and highest 5% of the revenue values extreme? Do you prefer a histogram or boxplot for showing this?\nRemove the lowest and highest 5% of the cases, draw a plot of the remaining data and discuss whether the resulting distribution looks symmetric.\nDraw a Normal quantile plot of the data after removal of the 5% at each end and comment on whether you would regard the remaining distribution as normal.\n\n\n\n\n\n Download data: engine\nThese data record the amounts of three pollutants - carbon monoxide CO, hydrocarbons HC, and nitrogen oxide NO - in grammes emitted per mile by 46 light-duty engines.\n\n\n\nAre the distributions for the three pollutants similar? To make this an easier question to answer, try to produce histograms of the variables, using the same class intervals and range for the horizontal axis in each case.\n\n\n\n\n\n Download data: chlorph\nThese data come from a semi-automated process for measuring the actual amount of chlorpheniramine maleate in tablets which are supposed to contain a 4mg dose.\nThe tablets used for the study were made by two different manufacturers. For each manufacturer, a composite was produced by grinding together a number of tablets. Each composite was split into seven pieces each of the same weight as a tablet and the pieces were sent to seven different laboratories. Each laboratory made 10 separate measurements on each composite.\nThe data contain three variables: * chlorpheniramine - the amount measured * manufacturer - the tablet manufacturer as a factor (A or B) * laboratory - the laboratory which performed the measurement as a factor (1 to 7)\n\n\n\nProduce box-plots of the chlorpheniramine measurements split by laboratory. What, if anything, do they suggest?\nNow produce box-plots by manufacturer. Anything noticeable?\nThe problem here is that we really need a separate box-plot for each combination of manufacturer and laboratory. We can do this by boxplot(chlorpheniramine~laboratory*manufacturer, data=chlorph) Try this (or some abbreviated version of it).\nTry reversing the order of laboratory and manufacturer. Which way round is better? Try colouring the box-plots by manufacturer. Does that help?\nDo the data suggest that the two manufacturers actually put different amounts of the drug into supposed 4mg tablets?\nAre there obvious differences between different laboratories? If so, what kind of differences do you observe?\nIf you had to choose a single laboratory to make some measurements for you, which would you choose and why?"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-1-standard-plots-in-r",
    "href": "Lab1_ExplContVar.html#exercise-1-standard-plots-in-r",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "To begin with, let’s first see how to use R to produce standard plots of a variable, namely histograms, boxplots, and quantile (or QQ) plots.\nFor illustration, let us use the R built-in mtcars data set, which contains information on the characteristics of 23 cars.\n\ndata(mtcars)\n\n\n\nA histogram consists of parallel vertical bars that graphically shows the frequency distribution of a quantitative variable. The area of each bar is proportional to the frequency of items found in each class. A histogram is useful to look at when we want to see more detail on the full distribution of the data, and features relating to its shape.\nTo plot a histogram, we apply the hist function to the data vector. We can extract the mpg (miles-per-gallon) variable from the mtcars data set using the $ operator, and so we can draw a histogram as follows:\n\nhist(mtcars$mpg)\n\n\n\n\n\n\n\n\nThis seems to suggest that we have a peak somewhere between 15 and 20 mpg, and potentially another peak between 30 and 35 mpg - perhaps suggesting groups of ‘fuel efficient’ and ‘fuel inefficient’ cars.\nOne way to assess if the number of bars in the histogram is appropriate is to show the location of the data points on the horizontal axis. We can add a ‘rug plot’ to our histogram, which marks the positions of the data with lines on the axis:\n\nhist(mtcars$mpg)\nrug(mtcars$mpg) ## Note: the 'rug' function draws on top of an existing histogram\n\n\n\n\n\n\n\n\nNow we can also see where the data fall within the bars!\nThe default settings of hist will determine the number of bars to display algorithmically, and in this case it has drawn only 5. In general, this is probably too few to show any detail, but we don’t have many data points here. Fortunately, the display of the histogram can be adjusted by a number of arguments:\n\nbreaks - allows us to control the number of bars in the histogram. breaks can take a variety of different inputs:\n\nIf breaks is set to a single number, this will be used to (suggest) the number of bars in the histogram.\nIf breaks is set to a vector, the values will be used to indicate the endpoints of the bars of the histogram.\n\nfreq - if TRUE the histogram shows the simple frequencies or counts within each bar; if FALSE then the histogram shows probability densities rather than counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the hist function to draw histograms of miles-per-gallon which match those shown above (don’t worry about the labels).\n\n\n\nClick for solution\n\n\nhist(mtcars$mpg,freq=FALSE,main='freq=FALSE')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=10,main='breaks=10')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=50,main='breaks=50')\n\n\n\n\n\n\n\nhist(mtcars$mpg,breaks=c(10,12,20,30,35), main=\"breaks=c(10,12,20,30,35)\")\n\n\n\n\n\n\n\n\n\n\n\n\nA five-number numerical summary can be computed with the fivenum function, which takes a vector of numbers as input. To add a little more information, the summary function includes the mean of the data for a 6-number summary.\n\n\n\nCompute summaries of the mpg data in the mtcars dataset using fivenum and summary. What is your interpretation of the result?\n\n\nClick for solution\n\n\nfivenum(mtcars$mpg)\n\n[1] 10.40 15.35 19.20 22.80 33.90\n\n\nThe values returned are the sample minimum, lower quartile, median, upper quartile and maximum. We can see that the median across all the cars in the dataset is about 20 miles per gallon. This is pretty terrible by modern standards, but the data are from 1974 and the USA.\n\nsummary(mtcars$mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\nThe inclusion of the mean can be quite helpful. If the data have an approximately symmetric distribution then the mean and median values should be close, which can be used as a quick check for any potential skewness in the data. Given that the mean is fairly close to the median, there doesn’t appear to be a dramatic amount of skewness in the distribution of MPG."
  },
  {
    "objectID": "Lab1_ExplContVar.html#boxplot",
    "href": "Lab1_ExplContVar.html#boxplot",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "A boxplot provides a graphical view of the median, quartiles, maximum, and minimum of a data set. In many ways, it is simply a direct visualisation of the five number summary constructed above. The whiskers (vertical lines) capture roughly 99% of a normal distribution, and observations outside this range are plotted as points representing outliers (see the figure below).\n\nBoxplots are created for single variables using the boxplot function, but can be used to easily compare many variables or groups within the data. To draw a boxplot of a single variable, multiple variables, or all variables in a data frame, we simply pass the data directly to the boxplot function:\n\nboxplot(mtcars$mpg)\n\n\n\n\n\n\n\n\nAs the boxplot is based on the simple 5-number summary, it lacks the detail of a histogram. However, we can inspect it for features such as symmetry or skewness - a symmetric distribution will give a boxplot with a whiskers of equal length, a centrally-positioned box evenly divided by the median line.\nHere, we see the box is slightly off-centre, suggesting some slight skewness. We also note that there are no obvious outliers.\n\n\nDraw a a boxplot of all the variables in mtcars by passing the entire data frame to the boxplot function. Can you see anything useful in the plot?\n\n\nClick for solution\n\n\nboxplot(mtcars)\n\n\n\n\n\n\n\n\nUnfortunately, because of the different scales of variables we can’t see what’s going on with the smaller variables.\n\nThe boxplot is most useful when comparing how a variable behaves in different groups (i.e., the levels of a categorical variable). For example, we can compare the MPG with the number of engine cylinders\n\ncyl &lt;- factor(mtcars$cyl) # make the 'cyl' variable categorical\nboxplot(mtcars$mpg ~ cyl)\n\n\n\n\n\n\n\n\nWhat do you conclude about fuel efficiency in cars with more engine cylinders?\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data."
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-2-data-analysis-of-galtons-heights",
    "href": "Lab1_ExplContVar.html#exercise-2-data-analysis-of-galtons-heights",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Francis Galton famously developed his ideas on correlation and regression using data which included the heights of parents and their children. This galton data set include data on heights for 928 children and their 205 ‘midparents’. Each ‘midparent’ height is the average of the father’s height and 1.08 times the mother’s height (to adjust for the usual gender differences). Similarly, the daughter’s heights have also been multiplied by 1.08. Note that we have one midparent height for each child, so that many midparent heights are repeated.\nThe variables are child and parent for the different heights recorded in inches.\n\n\n\nDownload the galton.Rda data set from the Ultra page and load the file\nDraw a boxplot of both variables in the data set.\nWhat features do you see? How do the heights compare in terms of location and spread?\nDoes this agree with what you expected?\nDraw and compare histograms of the two variables - can you detect any noticeable similarities or differences?\nRedraw your histograms and add a rugplot to each. Does this give you more information?\n\n\n\nClick for solution\n\n\nboxplot(galton)\n\n\n\n\n\n\n\n## parents are less spread than children, both seem to be centred around the \n## same values (about 68.5)\n## both appear symmetric\n## parent displays two outliers\n\n## expectations: similar heights? so yes.\n## not sure would we expect more variability in children than parents, though parents are an average!\n\nhist(galton$child)\nrug(galton$child)\n\n\n\n\n\n\n\nhist(galton$parent)\nrug(galton$parent)\n\n\n\n\n\n\n\n## histograms are clearly symmetric, normal distributions?\n## rugplot shows something strange - all of these data points are lining up on a few values - why?\n## lets look at the first 30 values\ngalton$child[1:30]\n\n [1] 61.7 61.7 61.7 61.7 61.7 62.2 62.2 62.2 62.2 62.2 62.2 62.2 63.2 63.2 63.2\n[16] 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2 63.2\n\n## is there rounding here? all the values are at integers +0.2...\n## how many unique values are there?\nunique(galton$parent)\n\n [1] 70.5 68.5 65.5 64.5 64.0 67.5 66.5 69.5 71.5 72.5 73.0\n\n## only 11 unique values!\n\n\n\n\n\n\nUsing the par() command, draw histograms of parents and child heights side-by-side on the same plot (you may need to adjust the width of your plot window.)\nDo your plots reveal anything interesting?\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\nhist(galton$parent)\nhist(galton$child)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the difficulties of comparing multiple independent plots is we need to do more work to ensure consistency of presentation. In particular, we should ensure that our histogram intervals and axis ranges are the same for both plots, as the default presentation will change from plot to plot.\nMany high level plotting functions (plot, hist, boxplot, etc.) allow you to include additional options to customise how the plot is drawn (as well as other graphical parameters). We have seen examples of these already with the axis label arguments xlab and ylab, however we can customise the following plot features for finer control of how a plot is drawn.\nAxis limits\nTo control the ranges of the horizontal and vertical axes, we can add the xlim and ylim arguments to our original plotting function To set the horixontal axis limits, we pass a vector of two numbers to represent the lower and upper limits, xlim = c(lower, upper) specifying numerical values for upper and lower, and repeat the same for ylim to customise the vertical axis.\nAxis labels\nTo specify a label for the x- and y-axes we can supply a string to the xlab and ylab arguments. To give a plot a title, we pass the title as a string to the main argument.\nIt is easier to compare the shape of distributions in histograms when they are arranged vertically, they use equal horizontal axis limits, and the same binwidths.\n\n\n\n\nUse par to setup a column of two plots.\nPlot a histogram of child and then parent from galton using:\n\nx-axis limits of 60 to 75\nA bar width of 1 unit\nAn appropriate x-axis label and plot title\n\nDoes the comparison yield any new information that wasn’t conveyed in the boxplot?\nTry reducing your bar widths - what do you find?\n\n\n\nClick for solution\n\n\npar(mfrow=c(2,1))\nhist(galton$parent,xlim=c(60,75),breaks=60:75,xlab='Parent heights', main='')\nhist(galton$child,xlim=c(60,75),breaks=60:75,xlab='Child heights', main='')\n\n\n\n\n\n\n\n## they seem to line up nicely, with the children more spread. Much like the boxplots\n## why are the parents less variable? One set of parents has many children, so \n## there's a lot more variability from many children (between 1 and 15) there\n## due to repetition of the parent values \n## its actually a difference of the order of 1/sqrt(2), which you would expect for\n## the mean of 2 parents!\n\nIn interpreting the data, it is worth noting that:\n\nGalton obtained this data “through the offer of prizes” for the “bext Extracts from their own Family Records”, so the sample is hardly a random one\nthe data are clearly heavily rounded for tabulation\nfamily sizes vary from 1 child up to 15, so that there is a lot of repetition in the midparent heights.\n\nYou might expect that if we had the individual un-adjusted heights and the genders of the parents and children we would find that the height data distributions would be neatly bimodal with one peak for females and one for males. They are not. Apparently, height distributions are rarely like that."
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-3-data-exploration-of-the-movies-dataset",
    "href": "Lab1_ExplContVar.html#exercise-3-data-exploration-of-the-movies-dataset",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Data science inevitably involves working with large data sets. The effort involved in preparing and making a large dataset usable for analysis should not be underestimated, but thankfully we’re going to look at a dataset “prepared earlier”. This movies data set (downloadable from Ultra) is reasonably large(ish), containing 24 different attribues of 28819 movies gathered from IMDB. One of the variables is the movie length in minutes, and it is interesting to look at this variable in some detail.\n\n\n\nLoad the movies data set and draw a histogram of the data.\nPlot a histogram of the length variable.\nWhat features can you see?\nAdd a rugplot to the histogram - what problems does this highlight?\nLet’s try a boxplot of the data - do we learn anything more?\nAre there any obvious outliers?\n\n\n\nClick for solution\n\n\nhist(movies$length)\nrug(movies$length)\n\n\n\n\n\n\n\n## all the data stack up at the left end near zero, but the range of the data is huge!\nboxplot(movies$length,horizontal = TRUE)\n\n\n\n\n\n\n\n## this confirms the same as the rugplot, but there are two ridiculously large outliers\n\nClearly, our data are distorted by some particularly egregious outliers.\n\n\n\n\nLook at the outliers more closely:\n\nWhat are the lengths of the two longest movies?\nDoes that seem sensible?\nExtract the subset of the dataframe containing all the variables for both of these movies\nInspect the variable values:\nThe variables r1 to r10 give the percentage of reviews which rated the movie as a 1 up to a 10 out of 10. Are these movies particularly popular?\nWhat are the names of the movies? Do a quick Google search to see if you can find out more.\n\n\n\nClick for solution\n\nThere are various way you can do this, I chose to sort the data and use the head or tail functions to extract the end/start of the list\n\ntail(sort(movies$length))\n\n[1]  647  773  873 1100 2880 5220\n\nhead(sort(movies$length,decreasing=TRUE))\n\n[1] 5220 2880 1100  873  773  647\n\n\nso 5220 and 2880 minutes - that’s 87 and 48 hours, or 14.5 and 8 days respectively. These are obviously not ‘genuine’ movies. Although it’s tempting to dismiss these as simple errors, it is worth checking if possible.\nTo look at all the variables for these two movies, we must subset the data frame using the square brackets\n\nmovies[movies$length&gt;2000,]\n\n                                                 title year length budget\n11937                           Cure for Insomnia, The 1987   5220     NA\n30574 Longest Most Meaningless Movie in the World, The 1970   2880     NA\n      rating votes   r1  r2  r3  r4 r5 r6 r7  r8  r9  r10 mpaa Action Animation\n11937    3.8    59 44.5 4.5 4.5 4.5  0  0  0 4.5 4.5 44.5           0         0\n30574    6.4    15 44.5 0.0 0.0 0.0  0  0  0 0.0 0.0 64.5           0         0\n      Comedy Drama Documentary Romance Short\n11937      0     0           0       0     0\n30574      0     0           0       0     0\n\n\nThe movies seem to be somewhat polarising, either rated 0 or 10, but only by a small number of reviews! Incidentally, this data set is no longer up-to-date and there are some even longer films now (though it’s a mystery why.)\n\nIn this case, the extreme outliers should be ignored, and for exploring the main distribution of movie lengths it makes sense to set some kind of upper limit. Over 99% of the data are less than three hours in length, so let’s restrict ourselve to those.\n\n\n\n\nExtract the all the movies of length at most three hours.\nDraw a histogram - what do you find?\n\n\n\nClick for solution\n\n\nmovies2 &lt;- movies[movies$length&lt;180,]\nhist(movies2$length)\n\n\n\n\n\n\n\n\nAt last, we see some structure! There are two clear peaks here (bi-modality): the first under 20 minutes, the second in [80,100] minutes. The latter group seems about right for the ‘average’ movie.\n\nUseful context:\n\nThe Oscars define a “short film” as anything under 40 minutes.\nAnimated shorts are typically between 5 and 8 minutes long, and are counted as individual movies (so, e.g., every ‘Tom and Jerry’ cartoon has its own entry).\n\n\n\n\n\nRedraw your histogram using a bin-width of 1 minute (you may need to enlarge your plot window).\nWhat do you see? How does the information above help explain the data?\nIs there any heaping in the data? At what values?\n\n\n\nClick for solution\n\nTo get a 1-minute bin width, I made a sequence of integers from 0 to 180 as my breakpoints. If you’re not familiar with this, look at the help for the ‘seq’ function in creating similar sequences.\n\nhist(movies2$length,breaks=0:180)\n\n\n\n\n\n\n\n\nThere’s a lot going on here! * there are still a fair few longer films over 2hrs, but its a minority * the clump of short movies correspond to the defined ‘short film’ and have a clear peak around 7mins. Coincidentally, most short cartoons are 6-8 mins long. * A big pronounced spike in values at exactly 90 minutes. This probably isn’t rounding, but rather that when making and editing movies they will have aimed to produce a film of that length. Perhaps we might have expected an ever sharper peak? * We also see some stacking around this peak, with certain lengths being favoured at 80, 85, 95, 100, etc"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-4-using-colour",
    "href": "Lab1_ExplContVar.html#exercise-4-using-colour",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Using colour in a plot can be very effective, for example to highlight different groups within the data. Colour is adjusted by setting the col optional arugment to the plotting function, and what R does with that information depends on the value we supply.\n\ncol is assigned a single value: all points on a scatterplot, all bars of a histogram, all boxplots are coloured with the new colour\ncol is a vector:\n\nin a scatterplot, if col is a vector of the same length as the number of data points then each data point is coloured individually\nin a histogram, if col is a vector of the same length as the number of bars then each bar is coloured individually\nin a boxplot, if col is a vector of the same length as the number of boxplots then each boxplot is coloured individually\nif the vector is not of the correct length, it will be replicated until it is and the above rules apply\n\n\nNow that we know how the col argument works, we need to know how to specify colours. Again, there are a number of ways and you can mix and match as appropriate\n\nIntegers: The integers 1:8 are interpreted as colours (black, red, green, blue, …) and can be used as a quick shorthand for a common colour. Type palette() to see the sequence of colours R uses.\nNames: R recognises over 650 named colours is specified as text, e.g.\"steelblue\", \"darkorange\". You can see the list of recognised names by typing colors(), and a document showing the actual colors is available here\nHexadecimal: R can recognise colours specified as hexadecimal RGB codes (as used in HTML etc), so pure red can be specified as \"#ff0000\" and cyan as \"#00ffff\".\nColour functions: R has a number of functions that will generate a number of colours for use in plotting. These functions include rainbow, heat.colors, and terrain.colors and all take the number of desired colours as argument.\n\n\n## Colour example\n## 3 plots in one row\npar(mfrow=c(1,3))\n## colour the cars data by number of gears\nplot(x=mtcars$wt, y=mtcars$mpg, col=mtcars$gear, xlab=\"Weight\", ylab=\"MPG\", \n     main=\"MPG vs Weight\")\n## manually colour boxplots\nboxplot(mpg~cyl, data=mtcars, col=c(\"orange\",\"violet\",\"steelblue3\"),\n        main=\"Car Milage Data\", xlab=\"Number of Cylinders\", \n        ylab=\"Miles Per Gallon\")\n## use a colour function to shade histogram bars\nhist(mtcars$mpg,col=rainbow(5),main='MPG')\n\n\n\n\n\n\n\n\n\n\n\nShow the histograms of length of short movies next to that for ‘non-short’ movies, using a different colour for each histogram.\nExperiment with using the col argument to add colour to your histograms.\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,2))\n## again, we can use subsetting to select a subset of the data. Here we don't need a ',' as we're subsetting a vector instead of a matrix \nhist(movies2$length[movies2$length&lt;=40],col='royalblue',xlab='length',main='Short films')\nhist(movies2$length[movies2$length&gt;40],col='tomato',xlab='length', main='Regular films')"
  },
  {
    "objectID": "Lab1_ExplContVar.html#exercise-5-optional-german-opinion-polls",
    "href": "Lab1_ExplContVar.html#exercise-5-optional-german-opinion-polls",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: bundestag\nThese data contain the results of the 2009 elections for the German Bundestag, the first chamber of the German parliament. The contains the number of votes cast for the various political parties, for each state (“Bundesland”). Amongst the German political parties there are two on the left of the political spectrum, the SPD - similar to the UK’s Labour party - and Die Linke (“The Left”), a party even further to the left. Suppose we’re interested in the support for this “Die Linke” party.\n\n\n\nExplore the election support for ‘Die Linke’ party by examining the LINKE1 variable using. Compare the outputs and explain the difference you find:\n\nA histogram, with a rugplot\nA stacked dotplot\nA beewswarm plot, setting horizontal=TRUE\n\n\n\n\nClick for solution\n\n\nhist(bundestag$LINKE1)\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## clear concentration of points around 10k. let's try narrower bins for more detail\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=2000))\nrug(bundestag$LINKE1)\n\n\n\n\n\n\n\n## that's a bit more useful\n\n\nstripchart(bundestag$LINKE1,method='stack',pch=16)\n\n\n\n\n\n\n\n## this looks mostly like the rugplot information on the plot above, but even with stacking this doesn't resemble the histogram\n## Why not? The histogram is grouping nearby points into its bars, but the dotplot/stripchart does not and is treating each unique value as a separate point.\n\n\nlibrary(beeswarm)\nbeeswarm(bundestag$LINKE1,horiz=TRUE)\n\n\n\n\n\n\n\n## the shape of the beewswarm more closely resembles the histogram, albeit rather than stacking points up from the bottom it works from the middle out towards the edges\n\n\nA stem and leaf plot is a technique for displaying the data in a similar fashion to a histogram, while preserving the information ofthe individual numerical values. Where the histogram summarises the data by the counts in its various intervals, the stem and leaf plot retains the original data values up to two significant figures.\n\n\n\n\nDraw a stem and leaf plot of the German election support for ‘Die Linke’ data using the stem() function.\nHow does the stem and leaf plot represent these data? You may want to look at the data value to help understand.\nNow draw a histogram, adjusting the histogram to have axis range and bar width to match the stem and leaf plot.\n\n\n\nClick for solution\n\n\nstem(bundestag$LINKE1)\n\n\n  The decimal point is 4 digit(s) to the right of the |\n\n  0 | \n  0 | 55566666667777777777777888888888888888888888888888888888999999999999+14\n  1 | 00000000000000000000000000000000000001111111111111111111111111111111+51\n  1 | 5555566\n  2 | 122234\n  2 | 588889\n  3 | 01223333344444\n  3 | 556666677777788\n  4 | 0000011233444\n  4 | 6677899\n  5 | \n  5 | \n  6 | 0233\n\n## This is effectively a histogram with bins of width 5000 units. The number indicate the actual data values, with the \"stem\" being the leading digit to the left of the | symbol, and the values of the next digits of the data being indicated to the right of the |\n## We can see some structure now *within* the bars, which is a bit more detail than we get from the histogram.\n\n\nhist(bundestag$LINKE1,breaks=seq(0,65000,by=5000))\n\n\n\n\n\n\n\n\nAs with the beeswarm plot, the stem and leaf plot is only suitable for relatively modestly sized data sets due to the fact it is literally writing out all of the data values on the screen!"
  },
  {
    "objectID": "Lab1_ExplContVar.html#stripplots-or-stripcharts",
    "href": "Lab1_ExplContVar.html#stripplots-or-stripcharts",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "‘Stripplots’ or ‘Stripcharts’ are very similar to the rugplot we applied to our histograms, and display the individual data points along a single axis. They can be used in much the same way as a boxplot, but rather than showing the data summaries they display everything!\nThe built-in faithful data set contains measurements on the waiting times between the eruptions of the Old Faithful geyser.\n\ndata(faithful)\nstripchart(faithful$waiting,ylab='Waiting Time', pch=16) \n\n\n\n\n\n\n\n\nPlotting symbols\nThe symbols used for points in plots can be changed by specifying a value for the argument pch {#pch} (which stands for plot character). Specifying values for pch works in the same way as col, though pch only accepts integers between 1 and 20 to represent different point types. The default is usually pch=1 which is a hollow circle, in the plot above we changed it to 15 which is a filled circle.\nHowever, when we have a lot of data points concentrated in a small interval, the stripplot suffers from problems of ‘overplotting’ where many points with similar values are drawn on top of each other.\nA partial solution to this is to add random noise (known as ‘jittering’) to spread out the points.\n\nMake a stripplot of the movie length data - how does the overplotting problem manifest here? You may want to compare to your histogram.\n\nA better solution is to stack the dots that fall close together, producing an alternative plot to a histogram - sometimes called a ‘dotplot’ or ‘stacked dotplot’\n\nstripchart(faithful$waiting, method='stack',pch=16)\n\n\n\n\n\n\n\n\n\nTry this out with the movies data."
  },
  {
    "objectID": "Lab1_ExplContVar.html#beeswarm-plots",
    "href": "Lab1_ExplContVar.html#beeswarm-plots",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "An evolution of the stripplot is the ‘beeswarm’ plot, available from the beeswarm package. A bee swarm plot is similar to stripplot, but with various methods to separate nearby points such that each point is visible.\n\nlibrary(beeswarm)\nbeeswarm(faithful$waiting)\n\n\n\n\n\n\n\n\nOne limitation of the beeswarm plot is that the computations to arrange all the points do not scale well with large data sets. Do not try this with the movies data, or you will be waiting for a very long time!"
  },
  {
    "objectID": "Lab1_ExplContVar.html#stem-and-leaf-plots",
    "href": "Lab1_ExplContVar.html#stem-and-leaf-plots",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "To see how this works, let’s look at the Old Faithful data, sorted from smallest to largest.\n\nsort(faithful$waiting)\n\n  [1] 43 45 45 45 46 46 46 46 46 47 47 47 47 48 48 48 49 49 49 49 49 50 50 50 50\n [26] 50 51 51 51 51 51 51 52 52 52 52 52 53 53 53 53 53 53 53 54 54 54 54 54 54\n [51] 54 54 54 55 55 55 55 55 55 56 56 56 56 57 57 57 58 58 58 58 59 59 59 59 59\n [76] 59 59 60 60 60 60 60 60 62 62 62 62 63 63 63 64 64 64 64 65 65 65 66 66 67\n[101] 68 69 69 70 70 70 70 71 71 71 71 71 72 73 73 73 73 73 73 73 74 74 74 74 74\n[126] 74 75 75 75 75 75 75 75 75 76 76 76 76 76 76 76 76 76 77 77 77 77 77 77 77\n[151] 77 77 77 77 77 78 78 78 78 78 78 78 78 78 78 78 78 78 78 78 79 79 79 79 79\n[176] 79 79 79 79 79 80 80 80 80 80 80 80 80 81 81 81 81 81 81 81 81 81 81 81 81\n[201] 81 82 82 82 82 82 82 82 82 82 82 82 82 83 83 83 83 83 83 83 83 83 83 83 83\n[226] 83 83 84 84 84 84 84 84 84 84 84 84 85 85 85 85 85 85 86 86 86 86 86 86 87\n[251] 87 88 88 88 88 88 88 89 89 89 90 90 90 90 90 90 91 92 93 93 94 96\n\n\nNote that the smallest value is 43, followed by three values of 45. A stem and leaf plot of these data looks like this\n\nstem(faithful$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n  4 | 3\n  4 | 55566666777788899999\n  5 | 00000111111222223333333444444444\n  5 | 555555666677788889999999\n  6 | 00000022223334444\n  6 | 555667899\n  7 | 00001111123333333444444\n  7 | 555555556666666667777777777778888888888888889999999999\n  8 | 000000001111111111111222222222222333333333333334444444444\n  8 | 55555566666677888888999\n  9 | 00000012334\n  9 | 6\n\n\nEach row of this plot is called a ‘stem’ and the values to the right of the ‘|’ symbol are the leaves. Be sure to read where R places the decimal point for the output. For this result, the decimal is placed one digit to the right of the vertical bar. Thus, the first row of the table then consists of data values of the form \\(4x\\), and the only leaf is a \\(3\\) corresponding to the value \\(43\\) in the data. The next stem groups the values \\(45-49\\), and we notice the three observations of \\(45\\) are represented by the \\(555\\) at the start of the second stem.\nNotice that each stem part is representing an interval of width 5, much like a histogram. As usual, R figures out how best to increment the stem part unless you specify otherwise. Finally, notice how the shape of the stem and leaf plot mirrors that of a histogram with interval width 5 - the only difference is that here we can see the values inside the bars."
  },
  {
    "objectID": "Lab1_ExplContVar.html#student-survey",
    "href": "Lab1_ExplContVar.html#student-survey",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "The data come from an old survey of 237 students taking their first statistics course. The dataset is called survey in the package MASS.\n\n\n\nLoad the data with data(survey, package='MASS')\nDraw a histogram of student heights - do you see evidence of bimodality?\nExperiment with different binwidths for the histogram. Which choice do you think is the best for conveying the information in the data?\nCompare male and femal heights using separate histograms with a common scale and binwidths."
  },
  {
    "objectID": "Lab1_ExplContVar.html#diamonds",
    "href": "Lab1_ExplContVar.html#diamonds",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: diamonds\nThe set diamonds includes information on the weight in carats (carat) and price of 53,940 diamonds.\n\n\n\nIs there anything unusual about the distribution of diamond weights? Which plot do you think shows it best? How might you explain the pattern you find?\nWhat about the distribution of prices? With a bit of detective work you ought to be able to uncover at least one unexpected feature. How you discover it, whether with a histogram, a dotplot, or whatever, is unimportant, the important thing is to find it. Having found it, what plot would you draw to present your results to someone else? Can you think of an explanation for the feature?"
  },
  {
    "objectID": "Lab1_ExplContVar.html#zuni-educational-funding",
    "href": "Lab1_ExplContVar.html#zuni-educational-funding",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: zuni\nThe zuni dataset seems quite simple. There are three pieces of information about each of 89 school districts in the US State of New Mexico: the name of the district, the average revenue per pupil in dollars, and the number of pupils. The apparent simplicity hides an interesting story. The data were used to determine how to allocate substantial amounts of money and there were intense legal disgreements about how the law should be interpreted and how the data should be used. Gastwirth was heavily involved and has written informatively about the case from a statistical point of view Gastwirth, 2006 and Gastwirth, 2008.\nOne statistical issue was the rule that before determining whether district revenues were sufficiently equal, the largest and smallest 5% of the data should first be deleted.\n\n\n\nAre the lowest and highest 5% of the revenue values extreme? Do you prefer a histogram or boxplot for showing this?\nRemove the lowest and highest 5% of the cases, draw a plot of the remaining data and discuss whether the resulting distribution looks symmetric.\nDraw a Normal quantile plot of the data after removal of the 5% at each end and comment on whether you would regard the remaining distribution as normal."
  },
  {
    "objectID": "Lab1_ExplContVar.html#pollutants-from-engines",
    "href": "Lab1_ExplContVar.html#pollutants-from-engines",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: engine\nThese data record the amounts of three pollutants - carbon monoxide CO, hydrocarbons HC, and nitrogen oxide NO - in grammes emitted per mile by 46 light-duty engines.\n\n\n\nAre the distributions for the three pollutants similar? To make this an easier question to answer, try to produce histograms of the variables, using the same class intervals and range for the horizontal axis in each case."
  },
  {
    "objectID": "Lab1_ExplContVar.html#dosage-of-chlorpheniramine-maleate",
    "href": "Lab1_ExplContVar.html#dosage-of-chlorpheniramine-maleate",
    "title": "Practical 1 – Exploring continuous variables",
    "section": "",
    "text": "Download data: chlorph\nThese data come from a semi-automated process for measuring the actual amount of chlorpheniramine maleate in tablets which are supposed to contain a 4mg dose.\nThe tablets used for the study were made by two different manufacturers. For each manufacturer, a composite was produced by grinding together a number of tablets. Each composite was split into seven pieces each of the same weight as a tablet and the pieces were sent to seven different laboratories. Each laboratory made 10 separate measurements on each composite.\nThe data contain three variables: * chlorpheniramine - the amount measured * manufacturer - the tablet manufacturer as a factor (A or B) * laboratory - the laboratory which performed the measurement as a factor (1 to 7)\n\n\n\nProduce box-plots of the chlorpheniramine measurements split by laboratory. What, if anything, do they suggest?\nNow produce box-plots by manufacturer. Anything noticeable?\nThe problem here is that we really need a separate box-plot for each combination of manufacturer and laboratory. We can do this by boxplot(chlorpheniramine~laboratory*manufacturer, data=chlorph) Try this (or some abbreviated version of it).\nTry reversing the order of laboratory and manufacturer. Which way round is better? Try colouring the box-plots by manufacturer. Does that help?\nDo the data suggest that the two manufacturers actually put different amounts of the drug into supposed 4mg tablets?\nAre there obvious differences between different laboratories? If so, what kind of differences do you observe?\nIf you had to choose a single laboratory to make some measurements for you, which would you choose and why?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DEVUL 2025/26",
    "section": "",
    "text": "This website contains the HTML version of lecture notes for the module in 2025/26. Use the left bar to navigate through the different lectures and the right bar to navigate through different sections in the same lecture.\nThe content of these notes integrates the material of previous lecturers of this module (Jonathan Cummings, Emmanuel Ogundimu and Hyeyoung Maeng). Without their input, the overall quality of the material would be much worse. It shall be noted however that any mistake in the notes is my responsibility: if you spot one, please let me know at daniele.turchetti@durham.ac.uk.\n\n\n\n\n\nLecture 1a\nLecture 1b\nPractical 1\nWorkshop 1\n\n\n\n\n\nLecture 2a\nLecture 2b\nPractical 2\nWorkshop 2"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "DEVUL 2025/26",
    "section": "",
    "text": "Lecture 1a\nLecture 1b\nPractical 1\nWorkshop 1\n\n\n\n\n\nLecture 2a\nLecture 2b\nPractical 2\nWorkshop 2"
  },
  {
    "objectID": "Workshop1.html",
    "href": "Workshop1.html",
    "title": "Workshop 1 - Challenge 1",
    "section": "",
    "text": "For this challenge you are going to work with the Marriage data set:\nload('Marriage.Rda')\nAmong the categorical variables in this data set, there is one called sign\nMarriage$sign\n\n\nAriesLeoPiscesGeminiSaggitariusPiscesLibraAquariusSaggitariusCancerAquariusScorpioVirgoLibraScorpioSaggitariusAquariusTaurusPiscesTaurusAriesSaggitariusLibraCapricornSaggitariusLeoLibraLibraAriesPiscesPiscesVirgoAriesGeminiAquariusSaggitariusGeminiPiscesGeminiVirgoPiscesPiscesGeminiVirgoPiscesVirgoGeminiCancerSaggitariusPiscesTaurusPiscesTaurusCapricornScorpioSaggitariusVirgoAriesLeoAriesTaurusTaurusAquariusCancerPiscesAriesAriesScorpioCancerVirgoLibraGeminiAriesPiscesLibraLeoCancerSaggitariusScorpioScorpioAriesPiscesCancerPiscesLeoLeoScorpioPiscesAquariusVirgoCancerLeoVirgoGeminiCancerAquariusGeminiVirgo\n\n\n    \n        Levels:\n    \n    \n    'Aquarius''Aries''Cancer''Capricorn''Gemini''Leo''Libra''Pisces''Saggitarius''Scorpio''Taurus''Virgo'\n1. Create a barchart of the counts for the categorical variable sign\n2. Create a piechart of the counts for the categorical variable sign\n3. Use the par() command to visualise the barchart and the piechart side by side"
  },
  {
    "objectID": "Workshop1.html#workshop-1---challenge-2",
    "href": "Workshop1.html#workshop-1---challenge-2",
    "title": "Workshop 1 - Challenge 1",
    "section": "Workshop 1 - Challenge 2",
    "text": "Workshop 1 - Challenge 2\nIn this challenge, you’ll work with the British election survey data set.\n\nload('beps.Rda')\n\nWe want to assess whether the attitude towards Europe changes along main party lines or not. Before doing anything - think a little about what you expect to see and write it down in the following box:\n 1. Extract the data corresponding to those individuals who intend to vote for Labour, and plot their attitudes to Europe. Add a main plot label to indicate the party.\n 2. Do the same for LibDem and Conservatives and show the three plots side by side using different colours."
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html",
    "href": "Workshop1_CategoricalVariables.html",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "",
    "text": "Extracting various types of subsets of a given data set\n\n\nDrawing simple barplots using barplot to show the distribution of a categorical variable\n\n\nUsing pie to draw pie charts to represent proportions"
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html#pie-charts",
    "href": "Workshop1_CategoricalVariables.html#pie-charts",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "Pie charts",
    "text": "Pie charts\nThe many issues with pie charts notwithstanding, generating a pie chart is relatively easy with the function pie. Note that the pie chart emphasises showing the data as proportions of the whole, rather than separate counts.\npie takes the same data input as the barplot, and can be supplied with labels, and given custom colours for the wedges. We’ll stick with the defaults for now, but feel free to experiment.\n\npie(offs)\n\n\n\n\n\n\n\n\nIf your goal is compare each category with the the whole (e.g., what portion of weddings are officiated by a Bishop compared to all participants), and the number of categories is small, then pie charts may work for you. However, the best alternative to a piechart is the barchart, but if you really want to show proportions of a whole via area, then a treemap is a better choice (see below).\n\n\nChallenge\n\nThis is your first data visualisation challenge. To start it simple, we’ll just do a recap of bar charts and pie charts. On your assignment on JupyterHub you will\n\nCreate a barchart of the counts for the categorical variable sign in the Marriage dataset.\nCreate a piechart of the same counts.\nUse the par() command to visualise the barchart and the piechart side by side.\n\n\n\nA refresher on how the par() command works\n\nR makes it easy to combine multiple plots into one overall graph, using either the par or layout functions.\nWith the par function, we specify the argument mfrow=c(nr, nc) to split the plot window into a grid of nr x nc plots that are filled in by row.\nNote: if you don’t want to arrange plots in a simple regular grid and have something more complex in mind, you can use the layout function.\n\n\n\n\nSolution of the Challenge\n\n\ndatacount &lt;- xtabs(~sign, data=Marriage) ## save the table to `offs`\nbarplot(datacount, las=2, col='red')\n\n\n\n\n\n\n\n\n\npie(datacount)\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nbarplot(datacount, las=2, col='red')\npie(datacount)"
  },
  {
    "objectID": "Workshop1_CategoricalVariables.html#data-analysis-british-election-survey-data",
    "href": "Workshop1_CategoricalVariables.html#data-analysis-british-election-survey-data",
    "title": "Workshop 1 - Exploring Categorical Variables",
    "section": "Data analysis: British Election survey data",
    "text": "Data analysis: British Election survey data\nIn many surveys there is a string of questions to which respondents give answers on integer scales from, say, 1 to 5. This is so commonplace that one such scale even has a name - the Likert scale.\nThe beps dataset includes seven questions put to 1525 voters in the British Election Panel Study for 1997-2001. One question was on a scale from 1 to 11, one from 0 to 3, and the rest were from 1 to 5.\nThe leaders of the main political parties at the time were Tony Blair for Labour (the Prime Minister at the time), William Hague for the Conservatives, and Charles Kennedy for the Liberal Democrats. Each surveyed individual assessed each party leader on a scale of 1 to 5 (5=best).\n\nLoad the beps data you can find on Ultra. Use the head function to get a quick look at the first few rows of the data to see what the variables are and how they are represented.\nLet’s begin with the party leader data:\n\nSplit your plot display to show a single row of three plots.\nDraw a barplot of each of the party leader’s ratings as contained in the three variables Blair, Hague and Kennedy. Don’t forget to call the xtabs function to summarise the data before plotting.\nColour your barplots by the corresponding party colours (‘red’, ‘blue’, and ‘orange’ respectively.).\n\nCan you see any similarities in the distributions of the assessments of Blair and Hague? How would you interpret these patterns?\nWhat do you find about the assessments of Kennedy?\nRepeat the plots using pie charts - which plots do you find easier to read and interpret?\n\n\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\npar(mfrow=c(1,3))\nbarplot(xtabs(~Blair,data=beps),col='red')\nbarplot(xtabs(~Hague,data=beps),col='blue')\nbarplot(xtabs(~Kennedy,data=beps),col='orange')\n\n\n\n\n\n\n\n## Blair and Hague are very polarised - either v popular or v not, rarely in the middle at '3'\n## Kennedy is less polarised, more middle/positive\n\n\npar(mfrow=c(1,3))\npie(xtabs(~Blair,data=beps))\npie(xtabs(~Hague,data=beps))\npie(xtabs(~Kennedy,data=beps))\n\n\n\n\n\n\n\n# pie give us information on the breakdown of the ratings, but not super helpful when it comes to comparisons\n\nIn addition to assessing the party leaders, two further variables concerned Europe. The first asked the respondents to quantify their knowledge of the parties’ policies on European integration from low knowledge (0) to high knowledge (3), and the second measures the individuals attitudes to European integration from 1 to 11, where higher values are more Eurosceptic.\n\nFirst, we investigate the respondents knowledge of the parties’ policies on Europe as contained in the political.knowledge variable.\nThen we investigate the Europe variable representing individual attitudes towards Europe. Do any features stand out?\nThe data set also contains a column representing the voting intentions of the individuals surveyed in the vote variable. We are going to plot the voting intentions.\n\n\n## Political knowledge of party's European policy\nbarplot(xtabs(~political.knowledge,data=beps),col='forestgreen')\n\n\n\n\n\n\n\n## many 0s and 2s - again, quite split between 'none' and 'some' knowledge of European policy\n\n\n## Attitudes towards European integration\nbarplot(xtabs(~Europe,data=beps),col=c('thistle'))\n\n\n\n\n\n\n\n## Note: 1=pro europe, 11=europsceptic.\n## BIG spike on 11 so strong Eurosceptic sentiment in the sample, with a lesser spike at a neutral position of 6.\n## Noticeably no strong pro-European sentiment at the time\n\n\n## Voting intentions\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'))\n\n\n\n\n\n\n\n## strong Labour support - not surprising as Blair is during his first term as PM\npie(xtabs(~vote,data=beps),col=c('blue','red','orange'))\n\n\n\n\n\n\n\n## this is probably a good use of a pie chart, with few categories and an easy obvious comparison\n\n\n\nChallenge\n\n\nNow, let’s dig a little deeper and think about how attitudes to Europe may differ between the different party voters:\n\nBefore doing anything - think a little about what you expect to see.\nNow, split the plot window into three, extract the data corresponding to those individuals who intend to vote for Labour, and plot their attitudes to Europe, using party colours as above. Add a main plot label to indicate the party.\nRepeat for Conservative and Liberal Democrat voters.\nWhat features do you see? Are there any surprises, or does this confirm what you expected?\n\n\n\n\n\nClick for solution\n\n\npar(mfrow=c(1,3))\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Labour\",]),col='red')\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Conservative\",]),col='blue')\nbarplot(xtabs(~Europe,data=beps[beps$vote==\"Liberal Democrat\",]),col='orange')\n\n\n\n\n\n\n\n## Conservatives are very eurosceptic, and almost never pro-Europe\n## Labour is a mix of either slightly pro/neutral, and strongly against\n## LibDem is also similar to Labour - though surprisingly (given recent years) less pro-European"
  },
  {
    "objectID": "Lecture1b_Variables.html",
    "href": "Lecture1b_Variables.html",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "Data is organised into variables, representing attributes or measurements - e.g. age, weight, income, temperature, time, etc.\nTo begin with, we’ll focus on using standard techniques to explore a single variable at a time. Specifically:\n\nExploring Continuous Variables - using statistical summaries, box plots, histograms, and quantile plots\nExploring Categorical Variables - using bar plots, pie charts, and stacked bars.\n\nThese techniques are quite simple, so our focus is on using them effectively to learn about data features and how to do so using R.\nThere are a number of Graphics packages in R that we could use to visualise our data:\n\nBase R covers most standard statistical visualisations\nggplot2 - GGPlot and related packages provide more modern graphics, but it has unusual syntax that can be more difficult to learn\nplotly - similar to ggplot. However, a bit easier to use\nOther custom packages will provide support for specific visualisations\n\nWe will focus on the base R functions, as those always available. However, you should experiment with the other packages, but be aware they work differently.\n\n\n\nWe focus first on quantitative data that is continuous (i.e. real-valued).\nWe view our data as a sample from an underlying continuous distribution - the goal of our explorations here is to seek some clues about the features of that distribution.\nMany possible ways to explore this - graphically, we will focus on histograms and boxplots.\nWhen we have a particular type of distribution in mind, we can also draw a quantile plot to see how plausible it is.\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. the birthweight of babies isn’t.\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\nA histogram is an approximate representation of the distribution of continuous data where we divide the range of values into bins (or buckets) and draw a bar over each bin with area proportional to the frequency of cases in each bin.\nA histogram is an effective tool for visualising features relating to the shape of the data distribution.\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n\nThis data set contains the various information on housing values and related quantities for the 506 suburban areas in Boston. The main interest is in the `median values of owner-occupied homes’, but there are 14 variables to explore here.\nLooking first at the median housing value, we can produce the histogram below. Some obvious features of note are:\n\nA concentration of housing values around 20-25, then a sudden drop-off – is there an explanation for this, such as changes in tax levels?\nPossible multi-modality, with modes around 25 and 30 – are there two classes or groups of housing?\nA further spike in values occur in the upper tail of the data, at 50 – this seems dubious, and could be some crude rounding or grouping of all values ‘’50 and over’’.\n\n\n\n\n\n\n\n\n\n\nChoosing the width of the bars can substantially affect the detail of the histogram. If we choose too few bins, then we can obscure key features by over-smoothing the data. Alterntaively, if we have too many bins then we can introduce too much noise to the plot that it obscures more general features and patterns. Unfortunately, the only way to find a good compromise is to experiment – R and other software will default to a ‘best guess’, but this invariably needs adjusting. The histograms below show the same data, but using bar widths of 5, 2.5, and 1 unit respectively. We’ll see more sophisticated methods for smoothing the data later.\n\n\n\n\n\n\n\n\n\nWith 14 variables in the data set, we could inspect the histograms of each of the variables. We could do this one-at-a-time, or arrange them in a grid or matrix as below:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe other variables in the data set show a variety of the features discussed above that we may want to investigate further:\n\nThe ‘age’ variable - in position \\((1,1)\\) - shows obvious left-skewness.\nThe ‘dis’ variable - in position \\((1,2)\\) - shows right skewness\nThe ‘rm’ variable - in position \\((3,4)\\) - looks symmetric, and possible approximate Normality\nIn the variables in the bottom two rows, we see a lot of gaps in the data and heaping on particular values.\nThe ‘chas’ variable - in position \\((1,3)\\) - exhibits heaping on only two values, 0 and 1. This is a discrete binary variable, not continuous a continuous one! We should use different methods to explore this variable.\n\nIn a full analysis, we would want to look at the relationships between the variables to determine how the features observed relate to each other. We’ll return to this later…\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe geyser data set contains 272 observations of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The variables are:\n\nduration - Length of eruption in mins\nwaiting - Waiting time to next eruption\n\nTo draw a histogram, we use the hist function:\n\nhist(geyser$waiting)\n\n\n\n\n\n\n\n\nHere we can see the data appear to come in two groups: one group with smaller values (shorter waiting times), and one group of larger (longer waiting times).\nWe can add a rug plot to an existing histogram to add more detail and to show where the individual data values fall inside the bars:\n\nhist(geyser$waiting)\nrug(geyser$waiting)\n\n\n\n\n\n\n\n\nNow, each individual mark on the horizontal axis shows where we have observed a data value.\nHistograms can be customised in many ways, but the most important one is changing the configuration of the bars drawn. This is controlled by the breaks parameter. We can set breaks to a number to indicate an approximate number of bars to draw:\n\nhist(geyser$waiting,breaks=20)\n\n\n\n\n\n\n\n\nOr we can be more specific and state where each individual bar should begin and end by listing the breakpoints of the bars as a vector:\n\nhist(faithful$waiting, breaks=c(30,45,47,50,52,55,60,75,80,85,95,105))\n\n\n\n\n\n\n\n\nAdditionally, almost all plot functions can take the following arguments to customise the plot:\n\nxlab, ylab - sets the x and y axis labels\nmain - sets the main title\nxlim, ylim - set x and y axis limits, e.g. c(0,10)\ncol - sets the plot colour(s)\n\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. zero-inflation\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\n\n\n\nWe have considered the shape of a distribution already, and have seen that histograms are a handy way of exploring shape.\nWe come now to several summary statistics (recall ISDS):\n\nthe average and median summarise the centre or location of the distribution.\nthe standard deviation and inter-quartile range summarise spread around the centre of the distribution.\n\nThe average is the most (over-)used statistic. It is useful especially where distributions are reasonably symmetric.\nFor distributions with long tails, it can give misleading signals, e.g. average household income will include households like Buckingham palace which distort the average.\nThe median is a measure of the midpoint of a distribution, and, unlike the mean, it is quite resistant to extreme values.\nA robust summary of spread is given by the interquartile range (IQR), the difference between the first and third quartiles. Every distribution has three quartiles:\n\nThe first quartile, \\(Q_1\\), is a number such that 25% of the values in the distribution do not exceed this number. This is called a lower quartile.\nThe second quartile, \\(Q_2\\), is a number such that 50% of the values in the distribution do not exceed this number. This is the same as the median.\nThe third quartile, \\(Q_3\\), is a number such that 75% of the values in the distribution do not exceed this number. This is called an upper quartile.\n\nTukey suggested that we combine the three quartiles with the minimum and maximum values in the data to form a five-number summary.\n\nIn R, the fivenum function gives the standard 5-number summary:\n\nfivenum(geyser$waiting)\n\n[1]  43  59  76  83 108\n\n\nThe summary function adds a 6th number (the mean):\n\nsummary(geyser$waiting)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  43.00   59.00   76.00   72.31   83.00  108.00 \n\n\nMin. 1st Qu. Median Mean 3rd Qu. Max. 43.0 58.0 76.0 70.9 82.0 96.0\n\n\n\n\nA boxplot (or box-and-whisker plot) is constructed from the 5-number summary by: * Drawing a box with lower boundary at \\(Q_1\\) and upper boundary \\(Q_3\\). * Drawing a line inside the box at the median (\\(Q_2\\)). * Drawing lines (whiskers) from the edges of the box to the most extreme data point that is within \\(1.5\\times IQR\\) of the edge of the box (often the minimum and maximum values).\n\nWe can draw a boxplot by using the boxplot function on a vector of data values:\n\nboxplot(geyser$waiting)\n\n\n\n\n\n\n\n\nOr we can pass all the columns of a data set to boxplot to draw everything at once:\n\nboxplot(geyser)\n\n\n\n\n\n\n\n\nNow, all variables are shown together on a common axis scale. This can be useful if all the variables take values of a similar size, but - as we see here - when the variables are quite different it can obscure the features of some variables by drawing everything together.\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data. Though usually, you have the same number of data points in each column of the data set so this is not often very helpful.\n\n\n\nTo show the relationship between a histogram and a boxplot, we have a data set comprising the length (mm) of 100 cuckoo eggs. Drawng the histogram and boxplot together, we can see that the histogram gives far more detail on the shape of the distribution and the boxplot is more of a summary visualisation. The median is indicated in red and the upper/lower quartiles in green, which shows how these quantities align between the plots. Note that the smallest value in the data is flagged as an outlier and drawn separately as a circle on the boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValues too far from the centre of the distribution are known as outliers.\nData values further than \\(1.5\\times IQR\\) from a lower/upper quartile are called an outlier, and usually indicated with a circle or point.\nAnything beyond \\(3\\times IQR\\) is an extreme outlier, and usually indicated with a star or asterisk.\nThis definition of outliers is a simple rule-of-thumb. It leads to about 0.7% of (normally distributed) observations being described as extreme for large samples.\nHowever, if you have a million data points then 0.7% of your data is still a lot! This doesn’t necessarily mean those points are all genuinely extreme or outliers.\n\n\n\nOne advantage of the boxplot, is that as it is a simple summary plot it is much easier to use boxplots to compare many variables at once. For example, the data plotted below are boxplots of the heights in inches of the singers in the New York Choral Society in 1979. The data are grouped according to the voice part they play in the choir. The vocal range for each voice part increases in pitch according to the following order: Bass 2, Bass 1, Tenor 2, Tenor 1, Alto 2, Alto 1, Soprano 2, Soprano 1. We can see immediately that the lowest pitch voices are associated with the taller singers and the higher pitches with smaller singers, which makes a lot of intuitive sense. There will also be a strong correspondance with Gender here too, though that information is not recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should examine a single boxplot for the following features:\n\nIs the median line in the centre of the 50% portion of the distribution? If not, some degree of skew or asymmetry is indicated.\nAre the whiskers the same length? If not, some degree of skew or asymmetry is indicated.\nAre there any outliers?\n\nWe should examine several boxplots for the following features:\n\nDo the groups have similar shapes? Some might seem symmetric, some skewed.\nAre the median lines about the same, or do there seem to be differences in location** between groups?\nAre the IQRs about the same, or do there seem to be differences in spread between groups?\nAre there some groups with more outliers  than others? Such inspection can reveal important differences between groups.\n\n\n\n\n\nMany statistical methods require our data be approximately Normally distributed - but how can we tell whether the approximation is reasonable?\nNormal-quantile plots provide a simple and informal way of doing this, without having to do a formal hypothesis test — though that may be a natural next step.\nA Normal Quantile (or Q-Q plot) can be used to informally assess the normality of a data set. We construct the plot as follows:\n\nOrder the \\(n\\) data values, giving the ordered data \\(x_{(1)},x_{(2)},\\dots,x_{(n)}\\).\nCalculate the values \\(\\frac{1}{n+1},\\frac{2}{n+1},\\dots,\\frac{n}{n+1}\\), which gives \\(n+1\\) divisions of the \\((0,1)\\) interval.\nUse Normal tables or computer to find the value \\(z_k\\) such that \\(\\Phi(z_k)=\\frac{k}{n+1}\\) for the values \\(k=1,2,\\dots,n\\). The \\(n\\) values \\(z_1,z_2,\\dots,z_n\\) are the \\(\\frac{k}{n+1}\\) quantiles of the Normal distribution.\nPlot the \\(n\\) pairs of points \\((x_{(1)},z_1), (x_{(2)},z_2), \\dots, (x_{(n)},z_n)\\).\n\nThe basic idea is that these plots have the property that plotted points for Normally distributed data should fall roughly on a straight line.\nThis is because \\(x_{(k)}\\) are the sample quantiles of our data, and \\(Z_k\\) are the theoretical quantiles of a Normal distribution. If our sample distribution is approximately normal, these pairs of values will be in agreement.\nSystematic deviations from the straight line indicate non-normality.\nWe don’t need the points to lie on a perfect straight line – we often use the `fat pen test’, meaning that if the points are covered by placing a fat pen over the top, then that’s enough to conclude approximate normality!\n\n\n\nLet’s inspect the cuckoo egg data for normality. We can draw a histogram first, and compare its shape to a normal curve (blue line). While there looks to be approximate agreement (and we only ever need approximate Normality…), we see some divergence for small values of the data.\nDrawing the Normal quantile plot confirms these observations - things look reasonably close to a straight line, but deviate from Normality for small values. However, this would probably be enough to pass our ‘fat pen test’.\n\n\n\n\n\n\n\n\n\nIn R, we can use the qqnorm function to draw a Normal quantile plot of a single variable. Calling qqline after adds the theoretical straight line for comparison\n\nqqnorm(mtcars$mpg)\nqqline(mtcars$mpg)\n\n\n\n\n\n\n\n\nFor this variable (miles per gallon of various cars) we see strong departure from Normality as the points lie far from the desired straight line.\n\n\n\nTo illustrate what happens when our data are very non-Normal, consider this data set of ‘monthly mean relative sunspot numbers’ recorded from from 1749 to 1983. The data are counts of a quantity that is usually quite small in magnitude, this means the data are usually concentrated on small values with an ‘invisible wall’ at 0 - since we cannot observe negative counts! This gives rise to a heavily skewed distribution, as we see in the histogram and boxplot below. The Normal quantile plot (right) now shows strong curvature - not the straight-line feature we would expect if the data were Normally distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is occasionally possible to transform the data so that the transformed data are more approximately Normal.\nThe kinds of transformation we might employ are simple one-to-one power transformations, such \\(\\ln x\\), \\(\\sqrt{x}\\), \\(1/x\\), etc.\nWe can decide informally whether a transformation is useful by examining a Normal quantile plot of the transformed data.\nGraphical displays can help appraise the effectiveness of the transformation, but the cannot tell you if it makes sense - you should consider the interpretation as well as the statistical properties!\n\n\n\n\n\n\nThe frequency distributions of single continuous variables can exhibit a lot of different features\nHistograms are great at for emphasising features of the raw data, but struggle when comparing multiple variables or groups.\nBoxplots show less detail than histograms, but excel at comparing distributions across variables and for identifying outliers.\nQuantile plots can be used to see if data (approximately) follow a particular distribution.\n\n\n\n\nTests using the summary statistics - tests of means and variances against hypothesised values may be appropriate. Normality - formal tests of Normality, such as the KS test, could be applied if following a Normal distribution is important\nOutliers - formal tests for outliers exist (‘outliers’ package) and could be applied, plus the effect of excluding outliers from any analysis could be explored\nMultimodality - methods for assessing multimodality also exist (e.g. ‘diptest’ package), but care must be taken to avoid interpreting data noise as structure\nSample size - many of the methods can be overly sensitive for very large samples\n\n\n\n\n\nCategorical or qualitative variables can only take one of a limited number of possible values (categories). The possible values a variable can take are known as the categories, levels, or labels.\nCategorical data comes in various forms depending on how the categories relate to each other:\n\nNominal - the categories have no standard order (e.g. eye colour)\nOrdinal - the categories have an intrinsic order (e.g. age recorded as “young”, “middle-aged”, and “old”)\nDiscrete - the categories are numerical, and hence ordered, but can only take a finite number of values (e.g. number of people per household)\n\nFor example, the data set below contains the responses to seven questions put to 1525 voters in an election survey for 1997-2001. All the variables in the data are categorical of different types. vote is the party the voter would vote for is clearly categorical, as is gender. The others are ordinal, but with numerical values. In some cases, where the variable is discrete, numerically valued, with a large number of categories - like age - it does make sense to treat it as continuous. Variables such as age are often thought of as being fundamentally continuous albeit recorded as discrete values.\n\nload(\"beps.Rda\")\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\n\nThe levels of a categorical variables are expressed by a factor coding to represent the different categories. Numerical codings are sometimes used, e.g. for a variable on marital status we could define:\n\nsingle\nmarried\nseparated\ndivorced\n…\n\nWhile this helps abbreviate the categories, it is important to remember that variables expressed this way are not the same as a numerical variable that can take these values. We should never treat these values as if they were continuous - for example, it could be tempting to take an average and, say, get a result of 2.6. But this is meaningless as it doesn’t correspond to any of the possibilities, the value we get depends entirely on how we coded our factors, and the rules of arthimetic make no sense for these variables – 1+3 may equal 4, but that does not mean single+separated=divorced!\nInstead, using text strings to represent the categories is safest and avoids mistakes such as these. However, care must be taken to ensure that there is consistency in how these levels are labelled, e.g. do we treat the labels “Female”, “female”, “F” as the same?\n\n\nCompared to continuous variables, categorical variables are relatively simplistic and usually contain little useful information on their own. As data, they usually reduce to the counts of the number of observations in the various categories.\nWe can obtain summary tables of the frequency of each category by using the table function.\n\ntable(beps$vote)\n\n\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\nThe R output is useful for a quick summary, but will need some manual re-formatting to make it presentable. R output is seldom an acceptable way to present information to others, and almost always should be transformed (e.g. into a summary table of relevant information) or summaries (e.g. by reporting only the relevant information R has given you). Here we can transform the ugly R code into a small data table:\n\n\n\n\n\nConservative\nLabour\nLiberal Democrat\n\n\n\n\n462\n720\n343\n\n\n\n\n\nThe xtabs function does something similar, and will be more useful later on when we have multiple variables at once:\n\nxtabs(~vote,data=beps)\n\nvote\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\n\n\n\nVisualisation of categorical variables usually focuses on plotting the counts of the categories, or the proportions that each category contribute to the total.\nThe range of useful graphics for such data is usually limited to:\n\nBarcharts - depict the counts or proportions by the size of the bar\nPiecharts - display the proportions of categories as fractions of the whole\nVariations of the above (stacked bars, treemaps)\n\nWhat features to look for?\n\nExtremes - the largest (smallest) category is often of particular interest.\nUneven distributions - Observational studies can often observe many more cases of one category than others. Some categories may not be observed\nUnexpected patterns of results - surprisingly large or small numbers for particular categories\nLarge numbers of categories - these may require grouping together or filtering out\nDon’t knows, missing values - Missing, unknown, or unavailable data is common in e.g. surveys and opinion polls.\nErrors in factor codings - e.g. gender could be denoted ‘M’ or ‘F’, but we may find values of ‘m’, or ‘female’.\n\n\n\n\nA bar chart or barplot simply draws the distribution of counts per category. We can use barplot to draw a barplot from a summary table of counts generated by the xtabs function\n\nbarplot(xtabs(~vote,data=beps))\n\n\n\n\n\n\n\n\nbarplot takes a number of additional arguments to customise the plot:\n\nnames - a vector of labels for each bar\nhoriz - set to TRUE to show a horizontal barplot.\nwidth - a vector of values to specify the widths of the bars\nspace - a vector of values to specify the spacing between bars\ncol - a vector of colours for the bars\n\n\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'), horiz=TRUE, names=c('Con','Lab','LibDem'))\n\n\n\n\n\n\n\n\n\n\n\nHere we have the number of eligible voters in each of the 16 Bundesländer (states) in the German Federal elections in 2009.\n\nload('btw9s.Rda')\nhead(btw9s)\n\n   Bundesland  Voters   EW State1\nBW         BW 7633818 West     BW\nBY         BY 9382583 West     BY\nBE         BE 2471665 East     BE\nBB         BB 2128715 East     BB\nHB         HB  487978 West     HB\nHH         HH 1256634 West     HH\n\n\n\n\n\n\n\n\n\n\n\nAs states are categorical the ordering of the bars is arbitrary, which limits what we can interpret - but clearly there are some very large and very small states.\nWe notice wide variation in populations - the largest is ‘NW’ (Nordrhein-Westfalen) which includes many major cities like Cologne and Düsseldorf; the smallest is ‘HB’ which is Bremen, a smaller city-state.\nThe ordering of bars can be used for emphasis - here its alphabetical. Ordering the bars by size gives a much better impression of the relative sizes of the sixteen states:\n\n\n\n\n\n\n\n\n\nUsing problem-specific structure can help give you context to your analysis. For example, we can separate the states belonging to the former East (left) and West Germany (right) and use a little colour for emphasis:\n\n\n\n\n\n\n\n\n\nClearly the West German states are substantially more populous.\n\n\n\n\ntitanic &lt;- data.frame(Titanic)\n\nThe Titanic data contains information on the fate of survivors fatal maiden voyage of the ocean liner Titanic. The data are in the form of counts of survivors (and not) summarised by economic status (class), sex and age. The variables are all categorical and defined as\n\nClass - 1st, 2nd, 3rd, or crew\nSex - Female, Male\nAge - Adult, Child\nSurvived - No, Yes\n\nBefore we analyse the data, think about what you expect the results to show. Do we expect more Male or Female survivors? Do we expect those in 1st class to fare better than those in 3rd class? Would we expect the Crew to fare better or worse than the passengers?\nBy thinking about what we might expect before looking at the data, we allow ourselves to be surprised when we find features we did not expect!\n\npar(mfrow=c(1,4))\nbarplot(xtabs(Freq~Survived,data=titanic), col=\"red\",main=\"Survived\")\nbarplot(xtabs(Freq~Sex,data=titanic), col=\"green\",main=\"Sex\")\nbarplot(xtabs(Freq~Age,data=titanic), col=\"orange\",main=\"Sex\")\nbarplot(xtabs(Freq~Class,data=titanic), col=\"blue\",main=\"Sex\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nMore than twice as many passengers died as survived\nMore Male than Female passengers on board - more than 3x as many!\nVery few Child passengers\nMore people in Crew than any other class; fewest in second class\n\nThe interesting questions arise when we try to think whether Survived is related to the other variables. But we’ll have to come back to this later on.\n\n\n\nProportions are of particular interest in opinion polls, or studies where the composition of a larger population is of interest. In elections, the values of the counts are far less important than the share of the total - in particular the size of the largest proportion.\nVisualisations of proportions are based around the simple idea of dividing the larger whole into pieces which reflect the corresponding fractions.\n\npiecharts - slices of a circle\ncomposite or stacked barcharts - fractions of a bar\ntreemaps - tiles within a square or rectangle\n\nA stacked barplot is a variation of a standard barplot where the individual bars are broken up into portions reflecting the different. When we just have one variable, the effect is to stack the individual bars on top of each other.\n\nbarplot(as.matrix(xtabs(~vote,data=beps)), ## note we have to conver to a matrix here\n  beside=FALSE,\n  horiz=TRUE,\n  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nHere we can see the individual sizes of the bars, as well as a clear indication of their contribution to the overall total. So, it’s easy to read that the red category (Labour) has the highest share of the vote in this poll.\nThe traditional pie chart can be generated using pie:\n\npie(xtabs(~vote,data=beps),  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nWe’re probably all familiar with this, but the basic idea is to use slices of the ‘pie’ to represent the proportion. Unfortunately, pie charts are often not the best visualisation as it can be difficult to detect differences between similarly-sized slices of a circle when compared to similiarly sized rectangles.\nHistorical aside: the invention of the pie chart is often attributed to Florence Nightingale. In one of the earliest conventional uses of data visualisation, she used an early version of the pie chart highlight the poor conditions of soldiers in field hospitals during the Crimean War (1854-6). Florence was an early pioneer of statistics and the first female member of the Royal Statistical Society – this aspect of her life and achivements is often overlooked given her frequent association with nursing.\n\n\n\n\n\n\n\nA stem and leaf plot presents the numerical values of the data in a similar form to a histogram.\n\nstem(geyser$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   4 | 3\n   4 | 577888889999999\n   5 | 00000000000011111222223333333444444444\n   5 | 5556677777777788888999\n   6 | 0000001112222234\n   6 | 5555555668889999\n   7 | 01111122222233333344444444\n   7 | 5555555556666666677777777778888888888888888899999999999\n   8 | 00000000000001111111111112222222233333344444444444\n   8 | 5555555666667777777777777788888889999999\n   9 | 0011222333333334\n   9 | 668\n  10 | \n  10 | 8\n\n\nThis plot resembles a sideways histogram, only it shows extra information on the values within each bar.\n\n\n\nA stripplot is similar again, but displays the data as points rather than the numerical value\n\nstripchart(geyser$waiting, method='stack')\n\n\n\n\n\n\n\n\nBy default, it draws a hollow box for each data point which can make it difficult to read. We can modify the shape of the points by the pch (plot character) argument can help improve readability:\n\nstripchart(geyser$waiting, method='stack', pch=16)\n\n\n\n\n\n\n\n\n\n\n\nA beeswarm plot is similar to stripplot, but uses various techniques to separate nearby points such that each point remains visible. It also draws the plot symmetric about a central axis, rather than stacking up points from a baseline.\n\nlibrary(beeswarm)\nbeeswarm(geyser$waiting,horizontal = TRUE)"
  },
  {
    "objectID": "Lecture1b_Variables.html#exploring-continuous-variables",
    "href": "Lecture1b_Variables.html#exploring-continuous-variables",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "We focus first on quantitative data that is continuous (i.e. real-valued).\nWe view our data as a sample from an underlying continuous distribution - the goal of our explorations here is to seek some clues about the features of that distribution.\nMany possible ways to explore this - graphically, we will focus on histograms and boxplots.\nWhen we have a particular type of distribution in mind, we can also draw a quantile plot to see how plausible it is.\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. the birthweight of babies isn’t.\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\nA histogram is an approximate representation of the distribution of continuous data where we divide the range of values into bins (or buckets) and draw a bar over each bin with area proportional to the frequency of cases in each bin.\nA histogram is an effective tool for visualising features relating to the shape of the data distribution.\n\n\n\n\nlibrary(MASS)\ndata(Boston)\n\nThis data set contains the various information on housing values and related quantities for the 506 suburban areas in Boston. The main interest is in the `median values of owner-occupied homes’, but there are 14 variables to explore here.\nLooking first at the median housing value, we can produce the histogram below. Some obvious features of note are:\n\nA concentration of housing values around 20-25, then a sudden drop-off – is there an explanation for this, such as changes in tax levels?\nPossible multi-modality, with modes around 25 and 30 – are there two classes or groups of housing?\nA further spike in values occur in the upper tail of the data, at 50 – this seems dubious, and could be some crude rounding or grouping of all values ‘’50 and over’’.\n\n\n\n\n\n\n\n\n\n\nChoosing the width of the bars can substantially affect the detail of the histogram. If we choose too few bins, then we can obscure key features by over-smoothing the data. Alterntaively, if we have too many bins then we can introduce too much noise to the plot that it obscures more general features and patterns. Unfortunately, the only way to find a good compromise is to experiment – R and other software will default to a ‘best guess’, but this invariably needs adjusting. The histograms below show the same data, but using bar widths of 5, 2.5, and 1 unit respectively. We’ll see more sophisticated methods for smoothing the data later.\n\n\n\n\n\n\n\n\n\nWith 14 variables in the data set, we could inspect the histograms of each of the variables. We could do this one-at-a-time, or arrange them in a grid or matrix as below:\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe other variables in the data set show a variety of the features discussed above that we may want to investigate further:\n\nThe ‘age’ variable - in position \\((1,1)\\) - shows obvious left-skewness.\nThe ‘dis’ variable - in position \\((1,2)\\) - shows right skewness\nThe ‘rm’ variable - in position \\((3,4)\\) - looks symmetric, and possible approximate Normality\nIn the variables in the bottom two rows, we see a lot of gaps in the data and heaping on particular values.\nThe ‘chas’ variable - in position \\((1,3)\\) - exhibits heaping on only two values, 0 and 1. This is a discrete binary variable, not continuous a continuous one! We should use different methods to explore this variable.\n\nIn a full analysis, we would want to look at the relationships between the variables to determine how the features observed relate to each other. We’ll return to this later…\n\n\n\n\nlibrary(MASS)\ndata(geyser)\n\nThe geyser data set contains 272 observations of the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. The variables are:\n\nduration - Length of eruption in mins\nwaiting - Waiting time to next eruption\n\nTo draw a histogram, we use the hist function:\n\nhist(geyser$waiting)\n\n\n\n\n\n\n\n\nHere we can see the data appear to come in two groups: one group with smaller values (shorter waiting times), and one group of larger (longer waiting times).\nWe can add a rug plot to an existing histogram to add more detail and to show where the individual data values fall inside the bars:\n\nhist(geyser$waiting)\nrug(geyser$waiting)\n\n\n\n\n\n\n\n\nNow, each individual mark on the horizontal axis shows where we have observed a data value.\nHistograms can be customised in many ways, but the most important one is changing the configuration of the bars drawn. This is controlled by the breaks parameter. We can set breaks to a number to indicate an approximate number of bars to draw:\n\nhist(geyser$waiting,breaks=20)\n\n\n\n\n\n\n\n\nOr we can be more specific and state where each individual bar should begin and end by listing the breakpoints of the bars as a vector:\n\nhist(faithful$waiting, breaks=c(30,45,47,50,52,55,60,75,80,85,95,105))\n\n\n\n\n\n\n\n\nAdditionally, almost all plot functions can take the following arguments to customise the plot:\n\nxlab, ylab - sets the x and y axis labels\nmain - sets the main title\nxlim, ylim - set x and y axis limits, e.g. c(0,10)\ncol - sets the plot colour(s)\n\n\n\n\n\nSymmetry or asymmetry - is the distribution skewed to the left or right? e.g. distributions of income are skewed.\nOutliers - are there one or more vales that are far from the rest of the data?\nMultimodality - does the distribution have more than one peak? This could suggest an underlying group structure.\nGaps - are there ranges of values within the data where no cases are recorded? e.g. exam marks for an exam which nobody fails.\nHeaping - do some values occur unexpectedly often? e.g. zero-inflation\nRounding - are only certain values are found? e.g. ages are usually only reported as integers.\nImpossibilities - are there values outside of the feasible ranges? e.g. negative values for strictly positive quantities such as age, rainfall, etc\nErrors - values which look wrong for one reason or another.\n\n\n\n\n\n\n\n\nWe have considered the shape of a distribution already, and have seen that histograms are a handy way of exploring shape.\nWe come now to several summary statistics (recall ISDS):\n\nthe average and median summarise the centre or location of the distribution.\nthe standard deviation and inter-quartile range summarise spread around the centre of the distribution.\n\nThe average is the most (over-)used statistic. It is useful especially where distributions are reasonably symmetric.\nFor distributions with long tails, it can give misleading signals, e.g. average household income will include households like Buckingham palace which distort the average.\nThe median is a measure of the midpoint of a distribution, and, unlike the mean, it is quite resistant to extreme values.\nA robust summary of spread is given by the interquartile range (IQR), the difference between the first and third quartiles. Every distribution has three quartiles:\n\nThe first quartile, \\(Q_1\\), is a number such that 25% of the values in the distribution do not exceed this number. This is called a lower quartile.\nThe second quartile, \\(Q_2\\), is a number such that 50% of the values in the distribution do not exceed this number. This is the same as the median.\nThe third quartile, \\(Q_3\\), is a number such that 75% of the values in the distribution do not exceed this number. This is called an upper quartile.\n\nTukey suggested that we combine the three quartiles with the minimum and maximum values in the data to form a five-number summary.\n\nIn R, the fivenum function gives the standard 5-number summary:\n\nfivenum(geyser$waiting)\n\n[1]  43  59  76  83 108\n\n\nThe summary function adds a 6th number (the mean):\n\nsummary(geyser$waiting)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  43.00   59.00   76.00   72.31   83.00  108.00 \n\n\nMin. 1st Qu. Median Mean 3rd Qu. Max. 43.0 58.0 76.0 70.9 82.0 96.0\n\n\n\n\nA boxplot (or box-and-whisker plot) is constructed from the 5-number summary by: * Drawing a box with lower boundary at \\(Q_1\\) and upper boundary \\(Q_3\\). * Drawing a line inside the box at the median (\\(Q_2\\)). * Drawing lines (whiskers) from the edges of the box to the most extreme data point that is within \\(1.5\\times IQR\\) of the edge of the box (often the minimum and maximum values).\n\nWe can draw a boxplot by using the boxplot function on a vector of data values:\n\nboxplot(geyser$waiting)\n\n\n\n\n\n\n\n\nOr we can pass all the columns of a data set to boxplot to draw everything at once:\n\nboxplot(geyser)\n\n\n\n\n\n\n\n\nNow, all variables are shown together on a common axis scale. This can be useful if all the variables take values of a similar size, but - as we see here - when the variables are quite different it can obscure the features of some variables by drawing everything together.\nOptional arguments for boxplot include:\n\nhorizontal - if TRUE the boxplots are drawn horizontally rather than vertically.\nvarwidth - if TRUE the boxplot widths are drawn proportional to the square root of the samples sizes, so wider boxplots represent more data. Though usually, you have the same number of data points in each column of the data set so this is not often very helpful.\n\n\n\nTo show the relationship between a histogram and a boxplot, we have a data set comprising the length (mm) of 100 cuckoo eggs. Drawng the histogram and boxplot together, we can see that the histogram gives far more detail on the shape of the distribution and the boxplot is more of a summary visualisation. The median is indicated in red and the upper/lower quartiles in green, which shows how these quantities align between the plots. Note that the smallest value in the data is flagged as an outlier and drawn separately as a circle on the boxplot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValues too far from the centre of the distribution are known as outliers.\nData values further than \\(1.5\\times IQR\\) from a lower/upper quartile are called an outlier, and usually indicated with a circle or point.\nAnything beyond \\(3\\times IQR\\) is an extreme outlier, and usually indicated with a star or asterisk.\nThis definition of outliers is a simple rule-of-thumb. It leads to about 0.7% of (normally distributed) observations being described as extreme for large samples.\nHowever, if you have a million data points then 0.7% of your data is still a lot! This doesn’t necessarily mean those points are all genuinely extreme or outliers.\n\n\n\nOne advantage of the boxplot, is that as it is a simple summary plot it is much easier to use boxplots to compare many variables at once. For example, the data plotted below are boxplots of the heights in inches of the singers in the New York Choral Society in 1979. The data are grouped according to the voice part they play in the choir. The vocal range for each voice part increases in pitch according to the following order: Bass 2, Bass 1, Tenor 2, Tenor 1, Alto 2, Alto 1, Soprano 2, Soprano 1. We can see immediately that the lowest pitch voices are associated with the taller singers and the higher pitches with smaller singers, which makes a lot of intuitive sense. There will also be a strong correspondance with Gender here too, though that information is not recorded.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should examine a single boxplot for the following features:\n\nIs the median line in the centre of the 50% portion of the distribution? If not, some degree of skew or asymmetry is indicated.\nAre the whiskers the same length? If not, some degree of skew or asymmetry is indicated.\nAre there any outliers?\n\nWe should examine several boxplots for the following features:\n\nDo the groups have similar shapes? Some might seem symmetric, some skewed.\nAre the median lines about the same, or do there seem to be differences in location** between groups?\nAre the IQRs about the same, or do there seem to be differences in spread between groups?\nAre there some groups with more outliers  than others? Such inspection can reveal important differences between groups.\n\n\n\n\n\nMany statistical methods require our data be approximately Normally distributed - but how can we tell whether the approximation is reasonable?\nNormal-quantile plots provide a simple and informal way of doing this, without having to do a formal hypothesis test — though that may be a natural next step.\nA Normal Quantile (or Q-Q plot) can be used to informally assess the normality of a data set. We construct the plot as follows:\n\nOrder the \\(n\\) data values, giving the ordered data \\(x_{(1)},x_{(2)},\\dots,x_{(n)}\\).\nCalculate the values \\(\\frac{1}{n+1},\\frac{2}{n+1},\\dots,\\frac{n}{n+1}\\), which gives \\(n+1\\) divisions of the \\((0,1)\\) interval.\nUse Normal tables or computer to find the value \\(z_k\\) such that \\(\\Phi(z_k)=\\frac{k}{n+1}\\) for the values \\(k=1,2,\\dots,n\\). The \\(n\\) values \\(z_1,z_2,\\dots,z_n\\) are the \\(\\frac{k}{n+1}\\) quantiles of the Normal distribution.\nPlot the \\(n\\) pairs of points \\((x_{(1)},z_1), (x_{(2)},z_2), \\dots, (x_{(n)},z_n)\\).\n\nThe basic idea is that these plots have the property that plotted points for Normally distributed data should fall roughly on a straight line.\nThis is because \\(x_{(k)}\\) are the sample quantiles of our data, and \\(Z_k\\) are the theoretical quantiles of a Normal distribution. If our sample distribution is approximately normal, these pairs of values will be in agreement.\nSystematic deviations from the straight line indicate non-normality.\nWe don’t need the points to lie on a perfect straight line – we often use the `fat pen test’, meaning that if the points are covered by placing a fat pen over the top, then that’s enough to conclude approximate normality!\n\n\n\nLet’s inspect the cuckoo egg data for normality. We can draw a histogram first, and compare its shape to a normal curve (blue line). While there looks to be approximate agreement (and we only ever need approximate Normality…), we see some divergence for small values of the data.\nDrawing the Normal quantile plot confirms these observations - things look reasonably close to a straight line, but deviate from Normality for small values. However, this would probably be enough to pass our ‘fat pen test’.\n\n\n\n\n\n\n\n\n\nIn R, we can use the qqnorm function to draw a Normal quantile plot of a single variable. Calling qqline after adds the theoretical straight line for comparison\n\nqqnorm(mtcars$mpg)\nqqline(mtcars$mpg)\n\n\n\n\n\n\n\n\nFor this variable (miles per gallon of various cars) we see strong departure from Normality as the points lie far from the desired straight line.\n\n\n\nTo illustrate what happens when our data are very non-Normal, consider this data set of ‘monthly mean relative sunspot numbers’ recorded from from 1749 to 1983. The data are counts of a quantity that is usually quite small in magnitude, this means the data are usually concentrated on small values with an ‘invisible wall’ at 0 - since we cannot observe negative counts! This gives rise to a heavily skewed distribution, as we see in the histogram and boxplot below. The Normal quantile plot (right) now shows strong curvature - not the straight-line feature we would expect if the data were Normally distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is occasionally possible to transform the data so that the transformed data are more approximately Normal.\nThe kinds of transformation we might employ are simple one-to-one power transformations, such \\(\\ln x\\), \\(\\sqrt{x}\\), \\(1/x\\), etc.\nWe can decide informally whether a transformation is useful by examining a Normal quantile plot of the transformed data.\nGraphical displays can help appraise the effectiveness of the transformation, but the cannot tell you if it makes sense - you should consider the interpretation as well as the statistical properties!"
  },
  {
    "objectID": "Lecture1b_Variables.html#summary",
    "href": "Lecture1b_Variables.html#summary",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "The frequency distributions of single continuous variables can exhibit a lot of different features\nHistograms are great at for emphasising features of the raw data, but struggle when comparing multiple variables or groups.\nBoxplots show less detail than histograms, but excel at comparing distributions across variables and for identifying outliers.\nQuantile plots can be used to see if data (approximately) follow a particular distribution.\n\n\n\n\nTests using the summary statistics - tests of means and variances against hypothesised values may be appropriate. Normality - formal tests of Normality, such as the KS test, could be applied if following a Normal distribution is important\nOutliers - formal tests for outliers exist (‘outliers’ package) and could be applied, plus the effect of excluding outliers from any analysis could be explored\nMultimodality - methods for assessing multimodality also exist (e.g. ‘diptest’ package), but care must be taken to avoid interpreting data noise as structure\nSample size - many of the methods can be overly sensitive for very large samples"
  },
  {
    "objectID": "Lecture1b_Variables.html#exploring-categorical-variables",
    "href": "Lecture1b_Variables.html#exploring-categorical-variables",
    "title": "Lecture 1b - Continuous and Categorical Variables",
    "section": "",
    "text": "Categorical or qualitative variables can only take one of a limited number of possible values (categories). The possible values a variable can take are known as the categories, levels, or labels.\nCategorical data comes in various forms depending on how the categories relate to each other:\n\nNominal - the categories have no standard order (e.g. eye colour)\nOrdinal - the categories have an intrinsic order (e.g. age recorded as “young”, “middle-aged”, and “old”)\nDiscrete - the categories are numerical, and hence ordered, but can only take a finite number of values (e.g. number of people per household)\n\nFor example, the data set below contains the responses to seven questions put to 1525 voters in an election survey for 1997-2001. All the variables in the data are categorical of different types. vote is the party the voter would vote for is clearly categorical, as is gender. The others are ordinal, but with numerical values. In some cases, where the variable is discrete, numerically valued, with a large number of categories - like age - it does make sense to treat it as continuous. Variables such as age are often thought of as being fundamentally continuous albeit recorded as discrete values.\n\nload(\"beps.Rda\")\nhead(beps)\n\n              vote age economic.cond.national economic.cond.household Blair\n1 Liberal Democrat  43                      3                       3     4\n2           Labour  36                      4                       4     4\n3           Labour  35                      4                       4     5\n4           Labour  24                      4                       2     2\n5           Labour  41                      2                       2     1\n6           Labour  47                      3                       4     4\n  Hague Kennedy Europe political.knowledge gender\n1     1       4      2                   2 female\n2     4       4      5                   2   male\n3     2       3      3                   2   male\n4     1       3      4                   0 female\n5     1       4      6                   2   male\n6     4       2      4                   2   male\n\n\nThe levels of a categorical variables are expressed by a factor coding to represent the different categories. Numerical codings are sometimes used, e.g. for a variable on marital status we could define:\n\nsingle\nmarried\nseparated\ndivorced\n…\n\nWhile this helps abbreviate the categories, it is important to remember that variables expressed this way are not the same as a numerical variable that can take these values. We should never treat these values as if they were continuous - for example, it could be tempting to take an average and, say, get a result of 2.6. But this is meaningless as it doesn’t correspond to any of the possibilities, the value we get depends entirely on how we coded our factors, and the rules of arthimetic make no sense for these variables – 1+3 may equal 4, but that does not mean single+separated=divorced!\nInstead, using text strings to represent the categories is safest and avoids mistakes such as these. However, care must be taken to ensure that there is consistency in how these levels are labelled, e.g. do we treat the labels “Female”, “female”, “F” as the same?\n\n\nCompared to continuous variables, categorical variables are relatively simplistic and usually contain little useful information on their own. As data, they usually reduce to the counts of the number of observations in the various categories.\nWe can obtain summary tables of the frequency of each category by using the table function.\n\ntable(beps$vote)\n\n\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\nThe R output is useful for a quick summary, but will need some manual re-formatting to make it presentable. R output is seldom an acceptable way to present information to others, and almost always should be transformed (e.g. into a summary table of relevant information) or summaries (e.g. by reporting only the relevant information R has given you). Here we can transform the ugly R code into a small data table:\n\n\n\n\n\nConservative\nLabour\nLiberal Democrat\n\n\n\n\n462\n720\n343\n\n\n\n\n\nThe xtabs function does something similar, and will be more useful later on when we have multiple variables at once:\n\nxtabs(~vote,data=beps)\n\nvote\n    Conservative           Labour Liberal Democrat \n             462              720              343 \n\n\n\n\n\nVisualisation of categorical variables usually focuses on plotting the counts of the categories, or the proportions that each category contribute to the total.\nThe range of useful graphics for such data is usually limited to:\n\nBarcharts - depict the counts or proportions by the size of the bar\nPiecharts - display the proportions of categories as fractions of the whole\nVariations of the above (stacked bars, treemaps)\n\nWhat features to look for?\n\nExtremes - the largest (smallest) category is often of particular interest.\nUneven distributions - Observational studies can often observe many more cases of one category than others. Some categories may not be observed\nUnexpected patterns of results - surprisingly large or small numbers for particular categories\nLarge numbers of categories - these may require grouping together or filtering out\nDon’t knows, missing values - Missing, unknown, or unavailable data is common in e.g. surveys and opinion polls.\nErrors in factor codings - e.g. gender could be denoted ‘M’ or ‘F’, but we may find values of ‘m’, or ‘female’.\n\n\n\n\nA bar chart or barplot simply draws the distribution of counts per category. We can use barplot to draw a barplot from a summary table of counts generated by the xtabs function\n\nbarplot(xtabs(~vote,data=beps))\n\n\n\n\n\n\n\n\nbarplot takes a number of additional arguments to customise the plot:\n\nnames - a vector of labels for each bar\nhoriz - set to TRUE to show a horizontal barplot.\nwidth - a vector of values to specify the widths of the bars\nspace - a vector of values to specify the spacing between bars\ncol - a vector of colours for the bars\n\n\nbarplot(xtabs(~vote,data=beps),col=c('blue','red','orange'), horiz=TRUE, names=c('Con','Lab','LibDem'))\n\n\n\n\n\n\n\n\n\n\n\nHere we have the number of eligible voters in each of the 16 Bundesländer (states) in the German Federal elections in 2009.\n\nload('btw9s.Rda')\nhead(btw9s)\n\n   Bundesland  Voters   EW State1\nBW         BW 7633818 West     BW\nBY         BY 9382583 West     BY\nBE         BE 2471665 East     BE\nBB         BB 2128715 East     BB\nHB         HB  487978 West     HB\nHH         HH 1256634 West     HH\n\n\n\n\n\n\n\n\n\n\n\nAs states are categorical the ordering of the bars is arbitrary, which limits what we can interpret - but clearly there are some very large and very small states.\nWe notice wide variation in populations - the largest is ‘NW’ (Nordrhein-Westfalen) which includes many major cities like Cologne and Düsseldorf; the smallest is ‘HB’ which is Bremen, a smaller city-state.\nThe ordering of bars can be used for emphasis - here its alphabetical. Ordering the bars by size gives a much better impression of the relative sizes of the sixteen states:\n\n\n\n\n\n\n\n\n\nUsing problem-specific structure can help give you context to your analysis. For example, we can separate the states belonging to the former East (left) and West Germany (right) and use a little colour for emphasis:\n\n\n\n\n\n\n\n\n\nClearly the West German states are substantially more populous.\n\n\n\n\ntitanic &lt;- data.frame(Titanic)\n\nThe Titanic data contains information on the fate of survivors fatal maiden voyage of the ocean liner Titanic. The data are in the form of counts of survivors (and not) summarised by economic status (class), sex and age. The variables are all categorical and defined as\n\nClass - 1st, 2nd, 3rd, or crew\nSex - Female, Male\nAge - Adult, Child\nSurvived - No, Yes\n\nBefore we analyse the data, think about what you expect the results to show. Do we expect more Male or Female survivors? Do we expect those in 1st class to fare better than those in 3rd class? Would we expect the Crew to fare better or worse than the passengers?\nBy thinking about what we might expect before looking at the data, we allow ourselves to be surprised when we find features we did not expect!\n\npar(mfrow=c(1,4))\nbarplot(xtabs(Freq~Survived,data=titanic), col=\"red\",main=\"Survived\")\nbarplot(xtabs(Freq~Sex,data=titanic), col=\"green\",main=\"Sex\")\nbarplot(xtabs(Freq~Age,data=titanic), col=\"orange\",main=\"Sex\")\nbarplot(xtabs(Freq~Class,data=titanic), col=\"blue\",main=\"Sex\")\n\n\n\n\n\n\n\n\nWhat do we see here?\n\nMore than twice as many passengers died as survived\nMore Male than Female passengers on board - more than 3x as many!\nVery few Child passengers\nMore people in Crew than any other class; fewest in second class\n\nThe interesting questions arise when we try to think whether Survived is related to the other variables. But we’ll have to come back to this later on.\n\n\n\nProportions are of particular interest in opinion polls, or studies where the composition of a larger population is of interest. In elections, the values of the counts are far less important than the share of the total - in particular the size of the largest proportion.\nVisualisations of proportions are based around the simple idea of dividing the larger whole into pieces which reflect the corresponding fractions.\n\npiecharts - slices of a circle\ncomposite or stacked barcharts - fractions of a bar\ntreemaps - tiles within a square or rectangle\n\nA stacked barplot is a variation of a standard barplot where the individual bars are broken up into portions reflecting the different. When we just have one variable, the effect is to stack the individual bars on top of each other.\n\nbarplot(as.matrix(xtabs(~vote,data=beps)), ## note we have to conver to a matrix here\n  beside=FALSE,\n  horiz=TRUE,\n  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nHere we can see the individual sizes of the bars, as well as a clear indication of their contribution to the overall total. So, it’s easy to read that the red category (Labour) has the highest share of the vote in this poll.\nThe traditional pie chart can be generated using pie:\n\npie(xtabs(~vote,data=beps),  col=c('blue','red','orange'))\n\n\n\n\n\n\n\n\nWe’re probably all familiar with this, but the basic idea is to use slices of the ‘pie’ to represent the proportion. Unfortunately, pie charts are often not the best visualisation as it can be difficult to detect differences between similarly-sized slices of a circle when compared to similiarly sized rectangles.\nHistorical aside: the invention of the pie chart is often attributed to Florence Nightingale. In one of the earliest conventional uses of data visualisation, she used an early version of the pie chart highlight the poor conditions of soldiers in field hospitals during the Crimean War (1854-6). Florence was an early pioneer of statistics and the first female member of the Royal Statistical Society – this aspect of her life and achivements is often overlooked given her frequent association with nursing.\n\n\n\n\n\n\n\nA stem and leaf plot presents the numerical values of the data in a similar form to a histogram.\n\nstem(geyser$waiting)\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   4 | 3\n   4 | 577888889999999\n   5 | 00000000000011111222223333333444444444\n   5 | 5556677777777788888999\n   6 | 0000001112222234\n   6 | 5555555668889999\n   7 | 01111122222233333344444444\n   7 | 5555555556666666677777777778888888888888888899999999999\n   8 | 00000000000001111111111112222222233333344444444444\n   8 | 5555555666667777777777777788888889999999\n   9 | 0011222333333334\n   9 | 668\n  10 | \n  10 | 8\n\n\nThis plot resembles a sideways histogram, only it shows extra information on the values within each bar.\n\n\n\nA stripplot is similar again, but displays the data as points rather than the numerical value\n\nstripchart(geyser$waiting, method='stack')\n\n\n\n\n\n\n\n\nBy default, it draws a hollow box for each data point which can make it difficult to read. We can modify the shape of the points by the pch (plot character) argument can help improve readability:\n\nstripchart(geyser$waiting, method='stack', pch=16)\n\n\n\n\n\n\n\n\n\n\n\nA beeswarm plot is similar to stripplot, but uses various techniques to separate nearby points such that each point remains visible. It also draws the plot symmetric about a central axis, rather than stacking up points from a baseline.\n\nlibrary(beeswarm)\nbeeswarm(geyser$waiting,horizontal = TRUE)"
  },
  {
    "objectID": "Lecture1a_IntroViz.html",
    "href": "Lecture1a_IntroViz.html",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "In this lecture, we look into the concept of exploratory data analysis and learn what makes good (and bad) data visualisation.\n\n\n\nExploratory data analysis (EDA) should be one of the first steps in analysing any data set and was pioneered as a discipline of its own by John Tukey in the 1960s and 1970s.\n\n\n\n\nExploratory Data Analysis chart\n\n\n\nIn the words of Tukey:\n\n“Exploratory data analysis is detective work — in the purest sense — finding and revealing the clues.”\n“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone — as the first step.”\n\nSo, the definition of EDA is fairly self-explanatory. We seek to explore the data, by asking questions and looking for clues to help us better understand the data. The purpose is generally to gain sufficient information to make reasoned and justifiable hypotheses to explore with more formal methods, or to identify sensible modelling approaches (and exclude others). The features and insights we gather while exploring will suggest what appropriate strategies for our subsequent analysis.\nTherefore, it is simply good practice to try to understand and gather as many insights from the data as possible before even attempting before any modelling or inference. Without a solid understanding of the data, we do not know if the techniques we apply are appropriate, and so we risk our inferences and conclusions being invalid.\nThat said, EDA is not a one-off process. It is often iterative, going back and forth between exploration and modelling/analysis as each part of the process can suggest new questions or hypotheses to investigate.\nExploratory Data Analysis is not a formal process and it does not have strict rules to follow or methods to apply: the overarching goal is simply to develop an understanding of the data set. However, typically we do try and do this without fitting any complex models or making assumptions about our data. We’re looking to see what the data tell us, not what our choice of technique or model says. We have insufficient knowledge of the properties of our data to exploit sophisticated techniques.\nThe “detective work” of EDA is essentially looking for clues in the data that will reveal insights about what is actually going on with the problem from which they come — but this requires looking both in the right places and with the right magnifying glass.\nData sets rarely arrive with a manual, or a shopping list of specific features to investigate. Absent any formal structure, the best approach is to pursue any lead that occurs to you, and ask questions - lots of them — some will lead to insights, others will be dead ends.\nSome obvious ‘clues’ and features to investigate in an unseen data set are: 1 Features and properties of individual variables and collections 2 Identifying important and unimportant variables 3 Identifying structure, patterns and relationships between variables 4 Anomalies, errors, and outliers 5 Variation and distributions 6 Missing values\nAs we’re only exploring the data with minimal assumptions, the tools of EDA must be mathematically quite simple and robust as we should not be relying on assumptions of distributions or structure that may not be justified. We rely primarily on:\n\nStatistical summaries\nGraphics and visualisations\n\nOur focus will be extensively on making a graphical exploration of the data.\n\n\n\n\n\nData visualisation is the creation and study of the visual representation of data.\nLike EDA, there is no complex theory about graphics — in fact, there is not much theory at all! The topics are not usually covered in depth in books or lectures as they build on relatively simple statistical concepts. Once the basic graphical forms have been described, textbooks usually move on to more mathematical ideas such as proving the central limit theorem.\nExploratory Data Analysis through investigation of data graphics is sometimes called Graphical Data Analysis (GDA).\nThere are some standard plots and graphics that are applicable in some fairly generaly situations, but\nA good visualisation reveals the data, and communicates complex ideas with clarity, precision and efficiency. Some features of a good data visualisation would be\n\nShow the data!\nInduce the viewer to think about the substance rather than the methodology, design, etc.\nAvoid distorting what the data have to say\nPresent many numbers in a small space\nMake large data sets coherent\nEncourage comparisons between different pieces of data\n\n\nGood data visualisations can communicate the key features of complex data sets more convincingly and more effectively than the raw data often can achieve.\nTypically, data visualisation is used for one of two purposes:\n1- Analysis - used to find patterns, trends, aid in data description, interpretation * Goal: the “Eureka!” moment * Many images for the analyst only, linked to analysis/modelling * Typically many rough and simple plots used to detect interesting features of the data and suggest directions for future investigation, analysis or modelling\n2- Presentation - used to attract attention, make a point, illustrate a conclusion * Goal: The “Wow!” moment. * A single image suitable for a large audience which tells a clear story * Once the key features and behaviours of the data are known, the best graphic can be produced to show those features in a clear way. Often targetting a less technical audience.\nFor example, the visualisation below shows a presentation of the number of cases of measles per 100,000 for the 50 US states over time. The impact of vaccination on the levels of measles is striking and clear.\n\nPresentation quality graphics can venture into the realm of data art, but this is rather beyond what we could hope to achieve in our short course. These visualisations, often called infographics, try to present data in a non-technical way that can easily be understood by non-experts. For example, the following graph illustrates the scale of the amount of waste plastic from plastic bottle sales over 10 years, relative to New York.\n\n\n\n\nFirst, a little bit of historical context. Data visualisation (and statistics) are relatively new disciplines - relative to the rest of mathematics. For data visualisation, William Playfair (1759-1823) is often credited for pioneering many of the graphical forms we still use today. Slightly later, Florence Nightingale (1820-1910) became one of the first people to persuade the public and influence public policy with data visualisation. Despite being better known for her achievements in nursing, Florence was the first female member of the Royal Statistical Society. Her rose diagrams were an innovative combination of pie chart and time series, and were used to illustrate the terrible conditions suffered by soldiers in the Crimean war.\nThese early visualisations were difficult to produce and required a combination of art and intuition. The 20th century brought computers and the ability to process and visualise increasing amounts of data with ease. Ultimately, a number of standard graphics were developed that exploit our visual perception to interpret complex data - most of which we have seen in the course so far.\nSo, despite having a long history, what is it about data visualisation that is so effective that we continue to do it? The answer is that depicting data graphically can be extremely effective as it takes advantage of the human brain’s natural strengths at quickly and efficiently processing visual information. Understanding this will help you make better visualisations!\nThe human brain has developed many subconscious natural abilities to process visual information and make sense of the world around us. We are constantly processing and interpreting the visual signals from our eyes and much of this happens sub-consciously without any actual effort. The reason for this is that this analysis relies on the visual perception part of our brain, rather than cognitive “thinking” part.\nVisual perception is the ability to see, interpret, and organise our environment. It’s fast and efficient, and happens automatically. Cognition, which is handled by the cerebral cortex, is much slower and requires more effort to process information. So, presenting data visually exploits our quick perception capabilities and helps to reduce cognitive load.\nTo illustrate these difference consider the following table and plots. From which of the three presentations of the data is it easiest to identify which 3 regions have the highest available renewable water resources?\n\n\n\n\n\nregion\nkm3\n\n\n\n\nCentral America and Caribbean\n735\n\n\nCentral Asia\n242\n\n\nEast Asia\n3410\n\n\nEastern Europe\n4448\n\n\nMiddle East\n484\n\n\nNorth America\n6077\n\n\nNorthern Africa\n47\n\n\nOceania\n902\n\n\nSouth America\n12724\n\n\nSouth Asia\n1935\n\n\nSub-Saharan Africa\n3884\n\n\nWestern & Central Europe\n2129\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table takes longer to process, as we must read each row, process that information into numbers, that we then compare. The first plot abstracts this for us by using bigger bars for bigger numbers - this makes it much easier to assess the sizes, but we must compare many sets of bars to decide which are the largest. The final plot simplifies this for us, by sorting the bars by size.\nThe difference in speeds at which our human senses can process information was compared by Danish physicist Tor Nørretranders to standard computer throughputs.\n\nNotice how sight comes out on top as it has the same bandwidth as a computer network. This is followed by touch, and hearing, with taste having the same processing power as a pocket calculator. The small white square in the bottom-right corner is the portion of this processing of which we are cognitively aware.\nNot only do our visual senses dominate our sensory processing, but the amount of data and the speed with which we process are far higher than we are aware of. This is known as pre-attentive processing. Pre-attentive processing is subconscious and fast - it take 50-500ms for the eye and brain to process and react to simple visual stimuli. This is clearly much faster thanour brain could process the data table in the small example above. So, turning our data into visual representations means we can process far more information much more quickly.\n\n\n\nThe key idea of data visualisation is that quantitative and categorical information is encoded by visual variables that can be easily perceived. Visual variables are “the differences in elements of a visual as perceived by the human eye”. Essentially these are the fundamental ways in which graphic symbols can be distinguished. When we view a graph, the goal is to decode the graphical attributes and extract information about the data that was encoded\nA number of authors have proposed sets of visual variables that are easy to detect visually:\n\nPosition\nLength\nDirection\nAngle\nArea\nShape\nColour, Texture\nVolume\n\n\n\nWhen using a common coordinate system, position is the easiest feature to recognise and evaluate with regard to elements in space.\nExample: Scatter plots, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s easy to compare separate scales repeated with the same axis even if they are not aligned.\nExample: Lattice/Grid/Facet plots\n\n\n\n\n\n\n\n\n\n\n\n\nLength can effectively represent quantitative information. The human brain easily recognises proportions and evaluates length, even if the objects are not aligned.\nExample: Bar charts, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nAngles help to make comparisons by providing a sense of proportion. Angles are harder to evaluate than length or position, but pie charts are as efficient with small numbers of categories.\nExample: Pie charts\n\n\n\n\n\n\n\n\n\n\n\n\nThe relative magnitude of areas is harder to compare versus the length of lines. The second direction requires more effort to process and interpret.\nExample: Bubble plots, Treemaps, Mosaic plots, Corrplots\n\n\n\n\n\n\n\n\n\n\n\n\nHue is what we usually mean by ‘colour’. Hue can be used to highlight, identify, and group elements in a visual display.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nColour has many aspects. Saturation is the intensity of a single hue. Increasing intensities of colour can be perceived intuitively as numbers of increasing value.\nExample: Heatmaps\n\n\n\n\n\n\n\n\n\n\n\n\nGroups can be distinguished by different shapes, though comparison requires cognition which makes it less effective than with colour.\nExample: Glyph plots, Scatterplots\n\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen colour is not available, different shadings or fills can be applied where previously we would use hue. Generally, these textures are seldom used in modern visualisations as they are less effective than colour.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nVolume refers to using 3D shapes to represent information. But 3D objects are hard to evaluate in a 2D space, making them particularly difficult to read effectively.\nExample: 3D charts\n To make plots appear 3D in a 2D plot, we must introduce a forced perspective, which distorts the quantitative information that we’re trying to present. For example, consider the following 3D barplot:\n\n\nSome bars get hidden behind others\nThe perspective effect makes bars at the front appear taller than those at the back\nIts difficult to read the numerical values\nThe quantitative data is only 1D -the vertical axis. The 3D chart has added 2 un-needed dimensions to the plot, which has compromised its ability to present the data without distortion.\n\n\n3D pie charts are even worse\n\n\n\nThe different visual variables have different levels of efficiency when visually interpreting values of different size. Different tasks will have different rankings. In general, we should use encodings at the top of the scale where possible (and sensible). For instance, when assessing the magnitude of a quantitative variable, we would rank the encodings something like this:\n\nPosition: Common Scale\nPosition: Non-aligned Scale\nLength\nDirection\nAngle\nArea\nColour: Hue\nColour: Saturation\nShape\nTexture\nVolume\n\n\n\n\nWhen displaying multiple quantities, we can combine encodings:\n\nSome encodings can be combined and visually decoded separately. These are separable encodings.\nOther combinations cannot be easily decoded separately, and are integral encodings.\n\n\nSuppose the red point is of interest - finding it among a low number of points is relatively easy (top left). With only two encodings and low density, it is easy to spot the unusual point.\nIncreasing the number of points makes it a little harder to find, but the contrast between colours helps (top centre). Position and hue are clearly separable encodings.\nIf we repeat the same experiment but changing point shape instead, the task becomes harder (top right). Shape requires more effor to process as an encoding.\nAmong 100 points, the triangle is almost lost. Shape is a more challenging feature to distinguish. (bottom left)\nMixing colour and shape compounds the problem further! (bottom centre)\n\n\n\n\n\n\n\n\n\n\nHere, we are juggling many data encodings at once. Horizontal and vertical position of the points indicate numerical values of two variables Colour and point shape indicating values of two categorical variables. Clearly, some aspects are more separable than others (e.g. x and y postition). Using many encodings with multiple different options to show your data become rapidly uninterpretable (below left), unless your data has a great deal of structure to help make sense of things (below right).\n\nN &lt;- 150\nlibrary(mvtnorm)\nxs &lt;- rmvnorm(150, c(0,0),matrix(c(1,0.5,0.5,1),nr=2))\ncs &lt;- cols[sample(1:4,N,replace=TRUE)]\nps &lt;- c(3,15,16,17)[sample(1:4,N,replace=TRUE)]\npar(mfrow=c(1,2))\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=ps,col=cs);graphics::box()\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=c(3,15,16,17)[cut(xs[,2],4)],col=cols[cut(xs[,1],4)]);graphics::box()\n\n\n\n\n\n\n\n\nThe plot below shows the prevalence of Diabetes and Obesity by county in the USA. Here, multiple inseparable encodings have been used, namely two colour hues with blue indicating obesity, and red indicating Diabetes, with intensity of the colours and their combinations showing the level of prevalance in each county. It is almost impossible to disentangle the obesity information from the diabetes information - these are integral encodings. The only obvious features are dark vs light shades of colour - the saturation.\n\n\n\n\n\nThe human brain is wired to see structure, logic, and patterns. It attempts to simplify what it sees by subconsciously arranging parts of an image into an organised whole, rather than just a series of disparate elements. The Gestalt principles were developed to explain how the brain does this, and can be used to aid (and break) data visualisation.\n\nProximity\nSimilarity\nEnclosure\nConnectedness\nClosure\nContinuity\nFigure and Ground\nFocal Point\n\n\n\nThe Proximity principle says that we perceive visual elements which are close together as belonging to the same group. This is easily seen in scatterplots, where we associatethe proximity in the plot with similarity of the object.\n\n\n\n\n\n\n\n\n\nThe same idea applies more generally and can be applied to other plots, where we can arrange the plot to group items we want to perceive as belonging together. Which of the two plots below best compares the sales per country?\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nSpatial proximity takes precedence over all other principles of grouping.\nUse proximity to focus on the visualisation goal, by keeping the main data points closer together.\n\n\n\n\nWe perceive elements with shared visual properties as being “similar”. Objects of similar colors, similar shapes, similar sizes, and similar orientations are instinctively perceived as a group. For example, in the scatterplots below the use of colour reinforces a sense of commonality with points of the same colour that is stronger than the three loose clusters we observe with proximity alone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nUse colour, shape, or size to group visual objects together.\nThe Similarity Principle can help you more readily identify which groups the displayed data belong to.\nColour can be used effectively when associated with intuitive quantities, e.g. red=financial loss, blue=negative temperature. Beware that association of colour with particular concepts varies around the world. An important example is that in Europe and the Americas coloring an upward trend in finanical markets usually uses green or blue is used to denote an upward trend and red is used to denote a downward trend, but in mainland China, Japan, South Korea, and Taiwan, the reverse is true.\nThe idea can be useful when similar colours, shapes, sizes are used consistently across multiple graphics.\n\n\n\n\nThe enclosure principle addresses the fact that enclosing a group of objects brings our attention to them, they are processed together, and our mind perceives them as connected.\n\nRecommendations to aid effective data visualisation:\n\nEnclose objects that you want to be perceived as grouped in a container.\nEnclosures can be used to highlight regions of a visualisation that can be “zoomed in” to give extra detail.\n\n\n\n\nConnectedness says that objects that are connected in some way, such as by a line, are seen as part of the same group. This supersedes other principles like proximity and similarity in terms of visual grouping perception because putting a direct connection between objects is a strong factor in determining the grouping of objects.\nRecommendations to aid effective data visualisation:\n\nConnecting grouped elements by lines is one of the strongest ways to visualise a grouping in the data\nThis is particularly natural with time series, but generally should be avoided unless the x-axis has a similar meaning\nParallel coordinate plots exploit this principle to connect individual data observations\n\n\n\n\nClosure states that when the brain sees complex arrangements of elements, it organises them into recognisable patterns. While this is usually helpful, it can occasionally cause problems. When the human brain is confronted with an incomplete image, it will fill in the blanks to complete the image and make it make sense.\nConsider the three plots of a time series below. The first plot is incomplete, showing a gap around 2013. Absent any other information, our brains would intuitively connect the lines on either side to give the impression of a behaviour like the plot in the middle. The true data actually followed the right plot. If we ignored the gap entirely and plotted all the data, we would draw a time series like the middle curve which would be highly misleading.\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nBe careful when showing graphs with breaks because the human mind tends to form complete shapes even if the shape is incomplete.\nSimilarly, beware joining all points up with lines when there are large gaps in the data - this is just falling into the same trap!\nIf your data have a lot of gaps, use points not lines.\n\n\n\n\nContinuity states that human brains tend to perceive any line or trend as continuing its established direction. The eye follows lines, curves, or a sequence of shapes to determine a relationship between elements.\nFor example, compare the two barplots below. The plot on the right is more easily readable than the one on the left.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nArrange visual objects in a line to simplify grouping and comparison. This happens naturally on scatterplots with obvious trends. Another example would be using bar chars ordered by y-value.\nUsing lines in time series graphs exploits continuity by joining points into a series\nThis can be paired with using colour saturation to emphasise the continuity along a secondary encoding.\n\n\n\n\nThe Figure and Ground principle says the brain will unconsciously place objects either in the foreground or the background. Background subtraction is a “brain technique” which allows an images foreground shapes to be extracted for further processing. To ensure that we can easily recognise patterns and features in a visualisation, we must ensure that the background and foreground elements are sufficiently different that we can easily identify the data from the background of the plot.\nThe plots below are two examples of doing this badly. The low contrast between the background and the data points makes it difficult to read. In particular, beware using yellow or other pale shades on a white background as they can be rendered nearly invisible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nEnsure there is enough contrast between your foreground and background to make charts and graphs more legible and not misleading.\nChoose colours and contrast levels that make your foreground image stand out.\nTransparency can help push less important features to the background.\nAvoid colour overload with many different and contrasting colours. Stick to a small number of distinct hues, or a scale of different intensities.\n\n\n\n\nA relatively recent addition, the focal point principle says that elements that visually stand out are the first thing we see and process in an image. This is related to the Figure and Ground principle, where we make particular elements stand out prominently from the background.\n\n\nLoading required package: grid\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nDistinctive characteristics (e.g., a different color or a different shape) can be used to highlight and create focal points.\nMosaicplots with \\(\\chi^2\\) shading automatically highlight “interesting” features, which immediately draws the eye\nUsing a substantially different colour or intensity can make the feature ‘pop out’ into the foreground\n\n\n\n\n\nThe idea of ‘graphical excellence’ was developed by Edward Tufte. Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. In particular, he said that good graphical displays of data should:\n\nshow the data,\ninduce the viewer to think about the substance,\navoid distorting what the data says,\npresent many numbers in small space,\nmake large data sets coherent,\nencourage comparison between data,\nreveal the data at several levels of detail,\nhave a clear purpose: description, exploration, tabulation or decoration.\n\nUnfortunately, it’s all too easy (and sometimes tempting) to ignore some of these principles to try and prove a particular point.\n\n\nRecall the Anscombe quartet data were identical when examined using simple summary statistics, but vary considerably when graphed\n\nlibrary(datasets)\ndata(anscombe)\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn ill-specified hypothesis or model cannot be rescued by a graphic. No matter how clever or fancy they are!\n\n\n\n\n\n\n\n\n\nThe correlation between these two series is \\(0.952\\), however there is obviously no genuine relationship here. Beware spurious correlations that lead to spurious conclusions, and remember that correlation does not imply causation!\n\n\n\nGraphs rely on our understanding that a number is represented visually by the magnitude of some graphical element.\n“The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the quantities represented.” — E Tufte\nTufte proposed measuring the violation of this principle by: \\[ \\text{Lie factor} = \\frac{\\text{size of effect in graphic}}{\\text{size of effect in data}}\\]\nA good graph should be between 0.95 and 1.05. Anything outside of this is distorting the numerical effect in the data.\nThe image below is hopelessly distoring the proportions, which don’t even add up to one. The graphical element of six equal sized segments bear no resemblance to the data whatsoever!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelective choice of axis ranges is one of the most common abuses of data graphics by disproportionately exaggerating visual effects. Where possible common axes should be used, and when emphasising relative values the inclusion of the origin (0) is recommended.\nThis plot from the Daily Mail below substantially distorts the data by starting the horizontal axis at 0.55 rather than zero. Notice how this substantially inflates the size of the blue bar relative to the red one.\n\n\n\n\n\n\n\n\n\nA more truthful plot would look like this.\n\n\n\n\n\n\n\n\n\nThough it is questionable as to whether a plot is needed to compare \\(0.6\\) with \\(0.7\\)! We probably don’t need the machinery of data visualisation to assess this difference.\n\n\n\nOmitting axis labels is perhaps even worse than being selective about what ranges you draw. Omitting numerical labels on the axes makes any meaningful comparison or interpretation impossible by removing the connection of the graphic with the numerical quantity it represents.\nPolitics is a common source of badly presented data distorted to prove a point. For instance, this tweet showing a selective part of a data set with no numbers to give any sens of scale:\n\n\n\n\n\n\n\n\n\nHow big is the difference between these time series? 0.1? 1? 100? Accurate interpretation is impossible.\nWhereas this shows a more complete picture, with a longer history:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe rely on our visual perception to interpret the graphical elements in terms of numerical values. Plotting simple data using 3D plots introduce a forced perspective, unnecessarily distorting our perception of the data. In 3D plots, the plot region is no longer a rectangle but is distorted - compressing distances at one side and expanding them at the other.\n\n\n\n\n\n\n\n\n\nThis is only plotting the integers 1 to 4, but it is not easy to identify the sizes of the bars. All of the bars appear to be smaller than their defined values, and it is difficult to assess relative sizes - does bar D really look \\(4\\times\\) larger than bar A? Do we really need a barplot with a fake 3rd dimension to compare four integers?\n\n\n\nChartjunk is defined as content-free decoration of a data graphic that hinders the interpretation. This sort of nuisance decoration becomes problematic as it becomes hard to extract the data in the foreground of the plot when it is cluttered and surrounded by other decorations (see the Figure and Ground principle earlier). In general, avoid unnecessary distraction and focus on the data!\n\n\n\n\n\n\n\n\n\nThere are a lot of unnecessary and confusing elements to this simple plot:\n\nA very heavy 3D projection which seriously distorts the plot region\nA drop shadow that has no value\nRedundant use of bar labels and a plot legend.\nColouring the bars is probably not even necessary, as the bars are already labelled and each bar has a unique colour\nPoor choice of colours - Europe and Oceania are two shades of red, implying a degree of similarity\n\n\n\n\nA plot isn’t always the best way to show simple data - ask yourself if a drawing a plot is necessary. In particular, if the data are simple then keen the plot simple - contriving elaborate plots out of very little information is just confusing.\nThe plot below shows the proportion of students enrolling at a US college for two age groups - below 25, and 25+.\n\n\n\n\n\n\n\n\n\nAccording to Edward Tufte in his book `The Visual Display of Quantitative Information’ (2002):\n“This may well be the worst graphic ever to find its way into print.” — E Tufte\nSince all students will fall into one group or the other, it is clear that we can get one time series by subtracting the other from 100%. So, this is trying to show a single time series of 4 points. However, they do just about everything wrong:\n\nThe two time series being plotted are complementary (i.e. they sum to one), so plotting both series is redundant\nAn exaggerated 3D effect is used for no reason\nEach series is shaded using two different colours\nThe \\(y\\) axes ranges includes neither 0 nor 100, so and skips over two sizeable ranges of values (notice the squiggles)\nThe data are interpolated with smooth lines in a rather strange way\n\nIf we were to just plot the data, we would simply see the following\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have extensively used colour in our graphics to highlight features of interest, distinguish different groups, or even indicate values of quantitative variables. Encoding features with colour can be effective, but colour needs to be used appropriately to be successful.\n\n\nThere are three common patterns of use of colour encodings:\n\nCategorical - distinguish different groups\nSequential - indicate different levels of a quantitative variable\nDiverging - indicate different levels and direction of a quantitative variable\n\nCategorical encodings are used to distinguish multiple different groups that have no intrinsic ordering, e.g. levels of a categorical variable. It is important to use a collection of sufficiently distinct and easily recognisable hues (colours) to easily distinguish multiple groups in your visualisation. The groups have no relationship, the hues should be as different as possible and preferably of a similar intensity. The main challenge with this encoding is that only a small number of categories can be encoded this way before we run out of sufficiently different colours.\n\n\n\n\n\n\n\n\n\nSequential encodings are used to represent values of a quantitative variable. By varying the intensity of the colour we can indicate a quantitivate variable by associating large values with more intense (saturated) colour, and lower values with less intense colours. A common example of this is on heatmaps, or more general maps such as of rainfall levels in weather forecasts. Typically, we restrict the colour to many shades of a single hue, but additional shades can be used if meaningful (e.g. to indicate extreme rainfall). While effective at highlighting major differences by major contrasts in the colour intensity, it is more challenging to detect smaller differences as subtle changes in colour and to decode numerical information from the plot.\n\n\n\n\n\n\n\n\n\nDiverging encodings are used to represent the values and direction of a quantitative variable. Combining the two previous ideas, we use two sequential schemes based on substantially different hues (e.g. red and blue) that meet in the middle. Now the colour intensity indicates the magnitude of the value, and the hue of the colour indicates its direction. For example, we have seen this used already in plots of correlation matrices, where strong colour indicated strong correlations and red/blue indicated negative/positive correlation. Another common example is a maps of temperatures in weather forecasts, where warm temperatures use one hue and cold temperatures use another, and they meet in the middle.\n\n\n\n\n\n\n\n\n\n\n\n\nWhile we may have a particular set of colours in mind to use with our visualisations, it can be difficult to set this up in R. There are many different ways to specify colours, and not all of them are intuitive to use:\n\nIntegers codes: R interprets integer values as particular colours from its default palette. For instance, 1=“black”, 2=“red”, 3=“green”, etc.\nColour names: R will a long list of named colours, e.g. “black”, “red”, “green3”, “skyblue“, “cyan”. The full list of names can be found http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\nRGB and similar: Any colour can be represented as a combination of proportions of red, green and blue. R’s rgb function will convert those red, green and blue amounts to a usable colour: black=rgb(0,0,0); green3=rgb(0, 205, 0, max=255), cyan=rgb(0, 255, 255,max=255). The hsv and hcl provide similar functions using alternative colour specifications.\nHexadecimal: The RGB values can also be expressed as a hexademical code: black=“#000000”; cyan=“#00FFFF”.\n\n\n\nR has a default palette of colours used for the default colours in graphs. Integer colour codes are the corresponding colour in this default list. The default palette contains the following eight elements.\n\npalette()\n\n[1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n[8] \"gray\"   \n\npie(rep(1, 8), labels = sprintf(\"%d (%s)\", 1:8, palette()), col = 1:8)\n\n\n\n\n\n\n\n\nWe can replace the standard palette with a vector of our own colours.\n\npalette(c(\"black\",\"#3366cc\",\"#61D04F\",\"#C45560\", \"#F5C710\", \"#CD0BBC\"))\npie(rep(1, 6), labels = sprintf(\"%d (%s)\", 1:6, palette()), col = 1:6)\n\n\n\n\n\n\n\n\nR also has a number of functions that will generate a list of n colours according to a particular scheme. These can be used to replace the default palette, or as input for a particular plot.\n\n\n\n\n\n\n\n\n\nWhich of these are better for: * categorical? * sequential? * diverging?\nIt is also worth noting that we’ve used these to colour area. Their effectiveness may vary when colouring points or lines.\n\n\n\n\n\n\nThe human brain can read and process visual information far faster than any other form - this is why data visualisation is so effective.\nData visualisation uses visual variables to encode the data in a graphical way.\nSome encodings are more effective than others. Some encodings work well together, and others less so.\nThe Gestalt principles describe how the brain sees patterns - we can use this to create better/worse visualisations.\nColour is an effective encoding, but we should carefully choose the colour palette to get the most out of it"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#exploratory-data-analysis",
    "href": "Lecture1a_IntroViz.html#exploratory-data-analysis",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "Exploratory data analysis (EDA) should be one of the first steps in analysing any data set and was pioneered as a discipline of its own by John Tukey in the 1960s and 1970s.\n\n\n\n\nExploratory Data Analysis chart\n\n\n\nIn the words of Tukey:\n\n“Exploratory data analysis is detective work — in the purest sense — finding and revealing the clues.”\n“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone — as the first step.”\n\nSo, the definition of EDA is fairly self-explanatory. We seek to explore the data, by asking questions and looking for clues to help us better understand the data. The purpose is generally to gain sufficient information to make reasoned and justifiable hypotheses to explore with more formal methods, or to identify sensible modelling approaches (and exclude others). The features and insights we gather while exploring will suggest what appropriate strategies for our subsequent analysis.\nTherefore, it is simply good practice to try to understand and gather as many insights from the data as possible before even attempting before any modelling or inference. Without a solid understanding of the data, we do not know if the techniques we apply are appropriate, and so we risk our inferences and conclusions being invalid.\nThat said, EDA is not a one-off process. It is often iterative, going back and forth between exploration and modelling/analysis as each part of the process can suggest new questions or hypotheses to investigate.\nExploratory Data Analysis is not a formal process and it does not have strict rules to follow or methods to apply: the overarching goal is simply to develop an understanding of the data set. However, typically we do try and do this without fitting any complex models or making assumptions about our data. We’re looking to see what the data tell us, not what our choice of technique or model says. We have insufficient knowledge of the properties of our data to exploit sophisticated techniques.\nThe “detective work” of EDA is essentially looking for clues in the data that will reveal insights about what is actually going on with the problem from which they come — but this requires looking both in the right places and with the right magnifying glass.\nData sets rarely arrive with a manual, or a shopping list of specific features to investigate. Absent any formal structure, the best approach is to pursue any lead that occurs to you, and ask questions - lots of them — some will lead to insights, others will be dead ends.\nSome obvious ‘clues’ and features to investigate in an unseen data set are: 1 Features and properties of individual variables and collections 2 Identifying important and unimportant variables 3 Identifying structure, patterns and relationships between variables 4 Anomalies, errors, and outliers 5 Variation and distributions 6 Missing values\nAs we’re only exploring the data with minimal assumptions, the tools of EDA must be mathematically quite simple and robust as we should not be relying on assumptions of distributions or structure that may not be justified. We rely primarily on:\n\nStatistical summaries\nGraphics and visualisations\n\nOur focus will be extensively on making a graphical exploration of the data."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#data-visualisation",
    "href": "Lecture1a_IntroViz.html#data-visualisation",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "Data visualisation is the creation and study of the visual representation of data.\nLike EDA, there is no complex theory about graphics — in fact, there is not much theory at all! The topics are not usually covered in depth in books or lectures as they build on relatively simple statistical concepts. Once the basic graphical forms have been described, textbooks usually move on to more mathematical ideas such as proving the central limit theorem.\nExploratory Data Analysis through investigation of data graphics is sometimes called Graphical Data Analysis (GDA).\nThere are some standard plots and graphics that are applicable in some fairly generaly situations, but\nA good visualisation reveals the data, and communicates complex ideas with clarity, precision and efficiency. Some features of a good data visualisation would be\n\nShow the data!\nInduce the viewer to think about the substance rather than the methodology, design, etc.\nAvoid distorting what the data have to say\nPresent many numbers in a small space\nMake large data sets coherent\nEncourage comparisons between different pieces of data\n\n\nGood data visualisations can communicate the key features of complex data sets more convincingly and more effectively than the raw data often can achieve.\nTypically, data visualisation is used for one of two purposes:\n1- Analysis - used to find patterns, trends, aid in data description, interpretation * Goal: the “Eureka!” moment * Many images for the analyst only, linked to analysis/modelling * Typically many rough and simple plots used to detect interesting features of the data and suggest directions for future investigation, analysis or modelling\n2- Presentation - used to attract attention, make a point, illustrate a conclusion * Goal: The “Wow!” moment. * A single image suitable for a large audience which tells a clear story * Once the key features and behaviours of the data are known, the best graphic can be produced to show those features in a clear way. Often targetting a less technical audience.\nFor example, the visualisation below shows a presentation of the number of cases of measles per 100,000 for the 50 US states over time. The impact of vaccination on the levels of measles is striking and clear.\n\nPresentation quality graphics can venture into the realm of data art, but this is rather beyond what we could hope to achieve in our short course. These visualisations, often called infographics, try to present data in a non-technical way that can easily be understood by non-experts. For example, the following graph illustrates the scale of the amount of waste plastic from plastic bottle sales over 10 years, relative to New York."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#making-effective-data-visualisation",
    "href": "Lecture1a_IntroViz.html#making-effective-data-visualisation",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "First, a little bit of historical context. Data visualisation (and statistics) are relatively new disciplines - relative to the rest of mathematics. For data visualisation, William Playfair (1759-1823) is often credited for pioneering many of the graphical forms we still use today. Slightly later, Florence Nightingale (1820-1910) became one of the first people to persuade the public and influence public policy with data visualisation. Despite being better known for her achievements in nursing, Florence was the first female member of the Royal Statistical Society. Her rose diagrams were an innovative combination of pie chart and time series, and were used to illustrate the terrible conditions suffered by soldiers in the Crimean war.\nThese early visualisations were difficult to produce and required a combination of art and intuition. The 20th century brought computers and the ability to process and visualise increasing amounts of data with ease. Ultimately, a number of standard graphics were developed that exploit our visual perception to interpret complex data - most of which we have seen in the course so far.\nSo, despite having a long history, what is it about data visualisation that is so effective that we continue to do it? The answer is that depicting data graphically can be extremely effective as it takes advantage of the human brain’s natural strengths at quickly and efficiently processing visual information. Understanding this will help you make better visualisations!\nThe human brain has developed many subconscious natural abilities to process visual information and make sense of the world around us. We are constantly processing and interpreting the visual signals from our eyes and much of this happens sub-consciously without any actual effort. The reason for this is that this analysis relies on the visual perception part of our brain, rather than cognitive “thinking” part.\nVisual perception is the ability to see, interpret, and organise our environment. It’s fast and efficient, and happens automatically. Cognition, which is handled by the cerebral cortex, is much slower and requires more effort to process information. So, presenting data visually exploits our quick perception capabilities and helps to reduce cognitive load.\nTo illustrate these difference consider the following table and plots. From which of the three presentations of the data is it easiest to identify which 3 regions have the highest available renewable water resources?\n\n\n\n\n\nregion\nkm3\n\n\n\n\nCentral America and Caribbean\n735\n\n\nCentral Asia\n242\n\n\nEast Asia\n3410\n\n\nEastern Europe\n4448\n\n\nMiddle East\n484\n\n\nNorth America\n6077\n\n\nNorthern Africa\n47\n\n\nOceania\n902\n\n\nSouth America\n12724\n\n\nSouth Asia\n1935\n\n\nSub-Saharan Africa\n3884\n\n\nWestern & Central Europe\n2129\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table takes longer to process, as we must read each row, process that information into numbers, that we then compare. The first plot abstracts this for us by using bigger bars for bigger numbers - this makes it much easier to assess the sizes, but we must compare many sets of bars to decide which are the largest. The final plot simplifies this for us, by sorting the bars by size.\nThe difference in speeds at which our human senses can process information was compared by Danish physicist Tor Nørretranders to standard computer throughputs.\n\nNotice how sight comes out on top as it has the same bandwidth as a computer network. This is followed by touch, and hearing, with taste having the same processing power as a pocket calculator. The small white square in the bottom-right corner is the portion of this processing of which we are cognitively aware.\nNot only do our visual senses dominate our sensory processing, but the amount of data and the speed with which we process are far higher than we are aware of. This is known as pre-attentive processing. Pre-attentive processing is subconscious and fast - it take 50-500ms for the eye and brain to process and react to simple visual stimuli. This is clearly much faster thanour brain could process the data table in the small example above. So, turning our data into visual representations means we can process far more information much more quickly."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#encoding-data",
    "href": "Lecture1a_IntroViz.html#encoding-data",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The key idea of data visualisation is that quantitative and categorical information is encoded by visual variables that can be easily perceived. Visual variables are “the differences in elements of a visual as perceived by the human eye”. Essentially these are the fundamental ways in which graphic symbols can be distinguished. When we view a graph, the goal is to decode the graphical attributes and extract information about the data that was encoded\nA number of authors have proposed sets of visual variables that are easy to detect visually:\n\nPosition\nLength\nDirection\nAngle\nArea\nShape\nColour, Texture\nVolume\n\n\n\nWhen using a common coordinate system, position is the easiest feature to recognise and evaluate with regard to elements in space.\nExample: Scatter plots, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s easy to compare separate scales repeated with the same axis even if they are not aligned.\nExample: Lattice/Grid/Facet plots\n\n\n\n\n\n\n\n\n\n\n\n\nLength can effectively represent quantitative information. The human brain easily recognises proportions and evaluates length, even if the objects are not aligned.\nExample: Bar charts, boxplots\n\n\n\n\n\n\n\n\n\n\n\n\nAngles help to make comparisons by providing a sense of proportion. Angles are harder to evaluate than length or position, but pie charts are as efficient with small numbers of categories.\nExample: Pie charts\n\n\n\n\n\n\n\n\n\n\n\n\nThe relative magnitude of areas is harder to compare versus the length of lines. The second direction requires more effort to process and interpret.\nExample: Bubble plots, Treemaps, Mosaic plots, Corrplots\n\n\n\n\n\n\n\n\n\n\n\n\nHue is what we usually mean by ‘colour’. Hue can be used to highlight, identify, and group elements in a visual display.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nColour has many aspects. Saturation is the intensity of a single hue. Increasing intensities of colour can be perceived intuitively as numbers of increasing value.\nExample: Heatmaps\n\n\n\n\n\n\n\n\n\n\n\n\nGroups can be distinguished by different shapes, though comparison requires cognition which makes it less effective than with colour.\nExample: Glyph plots, Scatterplots\n\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen colour is not available, different shadings or fills can be applied where previously we would use hue. Generally, these textures are seldom used in modern visualisations as they are less effective than colour.\nExample: any\n\n\n\n\n\n\n\n\n\n\n\n\nVolume refers to using 3D shapes to represent information. But 3D objects are hard to evaluate in a 2D space, making them particularly difficult to read effectively.\nExample: 3D charts\n To make plots appear 3D in a 2D plot, we must introduce a forced perspective, which distorts the quantitative information that we’re trying to present. For example, consider the following 3D barplot:\n\n\nSome bars get hidden behind others\nThe perspective effect makes bars at the front appear taller than those at the back\nIts difficult to read the numerical values\nThe quantitative data is only 1D -the vertical axis. The 3D chart has added 2 un-needed dimensions to the plot, which has compromised its ability to present the data without distortion.\n\n\n3D pie charts are even worse\n\n\n\nThe different visual variables have different levels of efficiency when visually interpreting values of different size. Different tasks will have different rankings. In general, we should use encodings at the top of the scale where possible (and sensible). For instance, when assessing the magnitude of a quantitative variable, we would rank the encodings something like this:\n\nPosition: Common Scale\nPosition: Non-aligned Scale\nLength\nDirection\nAngle\nArea\nColour: Hue\nColour: Saturation\nShape\nTexture\nVolume\n\n\n\n\nWhen displaying multiple quantities, we can combine encodings:\n\nSome encodings can be combined and visually decoded separately. These are separable encodings.\nOther combinations cannot be easily decoded separately, and are integral encodings.\n\n\nSuppose the red point is of interest - finding it among a low number of points is relatively easy (top left). With only two encodings and low density, it is easy to spot the unusual point.\nIncreasing the number of points makes it a little harder to find, but the contrast between colours helps (top centre). Position and hue are clearly separable encodings.\nIf we repeat the same experiment but changing point shape instead, the task becomes harder (top right). Shape requires more effor to process as an encoding.\nAmong 100 points, the triangle is almost lost. Shape is a more challenging feature to distinguish. (bottom left)\nMixing colour and shape compounds the problem further! (bottom centre)\n\n\n\n\n\n\n\n\n\n\nHere, we are juggling many data encodings at once. Horizontal and vertical position of the points indicate numerical values of two variables Colour and point shape indicating values of two categorical variables. Clearly, some aspects are more separable than others (e.g. x and y postition). Using many encodings with multiple different options to show your data become rapidly uninterpretable (below left), unless your data has a great deal of structure to help make sense of things (below right).\n\nN &lt;- 150\nlibrary(mvtnorm)\nxs &lt;- rmvnorm(150, c(0,0),matrix(c(1,0.5,0.5,1),nr=2))\ncs &lt;- cols[sample(1:4,N,replace=TRUE)]\nps &lt;- c(3,15,16,17)[sample(1:4,N,replace=TRUE)]\npar(mfrow=c(1,2))\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=ps,col=cs);graphics::box()\nplot(x=xs[,1],y=xs[,2],axes=FALSE,xlab='',ylab='',pch=c(3,15,16,17)[cut(xs[,2],4)],col=cols[cut(xs[,1],4)]);graphics::box()\n\n\n\n\n\n\n\n\nThe plot below shows the prevalence of Diabetes and Obesity by county in the USA. Here, multiple inseparable encodings have been used, namely two colour hues with blue indicating obesity, and red indicating Diabetes, with intensity of the colours and their combinations showing the level of prevalance in each county. It is almost impossible to disentangle the obesity information from the diabetes information - these are integral encodings. The only obvious features are dark vs light shades of colour - the saturation."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#gestalt-principles",
    "href": "Lecture1a_IntroViz.html#gestalt-principles",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The human brain is wired to see structure, logic, and patterns. It attempts to simplify what it sees by subconsciously arranging parts of an image into an organised whole, rather than just a series of disparate elements. The Gestalt principles were developed to explain how the brain does this, and can be used to aid (and break) data visualisation.\n\nProximity\nSimilarity\nEnclosure\nConnectedness\nClosure\nContinuity\nFigure and Ground\nFocal Point\n\n\n\nThe Proximity principle says that we perceive visual elements which are close together as belonging to the same group. This is easily seen in scatterplots, where we associatethe proximity in the plot with similarity of the object.\n\n\n\n\n\n\n\n\n\nThe same idea applies more generally and can be applied to other plots, where we can arrange the plot to group items we want to perceive as belonging together. Which of the two plots below best compares the sales per country?\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nSpatial proximity takes precedence over all other principles of grouping.\nUse proximity to focus on the visualisation goal, by keeping the main data points closer together.\n\n\n\n\nWe perceive elements with shared visual properties as being “similar”. Objects of similar colors, similar shapes, similar sizes, and similar orientations are instinctively perceived as a group. For example, in the scatterplots below the use of colour reinforces a sense of commonality with points of the same colour that is stronger than the three loose clusters we observe with proximity alone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nUse colour, shape, or size to group visual objects together.\nThe Similarity Principle can help you more readily identify which groups the displayed data belong to.\nColour can be used effectively when associated with intuitive quantities, e.g. red=financial loss, blue=negative temperature. Beware that association of colour with particular concepts varies around the world. An important example is that in Europe and the Americas coloring an upward trend in finanical markets usually uses green or blue is used to denote an upward trend and red is used to denote a downward trend, but in mainland China, Japan, South Korea, and Taiwan, the reverse is true.\nThe idea can be useful when similar colours, shapes, sizes are used consistently across multiple graphics.\n\n\n\n\nThe enclosure principle addresses the fact that enclosing a group of objects brings our attention to them, they are processed together, and our mind perceives them as connected.\n\nRecommendations to aid effective data visualisation:\n\nEnclose objects that you want to be perceived as grouped in a container.\nEnclosures can be used to highlight regions of a visualisation that can be “zoomed in” to give extra detail.\n\n\n\n\nConnectedness says that objects that are connected in some way, such as by a line, are seen as part of the same group. This supersedes other principles like proximity and similarity in terms of visual grouping perception because putting a direct connection between objects is a strong factor in determining the grouping of objects.\nRecommendations to aid effective data visualisation:\n\nConnecting grouped elements by lines is one of the strongest ways to visualise a grouping in the data\nThis is particularly natural with time series, but generally should be avoided unless the x-axis has a similar meaning\nParallel coordinate plots exploit this principle to connect individual data observations\n\n\n\n\nClosure states that when the brain sees complex arrangements of elements, it organises them into recognisable patterns. While this is usually helpful, it can occasionally cause problems. When the human brain is confronted with an incomplete image, it will fill in the blanks to complete the image and make it make sense.\nConsider the three plots of a time series below. The first plot is incomplete, showing a gap around 2013. Absent any other information, our brains would intuitively connect the lines on either side to give the impression of a behaviour like the plot in the middle. The true data actually followed the right plot. If we ignored the gap entirely and plotted all the data, we would draw a time series like the middle curve which would be highly misleading.\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nBe careful when showing graphs with breaks because the human mind tends to form complete shapes even if the shape is incomplete.\nSimilarly, beware joining all points up with lines when there are large gaps in the data - this is just falling into the same trap!\nIf your data have a lot of gaps, use points not lines.\n\n\n\n\nContinuity states that human brains tend to perceive any line or trend as continuing its established direction. The eye follows lines, curves, or a sequence of shapes to determine a relationship between elements.\nFor example, compare the two barplots below. The plot on the right is more easily readable than the one on the left.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nArrange visual objects in a line to simplify grouping and comparison. This happens naturally on scatterplots with obvious trends. Another example would be using bar chars ordered by y-value.\nUsing lines in time series graphs exploits continuity by joining points into a series\nThis can be paired with using colour saturation to emphasise the continuity along a secondary encoding.\n\n\n\n\nThe Figure and Ground principle says the brain will unconsciously place objects either in the foreground or the background. Background subtraction is a “brain technique” which allows an images foreground shapes to be extracted for further processing. To ensure that we can easily recognise patterns and features in a visualisation, we must ensure that the background and foreground elements are sufficiently different that we can easily identify the data from the background of the plot.\nThe plots below are two examples of doing this badly. The low contrast between the background and the data points makes it difficult to read. In particular, beware using yellow or other pale shades on a white background as they can be rendered nearly invisible.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nEnsure there is enough contrast between your foreground and background to make charts and graphs more legible and not misleading.\nChoose colours and contrast levels that make your foreground image stand out.\nTransparency can help push less important features to the background.\nAvoid colour overload with many different and contrasting colours. Stick to a small number of distinct hues, or a scale of different intensities.\n\n\n\n\nA relatively recent addition, the focal point principle says that elements that visually stand out are the first thing we see and process in an image. This is related to the Figure and Ground principle, where we make particular elements stand out prominently from the background.\n\n\nLoading required package: grid\n\n\n\n\n\n\n\n\n\nRecommendations to aid effective data visualisation:\n\nDistinctive characteristics (e.g., a different color or a different shape) can be used to highlight and create focal points.\nMosaicplots with \\(\\chi^2\\) shading automatically highlight “interesting” features, which immediately draws the eye\nUsing a substantially different colour or intensity can make the feature ‘pop out’ into the foreground"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#what-to-avoid",
    "href": "Lecture1a_IntroViz.html#what-to-avoid",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The idea of ‘graphical excellence’ was developed by Edward Tufte. Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. In particular, he said that good graphical displays of data should:\n\nshow the data,\ninduce the viewer to think about the substance,\navoid distorting what the data says,\npresent many numbers in small space,\nmake large data sets coherent,\nencourage comparison between data,\nreveal the data at several levels of detail,\nhave a clear purpose: description, exploration, tabulation or decoration.\n\nUnfortunately, it’s all too easy (and sometimes tempting) to ignore some of these principles to try and prove a particular point.\n\n\nRecall the Anscombe quartet data were identical when examined using simple summary statistics, but vary considerably when graphed\n\nlibrary(datasets)\ndata(anscombe)\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n10\n10\n10\n8\n8.04\n9.14\n7.46\n6.58\n\n\n8\n8\n8\n8\n6.95\n8.14\n6.77\n5.76\n\n\n13\n13\n13\n8\n7.58\n8.74\n12.74\n7.71\n\n\n9\n9\n9\n8\n8.81\n8.77\n7.11\n8.84\n\n\n11\n11\n11\n8\n8.33\n9.26\n7.81\n8.47\n\n\n14\n14\n14\n8\n9.96\n8.10\n8.84\n7.04\n\n\n6\n6\n6\n8\n7.24\n6.13\n6.08\n5.25\n\n\n4\n4\n4\n19\n4.26\n3.10\n5.39\n12.50\n\n\n12\n12\n12\n8\n10.84\n9.13\n8.15\n5.56\n\n\n7\n7\n7\n8\n4.82\n7.26\n6.42\n7.91\n\n\n5\n5\n5\n8\n5.68\n4.74\n5.73\n6.89\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn ill-specified hypothesis or model cannot be rescued by a graphic. No matter how clever or fancy they are!\n\n\n\n\n\n\n\n\n\nThe correlation between these two series is \\(0.952\\), however there is obviously no genuine relationship here. Beware spurious correlations that lead to spurious conclusions, and remember that correlation does not imply causation!\n\n\n\nGraphs rely on our understanding that a number is represented visually by the magnitude of some graphical element.\n“The representation of numbers, as physically measured on the surface of the graphic itself, should be directly proportional to the quantities represented.” — E Tufte\nTufte proposed measuring the violation of this principle by: \\[ \\text{Lie factor} = \\frac{\\text{size of effect in graphic}}{\\text{size of effect in data}}\\]\nA good graph should be between 0.95 and 1.05. Anything outside of this is distorting the numerical effect in the data.\nThe image below is hopelessly distoring the proportions, which don’t even add up to one. The graphical element of six equal sized segments bear no resemblance to the data whatsoever!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelective choice of axis ranges is one of the most common abuses of data graphics by disproportionately exaggerating visual effects. Where possible common axes should be used, and when emphasising relative values the inclusion of the origin (0) is recommended.\nThis plot from the Daily Mail below substantially distorts the data by starting the horizontal axis at 0.55 rather than zero. Notice how this substantially inflates the size of the blue bar relative to the red one.\n\n\n\n\n\n\n\n\n\nA more truthful plot would look like this.\n\n\n\n\n\n\n\n\n\nThough it is questionable as to whether a plot is needed to compare \\(0.6\\) with \\(0.7\\)! We probably don’t need the machinery of data visualisation to assess this difference.\n\n\n\nOmitting axis labels is perhaps even worse than being selective about what ranges you draw. Omitting numerical labels on the axes makes any meaningful comparison or interpretation impossible by removing the connection of the graphic with the numerical quantity it represents.\nPolitics is a common source of badly presented data distorted to prove a point. For instance, this tweet showing a selective part of a data set with no numbers to give any sens of scale:\n\n\n\n\n\n\n\n\n\nHow big is the difference between these time series? 0.1? 1? 100? Accurate interpretation is impossible.\nWhereas this shows a more complete picture, with a longer history:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe rely on our visual perception to interpret the graphical elements in terms of numerical values. Plotting simple data using 3D plots introduce a forced perspective, unnecessarily distorting our perception of the data. In 3D plots, the plot region is no longer a rectangle but is distorted - compressing distances at one side and expanding them at the other.\n\n\n\n\n\n\n\n\n\nThis is only plotting the integers 1 to 4, but it is not easy to identify the sizes of the bars. All of the bars appear to be smaller than their defined values, and it is difficult to assess relative sizes - does bar D really look \\(4\\times\\) larger than bar A? Do we really need a barplot with a fake 3rd dimension to compare four integers?\n\n\n\nChartjunk is defined as content-free decoration of a data graphic that hinders the interpretation. This sort of nuisance decoration becomes problematic as it becomes hard to extract the data in the foreground of the plot when it is cluttered and surrounded by other decorations (see the Figure and Ground principle earlier). In general, avoid unnecessary distraction and focus on the data!\n\n\n\n\n\n\n\n\n\nThere are a lot of unnecessary and confusing elements to this simple plot:\n\nA very heavy 3D projection which seriously distorts the plot region\nA drop shadow that has no value\nRedundant use of bar labels and a plot legend.\nColouring the bars is probably not even necessary, as the bars are already labelled and each bar has a unique colour\nPoor choice of colours - Europe and Oceania are two shades of red, implying a degree of similarity\n\n\n\n\nA plot isn’t always the best way to show simple data - ask yourself if a drawing a plot is necessary. In particular, if the data are simple then keen the plot simple - contriving elaborate plots out of very little information is just confusing.\nThe plot below shows the proportion of students enrolling at a US college for two age groups - below 25, and 25+.\n\n\n\n\n\n\n\n\n\nAccording to Edward Tufte in his book `The Visual Display of Quantitative Information’ (2002):\n“This may well be the worst graphic ever to find its way into print.” — E Tufte\nSince all students will fall into one group or the other, it is clear that we can get one time series by subtracting the other from 100%. So, this is trying to show a single time series of 4 points. However, they do just about everything wrong:\n\nThe two time series being plotted are complementary (i.e. they sum to one), so plotting both series is redundant\nAn exaggerated 3D effect is used for no reason\nEach series is shaded using two different colours\nThe \\(y\\) axes ranges includes neither 0 nor 100, so and skips over two sizeable ranges of values (notice the squiggles)\nThe data are interpolated with smooth lines in a rather strange way\n\nIf we were to just plot the data, we would simply see the following"
  },
  {
    "objectID": "Lecture1a_IntroViz.html#bonus-content-using-colour-effectively",
    "href": "Lecture1a_IntroViz.html#bonus-content-using-colour-effectively",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "We have extensively used colour in our graphics to highlight features of interest, distinguish different groups, or even indicate values of quantitative variables. Encoding features with colour can be effective, but colour needs to be used appropriately to be successful.\n\n\nThere are three common patterns of use of colour encodings:\n\nCategorical - distinguish different groups\nSequential - indicate different levels of a quantitative variable\nDiverging - indicate different levels and direction of a quantitative variable\n\nCategorical encodings are used to distinguish multiple different groups that have no intrinsic ordering, e.g. levels of a categorical variable. It is important to use a collection of sufficiently distinct and easily recognisable hues (colours) to easily distinguish multiple groups in your visualisation. The groups have no relationship, the hues should be as different as possible and preferably of a similar intensity. The main challenge with this encoding is that only a small number of categories can be encoded this way before we run out of sufficiently different colours.\n\n\n\n\n\n\n\n\n\nSequential encodings are used to represent values of a quantitative variable. By varying the intensity of the colour we can indicate a quantitivate variable by associating large values with more intense (saturated) colour, and lower values with less intense colours. A common example of this is on heatmaps, or more general maps such as of rainfall levels in weather forecasts. Typically, we restrict the colour to many shades of a single hue, but additional shades can be used if meaningful (e.g. to indicate extreme rainfall). While effective at highlighting major differences by major contrasts in the colour intensity, it is more challenging to detect smaller differences as subtle changes in colour and to decode numerical information from the plot.\n\n\n\n\n\n\n\n\n\nDiverging encodings are used to represent the values and direction of a quantitative variable. Combining the two previous ideas, we use two sequential schemes based on substantially different hues (e.g. red and blue) that meet in the middle. Now the colour intensity indicates the magnitude of the value, and the hue of the colour indicates its direction. For example, we have seen this used already in plots of correlation matrices, where strong colour indicated strong correlations and red/blue indicated negative/positive correlation. Another common example is a maps of temperatures in weather forecasts, where warm temperatures use one hue and cold temperatures use another, and they meet in the middle.\n\n\n\n\n\n\n\n\n\n\n\n\nWhile we may have a particular set of colours in mind to use with our visualisations, it can be difficult to set this up in R. There are many different ways to specify colours, and not all of them are intuitive to use:\n\nIntegers codes: R interprets integer values as particular colours from its default palette. For instance, 1=“black”, 2=“red”, 3=“green”, etc.\nColour names: R will a long list of named colours, e.g. “black”, “red”, “green3”, “skyblue“, “cyan”. The full list of names can be found http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf\nRGB and similar: Any colour can be represented as a combination of proportions of red, green and blue. R’s rgb function will convert those red, green and blue amounts to a usable colour: black=rgb(0,0,0); green3=rgb(0, 205, 0, max=255), cyan=rgb(0, 255, 255,max=255). The hsv and hcl provide similar functions using alternative colour specifications.\nHexadecimal: The RGB values can also be expressed as a hexademical code: black=“#000000”; cyan=“#00FFFF”.\n\n\n\nR has a default palette of colours used for the default colours in graphs. Integer colour codes are the corresponding colour in this default list. The default palette contains the following eight elements.\n\npalette()\n\n[1] \"black\"   \"red\"     \"green3\"  \"blue\"    \"cyan\"    \"magenta\" \"yellow\" \n[8] \"gray\"   \n\npie(rep(1, 8), labels = sprintf(\"%d (%s)\", 1:8, palette()), col = 1:8)\n\n\n\n\n\n\n\n\nWe can replace the standard palette with a vector of our own colours.\n\npalette(c(\"black\",\"#3366cc\",\"#61D04F\",\"#C45560\", \"#F5C710\", \"#CD0BBC\"))\npie(rep(1, 6), labels = sprintf(\"%d (%s)\", 1:6, palette()), col = 1:6)\n\n\n\n\n\n\n\n\nR also has a number of functions that will generate a list of n colours according to a particular scheme. These can be used to replace the default palette, or as input for a particular plot.\n\n\n\n\n\n\n\n\n\nWhich of these are better for: * categorical? * sequential? * diverging?\nIt is also worth noting that we’ve used these to colour area. Their effectiveness may vary when colouring points or lines."
  },
  {
    "objectID": "Lecture1a_IntroViz.html#summary",
    "href": "Lecture1a_IntroViz.html#summary",
    "title": "Lecture 1a - Introduction to Data Visualisation",
    "section": "",
    "text": "The human brain can read and process visual information far faster than any other form - this is why data visualisation is so effective.\nData visualisation uses visual variables to encode the data in a graphical way.\nSome encodings are more effective than others. Some encodings work well together, and others less so.\nThe Gestalt principles describe how the brain sees patterns - we can use this to create better/worse visualisations.\nColour is an effective encoding, but we should carefully choose the colour palette to get the most out of it"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]